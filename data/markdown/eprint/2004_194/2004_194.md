# A Note on an Encryption Scheme of Kurosawa and Desmedt<sup>∗</sup>

Rosario Gennaro† Victor Shoup‡

May 18, 2005

### Abstract

Recently, Kurosawa and Desmedt presented a new hybrid encryption scheme which is secure against adaptive chosen-ciphertext attack. Their scheme is a modification of the Cramer-Shoup encryption scheme. Its major advantage with respect to Cramer-Shoup is that it saves the computation of one exponentiation and produces shorter ciphertexts. However, the proof presented by Kurosawa and Desmedt relies on the use of information-theoretic key derivation and message authentication functions.

In this note we present a different proof of security which shows that the Kurosawa-Desmedt scheme can be instantiated with any computationally secure key derivation and message authentication functions, thus extending the applicability of their paradigm, and improving its efficiency.

# 1 Introduction

The notion of chosen-ciphertext security was introduced by Naor and Yung [6] and developed by Rackoff and Simon [7], and Dolev, Dwork, and Naor [4].

In a chosen ciphertext attack, the adversary is given access to a decryption oracle that allows him to obtain the decryptions of ciphertexts of his choosing. Intuitively, security in this setting means that an adversary obtains (effectively) no information about encrypted messages, provided the corresponding ciphertexts are never submitted to the decryption oracle.

As shown in [4], security against chosen-ciphertext attack is equivalent to the notion of non-malleability. An encryption scheme is said to be non-malleable if given a ciphertext c, it is infeasible to compute a ciphertext c <sup>0</sup> whose decryption is somewhat related to the decryption of c.

For these reasons, the notion of chosen-ciphertext security has emerged as the "right" notion of security for encryption schemes. Indeed it can be shown that in order to model encryption as a "secure envelope", then the encryption scheme used must be chosen-ciphertext secure.

A number of chosen ciphertext secure cryptosystems have been proposed in the literature. The first schemes were presented in [6, 7, 4], but they were quite impractical. The first

<sup>∗</sup>Original version: August 10, 2004.

<sup>†</sup> IBM T.J.Watson Research Center, Yorktown Heights, NY, USA. rosario@watson.ibm.com

<sup>‡</sup>Computer Science Dept. NYU. shoup@cs.nyu.edu

truly practical cryptosystem that is provably secure against chosen ciphertext attack was discovered by Cramer and Shoup [1]. The security of this scheme is based on the hardness of the decisional Diffie-Hellman problem. In [2] Cramer and Shoup show that their original scheme is an instance of a more generic paradigm, which can be also instantiated with the Quadratic Residuosity and N-Residuosity assumptions.

In [8] Shoup presents an hybrid variant of the Cramer-Shoup cryptosystem. This scheme uses the original public-key scheme to generate an encryption of a random group element κ. Then a key derivation function (KDF) is applied to κ to compute two keys k, K which are used to encrypt the actual message with a chosen-ciphertext secure symmetric encryption scheme (recalled below).

Differently than in the public-key case, symmetric encryption schemes which are secure against a chosen-ciphertext attack can be easily built out of weaker primitives. It is indeed well known that all you need is a symmetric encryption scheme E which is secure against passive adversaries, and a secure message authentication code (MAC). To encrypt a message m with keys k, K it is sufficient to encrypt m with K, i.e. compute e = EK(m), and then compute a message authentication tag for e using k, i.e. compute t = MAC <sup>k</sup>(e). The final ciphertext is (e, t). The receiver, who also holds k, K, first checks that the tag t is correct and only in that case decrypts e.

Recently Kurosawa and Desmedt [5] modified the hybrid scheme presented in [8]. The advantage of their modification is that the computation of a ciphertext in their scheme requires one less exponentiation and produces shorter ciphertexts.

However their proof of security relies on the use of information theoretically secure KDF and MAC functions in the symmetric step of the hybrid construction. There are several reasons why this is not desirable, among them:

efficiency The proof in [5] requires the key k to be statistically close to a random key. This means that we cannot use a pseudo-random generator to derive k from a random group element encrypted during the public-key phase. This in turns implies that the public key part of the scheme must be instantiated with larger security parameters which would result in slower execution times<sup>1</sup> ;

modularity we would like to have a scheme into which we can plug any secure component and it still remains secure. It would be hard to deploy a scheme in large-scale if it can be used only in conjunction with certain types of MACs and KDFs (and in particular, with KDFs and MACs that are not used at all by the designers of standard cryptographic algorithms).

In this note we show a new and different proof of security for the Kurosawa-Desmedt scheme. We show that it is indeed possible to use any secure key derivation function and message authentication code. This effectively improves the efficiency and applicability of their scheme.

<sup>1</sup>For typical security parameters, this increase in computation times totally offsets the gain from performing one less exponentiation, thus making the Kurosawa-Desmedt scheme as efficient as the original Cramer-Shoup

# 2 The scheme

In this section we recall the Kurosawa-Desmedt scheme from [5]. We describe it using generic building blocks and at the end of the section we point out where the proof of security in [5] requires information theoretic security. The scheme makes use of:

- a group G of prime order q, with (random) generators  $g_1$  and  $g_2$ . Security assumption (DDH): Hard to distinguish  $(g_1^r, g_2^r)$  from  $(g_1^r, g_2^{r'})$ , where r is a random element of  $\mathbb{Z}_q$  and r' is a random element of  $\mathbb{Z}_q \setminus \{r\}$ .
- a message authentication code MAC, which is a function that takes two inputs, a key k and message  $e \in \{0,1\}^*$ , and produces a "tag"  $t := MAC_k(e)$ .

  Security assumption: For random k, after obtaining  $t^* := MAC_k(e^*)$  for (at most one) adversarially chosen  $e^*$ , hard to compute a forgery pair, i.e., a pair (e,t) such that  $e \neq e^*$  and  $t = MAC_k(e)$ .
- a symmetric key encryption scheme, with encryption algorithm E and decryption algorithm D, such that for key K and plaintext  $m \in \{0,1\}^*$ ,  $e := E_K(m)$  is the encryption of m under K, and for key K and ciphertext  $e \in \{0,1\}^*$ ,  $m := D_K(m)$  is the decryption of e under K.
  - Security assumption (semantic security): hard to distinguish  $E_K(m_0)$  from  $E_K(m_1)$  for randomly chosen K and adversarially chosen  $m_0$  and  $m_1$  (where  $m_0$  and  $m_1$  are of equal length).
- a key derivation function KDF, such that for v ∈ G, KDF(v) = (k, K), where k is a
  message authentication key, and K is a symmetric encryption key.
   Security assumption: hard to distinguish KDF(v) from (k, K), where v, k and K are
  random.
- a hash function  $H: G \times G \to \mathbb{Z}_q$ . Security assumption (target collision resistance): given  $u_1^* := g_1^r$  and  $u_2^* := g_2^r$ , for random  $r \in \mathbb{Z}_q$ , hard to find  $(u_1, u_2) \in G \times G \setminus \{(u_1^*, u_2^*)\}$  such that  $H(u_1, u_2) = H(u_1^*, u_2^*)$ .

Note that the key space for the message authentication code is assumed to consist of all bit strings of a given length, so that by a random key k, we mean a random bit string of appropriate length. Similarly for the symmetric encryption keys.

Note also that *KDF* and *H* may have associated keys (which are publicly known).

**Key Generation:** The description of the group G is generated, along with random generators  $g_1$  and  $g_2$  for G. Any keys for KDF and H are also generated. Then:

$$x_1, x_2, y_1, y_2 \stackrel{\phi}{\leftarrow} \mathbb{Z}_q, \ c \leftarrow g_1^{x_1} g_2^{x_2}, \ d \leftarrow g_1^{y_1} g_2^{y_2}.$$

<sup>&</sup>lt;sup>2</sup>Since we are defining MAC as a function there is only one possible output for any input pair k, e. It is possible to define message authentication codes as two algorithms: a "tagging" and a "verifying" algorithm. The tagging algorithm could be randomized and thus one of several tags could be computed on the same input pair. The security property then would be that it is hard to compute any valid message/tag pair (e, t) other than  $(e^*, t^*)$ .

The public key consists of the description of G, the generators  $g_1$  and  $g_2$ , keys for KDF and H (if any), along with the group elements c and d. The private key consists of the public key, along with  $x_1, x_2, y_1, y_2$ .

Encryption of  $m \in \{0,1\}^*$ :

```
r \stackrel{\notin}{\leftarrow} \mathbb{Z}_q, \ u_1 \leftarrow g_1^r \in G, \ u_2 \leftarrow g_2^r \in G, \ \alpha \leftarrow H(u_1, u_2) \in \mathbb{Z}_q
v \leftarrow c^r d^{r\alpha} \in G, \ (k, K) \leftarrow KDF(v), \ e \leftarrow E_K(m), \ t \leftarrow MAC_k(e)
output C := (u_1, u_2, e, t)
```

**Decryption of**  $C = (u_1, u_2, e, t)$ :

```
\begin{aligned} &\alpha \leftarrow H(u_1,u_2) \in \mathbb{Z}_q, \ v \leftarrow u_1^{x_1+y_1\alpha}u_2^{x_2+y_2\alpha} \in G, \ (k,K) \leftarrow \mathit{KDF}(v) \\ &\text{if } t \neq \mathit{MAC}_k(e) \ \text{then} \\ &\text{output "reject"} \\ &\text{else} \\ &m \leftarrow D_K(e) \\ &\text{output } m \end{aligned}
```

In addition to the above computational security assumption, the proof of security in [5] requires the following *information theoretic* assumptions:

- information-theoretically secure KDF. If  $v \in G$  is random, then at least the first component k of the output of KDF(v) should be (statistically close to) uniform.
- information-theoretically secure MAC. For all e and t, if k is chosen at random, then  $\Pr[MAC_k(e) = t]$  is negligible.

Our proof of security, described in the next section, does *not* need these assumptions.

Kurosawa and Desmedt apparently introduce these assumptions to avoid a potential circularity in their proof. Both their proof and ours use a "hybrid argument," whereby the initial attack game is transformed in a sequence of small steps into a game in which the adversary's advantage is clearly negligible. At one point in their proof, in order to justify one of the steps in this sequence, they want to prove that the decryption oracle will reject certain ciphertexts; to prove this, they must first prove that certain MAC keys are randomly distributed and that certain tag values are unpredictable; to draw this conclusion, they must first prove that the above-mentioned ciphertexts are rejected. As it happens, the way out of this circularity chosen by Kurosawa and Desmedt is to introduce informationtheoretic security assumptions. However, we show how to avoid this using a technique that is perhaps not so well appreciated, which we might call "deferred analysis." We will point out below in the proof where this technique is employed. This technique has also been used before — for example, in [3], it is used in the proof of security of a version of the Cramer-Shoup encryption scheme that makes use of a universal one-way hash function (rather than a collision-resistant hash function). The original security proof in [1] was actually a bit incomplete regarding this issue.

In Appendix A we recall the Cramer-Shoup hybrid scheme from [8] and compare the two schemes. In particular we point out how for typical security parameters the gains posted by the Kurosawa-Desmedt scheme may be offset by the requirement that KDF and MAC be information theoretically secure.

# 3 Security proof

### Game 0

We now define a game, called  $Game\ \theta$ , which is an interactive computation between an adversary and a simulator. This game is simply the usual game used to define CCA security, in which the simulator provides the adversary's environment.

Initially, the simulator runs the key generation algorithm, obtaining the description of G, generators  $g_1$  and  $g_2$ , keys for KDF and H (if any), along with the values  $x_1, x_2, y_1, y_2 \in \mathbb{Z}_q$  and  $c, d \in G$ . The simulator gives the public key to the adversary.

During the execution of the game, the adversary makes a number of "decryption requests." Assume these requests are  $C^{(1)}, \ldots, C^{(Q)}$ , where

$$C^{(i)} = (u_1^{(i)}, u_2^{(i)}, e^{(i)}, t^{(i)}).$$

For each such request, the simulator decrypts the given ciphertext, and gives the adversary the result. We denote by  $\alpha^{(i)}$ ,  $v^{(i)}$ ,  $k^{(i)}$ , and  $K^{(i)}$  the corresponding intermediate quantities computed by the decryption algorithm on input  $C^{(i)}$ .

The adversary may also make a single "challenge request." For such a request, the adversary submits two messages  $m_0, m_1$ , which are bit strings of equal length, to the simulator; the simulator chooses  $b \in \{0, 1\}$  at random, and encrypts  $m_b$ , obtaining the "target ciphertext"  $C^* = (u_1^*, u_2^*, e^*, t^*)$ . The simulator gives  $C^*$  to the adversary. We denote by  $r^*$ ,  $\alpha^*$ ,  $v^*$ ,  $k^*$ , and  $K^*$  the corresponding intermediate quantities computed by the encryption algorithm.

The only restriction on the adversary's requests is that after it makes a challenge request, subsequent decryption requests must not be the same as the target ciphertext.

At the end of the game, the adversary outputs  $\hat{b} \in \{0, 1\}$ .

Let  $X_0$  be the event that  $\hat{b} = b$ . Security means that  $|\Pr[X_0] - 1/2|$  should be negligible.

We prove this by considering other games,  $Game\ 1$ ,  $Game\ 2$ , etc. These games will be quite similar to Game 0 in their overall structure, and will only differ from Game 0 in terms of how the simulator works. However, in each game, there will be well defined bits  $\hat{b}$  and b, so that in Game i, we always define  $X_i$  to the event that  $\hat{b} = b$  in that game. All of these games should be viewed as operating on the same underlying probability space.

Before moving on, we make a couple of additional assumptions about the internal structure of Game 0 that will be convenient down the road. First, we assume that  $g_2$  is computed as:

$$w \stackrel{\&}{\leftarrow} \mathbb{Z}_q^*, \ g_2 \leftarrow g_1^w.$$

Note that the value of w is never explicitly used in Game 0, except to compute  $g_2$ . Second, we assume that the quantities  $r^*$ ,  $u_1^*$ ,  $u_2^*$ ,  $\alpha^*$ ,  $v^*$ ,  $k^*$ , and  $K^*$  are computed at the very start of the game (they do not depend on the values  $m_0$ ,  $m_1$  provided later by the adversary, so this can be done).

#### Game 1

Game 1 is the same as Game 0, except that if the adversary ever submits  $C^{(i)}$  for decryption with

$$(u_1^{(i)}, u_2^{(i)}) \neq (u_1^*, u_2^*)$$
 and  $\alpha^{(i)} = \alpha^*$ ,

the simulator rejects the given ciphertext.

In Game 1, the simulator may reject ciphertexts that would not have been rejected in Game 0. Let us call Rejection Rule 0 the rule by which ciphertexts are rejected as in the ordinary decryption algorithm (i.e., the message authentication tags do not match). Let us call Rejection Rule 1 this new rejection rule, introduced in Game 1.

Let F<sup>1</sup> be the event that the simulator applies Rejection Rule 1 in Game 1 to a ciphertext to which Rejection Rule 0 does not apply. Game 0 and Game 1 proceed identically until the this event occurs; in particular, the events X<sup>0</sup> ∧ ¬F<sup>1</sup> and X<sup>1</sup> ∧ ¬F<sup>1</sup> are the same (recall that we view all games to operate on the same underlying probability space); therefore,

$$|\Pr[X_0] - \Pr[X_1]| \le \Pr[F_1]. \tag{1}$$

Moreover, we have

$$\Pr[F_1] = \epsilon_{\text{tcr}},\tag{2}$$

where tcr is the success probability that one can find a collision in H using resources similar to those of the given adversary. By assumption, tcr is negligible.

# Game 2

Game 2 is the same as Game 1, except that the simulator computes v <sup>∗</sup> as

$$v^* \leftarrow (u_1^*)^{x_1 + y_1 \alpha^*} (u_2^*)^{x_2 + y_2 \alpha^*}.$$

This change is purely conceptual, since v <sup>∗</sup> has the same value either way. In particular,

$$\Pr[X_1] = \Pr[X_2]. \tag{3}$$

# Game 3

Now generate u ∗ <sup>2</sup> by the rule

$$r' \stackrel{\&}{\leftarrow} \mathbb{Z}_q \setminus \{r^*\}, \ u_2^* \leftarrow g_2^{r'}.$$

We have

$$|\Pr[X_2] - \Pr[X_3]| = \epsilon_{\text{ddh}}, \tag{4}$$

where ddh is the advantage with which one can solve the DDH problem, using resources similar to those of the given adversary. By assumption, ddh is negligible.

The details. We can easily build a "hybrid" Game 2/3 that takes τ := (g1, g2, u<sup>∗</sup> 1 , u<sup>∗</sup> 2 ) as input, so that if τ is a random DH-tuple, Game 2/3 acts just like Game 2, and if τ is a random non-DH-tuple, then Game 2/3 acts just like Game 3. The distinguishing algorithm runs Game 2/3 on input τ , and outputs 1 if ˆb = b, and outputs 0 otherwise. The distinguishing advantage of this algorithm is exactly equal to |Pr[X2] − Pr[X3]|.

## Game 4

In this game, the simulator makes use of the value  $w \in \mathbb{Z}_q$ , where  $g_2 = g_1^w$ . The simulator did not need to make explicit use of this value in previous games. Indeed, we could not have used the DDH assumption if the simulator had to use w. However, we are now finished with the DDH assumption, and so the simulator is free to make use of w in this and subsequent games.

Game 4 is the same as Game 3, except that we introduce a new Rejection Rule 2: in responding to decryption requests, the simulator rejects any ciphertext  $C^{(i)}$  such that

$$(u_1^{(i)}, u_2^{(i)}) \neq (u_1^*, u_2^*)$$
 and  $(u_1^{(i)})^w \neq u_2^{(i)}$ .

Note that the condition  $(u_1^{(i)})^w \neq u_2^{(i)}$  is equivalent to  $\log_{g_1} u_1^{(i)} \neq \log_{g_2} u_2^{(i)}$ .

Define  $F_4$  to be the event that a ciphertext is rejected during Game 4 using Rejection Rule 2 to which Rejection Rules 0 and 1 are not applicable.

Clearly, Game 3 and Game 4 proceed identically until  $F_4$  occurs; in particular, the events  $X_3 \wedge \neg F_4$  and  $X_4 \wedge \neg F_4$  are the same, and so

$$|\Pr[X_3] - \Pr[X_4]| \le \Pr[F_4]. \tag{5}$$

We want to show that  $Pr[F_4]$  is negligible; however, we postpone this until later. This is the "deferred analysis" technique: instead of attempting to bound  $Pr[F_4]$  right now, we shall patiently wait until a later game, where it will be much easier. However, at this point we augment Game 4 just slightly, utilizing the well-known "plug and pray" technique: the simulator chooses  $j \in \{1, \ldots, Q\}$  at random, and we define  $F'_4$  to be the event that in Game 4, Rejection Rules 0 and 1 do not apply to  $C^{(j)}$ , but Rejection Rule 2 does apply to  $C^{(j)}$ . Clearly,

$$\Pr[F_4] \le Q \Pr[F_4'],\tag{6}$$

and so it suffices to show that  $\Pr[F_A']$  is negligible.

It will be helpful to write down in detail how Game 4 works, starting from scratch:

• The simulator begins by generating the description of G, along with a random generator  $g_1$ , and any keys for KDF and H. It then computes:

**I1:**
$$w \stackrel{\ensuremath{\leftarrow}}{\leftarrow} \mathbb{Z}_q^*, \ g_2 \leftarrow g_1^w$$

**12:**
$$x_1, x_2, y_1, y_2 \stackrel{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \$$

12:
$$x_1, x_2, y_1, y_2 \leftarrow \mathbb{Z}_q, \ c \leftarrow g_1^{s_1} g_2^{s_2}, \ d \leftarrow g_1^{s_1} g_2^{s_2}$$
13:  $r^* \leftarrow \mathbb{Z}_q, \ r' \leftarrow \mathbb{Z}_q \setminus \{r^*\}, \ u_1^* \leftarrow g_1^{r^*}, \ u_2^* \leftarrow g_1^{wr'}, \ \alpha^* \leftarrow H(u_1^*, u_2^*)$
14:  $v^* \leftarrow (u_1^*)^{x_1 + y_1 \alpha^*} (u_2^*)^{x_2 + y_2 \alpha^*}$

**I4:**
$$v^* \leftarrow (u_1^*)^{x_1+y_1\alpha^*}(u_2^*)^{x_2+y_2\alpha}$$

**I5:**
$$(k^*, K^*) \leftarrow KDF(v^*)$$

**I6:**
$$j \leftarrow \{1, ..., Q\}$$

The simulator gives the description of G, the generators  $g_1$  and  $g_2$ , keys for KDF and H (if any), along with c and d to the adversary.

• In processing a decryption request  $C^{(i)} = (u_1^{(i)}, u_2^{(i)}, e^{(i)}, t^{(i)})$ , the simulator proceeds as follows:

```
D01: \alpha^{(i)} \leftarrow H(u_1^{(i)}, u_2^{(i)})
              if (u_1^{(i)}, u_2^{(i)}) \neq (u_1^*, u_2^*) and \alpha^{(i)} = \alpha^* then return "reject"
D03:
              else if (u_1^{(i)}, u_2^{(i)}) = (u_1^*, u_2^*) then
D04:
                         if t^{(i)} \neq MAC_{k^*}(e^{(i)}) then return "reject"
D05:
                         return D_{K^*}(e^{(i)})
D06:
              else if (u_1^{(i)})^w \neq u_2^{(i)} then
v^{(i)} \leftarrow (u_1^{(i)})^{x_1+y_1\alpha^{(i)}} (u_2^{(i)})^{x_2+y_2\alpha^{(i)}}
(k^{(i)}, K^{(i)}) \leftarrow KDF(v^{(i)})
D07:
D08:
D09:
                         if t^{(i)} \neq MAC_{k^{(i)}}(e^{(i)}) then return "reject"
D10:
D11:
                          return "reject"
D12:
              else
                         \begin{array}{l} v^{(i)} \leftarrow (u_1^{(i)})^{x_1 + y_1 \alpha^{(i)}} (u_2^{(i)})^{x_2 + y_2 \alpha^{(i)}} \\ (k^{(i)}, K^{(i)}) \leftarrow \mathit{KDF}(v^{(i)}) \end{array}
D13:
D14:
                         if t^{(i)} \neq MAC_{k^{(i)}}(e^{(i)}) then return "reject"
D15:
                         return D_{K(i)}(e^{(i)})
D16:
```

Note that Rejection Rule 0 is applied at lines **D05**, **D10**, and **D15**, while Rejection Rule 1 is applied at line **D03**, and Rejection Rule 2 (and no other Rejection Rule) at line **D11**.

• In processing the challenge request, the adversary gives  $m_0, m_1$  to the simulator. The simulator computes

$$b \stackrel{\leftarrow}{\leftarrow} \{0,1\}, e^* \leftarrow E_{K^*}(m_b), t^* \leftarrow MAC_{k^*}(e^*),$$

and gives  $C^* := (u_1^*, u_2^*, e^*, t^*)$  to the adversary.

We have written the logic of the decryption oracle in this particular way to facilitate further analysis. Note that the computations at  $\mathbf{D08}$ - $\mathbf{D10}$  have no real effect, other than to determine if the event  $F'_4$  occurs; indeed, once line  $\mathbf{D08}$  is reached, the ciphertext is sure to be rejected, either at line  $\mathbf{D10}$  or at line  $\mathbf{D11}$ . The event  $F'_4$  is simply the event that line  $\mathbf{D11}$  executes in the jth decryption request.

#### Game 5

Game 5 is the same as Game 4, except that we change lines **I2** and **I4**, as follows:

**I2:**
$$x, y \stackrel{\phi}{\leftarrow} \mathbb{Z}_q, \ c \leftarrow g_1^x, \ d \leftarrow g_1^y$$

**I4:**  $v^* \stackrel{\phi}{\leftarrow} G$

as well as lines D08 and D13, as follows:

**D08:**
$$v^{(i)} \stackrel{\mbox{\tiny $\phi$}}{\leftarrow} G$$

**D13:**  $v^{(i)} \leftarrow (u_1^{(i)})^{x+y\alpha^{(i)}}$

Note that  $x_1, x_2, y_1, y_2$  are not used at all in Game 5.

We define  $F'_5$  to be the event that line **D11** is executed in the *j*th decryption request in Game 5. We claim that

$$\Pr[X_4] = \Pr[X_5] \tag{7}$$

and

$$\Pr[F_4'] = \Pr[F_5']. \tag{8}$$

This follows from a simple linear algebra argument, along the same lines as in [1]. The point is, we are simply swapping one set of 4-wise independent random variables for another; indeed, in both games, the variables c, d,  $v^*$ , and  $v^{(j)}$  (as computed at line **D08**) are mutually independent and uniformly distributed over G.

Now our sequence of games reaches a fork in the road. Games 6 and 7 below (the "left fork") are used to show that  $\Pr[X_5]$  is close to 1/2. Then we define Game 6′ (the "right fork"), which is another modification of Game 5, to show that  $\Pr[F_5]$  is small.

#### Game 6

Game 6 is the same as Game 5, except that we change line I5, as follows:

**I5:**
$$(k^*, K^*) \stackrel{\phi}{\leftarrow}$$
 "keys"

That is, we simply generate the keys  $k^*$  and  $K^*$  at random.

Observe that in Game 5,  $v^*$  is completely random, and is not used anywhere, except once as an input to KDF. Based on this, it is easy to see that

$$|\Pr[X_5] - \Pr[X_6]| = \epsilon_{\text{kdf}},\tag{9}$$

where  $\epsilon_{kdf}$  is the advantage with which one can distinguish the output of the KDF from a random key pair, using resources similar to those of the given adversary. By assumption,  $\epsilon_{kdf}$  is negligible.

### Game 7

Game 7 is the same as Game 6, except that we change line **D06**, as follows:

**D06:** return "reject"

Let  $F_7$  be the event that line **D06** is ever executed in Game 7, in any decryption request. Clearly, Game 6 and Game 7 proceed identically until  $F_7$  occurs; in particular, the events  $X_6 \wedge \neg F_7$  and  $X_7 \wedge \neg F_7$  are the same, and so

$$|\Pr[X_6] - \Pr[X_7]| \le \Pr[F_7]. \tag{10}$$

Moreover, if  $F_7$  occurs the adversary has effectively broken the message authentication code keyed by  $k^*$  (which in Game 7 is truly random). More precisely,

$$\Pr[F_7] \le Q\epsilon_{\max},\tag{11}$$

where  $\epsilon_{\text{mac}}$  is the advantage with which one can break the message authentication code using resources similar to those of the given adversary. By assumption,  $\epsilon_{\text{mac}}$  is negligible. The factor of Q in (11) comes from a standard "plug and pray" argument: the forging algorithm has to choose one of the pairs  $(e^{(i)}, t^{(i)})$  at random, and hope that it is actually a forgery pair.

Also, observe that the key  $K^*$  in Game 7 is truly random and is used for no other purpose that to encrypt  $m_b$ . Based on this, it is easy to see that

$$|\Pr[X_7] - 1/2| = \epsilon_{\text{enc}},\tag{12}$$

where  $\epsilon_{\rm enc}$  is the probability of breaking the semantic security of the underlying symmetric key encryption scheme, using resources similar to those of the given adversary. By assumption,  $\epsilon_{\rm enc}$  is negligible.

### Game 6'

We now backtrack to the fork in the road. Game 6' is the same as Game 5, except that we change line **D09**, as follows:

**D09:**
$$(k^{(i)}, K^{(i)}) \leftarrow$$
 "keys"

Define  $F'_{6'}$  to be the event that line **D11** is executed in the *j*th decryption request in Game 6'. Observe that at line **D08** in Game 5, the value  $v^{(j)}$  is completely random, and is not used anywhere, except once as an input to KDF. Based on this, it is easy to see that

$$|\Pr[F_5'] - \Pr[F_{6'}']| = \epsilon_{\text{kdf}}', \tag{13}$$

where  $\epsilon'_{kdf}$  is the advantage with which one can distinguish the output of the KDF from a random key pair, using resources similar to those of the given adversary. By assumption,  $\epsilon'_{kdf}$  is negligible.

Observe that in Game 6', the key  $k^{(j)}$  used in the message authentication code at line **D10** is completely random. From this, it easily follows that

$$\Pr[F'_{6'}] \le \epsilon'_{\text{mac}},\tag{14}$$

where  $\epsilon'_{\text{mac}}$  is the probability of breaking the message authentication code, using resources similar to those of the given adversary. By assumption,  $\epsilon'_{\text{mac}}$  is negligible. Indeed,  $\epsilon'_{\text{mac}}$  is the probability of breaking the message authentication code "blind" (without first obtaining one valid tag).

#### Completing the proof

We have

$$\Pr[F_4] \leq Q \Pr[F'_4] \qquad \text{[by (6)]} \\
= Q \Pr[F'_5] \qquad \text{[by (8)]} \\
\leq Q(\Pr[F'_{6'}] + \epsilon'_{\text{kdf}}) \qquad \text{[by (13)]} \\
\leq Q(\epsilon'_{\text{mac}} + \epsilon'_{\text{kdf}}) \qquad \text{[by (14)]}$$

Thus, we have

$$\Pr[F_4] \le Q(\epsilon'_{\text{mac}} + \epsilon'_{\text{kdf}}). \tag{15}$$

Finally, combining (15) with (1), (2), (3), (4), (5), (7), (9), (10), (11), and (12), we have:

$$|\Pr[X_0] - 1/2| \le \epsilon_{\text{tcr}} + \epsilon_{\text{ddh}} + \epsilon_{\text{kdf}} + \epsilon_{\text{enc}} + Q(\epsilon_{\text{mac}} + \epsilon'_{\text{mac}} + \epsilon'_{\text{kdf}}).$$
 (16)

By assumption, the right-hand side of (16) is negligible, which finishes the proof.

# 4 Hash Proof Systems

In [2] Cramer and Shoup showed that their original scheme in [1] was a special instance of a generic paradigm based on hash proof systems. We briefly recall here the basic ideas and how they can be applied to the scheme described in the previous section.

Smooth projective hashing [2]: Let X be a set and L ⊂ X a language. Loosely speaking, a hash function H<sup>a</sup> that maps X to some set is projective if there exists a projection key that defines the action of H<sup>a</sup> over the subset L of the domain X. That is, there exists a projection function α(·) that maps keys k into their projections s = α(a). The projection key s is such that for every x ∈ L it holds that the value of Ha(x) is uniquely determined by s and x. In contrast, nothing is guaranteed for x 6∈ L, and it may not be possible to compute Ha(x) from s and x. A smooth projective hash function has the additional property that for x /∈ L, the projection key s actually says nothing about the value of Ha(x). More specifically, given x and s = α(a), the value Ha(x) is uniformly distributed (or statistically close) to a random element in the range of Ha.

An interesting feature of smooth projective hashing is that if L is an NP-language, then for every x ∈ L it is possible to efficiently compute Ha(x) using the projection key s = α(a) and a witness of the fact that x ∈ L. Alternatively, given a itself, it is possible to efficiently compute Ha(x) even without knowing a witness.

Using the techniques from [2], Kurosawa and Desmedt in [5] generalize the above scheme can be generalized using smooth projective hashing as follows. The sets X, L and a projection key s = α(a) will be the public key. The key a will be the secret key.

To encrypt m, the sender chooses an element x ∈ L together with a witness. He then computes v = Ha(x) using the projection s and the witness. Then the keys (k, K) = KDF(v) are derived as above. The rest of the encryption procedure remains the same, i.e., e = EK(m) and t = MACk(e). The ciphertext is x, e, t.

The receiver on input (x, e, t) computes v <sup>0</sup> = Ha(x) and (k, K) = KDF(v 0 ). If t = MACk(e) then it decrypts m = DK(e).

Security Analysis. As in the proof in [2] the basic computational assumption underlying the security of this scheme is that it is hard to distinguish between random elements in L and random elements outside of L.

The proof of security in [5] requires the projective hash function to be strongly 2 universal, which is a stronger condition than smoothness. Basically it is required that for x /∈ L, even given s = α(a) and the value Ha(x 0 ) for x <sup>0</sup> ∈/ L and x <sup>0</sup> 6= x, the distribution of the value Ha(x) is statistically close to the uniform distribution over the range of Ha. Their generalized scheme, however, still requires information-theoretically secure KDF and MAC functions.

Our proof, which lifts such requirements on the KDF and MAC functions, also generalizes assuming strong 2-universal projective hashing, that one can efficiently sample elements outside of L, and there is a trapdoor that allows for efficiently testing language membership.

# References

- [1] R. Cramer and V. Shoup. A practical public key cryptosystem provably secure against adaptive chosen ciphertext attack. In CRYPTO'98.
- [2] R. Cramer and V. Shoup. Universal hash proofs and a paradigm for chosen ciphertext secure public key encryption. In EuroCrypt'02.
- [3] R. Cramer and V. Shoup. Design and analysis of practical public-key encryption schemes secure against adaptive chosen ciphertext attack. SIAM Journal of Computing 33:167-226, 2003
- [4] D. Dolev, C. Dwork, and M. Naor. Non-malleable cryptography. In STOC'91, pages 542–552, 1991.
- [5] K. Kurosawa and Y. Desmedt. A New Paradigm of Hybrid Encryption Scheme. In CRYPTO'04.
- [6] M. Naor and M. Yung. Public-key cryptosystems provably secure against chosen ciphertext attacks. In STOC'90.
- [7] C. Rackoff and D. Simon. Noninteractive zero-knowledge proof of knowledge and chosen ciphertext attack. In CRYPTO'91.
- [8] V. Shoup Using hash functions as a hedge against chosen ciphertext attack. In Euro-Crypt'00.

# A The original scheme

The Cramer-Shoup hybrid encryption scheme proposed in [1], and refined in [8], uses the same tools as the one described above. However key generation, encryption and decryption algorithms are different.

Key Generation: The description of the group G is generated, along with a random generator g<sup>1</sup> for G. Any keys for KDF and H are also generated. Then:

$$w, x, y, z \stackrel{\phi}{\leftarrow} \mathbb{Z}_q, \ g_2 \leftarrow g_1^w, \ c \leftarrow g_1^x, \ d \leftarrow g_1^y, \ h \leftarrow g_1^z.$$

The public key consists of the description of G, the generators g<sup>1</sup> and g2, keys for KDF and H (if any), along with the group elements c, d, h. The private key consists of the public key, along with w, x, y, z.

Encryption of  $m \in \{0,1\}^*$ :

```
r \stackrel{\protect}{\leftarrow} \mathbb{Z}_q, \ \kappa \leftarrow h^r, \ u_1 \leftarrow g_1^r \in G, \ u_2 \leftarrow g_2^r \in G, \ \alpha \leftarrow H(u_1, u_2) \in \mathbb{Z}_q
v \leftarrow c^r d^{r\alpha} \in G, \ (k, K) \leftarrow KDF(\kappa), \ e \leftarrow E_K(m), \ t \leftarrow MAC_k(e)
output C := (u_1, u_2, v, e, t)
```

**Decryption of**  $C = (u_1, u_2, v, e, t)$ :

```
\alpha \leftarrow H(u_1, u_2) \in \mathbb{Z}_q, \ v' \leftarrow u_1^{x+y\alpha} \in G, \ \kappa' \leftarrow u_1^z, \ (k, K) \leftarrow KDF(\kappa') if t \neq MAC_k(e) or v' \neq v or u_2 \neq u_1^w then reject else m \leftarrow D_K(e) output m
```

Notice that compared to the Kurosawa-Desmedt scheme, the encryption algorithm in this scheme computes an extra exponentiation (the computation of  $\kappa$ ) and a longer ciphertext (it includes the group element v). However, that does not translate into a direct gain in efficiency.

In the Cramer-Shoup scheme we can choose the prime q to be 160-bit long. This results in a random value  $\kappa$  which is computationally indistinguishable from a random group element. Then, under a suitable computational assumption on KDF, we can derive keys k, K of any required length using a pseudo-random number generator.

On the other hand, the key k in the Kurosawa-Desmedt scheme must be derived from v in an information-theoretic way. We can't apply a pseudo-random number generator, otherwise we lose the information-theoretic security. For common security parameters k is required to be at least 170-bits long. The only way we know how to do this is to map v into an 160-bit string using universal hashing and the Entropy Smoothing Theorem. But this requires v to come from a distribution with min-entropy at least, say, 320. Considering that from  $\kappa$  we also need to derive the key K (say, another 128 bits), then it seems that the group G must have order q of at least about 450 bits. This increase in the security parameter clearly offsets the gain obtained by dropping one exponentiation.

Using our proof, however, we can claim that the Kurosawa-Desmedt scheme can be used with a group G of order q where q is a 160-bit prime.

We also note that the scheme in [8] is optimized so that all exponentiations in the decryption algorithm are with respect to the same base — this allows for speedups using techniques for exponentiation with preprocessing. We believe that similar optimizations can be applied to the Kurosawa-Desmedt scheme. Also, the scheme in [8] can be proven secure in the random oracle model under the *computational* Diffie-Hellman assumption — we believe that the same can be proven for the Kurosawa-Desmedt scheme.