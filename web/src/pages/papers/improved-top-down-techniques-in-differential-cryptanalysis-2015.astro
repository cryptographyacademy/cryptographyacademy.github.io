---
import BaseLayout from '../../layouts/BaseLayout.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2015/268';
const CRAWLER = 'mistral';
const CONVERTED_DATE = '2026-02-16';
const TITLE_HTML = 'Improved Top-Down Techniques in Differential Cryptanalysis';
const AUTHORS_HTML = 'Itai Dinur, Orr Dunkelman, Masha Gutman, Adi Shamir';

const CONTENT = `    <p class="text-gray-300">Itai Dinur¹, Orr Dunkelman²,³,*, Masha Gutman³, and Adi Shamir³</p>

    <p class="text-gray-300">¹ Département d'Informatique, École Normale Supérieure, Paris, France ² Computer Science Department, University of Haifa, Israel ³ Computer Science department, The Weizmann Institute, Rehovot, Israel</p>

    <p class="text-gray-300">Abstract. The fundamental problem of differential cryptanalysis is to find the highest entries in the Difference Distribution Table (DDT) of a given mapping <span class="math">F</span> over <span class="math">n</span>-bit values, and in particular to find the highest diagonal entries which correspond to the best iterative characteristics of <span class="math">F</span>. The standard bottom-up approach to this problem is to consider all the internal components of the mapping along some differential characteristic, and to multiply their transition probabilities. However, this can provide seriously distorted estimates since the various events can be dependent, and there can be a huge number of low probability characteristics contributing to the same high probability entry. In this paper we use a top-down approach which considers the given mapping as a black box, and uses only its input/output relations in order to obtain direct experimental estimates for its DDT entries which are likely to be much more accurate. In particular, we describe three new techniques which reduce the time complexity of three crucial aspects of this problem: Finding the exact values of all the diagonal entries in the DDT for small values of <span class="math">n</span>, approximating all the diagonal entries which correspond to low Hamming weight differences for large values of <span class="math">n</span>, and finding an accurate approximation for any DDT entry whose large value is obtained from many small contributions. To demonstrate the potential contribution of our new techniques, we apply them to the SIMON family of block ciphers, show experimentally that most of the previously published bottom-up estimates of the probabilities of various differentials are off by a significant factor, and describe new differential properties which can cover more rounds with roughly the same probability for several of its members. In addition, we show how to use our new techniques to attack a 1-key version of the iterated Even-Mansour scheme in the related key setting, obtaining the first generic attack on 4 rounds of this well-studied construction.</p>

    <p class="text-gray-300">Keywords: differential cryptanalysis, difference distribution tables, iterative characteristics, Even-Mansour, SIMON.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>The second author was supported in part by the Israel Science Foundation through grants No. 827/12 and No. 1910/12.</li>

    </ul>

    <p class="text-gray-300">1 Introduction</p>

    <p class="text-gray-300">Differential cryptanalysis, which was first proposed in <em>[6]</em>, is one of the best known and most widely used tools for breaking the security of many types of cryptographic schemes (including block ciphers, stream ciphers, keyed and unkeyed hash functions, etc). Its main component is a Difference Distribution Table (abbreviated as <span class="math">DDT</span>) which describes how many times each input difference is mapped to each output difference by a given mapping <span class="math">F</span> over <span class="math">n</span>-bit values. The <span class="math">DDT</span> table has exponential size (with <span class="math">2^{n}</span> rows and <span class="math">2^{n}</span> columns), but we are usually interested only in its large entries: When we try to attack an existing scheme we try to find the largest <span class="math">DDT</span> entry, and when we develop a new cryptographic scheme we try to demonstrate that all the <span class="math">DDT</span> entries are smaller than some bound.</p>

    <p class="text-gray-300">For large value of <span class="math">n</span> such as 128, it is impractical to find the exact value of even a single entry in the table, but in most cases we are only interested in finding a sufficiently good approximation of its large values. There are many proposed algorithms for computing such approximations, but almost all of them are bottom-up techniques which start by analyzing the differential properties of small components such as a single S-box, and then combine them into large components such as a reduced-round version of the full scheme. To find the best differential attack, they use the detailed description of the scheme in order to identify a consistent collection of high probability differential properties of all the small components, and then multiply all these probabilities. In order to claim that there are no high probability differentials, they lower bound the number of multiplied probabilities, e.g., by showing that any differential characteristic has a large number of active S-boxes.</p>

    <p class="text-gray-300">A second problem is that in most cases, this bottom-up approach concentrates on a single differential characteristic and describes one particular way in which the given input difference can give rise to the given output difference by specifying all the intermediate differences. Moreover, this approach is also more susceptible to variations from the Markov cipher model, where dependence between different rounds can lead to an estimation of probability which is far from the correct value.</p>

    <p class="text-gray-300">In this paper we follow a different top-down approach, in which we consider the given mapping as a black box and ignore its internal structure. In particular, we do not multiply or add a large number of of probabilities associated with its smallest components, and thus we do not suffer from the three methodological problems listed above. Our goal is to use the smallest possible number of evaluations of the given mapping in order to compute either the precise value or a sufficiently good approximation of the most interesting entries in its <span class="math">DDT</span>.</p>

    <p class="text-gray-300">A straightforward black box algorithm can calculate the exact value of any particular entry in the <span class="math">DDT</span> table in <span class="math">2^{n}</span> time by evaluating the mapping for all the pairs of inputs with the desired input difference, and counting how many times we got the desired output difference. When we want to compute a set of <span class="math">k</span> entries in the <span class="math">DDT</span>, we can always repeat the computation for each entry separately and thus get a <span class="math">k2^{n}</span> upper bound on the time complexity. However,</p>

    <p class="text-gray-300">for some large sets of entries we can do much better. In particular, we can compute all the <span class="math">k=2^{n}</span> entries in a single row (which corresponds to a fixed input difference and arbitrary output differences) with the same <span class="math">2^{n}</span> time complexity by using the same algorithm. This also implies that the whole <span class="math">DDT</span> can be computed in <span class="math">2^{2n}</span> time, whereas a naive algorithm which computes each one of the <span class="math">2^{2n}</span> entries separately would require <span class="math">2^{3n}</span> time. If the mapping is a permutation and we are also given its inverse as a black box, we can similarly compute each column in the <span class="math">DDT</span> (which corresponds to a fixed output difference and arbitrary input differences) in <span class="math">2^{n}</span> time by applying the inverse black box to all the pairs with the desired output difference.</p>

    <p class="text-gray-300">Which other sets of entries in the <span class="math">DDT</span> can be simultaneously computed faster than via the naive algorithm? The first result we show in this paper is a new technique called the <em>diagonal algorithm</em>, which can calculate the exact values of all the <span class="math">2^{n}</span> diagonal entries in the <span class="math">DDT</span> (whose input and output differences are equal) with a total time complexity of about <span class="math">2^{n}</span>. These entries in the <span class="math">DDT</span> are particularly interesting in differential cryptanalysis, since they describe the probabilities of all the possible iterative characteristics which can be concatenated to themselves an arbitrarily large number of times in a consistent way. For many well known cryptosystems (such as DES), the best known differential attack on the scheme is based on such iterative characteristics. We then extend the diagonal algorithm to generalized diagonals which are defined as sets of <span class="math">2^{n}</span> <span class="math">DDT</span> entries in which the input difference and output difference are linearly related rather than equal. This can be particularly useful in schemes such as Feistel structures, in which we are often interested in output differences which are equal to the input differences but with swapped halves.</p>

    <p class="text-gray-300">In many applications of differential cryptanalysis, we can argue that only rows in the <span class="math">DDT</span> which correspond to input differences with low Hamming weight can contain large values (and thus lead to efficient attacks). Our next result is a new top-down algorithm which we call the <em>Hamming Ball algorithm</em>, which can efficiently identify all the large diagonal entries in the <span class="math">DDT</span> whose input and output differences have a low Hamming weight, and approximate their values.</p>

    <p class="text-gray-300">Our third result is a new <em>bins-in-the-middle(<span class="math">BITM</span>) algorithm</em> for computing in a more efficient way an improved approximation for any particular <span class="math">DDT</span> entry whose high value may be accumulated from a large number of differential characteristics which have much smaller probabilities. In this algorithm we assume that the given mapping is only quasi black box in the sense that it is the concatenation of two black boxes which can be computed separately. A typical example of such a situation is a cryptographic scheme which consists of many rounds, where we can choose in our analysis how many rounds we want to evaluate in the first black box, and then define the remaining rounds as the second black box.</p>

    <p class="text-gray-300">In our complexity analysis, we assume that most of the <span class="math">DDT</span> entries are distributed as if the mapping is randomly chosen, but a small number of entries have unusually large values which we would like to locate and to estimate by evaluating the mapping on the smallest possible number of inputs. This is analogous</p>

    <p class="text-gray-300">to classical models of random graphs in which we try to identify some planted structure such as a large clique which was artificially added to the random graph.</p>

    <p class="text-gray-300">To demonstrate the power of our new techniques, we used the relatively new but extensively studied proposal of the Simon family of lightweight block ciphers, which was developed by a team of experienced cryptographers from the NSA. Several previous papers <em>[1, 2, 9, 28]</em> tried to find the best possible differential properties of reduced-round variants of Simon with the bottom-up approach by analyzing its individual components. By using our new top-down techniques, we can provide strong experimental evidence that the previous probability estimates were inaccurate, and in fact we found new differential properties which are either longer by two rounds or have better probabilities for the same number of rounds compared to all the previously published results.</p>

    <p class="text-gray-300">The paper is organized as follows. After introducing our notation in Section 2, we survey in Section 3 the main bottom-up techniques for estimating differential probabilities which were proposed in the literature. Our three new top-down techniques are described in Section 4, Section 5, and Section 6. We describe the application of our new techniques to the Simon family of block ciphers in Section 7. Section 8 shows show how to use these top-down techniques in order to analyze the differential properties of the Even-Mansour scheme (whose random permutation is only given in the form of a black box), and to find the first generic attack on its 4-round 1-key version in the related key setting.</p>

    <h2 id="sec-2" class="text-2xl font-bold">2 Notations</h2>

    <p class="text-gray-300">In this section, we describe the notations used in the rest of this paper.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Given a function <span class="math">F:\\mathbb{GF}(2)^{n}\\to\\mathbb{GF}(2)^{n}</span>, the difference distribution table (<span class="math">DDT</span>) is a <span class="math">2^{n}\\times 2^{n}</span> table, where <span class="math">DDT[\\Delta_{I}][\\Delta_{O}]</span> counts the number of input pairs to <span class="math">F</span> with an <span class="math">n</span>-bit difference of <span class="math">\\Delta_{I}</span> whose <span class="math">n</span>-bit output difference is <span class="math">\\Delta_{O}</span>. More formally we define $DDT[\\Delta_{I},\\Delta_{O}]\\triangleq</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\{x\\in\\mathbb{GF}(2)^{n}:F(x)\\oplus F(x\\oplus\\Delta_{I})=\\Delta_{O}\\}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">We define the diagonal (<span class="math">DIAG</span>) of the <span class="math">DDT</span> as a vector of length <span class="math">2^{n}</span> which contains only the <span class="math">[\\Delta_{I},\\Delta_{O}]</span> entries for which <span class="math">\\Delta_{O}=\\Delta_{I}</span>, namely <span class="math">DIAG[\\Delta]\\triangleq DDT[\\Delta,\\Delta]</span>. Given an auxiliary function <span class="math">L:\\mathbb{GF}(2)^{n}\\to\\mathbb{GF}(2)^{n}</span>, we define the generalized diagonal (<span class="math">GDIAG</span>) of the <span class="math">DDT</span> as a table of size <span class="math">2^{n}</span>, which contains only the <span class="math">[\\Delta_{I},\\Delta_{O}]</span> entries for which <span class="math">\\Delta_{O}=L(\\Delta_{I})</span>, namely <span class="math">GDIAG_{L}[\\Delta]\\triangleq DDT[\\Delta,L(\\Delta)]</span>. Thus, the diagonal is a particular case of the generalized diagonal for which the auxiliary function <span class="math">L</span> is the identity. In this paper, we are mostly interested in generalized diagonals for linear functions <span class="math">L</span> (over <span class="math">\\mathbb{GF}(2)^{n}</span>), which can be computed efficiently using our algorithms.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Given an <span class="math">n</span>-bit word <span class="math">x</span>, we denote by <span class="math">ham(x)</span> its Hamming weight. Given two <span class="math">n</span>-bit words <span class="math">x,y</span>, we denote by <span class="math">dist(x,y)</span> their Hamming distance, i.e. <span class="math">ham(x\\oplus y)</span>. For an integer <span class="math">0\\leq r\\leq n</span>, we denote by <span class="math">B_{r}(y)</span> the Hamming ball of radius <span class="math">r</span> centered at <span class="math">c</span>, namely $B_{r}(c)\\triangleq\\{x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">dist(x,c)\\leq r\\}<span class="math">. The number of points in </span>B_{r}(c)<span class="math"> is denoted as </span>M_{r}^{n}\\triangleq</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">B_{r}(c)</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">=\\sum\\limits_{i=0}^{r}\\binom{n}{i}$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">We denote the <span class="math">n</span>-bit word with bits <span class="math">i_{1},...,i_{k}</span> set to <span class="math">1</span> and the rest set to <span class="math">0</span> by <span class="math">e_{i_{1},...,i_{k}}</span>.</p>

    <p class="text-gray-300">##</p>

    <p class="text-gray-300">3 Previous Work</p>

    <h3 id="sec-3" class="text-xl font-semibold mt-8">3.1 Bottom-Up Differential Characteristic Search</h3>

    <p class="text-gray-300">Since the early works on differential cryptanalysis (including the original work of <em>[6]</em>), there was a need to find good differential characteristics. This need was usually answered in the bottom-up approach: In <em>[21]</em> Matsui described the first general purpose differential characteristic search algorithm, which uses “bound-and-branch” approach. Matsui’s algorithm is assured to find the best characteristic, but its running time may be unbounded. Later works in the field was sometimes applied to specific ciphers (e.g., analyzing FEAL in <em>[3]</em>), or extending Matsui’s approach using basic properties of the block cipher (notably, the byte-oriented ciphers studied in <em>[7, 8, 17, 25]</em> or the ARX constructions studied in <em>[10, 14, 20, 22]</em>).</p>

    <p class="text-gray-300">Offering an upper bound on the probability of differential characteristics dates back to the early works of <em>[26]</em>, which suggested bounds for Feistel constructions, based on bounds on the probability of differential characteristics through the round function. This method is the basis of the approach of counting the number of active S-boxes (introduced in <em>[13]</em>), which is widely used today. Another approach introduced in <em>[24]</em> is the transformation of the problem into a linear-programming problem, and solving it for constraints. This technique was later extended in <em>[27, 28]</em>.</p>

    <p class="text-gray-300">Finally, we note that <em>[10]</em> also explored the concept of sampling the <span class="math">DDT</span> in the context of ARX constructions. If the word size is too big to be analyzed to obtain the full <span class="math">DDT</span>, one may pick a reduced set of entries and compute their probability (for ARX construction one can usually compute the probability of the transition without using input pairs).</p>

    <h3 id="sec-4" class="text-xl font-semibold mt-8">3.2 Top-Down Algorithms</h3>

    <p class="text-gray-300">The first top-down algorithm which we are aware of is due to <em>[5]</em> — the “Shrinking” algorithm that searches for impossible differentials. The main idea behind the shrinking algorithm is to take a scaled-down version of the cipher (e.g., with reduced word sizes and S-boxes). Such a scaled-down version allows evaluating the full difference distribution table, which in turn can be used to automatically identify impossible differentials. However, we note that many cryptosystems cannot be scaled down in an obvious way while maintaining properties of their DDT, and therefore the applicability of this algorithm is limited.</p>

    <h2 id="sec-5" class="text-2xl font-bold">4 The Diagonal Algorithm and its Extensions</h2>

    <h3 id="sec-6" class="text-xl font-semibold mt-8">4.1 The Diagonal Algorithm</h3>

    <p class="text-gray-300">We begin by describing our basic algorithm for calculating the exact values of all the diagonal entries in the difference distribution table with about the same time complexity as computing a single entry. The algorithm is given black box</p>

    <p class="text-gray-300">access to a function <span class="math">F:\\mathbb{GF}(2)^{n}\\to\\mathbb{GF}(2)^{n}</span>, and outputs the diagonal of the difference distribution table <span class="math">DIAG[\\Delta]\\triangleq DDT[\\Delta,\\Delta]</span>. The algorithm is based on the simple property that the equality <span class="math">x\\oplus y=F(x)\\oplus F(y)</span> along the diagonal is equivalent to the equality <span class="math">x\\oplus F(x)=y\\oplus F(y)</span>. Therefore, we can efficiently identify all the <span class="math">(x,y)</span> pairs with equal input and output differences <span class="math">\\Delta=x\\oplus y</span> (which contribute to the <span class="math">DIAG</span> table) by searching for all the collisions between values of <span class="math">x\\oplus F(x)</span> and <span class="math">y\\oplus F(y)</span>.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Initialize all the entries of the table <span class="math">DIAG</span> to zero, and set <span class="math">DIAG[0]</span> to <span class="math">2^{n}</span>.</li>

      <li>For each <span class="math">n</span>-bit value <span class="math">x</span>:</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Compute <span class="math">x\\oplus F(x)</span>, and store the pair <span class="math">(x\\oplus F(x),x)</span> in a hash table <span class="math">H</span>, i.e., add <span class="math">x</span> to the set of values stored at <span class="math">H[x\\oplus F(x)]</span>.</li>

      <li>For each <span class="math">n</span>-bit value <span class="math">b</span>:</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>For each pair <span class="math">(x,y)</span> of distinct values such that <span class="math">x,y\\in H[b]</span>, increment <span class="math">DIAG[x\\oplus y]</span> by 1.</li>

    </ol>

    <p class="text-gray-300">The time complexity of Steps 1 and 2 is <span class="math">2^{n}</span> each, and the time complexity of Step 3 is proportional to <span class="math">D</span>, which denotes the total number of pairs <span class="math">(x,y)</span> such that <span class="math">x\\oplus y=F(x)\\oplus F(y)</span> (which is the same as the sum of all the entries in <span class="math">DIAG</span>). Note that for a random function <span class="math">F</span>, the expected value of <span class="math">D</span> is about <span class="math">2^{2n-n}=2^{n}</span> as we have about <span class="math">2^{2n}</span> <span class="math">(x,y)</span> pairs, and the probability that a pair satisfies the <span class="math">n</span>-bit equality is <span class="math">2^{-n}</span>. Consequently, the expected time complexity of the algorithm for a random function is about <span class="math">2^{n}</span>, and the total memory complexity is also <span class="math">2^{n}</span>, which is the size of the hash table <span class="math">H</span> and the output table <span class="math">DIAG</span>.</p>

    <p class="text-gray-300">We note that there are several previous algorithms whose general structure resembles the diagonal algorithm. One such algorithm is impossible differential cryptanalysis of Feistel structures <em>[18]</em> and its various extensions, which use a data structure similar to <span class="math">H</span> to iterate over pairs with related input and output differences. However, in these algorithms <span class="math">H</span> is used in order to filter pairs required to attack specific cryptosystems, and not to explicitly calculate the <span class="math">DDT</span> (as we do in Step 3.(a)).</p>

    <h3 id="sec-7" class="text-xl font-semibold mt-8">4.2 The Generalized Diagonal Algorithm</h3>

    <p class="text-gray-300">We now extend the diagonal algorithm to compute a generalized diagonal <span class="math">GDIAG_{L}</span> for any given linear function <span class="math">L</span> over <span class="math">\\mathbb{GF}(2)^{n}</span>. In this case, we are interested in <span class="math">(x,y)</span> pairs such that <span class="math">L(F(x)\\oplus F(y))=x\\oplus y</span>, which is equivalent to the equality <span class="math">x\\oplus L(F(x))=y\\oplus L(F(y))</span>, since <span class="math">L</span> is linear. Therefore, the generalized diagonal algorithm is very similar to the diagonal algorithm above, and only differs in Step 2.(a), where we store the pair <span class="math">(x\\oplus L(F(x)),x)</span> in the hash table <span class="math">H</span> (instead of storing the pair <span class="math">(x\\oplus F(x),x)</span>). The complexity analysis of the generalized diagonal algorithm is essentially identical to the basic diagonal algorithm.</p>

    <p class="text-gray-300">5 The Hamming Ball Algorithm</p>

    <p class="text-gray-300">The (generalized) diagonal algorithm computes the exact value of the (generalized) diagonal of the <span class="math">DDT</span> of the function <span class="math">F</span> in about <span class="math">2^{n}</span> time, which is practical for <span class="math">n=32</span> but marginal for <span class="math">n=64</span>. In fact, it is easy to show that information theoretically, the only way to compute the precise value of a single <span class="math">DDT</span> entry is to test all the <span class="math">2^{n}</span> relevant pairs of inputs or outputs. However, if we assume that we only want to find large entries on the diagonal and to approximate their values, we can do much better.</p>

    <p class="text-gray-300">Assume that there exists some entry <span class="math">DDT[\\Delta,L(\\Delta)]</span> with a value of <span class="math">p\\cdot 2^{n}</span> (where <span class="math">0&lt;p\\leq 1</span> is the probability of an input pair with difference <span class="math">\\Delta</span> to have an output difference of <span class="math">L(\\Delta)</span>) for a fixed linear function <span class="math">L</span>. A trivial adaptation to the (generalized) diagonal algorithm evaluates and stores the pairs <span class="math">(x\\oplus L(F(x)),x)</span> for only <span class="math">0&lt;C\\leq 2^{n}</span> random values of <span class="math">x</span>. Clearly, we do not expect to generate a non-zero value in entry <span class="math">DDT[\\Delta,L(\\Delta)]</span> before evaluating at least <span class="math">p^{-1}</span> (<span class="math">x,x\\oplus\\Delta</span>) pairs. This gives a lower bound on <span class="math">C</span> and on the complexity of the algorithm, since after the evaluation of <span class="math">C</span> arbitrary values <span class="math">x</span>, we expect to have about <span class="math">C^{2}\\cdot 2^{-n}</span> pairs with randomly scattered input differences, and thus we require <span class="math">C^{2}\\cdot 2^{-n}\\geq p^{-1}</span> or <span class="math">C\\geq 2^{n/2}\\cdot p^{-1/2}</span>. Therefore, the time and memory complexity of our adaptation are still somewhat large for big domains, and in particular it is barely practical for <span class="math">n=128</span> even when <span class="math">p</span> is close to <span class="math">1</span> (as <span class="math">C\\geq 2^{n/2}=2^{64}</span>).</p>

    <p class="text-gray-300">We now describe a more efficient adaptation that requires the stronger assumption that the high probability entries <span class="math">DDT[\\Delta,L(\\Delta)]</span> occur at <span class="math">\\Delta</span>’s which have (relatively) low Hamming weight. The motivation behind this assumption is that we are interested in applying our algorithms to concrete cryptosystems in which a high probability entry <span class="math">DDT[\\Delta_{I},\\Delta_{O}]</span> typically indicates the existence of a high probability differential characteristic with the corresponding input-output differences. Such high probability characteristics in SP networks are likely to have a small number of active Sboxes, and thus <span class="math">\\Delta_{I}</span> and <span class="math">\\Delta_{O}</span> are likely to have low Hamming weights.</p>

    <p class="text-gray-300">In order to consider only <span class="math">DDT[\\Delta,L(\\Delta)]</span> entries where <span class="math">\\Delta</span> is of small Hamming weight, we pick an arbitrary center <span class="math">c</span> and a small radius <span class="math">r</span>, and evaluate <span class="math">F</span> only for inputs inside the Hamming ball <span class="math">B_{r}(c)</span>. All the pairs of points inside the Hamming ball have a small Hamming distance, and thus for a carefully chosen value of <span class="math">r</span>, we will obtain a quadratic number of relevant pairs from a linear number of values which have small Hamming distances <span class="math">d</span>.</p>

    <p class="text-gray-300">It is easy to see that the raw estimates we get with this approach for the entries in the <span class="math">DDT</span> are biased, since the Hamming ball has more pairs which differ only in their least significant bit than pairs which differ in their <span class="math">d</span> least significant bits for <span class="math">d&gt;1</span>. Given a difference <span class="math">\\Delta</span> such that <span class="math">ham(\\Delta)=d</span>, an important measure which is used by our Hamming ball algorithm is the number</p>

    <p class="text-gray-300">of pairs with difference <span class="math">\\Delta</span> in <span class="math">B_r(c)</span>. This measure, which we denote by <span class="math">P_{r,d}^n</span> (it does not depend on the actual values of <span class="math">c</span> or <span class="math">\\Delta</span>), is used in order to create from the experimental data unbiased estimates for the values of the entries <span class="math">DDT[\\Delta_I, \\Delta_O]</span>, as described below.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Initialize the entries of the table <span class="math">GDIAG_L</span> to zero.</li>

      <li>For each <span class="math">n</span>-bit value <span class="math">x \\in B_r(c)</span>:</li>

    </ol>

    <p class="text-gray-300">(a) Compute <span class="math">x \\oplus L(F(x))</span>, and store the pair <span class="math">(x \\oplus L(F(x)), x)</span> in a hash table <span class="math">H</span>, i.e., add <span class="math">x</span> to the set of values stored at <span class="math">H[x \\oplus L(F(x))]</span>.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>For each <span class="math">n</span>-bit value <span class="math">b</span> such that <span class="math">H[b]</span> contains at least 2 values:</li>

    </ol>

    <p class="text-gray-300">(a) For each pair <span class="math">(x, y)</span> such that <span class="math">x, y \\in H[b]</span>, increment <span class="math">GDIAG_L[x \\oplus y]</span> by 1.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>For each <span class="math">n</span>-bit value <span class="math">\\Delta</span> such that <span class="math">GDIAG_L[\\Delta] &amp;gt; 0</span>:</li>

    </ol>

    <p class="text-gray-300">(a) Denote <span class="math">ham(\\Delta) = d</span> and normalize the entry <span class="math">GDIAG_L[\\Delta]</span> by setting <span class="math">GDIAG_L[\\Delta] \\gets GDIAG_L[\\Delta] \\cdot (2^n / P_{r,d}^n)</span>.</p>

    <p class="text-gray-300">The time and memory complexities of Step 2 are <span class="math">M_r^n</span>. The time and memory complexities of steps 3 and 4 are determined by the number of collisions in the hash table <span class="math">H</span>, which depends on <span class="math">F</span>. For a random function, we expect to have <span class="math">(M_r^n)^2 \\cdot 2^{-n} \\leq M_r^n</span> such collisions, and therefore we generally do not expect steps 3 and 4 to dominate the time or memory complexities of the attack (especially for large domains where we select a small <span class="math">r</span> implying that <span class="math">M_r^n \\ll 2^n</span> and thus <span class="math">(M_r^n)^2 \\cdot 2^{-n} \\ll M_r^n</span>).</p>

    <p class="text-gray-300">In order to detect an entry <span class="math">DDT[\\Delta, L(\\Delta)]</span> with <span class="math">ham(\\Delta) = d</span> whose probability is <span class="math">p</span>, the most efficient method (assuming that we have sufficient memory) is to select a <span class="math">r</span> such that <span class="math">B_r(c)</span> contains about <span class="math">p^{-1}</span> pairs of points with input different <span class="math">\\Delta</span>, or <span class="math">P_{r,d}^n \\geq p^{-1}</span>.</p>

    <p class="text-gray-300">The efficiency of our algorithm for low Hamming weights is derived from the fact that Hamming balls are relatively closed under XOR's - pairs of points which are close to the origin are also close to each other. Similar efficiencies can be obtained for other sets with similar closure properties, such as arbitrary linear subspaces and sets of points which have short Hamming distance to linear subspaces.</p>

    <h2 id="sec-8" class="text-2xl font-bold">5.1 Analyzing Keyed Functions</h2>

    <p class="text-gray-300">The algorithms described so far analyze a keyless function <span class="math">F</span>. In order to obtain meaningful results for a keyed function <span class="math">F_K</span>, we assume the existence of high probability entries <span class="math">DDT[\\Delta_I, \\Delta_O]</span>, which are common to a large fraction of the keys. Such common high probability entries are typically the result of a high probability differential characteristics (with the corresponding input-output differences) in iterated block ciphers where the round keys are XORed into the state.³</p>

    <p class="text-gray-300">² The computation of <span class="math">P_{r,d}^n</span> is discussed in Appendix 10.</p>

    <p class="text-gray-300">³ In such cases, the probability of the characteristic can be estimated independently of the round keys, assuming the input values are selected at random.</p>

    <p class="text-gray-300">Based on this assumption, we can select a few keys <span class="math">K_{i}</span> at random, and independently run our algorithms on <span class="math">F_{K_{i}}</span> for each <span class="math">K_{i}</span>. Then, we look for high probability entries <span class="math">DDT[\\Delta_{I},\\Delta_{O}]</span> which are common to several keys. An additional possibility is to first run our algorithms on <span class="math">F_{K_{1}}</span>, and then to test the obtained high probability entries <span class="math">DDT[\\Delta_{I},\\Delta_{O}]</span> on <span class="math">F_{K_{i}}</span> for <span class="math">i&gt;1</span>, by encrypting sufficiently many pairs with input difference <span class="math">\\Delta_{I}</span> for each key.</p>

    <h2 id="sec-9" class="text-2xl font-bold">6 Improved Approximation of a Single Large DDT Entry</h2>

    <p class="text-gray-300">We now turn our attention to a related problem. Assume that we found a pair of input/output differences <span class="math">(\\Delta_{I},\\Delta_{O})</span> which are somehow related. For example, this can occur when an iterative characteristic is repeated several times. Given <span class="math">(\\Delta_{I},\\Delta_{O})</span>, we wish to estimate the probability of the transition <span class="math">\\Delta_{I}\\xrightarrow{r}\\Delta_{O}</span> (where <span class="math">r</span> is the number of rounds in the differential). The standard method to estimate this probability is to take many pairs with input difference <span class="math">\\Delta_{I}</span> and check how many of them have output difference <span class="math">\\Delta_{O}</span> (again, trying multiple keys). If the probability of the differential is <span class="math">p</span>, a good estimation requires <span class="math">O(p^{-1})</span> queries to the encryption algorithm.</p>

    <p class="text-gray-300">Now, assume that the cipher (or the rounds) for which we analyze this transition, can be divided into two (roughly equal) parts. In such a case, we can discuss the transition from <span class="math">\\Delta_{I}</span> to some <span class="math">\\Delta_{M}</span> after about <span class="math">r/2</span> rounds, and from <span class="math">\\Delta_{M}</span> to <span class="math">\\Delta_{O}</span> in the the remaining rounds. In other words, we look at <span class="math">\\Delta_{M}</span> after <span class="math">r^{\\prime}</span> rounds, and use the fact that:</p>

    <p class="text-gray-300"><span class="math">\\Pr[\\Delta_{I}\\xrightarrow{r}\\Delta_{O}]=\\sum_{\\Delta_{M}}\\Pr[\\Delta_{I}\\xrightarrow{r^{\\prime}}\\Delta_{M}\\xrightarrow{r-r^{\\prime}}\\Delta_{O}]</span> (1)</p>

    <p class="text-gray-300">which by the stochastic equivalence assumption (see <em>[19]</em>) we can re-write as</p>

    <p class="text-gray-300"><span class="math">\\Pr[\\Delta_{I}\\xrightarrow{r}\\Delta_{O}]=\\sum_{\\Delta_{M}}\\Pr[\\Delta_{I}\\xrightarrow{r^{\\prime}}\\Delta_{M}]\\cdot\\Pr[\\Delta_{M}\\xrightarrow{r-r^{\\prime}}\\Delta_{O}]</span> (2)</p>

    <p class="text-gray-300">To correctly evaluate the probability suggested by Equation (2), one needs to go over all possible <span class="math">\\Delta_{M}</span> values (which is usually infeasible for common block sizes), and for each one of them evaluate the probability of two shorter differentials, <span class="math">\\Delta_{I}\\xrightarrow{r^{\\prime}}\\Delta_{M}</span> and <span class="math">\\Delta_{M}\\xrightarrow{r-r^{\\prime}}\\Delta_{O}</span> (which in itself may be a hard task).</p>

    <p class="text-gray-300">Luckily, it was already observed in <em>[6]</em> that (in most cases) a high probability differential characteristic has several “close” high probability neighbors. This is explained by taking slightly different transitions through the active S-boxes with probability which is only slightly lower than the highest possible probability (used in the high probability characteristic). Similar behavior sometimes happen for differentials (especially for differentials which are based on a few “strong” characteristics, each having a few high probability “neighbors”).</p>

    <p class="text-gray-300">Hence, to give a lower bound on the value suggested by Equation (2), we can use the following computation:</p>

    <p class="text-gray-300"><span class="math">\\Pr[\\Delta_{I}\\xrightarrow{r}\\Delta_{O}]\\geq\\sum_{\\Delta_{M}\\in S}\\Pr[\\Delta_{I}\\xrightarrow{r^{\\prime}}\\Delta_{M}]\\cdot\\Pr[\\Delta_{M}\\xrightarrow{r-r^{\\prime}}\\Delta_{O}]</span> (3)</p>

    <p class="text-gray-300">where the set <span class="math">S</span> contains all the <span class="math">\\Delta_M</span> values for which the differentials <span class="math">\\Delta_I \\xrightarrow{r&#x27;} \\Delta_M</span> and <span class="math">\\Delta_M \\xrightarrow{r - r&#x27;} \\Delta_O</span> have a sufficiently high probability.⁴</p>

    <p class="text-gray-300">Obviously, this approximation relies on the fact that the two parts of the cipher are independent of each other. When taking into consideration a Markov-cipher assumption or the Stochastic Equivalence assumption (see [19] for more details), then the independence assumption immediately holds. However, in real life, one needs to verify it.</p>

    <p class="text-gray-300">One advantage of the Bins-in-the-Middle algorithm which is presented next over the standard analytical approach is the fact that we "reduce" the independence assumption only to the transition between the two parts of the cipher. This is to be compared with an analytical approach that computes the probability of each round independently, and then simply multiplies the probabilities of each round (i.e., approaches that assume that each round is independent of others). In the Bins-in-the-Middle algorithm, the probabilities which are multiplied are the sampled probabilities of differentials, i.e., probabilities that were experimentally verified.⁵</p>

    <h2 id="sec-10" class="text-2xl font-bold">6.1 The Bins-in-the-Middle (BITM) Algorithm</h2>

    <p class="text-gray-300">We now present an algorithm that finds all the "good" <span class="math">\\Delta_M</span> values in the set <span class="math">S</span> and experimentally estimates the probability of the two differentials <span class="math">\\Delta_I \\xrightarrow{r&#x27;} \\Delta_M</span> and <span class="math">\\Delta_M \\xrightarrow{r - r&#x27;} \\Delta_O</span>. The algorithm requires that the last <span class="math">r - r&#x27;</span> rounds are invertible (and thus, can be used only on permutations).</p>

    <p class="text-gray-300">The algorithm's basic idea is to actually produce a list of plausible <span class="math">\\Delta_M</span> by sampling random pairs with input difference <span class="math">\\Delta_I</span> (for the first <span class="math">r&#x27;</span> rounds) and a corresponding list by sampling random pairs with output difference <span class="math">\\Delta_O</span> (for the last <span class="math">r - r&#x27;</span>) rounds. We shall denote the two lists, <span class="math">L_1</span> and <span class="math">L_2</span>, respectively. The first list, <span class="math">L_1</span>, contains pairs of the form <span class="math">(\\Delta_{M_i}, p_i)</span> (i.e., the difference <span class="math">\\Delta_{M_i}</span> appears with probability <span class="math">p_i</span> given an input difference <span class="math">\\Delta_I</span>). Similarly, the second list, <span class="math">L_2</span>, contains pairs of the form <span class="math">(\\Delta_{M_j}, q_j)</span>.</p>

    <p class="text-gray-300">Given these two lists, we can define the set <span class="math">S</span> as all the differences which appear both in <span class="math">L_1</span> and <span class="math">L_2</span> with sufficiently high probability (which we denote by <span class="math">p_b</span>). Then, by using Equation (3), and the estimations for the <span class="math">p_i</span>'s and <span class="math">q_j</span>'s, we can compute an estimation for the probability of the differential <span class="math">\\Delta_I \\xrightarrow{r} \\Delta_O</span>:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Pick <span class="math">N</span> plaintext pairs⁶ of the form <span class="math">(x, x \\oplus \\Delta_I)</span>, and obtain their partial encryption after <span class="math">r&#x27;</span> rounds, <span class="math">(z, z&#x27;)</span>.</li>

      <li>Collect the differences <span class="math">z \\oplus z&#x27;</span>, and produce <span class="math">L_1</span>.</li>

      <li>Pick <span class="math">N</span> ciphertext pairs of the form <span class="math">(y, y \\oplus \\Delta_O)</span>, and obtain their partial decryption after <span class="math">r - r&#x27;</span> rounds, <span class="math">(w, w&#x27;)</span>.</li>

    </ol>

    <p class="text-gray-300">⁴ When using BITM to calculate the probability of a differential, one can choose the meeting round in a variety of ways. Usually setting <span class="math">r&#x27; \\approx r/2</span> gives the optimal results.</p>

    <p class="text-gray-300">⁵ Of course, we still need to assume independence between the two parts of the cipher.</p>

    <p class="text-gray-300">⁶ The value of <span class="math">N</span> is discussed later.</p>

    <p class="text-gray-300">10</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Collect the differences <span class="math">w \\oplus w&#x27;</span>, and produce <span class="math">L_2</span>.</li>

      <li>For all the differences that appear with probability above some bound <span class="math">p_b</span> in both <span class="math">L_1</span> and <span class="math">L_2</span>, compute the sum of all products <span class="math">(p_i \\cdot q_j)</span>.</li>

    </ol>

    <p class="text-gray-300">First, it is easy to see that both <span class="math">L_1</span> and <span class="math">L_2</span> contain two types of differences: High-probability differences (e.g., differences that appear with probability higher than <span class="math">p_b</span>) as well as low-probability differences that got sampled by chance. For an <span class="math">n</span>-bit block cipher, after sampling <span class="math">N</span> pairs, we expect low probability differences <span class="math">\\Delta_M</span> to be encountered only once (both in <span class="math">L_1</span> and in <span class="math">L_2</span>) as long as <span class="math">N &amp;lt; 2^{n/2}</span>. Moreover, as we later discuss, estimating the probabilities <span class="math">p_i</span>'s and <span class="math">q_j</span>'s can be done over many keys, offering a better estimation.</p>

    <p class="text-gray-300">Now, given <span class="math">p_b</span>, we wish to assure that we sample the high probability differences. This can be done, by looking for differences that appear at least twice during Steps 1-2 (for <span class="math">L_1</span>) or Steps 3-4 (for <span class="math">L_2</span>). Given that the number of "appearances" of an output difference follows the Poisson distribution, we need to take <span class="math">N = \\alpha / p_b</span> pairs, where <span class="math">\\alpha</span> determines the quality of our sampling. For example, if we pick <span class="math">\\alpha = 4</span>, i.e., we expect 4 pairs that follow the differential <span class="math">\\Delta_I \\xrightarrow{r&#x27;} \\Delta_M</span>, then with probability of <span class="math">90\\%</span>, <span class="math">\\Delta_M</span> would appear at least twice in Steps 1-2. Increasing the value of <span class="math">\\alpha</span> (and/or sampling using more keys) improves the quality of the values in <span class="math">L_1</span> and <span class="math">L_2</span>. For example, for <span class="math">\\alpha = 10</span>, the probability that a good <span class="math">\\Delta_M</span> will not appear at least twice is less than <span class="math">0.5\\%</span>.</p>

    <p class="text-gray-300">It is important to note that differences of low probability do not affect the overall estimation. This follows from the fact that we count only differences that appear in both lists <span class="math">L_1</span> and <span class="math">L_2</span>. Hence, even though there are some low probability differences in each list, it is extremely unlikely that the same low probability difference will appear in both lists simultaneously. Even in the extreme case that there are <span class="math">N</span> low probability <span class="math">\\Delta_M</span> values in each list, expected number of low probability <span class="math">\\Delta_M</span> appearing in both lists is <span class="math">N^2 / 2^n</span>, which is less than 1.</p>

    <p class="text-gray-300">We recall that similarly to all approaches that estimate the probability of differentials, we need to rely on some randomness assumptions. A round-by-round approach relies on the cipher being Markovian, whereas an experimental verification of the full differential does not require any assumption. The independence assumption needed by the BITM algorithm lies between these two extremes. We need to assume that the transition between the two parts of the cipher does not affect the probability estimations. In other words, even though the actual pairs in <span class="math">L_1</span> and <span class="math">L_2</span> are different, we can use a (reduced) Markov-cipher assumption to obtain an estimate for the total probability of the differential <span class="math">\\Delta_I \\xrightarrow{r} \\Delta_O</span>.</p>

    <p class="text-gray-300">11</p>

    <p class="text-gray-300">As mentioned earlier, as  <span class="math">\\alpha</span>  increases (or if the probability of the difference we check is higher than  <span class="math">p_b</span> ) the quality of the estimation of the probabilities in  <span class="math">L_1</span>  and  <span class="math">L_2</span>  improves. This is explained by the fact that we estimate the probability of an event which follows a Poisson distribution. If  <span class="math">X \\sim Poi(\\lambda)</span> , then  <span class="math">E[X] = Var[X] = \\lambda</span> , so the larger  <span class="math">\\lambda</span>  is, the closer  <span class="math">X</span>  is to its mean.</p>

    <p class="text-gray-300">Moreover, we note that the use of multiple keys can significantly improve the quality of the estimation. If we repeat the experiment with  <span class="math">t</span>  different keys, the expected number of times  <span class="math">\\Delta_M</span>  appeared in all  <span class="math">t</span>  experiments is increased by a factor  <span class="math">t</span> . As the sum of Poisson random variables is itself a Poisson random variable, we obtain a significantly better estimate for the actual probability of the difference. <span class="math">^8</span></p>

    <p class="text-gray-300">Hence, after sampling sufficiently many keys, one can obtain a better estimation of the actual probabilities of the various differences in  <span class="math">L_{1}</span>  and  <span class="math">L_{2}</span> , and discard the low probability differences. These probabilities can then be combined to offer a higher quality estimate of the probability of the differential  <span class="math">\\Delta_I \\stackrel{\\epsilon}{\\to} \\Delta_O</span> .</p>

    <p class="text-gray-300">A few improvements We first note that there is no need to actually store  <span class="math">L_{2}</span> . One can generate  <span class="math">L_{1}</span> , and for each  <span class="math">w \\oplus w&#x27;</span>  value of Steps 3-4, to increment the counter if  <span class="math">w \\oplus w&#x27;</span>  happens to be in  <span class="math">L_{1}</span> .</p>

    <p class="text-gray-300">We now turn our attention to the generation of  <span class="math">L_{1}</span> . It is easy to see that  <span class="math">L_{1}</span>  can take at most  <span class="math">O(N)</span>  memory cells. As  <span class="math">N</span>  increases this may be a practical bottleneck. Hence, once the used memory reaches the machine's limit (or the process' limit), we suggest to "extract" all the high probability differences encountered so far into a shorter list  <span class="math">L_{1}&#x27;</span> . Then, we sample more random pairs, but this time, we only deal with those pairs whose "output" difference is in the short list  <span class="math">L_{1}&#x27;</span> . The main advantage is now that we use almost no memory (as  <span class="math">L_{1}&#x27;</span>  tends to be small), we can actually increase the number of queries, thus obtaining a more accurate estimate.</p>

    <p class="text-gray-300">The final improvement in this front is to perform the previous idea in steps. We first sample many pairs, and store the differences  <span class="math">z \\oplus z&#x27;</span>  in a hash table (with less than  <span class="math">N</span>  bins). After finding the bins which were suggested more than others, we can dive into them by re-sampling more pairs.</p>

    <p class="text-gray-300">Comparison with Meet in the Middle Attacks We note that while the  <span class="math">BITM</span>  algorithm is is superficially similar to the meet in the middle (MITM) algorithm, it is quite different. In the MITM algorithm, we typically try to find some common value between the two parts of a cipher, and use this value to find the key (depending on the cryptanalytic task at hand, we may search for</p>

    <p class="text-gray-300">all the common values). In the BITM algorithm our goal is not to find these values, but to estimate the probability that they exist, in order to choose the best differential attack on the scheme.</p>

    <h3 id="sec-11" class="text-xl font-semibold mt-8">6.2 The Advantages of the <span class="math">BITM</span> Algorithm</h3>

    <p class="text-gray-300">The main advantage of the <span class="math">BITM</span> algorithm over a pure top-down algorithm which evaluates the full mapping is its greatly improved efficiency. Indeed, in order to estimate the differential <span class="math">\\Delta_{I}\\xrightarrow{r}\\Delta_{O}</span> requires <span class="math">O(p^{-1})</span> pairs. However, if we pick <span class="math">r^{\\prime}\\approx r/2</span>, and under the assumption that both parts are roughly of the same strength, we obtain <span class="math">p_{b}=O(\\sqrt{p})</span>. This is extremely important for the cases where a time complexity of <span class="math">p_{b}^{-1}</span> is still feasible but <span class="math">p_{b}^{-2}</span> is not (e.g., when <span class="math">p_{b}\\approx 2^{-40}</span>).</p>

    <p class="text-gray-300">Another advantage of the <span class="math">BITM</span> algorithm over bottom-up algorithms is that we take into account all the high probability differential characteristics simultaneously. Hence, the estimation for the differential probability is closer to the actual probability than an estimation which is based on the multiplication of many probabilities along a single differential characteristic.</p>

    <p class="text-gray-300">Finally, this method offers some experimental verification of the stochastic equivalence assumption. Indeed, for most ciphers (and most of the keys), the stochastic equivalence assumption tends to hold (or seem to “work” most of the time). However, when we discuss a single long characteristic, we may encounter some inconsistencies between different parts of the characteristic. In these situations, the real probability and computed probability will differ. Once we take into consideration multiple differential characteristics, the estimation becomes more resilient (though not 100% full-proof), as a “failure” of one of the longer characteristics does not invalidate the full differential. In addition, by running the algorithm with several different <span class="math">r^{\\prime}</span> can also help in validating the probability of the transition between the top half and the bottom half.</p>

    <h2 id="sec-12" class="text-2xl font-bold">7 Applying Our New Algorithms to the Simon Family of Block Ciphers</h2>

    <p class="text-gray-300">The Simon family of lightweight block ciphers, presented in <em>[4]</em>, is implemented using a balanced Feistel structure. The Simon round function is very simple and consists of only three operations: AND, XOR and constant rotations. All the ciphers in the Simon family use the same round function and differ only by the key size, the block size (which ranges from 32 to 128 bits) and the number of Feistel rounds which is dependant on the former two. As in any Feistel structure, the plaintext is divided into two blocks of size <span class="math">n</span>: <span class="math">P=(L_{0},R_{0})</span> and then every round <span class="math">1\\leq i\\leq r</span>:</p>

    <p class="text-gray-300"><span class="math">L_{i}=R_{i-1}\\oplus F(L_{i-1})\\oplus K_{i-1};\\quad R_{i}=L_{i-1}</span></p>

    <p class="text-gray-300">where the ciphertext is <span class="math">C=(L_{r},R_{r})</span> and <span class="math">F</span> is the Simon round function:</p>

    <p class="text-gray-300"><span class="math">F(x)=((x\\lll 1)\\wedge(x\\lll 8))\\oplus(x\\lll 2)</span></p>

    <p class="text-gray-300">An illustration of the round function is depicted in Figure 1.</p>

    <p class="text-gray-300">!<a href="img-0.jpeg">img-0.jpeg</a> Fig.1. The SIMON round function</p>

    <p class="text-gray-300">In this section we present the best differentials for SIMON64 SIMON96 and SIMON128 we found using our various diagonal estimation algorithms. We also describe more accurate BITM-based estimates for the differential probabilities of previously presented SIMON characteristics, which are substantially different from the original estimates.</p>

    <p class="text-gray-300">We applied the BITM algorithm to some of the previously known differentials for SIMON which were published in [1, 9, 27]. Since testing probabilities  <span class="math">&amp;lt; 2^{-80}</span>  is too expensive (it requires throwing  <span class="math">&amp;gt; 2^{40}</span>  balls into bins from each side), the longer differentials were evaluated by breaking the characteristic into the smallest possible number of parts, evaluating each part separately, and taking the product of the probabilities as the result. This is not a pure BITM approach, but it is much closer to a top-down computation compared to the bottom-up approach used by other researchers.</p>

    <p class="text-gray-300">For example, the 41-round SIMON128 characteristic from [1] was divided into a 9-round differential  <span class="math">(e_{12}, e_{6,10,14}) \\xrightarrow{9R} (e_{6,10,14}, e_{12})</span>  and a 7-round differential  <span class="math">(e_{6,10,14}, e_{12}) \\xrightarrow{7R} (e_{12}, e_{6,10,14})</span> . The probabilities for the differentials were tested many times:  <span class="math">2^{35}</span>  balls were thrown from each side for 30 different keys, and  <span class="math">2^{30} - 2^{32}</span>  balls were thrown from each side for  <span class="math">&amp;gt;100</span>  different keys. The results for the 9-round differential were in the range  <span class="math">[2^{-18.61}, 2^{-18.59}]</span>  with an average of  <span class="math">2^{-18.6}</span> . The results for the 7-round differential were in the range  <span class="math">[2^{-32.92}, 2^{-32.25}]</span>  with an average of  <span class="math">2^{-32.77}</span> . When testing the entire 16-round characteristic (first the 9-round differential, then the 7-round one) with the same number of experiments, the probability range was  <span class="math">[2^{-51.2}, 2^{-48.4}]</span>  with an average  <span class="math">2^{-49.9}</span> . This means that the entire 41-round characteristic has probability in the approximate range of  <span class="math">[2^{-121}, 2^{-115.6}]</span>  with an average of  <span class="math">\\approx 2^{-118.6}</span> .</p>

    <p class="text-gray-300">The advantage of using the BITM algorithm to evaluate the probabilities is the fact we do not consider what happens in the intermediate rounds, but only take interest in the probabilities between the first, last and middle round. Additionally, we do not make any independence assumptions about the round keys since the BITM experiments use the actual SIMON key-schedule.</p>

    <p class="text-gray-300">Table 1 compares all the previously published bottom-up estimates of the probabilities of various differential transitions with our experimentally obtained top-down results (where the numbers are the log to the base 2 of the probabilities). All the results (including those presented in the next subsection) were obtained by the same method as described for the SIMON128 differential (Many experiments with as many as  <span class="math">2^{35}</span>  balls thrown from each side with a narrow result range, and the average value is taken as the final probability).</p>

    <p class="text-gray-300">Table 1. The original and our improved estimates of the probabilities of the best previously published differentials</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Cipher</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Rounds</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Presented prob.</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">BITM prob.</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Source</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">SIMON64</td>

            <td class="px-3 py-2 border-b border-gray-700">21</td>

            <td class="px-3 py-2 border-b border-gray-700">-60.53</td>

            <td class="px-3 py-2 border-b border-gray-700">-56.05</td>

            <td class="px-3 py-2 border-b border-gray-700">[9]</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">SIMON64</td>

            <td class="px-3 py-2 border-b border-gray-700">21</td>

            <td class="px-3 py-2 border-b border-gray-700">-61.01</td>

            <td class="px-3 py-2 border-b border-gray-700">-56.05</td>

            <td class="px-3 py-2 border-b border-gray-700">[1]</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">SIMON64</td>

            <td class="px-3 py-2 border-b border-gray-700">21</td>

            <td class="px-3 py-2 border-b border-gray-700">-60.21</td>

            <td class="px-3 py-2 border-b border-gray-700">-59</td>

            <td class="px-3 py-2 border-b border-gray-700">[27]</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">SIMON96</td>

            <td class="px-3 py-2 border-b border-gray-700">30</td>

            <td class="px-3 py-2 border-b border-gray-700">-92.20</td>

            <td class="px-3 py-2 border-b border-gray-700">-88.5</td>

            <td class="px-3 py-2 border-b border-gray-700">[1]</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">SIMON128</td>

            <td class="px-3 py-2 border-b border-gray-700">41</td>

            <td class="px-3 py-2 border-b border-gray-700">-124.6</td>

            <td class="px-3 py-2 border-b border-gray-700">-118.6</td>

            <td class="px-3 py-2 border-b border-gray-700">[1]</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">This table shows that the previous estimates were too pessimistic (sometimes by a significant factor of  <span class="math">2^{6} = 64</span> ) since they considered only a limited number of differential characteristics. Since some of the differential probabilities that appear in the mentioned papers are significantly lower than the probabilities estimated by our BITM algorithm, we can extend the differentials to a larger number of rounds, while maintaining the probability above  <span class="math">2^{-n}</span> . However, even without extending the characteristics, the results of Table 1 automatically translate into better key recovery attacks on the SIMON members, as the previous attacks only depended on differentials for SIMON (and not on the internal characteristics).</p>

    <p class="text-gray-300">We applied the  <span class="math">GDIAG_{L}</span>  algorithm to SIMON, followed by estimating the probabilities using BITM. The result is an improvement by two rounds of the best previously known differential from [1] by 2 rounds while maintaining roughly the same probability.</p>

    <p class="text-gray-300">The application of <span class="math">GDIAG_{L}</span> was done with the function</p>

    <p class="text-gray-300"><span class="math">L(x)=(x\\lll n)</span></p>

    <p class="text-gray-300">(which is a half block rotation that swaps its two halves). The result of this function is some differential families of the type AB<span class="math">\\to</span> BA for various numbers of rounds. After applying a search for complementing differentials pairs AB <span class="math">\\to</span> BA <span class="math">\\to</span> AB, the most probable ones we found were:</p>

    <p class="text-gray-300"><span class="math">\\Pr\\left[\\left(e_{i,i+4},e_{i+6}\\right)\\xrightarrow{7\\rm R}\\left(e_{i+6},e_{i,i+4}\\right)\\right]</span> <span class="math">\\approx 2^{-14.6}</span> (4) <span class="math">\\Pr\\left[\\left(e_{i+6},e_{i,i+4}\\right)\\xrightarrow{9\\rm R}\\left(e_{i,i+4},e_{i+6}\\right)\\right]</span> <span class="math">\\approx 2^{-35.6}</span> (5) <span class="math">\\Pr\\left[\\left(e_{i+2},e_{i,i+4}\\right)\\xrightarrow{5\\rm R}\\left(e_{i,i+4},e_{i+2}\\right)\\right]</span> <span class="math">=2^{-8}</span> (6) <span class="math">\\Pr\\left[\\left(e_{i,i+4},e_{i+2}\\right)\\xrightarrow{11\\rm R}\\left(e_{i+2},e_{i,i+4}\\right)\\right]</span> <span class="math">\\approx 2^{-41.5}</span> (7)</p>

    <p class="text-gray-300">Both pairs result in a 16-round iterated differential.</p>

    <p class="text-gray-300">In order to construct the full characteristic we additionally use the following differential:</p>

    <p class="text-gray-300"><span class="math">\\Pr\\left[\\left(e_{i,i+4},e_{i+6}\\right)\\xrightarrow{6\\rm R}\\left(e_{i,i+4},e_{i+2}\\right)\\right]</span> <span class="math">\\approx 2^{-11.3}</span> (8)</p>

    <p class="text-gray-300">Concatenating differentials (4) and (5) results in a 16-round characteristic with probability of <span class="math">\\approx 2^{-48.6}</span>, thus the full characteristic</p>

    <p class="text-gray-300"><span class="math">\\left(e_{i,i+4},e_{i+6}\\right)\\xrightarrow[2^{-48.6}]{16\\rm R}\\left(e_{i,i+4},e_{i+6}\\right)\\xrightarrow[2^{-11.3}]{6\\rm R}\\left(e_{i,i+4},e_{i+2}\\right)</span></p>

    <p class="text-gray-300">has 22 rounds and probability of <span class="math">\\approx 2^{-59.9}</span>.</p>

    <p class="text-gray-300">A different 22-round characteristic can be obtained by using (6), (7), (6) and an additional round. Combining (6) with (7) results in a 16-round differential with probability <span class="math">\\approx 2^{-48}</span>, and adding <span class="math">(e_{i,i+4},e_{i+2})\\xrightarrow{1R}(e_{i+6},e_{i,i+4})</span> after (6) results in a 6-round differential with probability <span class="math">\\approx 2^{-11.3}</span>. The entire 22-round characteristic</p>

    <p class="text-gray-300"><span class="math">\\left(e_{i+2},e_{i,i+4}\\right)\\xrightarrow[2^{-48}]{16\\rm R}\\left(e_{i+2},e_{i,i+4}\\right)\\xrightarrow[2^{-11.3}]{6\\rm R}\\left(e_{i+6},e_{i,i+4}\\right)</span></p>

    <p class="text-gray-300">has probability <span class="math">\\approx 2^{-59.3}</span>.</p>

    <p class="text-gray-300">Note that both characteristics can be extended one round further ( <span class="math">(e_{i+6},e_{i,i+4,i+8})\\xrightarrow[2^{-2}]{1R}</span> <span class="math">(e_{i,i+4},e_{i+6})</span> and <span class="math">(e_{i+6},e_{i,i+4})\\xrightarrow[2^{-2}]{1R}(e_{i,i+4,i+8},e_{i+6})</span>, respectively) while maintaining a probability which is above <span class="math">2^{-64}</span>. The resultant 23-round differential is longer than all the previously found differentials for Simon64.</p>

    <p class="text-gray-300">SIMON128 The longest previously found differential for SIMON128 had 41 rounds. We can find a longer 43-round differential which is based on differentials (4), (5) and another short differential:</p>

    <div class="my-4 text-center"><span class="math-block">\\Pr \\left[ \\left(e _ {i + 6}, e _ {i, i + 4}\\right) \\xrightarrow {3 \\mathrm {R}} \\left(e _ {i, i + 8, i + 1 2}, e _ {i + 2, i + 1 0}\\right) \\right] = 2 ^ {- 1 2} \\tag {9}</span></div>

    <p class="text-gray-300">Combining (4) with (9) results in a 9-round characteristic of probability  <span class="math">\\approx 2^{-22.4}</span> . The full 43-round characteristic</p>

    <div class="my-4 text-center"><span class="math-block">\\left(e _ {i + 6}, e _ {i, i + 4, i + 8}\\right) \\xrightarrow [ 2 ^ {- 2} ]{\\mathrm {1 R}} \\left(e _ {i, i + 4}, e _ {i + 6}\\right) \\xrightarrow [ 2 ^ {- 9 7. 2} ]{\\mathrm {3 2 R}} \\left(e _ {i, i + 4}, e _ {i + 6}\\right) \\xrightarrow [ 2 ^ {- 2 6. 4} ]{\\mathrm {1 0 R}} \\left(e _ {i, i + 8, i + 1 2}, e _ {i + 2, i + 1 0}\\right)</span></div>

    <p class="text-gray-300">has probability of  <span class="math">\\approx 2^{-125.6}</span></p>

    <p class="text-gray-300">A different 43-round characteristic will require the same differentials that were used for SIMON64, combined with a 4-round differential as follows:</p>

    <div class="my-4 text-center"><span class="math-block">\\Pr \\left[ \\left(e _ {i + 2, i + 1 0}, e _ {i, i + 8, i + 1 2}\\right) \\xrightarrow {4 \\mathrm {R}} \\left(e _ {i + 2}, e _ {i, i + 4}\\right) \\right] = 2 ^ {- 1 5. 1} \\tag {10}</span></div>

    <p class="text-gray-300">and finishing with a single round  <span class="math">(e_{i + 6},e_{i,i + 4})\\xrightarrow[2^{-2}]{1R}(e_{i,i + 4,i + 8},e_{i + 6})</span> . The full characteristic is:</p>

    <div class="my-4 text-center"><span class="math-block">\\left(e _ {i + 2, i + 1 0}, e _ {i, i + 8, i + 1 2}\\right) \\xrightarrow [ 2 ^ {- 1 5. 1} ]{4 \\mathrm {R}} \\left(e _ {i + 2}, e _ {i, i + 4}\\right) \\xrightarrow [ 2 ^ {- 9 6} ]{3 2 \\mathrm {R}} \\left(e _ {i + 2}, e _ {i, i + 4}\\right) \\xrightarrow [ 2 ^ {- 1 3. 3} ]{7 \\mathrm {R}} \\left(e _ {i, i + 4, i + 8}, e _ {i + 6}\\right)</span></div>

    <p class="text-gray-300">and it has probability of  <span class="math">\\approx 2^{-124.4}</span> .</p>

    <p class="text-gray-300">Table 2. Summary of the  <span class="math">GDIAG_{L}</span>  results for SIMON</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Cipher</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Differential family</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Rounds</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Prob. (log_{2})</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">SIMON64</td>

            <td class="px-3 py-2 border-b border-gray-700">(e_{i,i+4},e_{i+6}) → (e_{i,i+4},e_{i+2})</td>

            <td class="px-3 py-2 border-b border-gray-700">22</td>

            <td class="px-3 py-2 border-b border-gray-700">−59.9</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">SIMON64</td>

            <td class="px-3 py-2 border-b border-gray-700">(e_{i+2},e_{i,i+4}) → (e_{i+6},e_{i,i+4})</td>

            <td class="px-3 py-2 border-b border-gray-700">22</td>

            <td class="px-3 py-2 border-b border-gray-700">−59.3</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">SIMON64</td>

            <td class="px-3 py-2 border-b border-gray-700">(e_{i+6},e_{i,i+4,i+8}) → (e_{i,i+4},e_{i+2})</td>

            <td class="px-3 py-2 border-b border-gray-700">23</td>

            <td class="px-3 py-2 border-b border-gray-700">−61.9</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">SIMON64</td>

            <td class="px-3 py-2 border-b border-gray-700">(e_{i+2},e_{i,i+4}) → (e_{i,i+4,i+8},e_{i+6})</td>

            <td class="px-3 py-2 border-b border-gray-700">23</td>

            <td class="px-3 py-2 border-b border-gray-700">−61.3</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">SIMON128</td>

            <td class="px-3 py-2 border-b border-gray-700">(e_{i+6},e_{i,i+4,i+8}) → (e_{i,i+8,i+12},e_{i+2,i+10})</td>

            <td class="px-3 py-2 border-b border-gray-700">43</td>

            <td class="px-3 py-2 border-b border-gray-700">−125.6</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">SIMON128</td>

            <td class="px-3 py-2 border-b border-gray-700">(e_{i+2,i+10},e_{i,i+8,i+12}) → (e_{i,i+4,i+8},e_{i+6})</td>

            <td class="px-3 py-2 border-b border-gray-700">43</td>

            <td class="px-3 py-2 border-b border-gray-700">−124.4</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">The Even-Mansour (EM) block cipher was proposed at Asiacrypt 1991 [16]. It uses a single publicly known random permutation  <span class="math">P</span>  on  <span class="math">n</span> -bit values and two secret  <span class="math">n</span> -bit keys  <span class="math">K_{1}</span>  and  <span class="math">K_{2}</span> , and defines the encryption of the  <span class="math">n</span> -bit plaintext  <span class="math">m</span>  as  <span class="math">E(m) = P(m \\oplus K_{1}) \\oplus K_{2}</span> . The decryption of the  <span class="math">n</span> -bit ciphertext  <span class="math">c</span>  is</p>

    <p class="text-gray-300">similarly defined as  <span class="math">D(c) = P^{-1}(c \\oplus K_2) \\oplus K_1</span> . It can be naturally generalized into an  <span class="math">r</span> -round iterated EM encryption function (a.k.a. a key-alternating scheme in [11]), which is defined using  <span class="math">r</span>  permutations  <span class="math">P_1, P_2, \\ldots, P_r</span>  and  <span class="math">r + 1</span>  keys  <span class="math">K_1, K_2, \\ldots, K_{r+1}</span>  as  <span class="math">E(m) = P_r(\\ldots P_2(P_1(m \\oplus K_1) \\oplus K_2) \\oplus K_3 \\ldots \\oplus K_r) \\oplus K_{r+1}</span> , where decryption is defined in an analogous way.</p>

    <p class="text-gray-300">At Asiacrypt 2013, Dinur et al. [15] analyzed several instances of iterated EM schemes, and in particular showed that the scheme in which all round keys are equal to  <span class="math">K</span>  (shown in Figure 2) must have at least 4 rounds in order to provide perfect  <span class="math">n</span> -bit security in the single-key model. In this section, we extend this result to the related-key model, and show that the scheme must have at least 5 rounds in order to provide perfect  <span class="math">(n - 1)</span> -bit security against related-key attacks that use 2 related keys. <span class="math">^{10}</span>  This is done by presenting a related-key differential attack, which is applicable to (almost) any 4-round EM scheme.</p>

    <p class="text-gray-300">!<a href="img-1.jpeg">img-1.jpeg</a> Fig. 2. An iterated EM with one key</p>

    <p class="text-gray-300">Our starting point is the attack of Mendel et al. [23] on the block cipher LED-64, which is a specific instance of the iterated EM scheme shown in Figure 2. This attack assumes that we have an iterative differential characteristic for  <span class="math">P_{2}</span>  with probability  <span class="math">p</span> , where the input and output difference is  <span class="math">\\Delta</span> , and extends it to a 3-round related-key characteristic with that same probability  <span class="math">p</span> . This is done by choosing both the key difference and the plaintext difference as  <span class="math">\\Delta</span> , which implies that the input difference to  <span class="math">P_{4}</span>  is  <span class="math">\\Delta</span>  with probability  <span class="math">p</span> . The 3-round characteristic can be used to attack 4 rounds of the scheme with time, data and memory complexities of about  <span class="math">2^{n/2} \\cdot p^{-1/2}</span>  using an extension of the attack of Daemen on the original EM scheme [12]. The full details of the attack are given in [23] and are not required in order to understand this section.</p>

    <p class="text-gray-300">The most relevant component of the attack of [23] to our analysis is the iterative differential characteristic for  <span class="math">P_{2}</span> . In [23], such a characteristic was efficiently found using the internal properties of LED, but here we notice that we can apply a similar attack to essentially any 4-round EM scheme with one key. Our general framework is similar to the one of [15], which analyzed the public permutations of an EM scheme in order to detect some property which is useful for attacking the full cipher. More specifically, we can use our new diagonal algorithm of Section 4.1 in order to analyze the specific choice of  <span class="math">P_{2}</span>  in a particular</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">incarnation of the Even-Mansour scheme, and find in it the highest probability iterative characteristic. However, applying the full diagonal algorithm results in an attack with complexity of at least <span class="math">2^{n}</span>, which is not faster than exhaustive search. Therefore, we apply the algorithm by evaluating only a fraction of the input space of <span class="math">P_{2}</span>. More specifically, we evaluate an arbitrary linear subspace of inputs <span class="math">X</span>, such that $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">X</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">=S<span class="math"> (for a parameter </span>S<span class="math">), and look for the entry </span>\\Delta<span class="math"> of the (partial) </span>DIAG<span class="math"> with a maximal value, denoted by </span>t<span class="math">. The value of </span>S<span class="math"> is carefully chosen in order to optimize the total time complexity of the attack, which is about </span>S+2^{n/2}\\cdot p^{-1/2}<span class="math">, where </span>p=t/2^{n}$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">In order to calculate <span class="math">t</span> as a function of <span class="math">S</span>, we first compute the expected value of an arbitrary cell of the (partial) <span class="math">DIAG</span>. Assuming that <span class="math">P_{2}</span> is a random permutation, then the function <span class="math">x\\oplus P_{2}(x)</span> is (very close to) a random function, and therefore we expect about <span class="math">S^{2}\\cdot 2^{-n}</span> collision in the hash table <span class="math">H</span> in Step 1 of the diagonal algorithm. Each such collision will contribute once to a <span class="math">DIAG</span> entry, but since we selected <span class="math">X</span> as a linear subspace, we know in advance that there are only <span class="math">S</span> relevant <span class="math">DIAG</span> entries (those which belong to the closed subspace <span class="math">X</span>). Therefore, the average value of a relevant entry is <span class="math">S^{2}\\cdot 2^{-n}/S=S\\cdot 2^{-n}</span> (whereas the other <span class="math">2^{n}-S</span> entries have zero values).</p>

    <p class="text-gray-300">Similarly to the analysis of <em>[15]</em>, based on some randomness assumptions on <span class="math">P_{2}</span>, the value of an entry in the (partial) <span class="math">DIAG</span> is distributed according to the Poisson distribution with an expectation <span class="math">\\lambda</span>, which is equal to the average value <span class="math">\\lambda=S\\cdot 2^{-n}</span>. Given a parameter <span class="math">t</span>, the probability that an arbitrary entry of the partial <span class="math">DIAG</span> will have a value of <span class="math">t</span> is estimated as <span class="math">(\\lambda^{t}e^{-\\lambda})/t!</span>. We have <span class="math">S</span> elements in the range, implying that we expect that about <span class="math">(S\\cdot\\lambda^{t}e^{-\\lambda})/t!</span> entries will have a value of <span class="math">t</span>. If we equate this number to 1, consider large values of <span class="math">S\\approx 2^{n}/n</span>, and ignore low order terms, we can deduce that the largest expected entry value <span class="math">t</span> satisfies <span class="math">t\\cdot\\log(t)=n</span>, and thus <span class="math">t</span> is approximately equal to <span class="math">n/\\log(n)</span>. When plugging <span class="math">S\\approx 2^{n}/n</span> and <span class="math">p=t\\cdot 2^{-n}\\approx n/(2^{n}\\cdot\\log(n))</span> into the complexity of the attack, we obtain <span class="math">S+2^{n/2}\\cdot p^{-1/2}\\approx 2^{n}/n+2^{n}/\\sqrt{n}\\approx 2^{n}/\\sqrt{n}</span>. In other words, we obtain a related key attack on any particular incarnation of a 4-round EM which is about <span class="math">\\sqrt{n}</span> times faster than exhaustive search, even if we have to include the complexity of analyzing the particular choice of <span class="math">P_{2}</span> in the total time complexity.</p>

    <p class="text-gray-300">We note that the analysis provided in this section can be used to improve the complexity of the original 4-round related-key attack on LED-64 <em>[23]</em>. However, the improvement factor is rather small and the main significance of our analysis is theoretical, namely, describing the first generic attack on the 4-round 1-key EM scheme which is (slightly) better than exhaustive search.</p>

    <h2 id="sec-16" class="text-2xl font-bold">9 Conclusions</h2>

    <p class="text-gray-300">In this paper we described and motivated the top-down approach to differential cryptanalysis, which tries to compute or approximate certain DDT values without looking at the internal structure of the given mapping. We introduced three novel techniques which can compute three types of interesting entries (on</p>

    <p class="text-gray-300">the diagonal, in low Hamming weight entries on the diagonal, and arbitrarily located entries with large values) with improved efficiency. We then applied the new BITM technique to Simon in order to obtain more accurate estimates of the probabilities of all the previously published differentials and combined it with the generalized diagonal algorithm to find better differentials for a larger number of rounds. This improves the best known cryptanalytic results for this scheme and demonstrates the power and versatility of our new top-down techniques. Finally, in Section 8 we described how to use our new algorithms to efficiently locate the highest diagonal entry in any given incarnation of the four round version of Even-Mansour scheme, in order to break the scheme with a related key attack which is faster than exhaustive search.</p>

    <h2 id="sec-17" class="text-2xl font-bold">References</h2>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[1] F. Abed, E. List, J. Wenzel, and S. Lucks. Differential Cryptanalysis of round-reduced Simon and Speck, 2014. Presented at FSE 2014. To Appear in Lecture Notes in Computer Science.</li>

      <li>[2] H. A. Alkhzaimi and M. M. Lauridsen. Cryptanalysis of the SIMON Family of Block Ciphers. Cryptology ePrint Archive, Report 2013/543, 2013.</li>

      <li>[3] K. Aoki, K. Kobayashi, and S. Moriai. Best Differential Characteristic Search of FEAL. In Fast Software Encryption, FSE ’97, volume 1267 of Lecture Notes in Computer Science, pages 41–53. Springer, 1997.</li>

      <li>[4] R. Beaulieu, D. Shors, J. Smith, S. Treatman-Clark, B. Weeks, and L. Wingers. The SIMON and SPECK Families of Lightweight Block Ciphers. Cryptology ePrint Archive, Report 2013/404, 2013.</li>

      <li>[5] E. Biham, A. Biryukov, and A. Shamir. Cryptanalysis of Skipjack Reduced to 31 Rounds Using Impossible Differentials. In Advances in Cryptology - EUROCRYPT ’99, volume 1592 of Lecture Notes in Computer Science, pages 12–23. Springer, 1999.</li>

      <li>[6] E. Biham and A. Shamir. Differential cryptanalysis of DES-like cryptosystems. Journal of CRYPTOLOGY, 4(1):3–72, 1991.</li>

      <li>[7] A. Biryukov and I. Nikolic. Automatic Search for Related-Key Differential Characteristics in Byte-Oriented Block Ciphers: Application to AES, Camellia, Khazad and Others. In Advances in Cryptology - EUROCRYPT 2010, volume 6110 of Lecture Notes in Computer Science, pages 322–344. Springer, 2010.</li>

      <li>[8] A. Biryukov and I. Nikolic. Search for Related-Key Differential Characteristics in DES-Like Ciphers. In Fast Software Encryption, FSE 2011, volume 6733 of Lecture Notes in Computer Science, pages 18–34. Springer, 2011.</li>

      <li>[9] A. Biryukov, A. Roy, and V. Velichkov. Differential Analysis of Block Ciphers SIMON and SPECK, 2014. Presented at FSE 2014. To Appear in Lecture Notes in Computer Science.</li>

      <li>[10] A. Biryukov and V. Velichkov. Automatic Search for Differential Trails in ARX Ciphers. In Topics in Cryptology - CT-RSA 2014, volume 8366 of Lecture Notes in Computer Science, pages 227–250. Springer, 2014.</li>

      <li>[11] A. Bogdanov, L. R. Knudsen, G. Leander, F. Standaert, J. P. Steinberger, and E. Tischhauser. Key-Alternating Ciphers in a Provable Setting: Encryption Using a Small Number of Public Permutations - (Extended Abstract). In Advances in Cryptology - EUROCRYPT 2012, volume 7237 of Lecture Notes in Computer Science, pages 45–62. Springer, 2012.</li>

      <li>[</li>

    </ul>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>J. Daemen. Limitations of the Even-Mansour Construction. In Advances in Cryptology - ASIACRYPT ’91, volume 739 of Lecture Notes in Computer Science, pages 495–498. Springer, 1993.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>J. Daemen, R. Govaerts, and J. Vandewalle. A New Approach to Block Cipher Design. In R. J. Anderson, editor, Fast Software Encryption, Cambridge Security Workshop, Cambridge, UK, December 9-11, 1993, Proceedings, volume 809 of Lecture Notes in Computer Science, pages 18–32. Springer, 1993.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>C. De Cannière and C. Rechberger. Finding SHA-1 Characteristics: General Results and Applications. In Advances in Cryptology - ASIACRYPT 2006, volume 4284 of Lecture Notes in Computer Science, pages 1–20. Springer, 2006.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>I. Dinur, O. Dunkelman, N. Keller, and A. Shamir. Key Recovery Attacks on 3-round Even-Mansour, 8-step LED-128, and Full <span class="math">AES^{2}</span>. In Advances in Cryptology - ASIACRYPT 2013, volume 8269 of Lecture Notes in Computer Science, pages 337–356. Springer, 2013.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>S. Even and Y. Mansour. A Construction of a Cipher from a Single Pseudorandom Permutation. J. Cryptology, 10(3):151–162, 1997.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>P. Fouque, J. Jean, and T. Peyrin. Structural Evaluation of AES and Chosen-Key Distinguisher of 9-Round AES-128. In Advances in Cryptology - CRYPTO 2013, volume 8042 of Lecture Notes in Computer Science, pages 183–203. Springer, 2013.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>L. Knudsen. DEAL - A 128-bit Block Cipher, 1998. NIST AES Proposal.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>X. Lai and J. L. Massey. Markov Ciphers and Differentail Cryptanalysis. In Advances in Cryptology - EUROCRYPT ’91, volume 547 of Lecture Notes in Computer Science, pages 17–38. Springer, 1991.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>G. Leurent. Analysis of Differential Attacks in ARX Constructions. In Advances in Cryptology - ASIACRYPT 2012, volume 7658 of Lecture Notes in Computer Science, pages 226–243. Springer, 2012.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>M. Matsui. On Correlation Between the Order of S-boxes and the Strength of DES. In Advances in Cryptology - EUROCRYPT ’94, volume 950 of Lecture Notes in Computer Science, pages 366–375. Springer, 1995.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>F. Mendel, T. Nad, and M. Schläffer. Finding SHA-2 Characteristics: Searching through a Minefield of Contradictions. In Advances in Cryptology - ASIACRYPT 2011, volume 7073 of Lecture Notes in Computer Science, pages 288–307. Springer, 2011.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>F. Mendel, V. Rijmen, D. Toz, and K. Varici. Differential Analysis of the LED Block Cipher. In Advances in Cryptology - ASIACRYPT 2012, volume 7658 of Lecture Notes in Computer Science, pages 190–207. Springer, 2012.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>N. Mouha, Q. Wang, D. Gu, and B. Preneel. Differential and Linear Cryptanalysis Using Mixed-Integer Linear Programming. In Information Security and Cryptology - Inscrypt 2011, volume 7537 of Lecture Notes in Computer Science, pages 57–76. Springer, 2012.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>I. Nikolic. Tweaking AES. In Selected Areas in Cryptography - SAC 2010, volume 6544 of Lecture Notes in Computer Science, pages 198–210. Springer, 2011.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>K. Nyberg and L. R. Knudsen. Provable Security Against Differential Cryptanalysis. In Advances in Cryptology - CRYPTO ’92, volume 740 of Lecture Notes in Computer Science, pages 566–574. Springer, 1993.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>S. Sun, L. Hu, M. Wang, P. Wang, K. Qiao, X. Ma, D. Shi, and L. Song. Automatic Enumeration of (Related-key) Differential and Linear Characteristics with Predefined Properties and Its Applications. 2014.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>S. Sun, L. Hu, P. Wang, K. Qiao, X. Ma, and L. Song. Automatic Security Evaluation and (Related-key) Differential Characteristic Search: Application to SIMON,</li>

    </ol>

    <p class="text-gray-300">PRESENT, LBlock, DES(L) and Other Bit-oriented Block Ciphers. Cryptology ePrint Archive, Report 2013/676, 2013. Accepted to ASIACRYPT 2014.</p>

    <h2 id="sec-18" class="text-2xl font-bold">10 Calculating <span class="math">P_{r,d}^{n}</span></h2>

    <p class="text-gray-300">The Hamming ball algorithm of Section 5 relies on the value of <span class="math">P_{r,d}^{n}</span>. We compute this value by distinguishing between two cases: when <span class="math">d &amp;gt; 2r</span>, then <span class="math">P_{r,d}^{n} = 0</span>, as the largest Hamming distance between points in <span class="math">B_{r}(c)</span> is <span class="math">2r</span>. Otherwise, <span class="math">d \\leq 2r</span>, and we consider the conditions on a point <span class="math">x</span> such that both <span class="math">x \\in B_{r}(c)</span> and <span class="math">x \\oplus \\Delta \\in B_{r}(c)</span>. We partition the coordinates of <span class="math">x \\oplus c</span> which are set to 1 into two groups: the <span class="math">d_{1} \\leq \\min(r, d)</span> coordinates which are common to <span class="math">x \\oplus c</span> and <span class="math">\\Delta \\oplus c</span>, and the remaining <span class="math">d_{2} \\leq \\min(r, n - d)</span> coordinates. Thus, we have <span class="math">dist(x, c) = d_{1} + d_{2}</span> and <span class="math">dist(x \\oplus \\Delta, c) = d + d_{2} - d_{1}</span>, implying that <span class="math">d_{1} + d_{2} \\leq r</span> and <span class="math">d + d_{2} - d_{1} \\leq r</span>. In particular, the last equality implies that <span class="math">d_{1} \\geq \\max(d - r, 0)</span>, and so <span class="math">\\max(d - r, 0) \\leq d_{1} \\leq \\min(r, d)</span>, while <span class="math">0 \\leq d_{2} \\leq \\min(r - d_{1}, r + d_{1} - d, n - d)</span>. Therefore, we obtain</p>

    <div class="my-4 text-center"><span class="math-block">P_{r,d}^{n} = \\sum_{d_{1} = m_{1}}^{m_{2}} \\sum_{d_{2} = 0}^{m_{3}} \\binom{d}{d_{1}} \\binom{n - d}{d_{2}}</span></div>

    <p class="text-gray-300">where <span class="math">m_{1} = \\max(d - r, 0)</span>, <span class="math">m_{2} = \\min(r, d)</span> and <span class="math">m_{3} = \\min(r - d_{1}, r + d_{1} - d, n - d)</span>.</p>

    <p class="text-gray-300">22</p>`;
---

<BaseLayout title="Improved Top-Down Techniques in Differential Cryptanalysis (2015/268)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2015 &middot; eprint 2015/268
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <p class="mt-4 text-xs text-gray-500">
        All content below belongs to the original authors. This page
        reproduces the paper for educational purposes. Always
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >cite the original</a>.
      </p>
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <Fragment set:html={CONTENT} />

  </article>
</BaseLayout>
