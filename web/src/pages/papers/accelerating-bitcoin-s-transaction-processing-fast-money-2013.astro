---
import BaseLayout from '../../layouts/BaseLayout.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2013/881';
const CRAWLER = 'mistral';
const CONVERTED_DATE = '2026-02-16';
const TITLE_HTML = 'Accelerating Bitcoin&#x27;s Transaction Processing. Fast Money Grows on Trees, Not Chains';
const AUTHORS_HTML = 'Yonatan Sompolinsky, Aviv Zohar';

const CONTENT = `    <p class="text-gray-300">Yonatan Sompolinsky [ School of Engineering and Computer Science, The Hebrew University of Jerusalem, Israel ] Aviv Zohar [ School of Engineering and Computer Science, The Hebrew University of Jerusalem, Israel Microsoft Research, Herzliya, Israel yoni_sompo@cs.huji.ac.il, avivz@cs.huji.ac.il ]</p>

    <h6 id="sec-2" class="text-base font-medium mt-4">Abstract</h6>

    <p class="text-gray-300">Bitcoin is a disruptive new crypto-currency based on a decentralized open-source protocol which has been gradually gaining momentum. Perhaps the most important question that will affect Bitcoin’s success, is whether or not it will be able to scale to support the high volume of transactions required from a global currency system. We investigate the implications of having a higher transaction throughput on Bitcoin’s security against double-spend attacks. We show that at high throughput, substantially weaker attackers are able to reverse payments they have made, even well after they were considered accepted by recipients. We address this security concern through the GHOST rule, a modification to the way Bitcoin nodes construct and re-organize the block chain, Bitcoin’s core distributed data-structure. GHOST has been adopted and a variant of it has been implemented as part of the Ethereum project, a second generation distributed applications platform.</p>

    <h2 id="sec-3" class="text-2xl font-bold">1 Introduction</h2>

    <p class="text-gray-300">Bitcoin is a disruptive protocol for distributed digital currency, which relies on cryptographic elements to secure its operation. Since its initial launch in 2009 by its mysterious creator Satoshi Nakamoto, general interest in the currency has been slowly increasing, and its uses have been slowly expanding.</p>

    <p class="text-gray-300">While several obstacles such as regulatory uncertainty and an under-developed infrastructure still need to be overcome, the main challenges that must be faced from a computer science perspective are related to Bitcoin’s ability to scale to higher transaction rates and to its ability to quickly process individual transactions. This paper aims to address both of these issues and the connections between them and Bitcoin’s security against double-spend attacks.</p>

    <p class="text-gray-300">The core idea behind the Bitcoin protocol is to replace the centralized control of money transmission ordinarily taken up by large organizations such as banks, credit card companies, and other money transmitters, by a large peer-to-peer network. The nodes of this network verify each other’s work and thus ensure that no single entity is able to misbehave. Bitcoin achieves this by maintaining</p>

    <p class="text-gray-300">a complete and public record of all its transactions at each node in the network. This ledger, which is known as the block chain, is composed of a growing sequence of blocks, each containing a set of approved transactions. The main challenge that Bitcoin overcomes is the synchronization of the ledger between the various nodes. Malicious parties may further try to interfere with this synchronization in order to double-spend—to redirect previously processed payments that will allow them to use the same money twice.</p>

    <p class="text-gray-300">To help solve the double-spend problem blocks are required to contain a proof-of-work, which is computationally difficult to generate. The difficulty of this task is adaptively set so that a block is created approximately once every 10 minutes in the entire network. Once created, blocks are propagated through the network. The 10 minute interval allows blocks to (usually) propagate to the vast majority of nodes before another block is created. If a node receives two conflicting blocks, which were created by distant nodes unaware of each other’s work (or perhaps by a malicious attacker), it resolves the conflict by picking the block pertaining to the longest block chain and adopting it. Satoshi Nakamoto’s original analysis of the protocol <em>[12]</em> shows that as long as any attacker holds less than 50% of the computational power in the network, the probability that double-spend attacks succeed decreases exponentially with time, which essentially allows payments to be considered accepted and irreversible after some period. The analysis, however, assumes that blocks are sent across the network much faster than they are created, and so it is ill-fitted to a scenario in which many transactions are processed by the network (which necessitates the frequent creation of larger blocks, taking longer to transmit).</p>

    <p class="text-gray-300">Indeed, capacity for additional transaction processing in Bitcoin is very much needed. As of December 2014, Bitcoin’s network processes around 90 thousand transactions per day <em>[2]</em>, a number which has been slowly growing, but still amounts to an average of roughly 1 transaction per second (TPS). In contrast, Visa’s global payment system handled a reported 150 million transactions per day in 2010 (just under 2000 TPS), and has grown steadily since. If Bitcoin is not able to scale to appropriate rates that match demand, transaction fees will rise, and users will be driven to use other forms of payment.</p>

    <p class="text-gray-300">Bitcoin’s current low number of transactions is mainly due to its small user-base. Once adoption increases, the system will need to scale to process transactions at a higher rate, and previous security guarantees may no longer hold. We investigate how susceptible the protocol is to double-spend attacks when more transactions are processed per second. We note that larger block sizes or more frequent block creation events (which are required in order to increase the transaction throughput) result in more conflicts between blocks, which severely reduces the level of security from attacks.</p>

    <p class="text-gray-300">To mitigate this, some methods for block compression were suggested by members in the Bitcoin community, e.g., transmitting only transaction hashes in blocks (an almost 16-fold reduction in size), or applying invertible Bloom lookup tables to communicate the differences between the subsets of transactions nodes are aware of <em>[3]</em>. Another approach is to use trustless off-chain transaction chan</p>

    <p class="text-gray-300">nels that slowly release money in minute portions to another party by updating a transaction that is only committed to the block chain once a reasonable sum of money has been transferred <em>[1]</em>. This approach has some downsides: money must be locked and is unusable for the duration of the channel’s existence, it only allows the aggregation of transactions between two parties that maintain a channel, and finally, it is not always useful for other protocols built on top of block chains (such as Ethereum) where individual updates cannot be aggregated in a similar fashion.</p>

    <p class="text-gray-300">We suggest an alternative to the longest-chain rule called GHOST, that changes the conflict-resolution procedure for the block chain. GHOST selects at each fork in the chain the heaviest subtree rooted at the fork. This protocol modification alleviates the above-mentioned security problem, and will help block-chain-based protocols grow further. A variant of GHOST has been adopted and implemented by the Ethereum project <em>[4]</em>, a second generation distributed applications platform that has recently received a great deal of attention. To best utilize the capacity of the block chain all solutions should ideally be combined. Our own improvement, GHOST, can be seen as a modification which allows an increase in the protocol’s block chain commitments, which in turn, will allow more transactions to take place at lower costs.</p>

    <p class="text-gray-300">A second aspect of our work involves the time until the transaction is authorized. As blocks are currently created on average once every 10 minutes, a given transaction is only included in the chain after a relatively long amount of time. Several alternative currencies that have forked the Bitcoin source-code have modified this parameter and have set lower block creation rates (e.g., once every 12 seconds in the case of FastCoin). We explore and quantify the security implications of such choices, from lower resilience to attacks to the required waiting time for a transaction to be considered accepted.</p>

    <p class="text-gray-300">It is important to note that in addition to the decreased difficulty of a double-spend attack, several other issues appear at high transaction rates: First, miners that are better connected to the network enjoy rewards slightly larger than their share of the hashing power, and second, the selfish mining strategy explored by Eyal and Sirer <em>[8]</em> can be employed by weaker miners. Both of these issues remain unsolved by the GHOST protocol alone. In a companion paper <em>[10]</em> we explore an additional modification (compatible with GHOST) that lowers the advantage of highly-connected miners, and provides an additional increase in throughput.</p>

    <h2 id="sec-4" class="text-2xl font-bold">2 Basics of the Bitcoin Protocol</h2>

    <p class="text-gray-300">The Block Chain. Bitcoin uses a public ledger to record the entire transaction history, which essentially consists of a sequence of blocks, the block chain. New blocks are created from time to time and are added successively to the ledger. Each block contains the transactions that have occurred since the last block and a cryptographic hash of the previous block in the sequence, which identifies</p>

    <p class="text-gray-300">the predecessor uniquely. A transaction is considered confirmed only once it is contained in some block which appears in this public log.</p>

    <p class="text-gray-300">The creation rate of blocks is set by requiring each block to contain a proof-of-work in its header, in the form of a solution to a computationally difficult problem (finding partial SHA-256 hash collisions). The problem depends on the most recent block, and is solved by randomly trying different inputs, thus ensuring some (random) time lag between successful block creation events. The reader is referred to <em>[12]</em> for a full explanation of the proof-of-work mechanism.</p>

    <p class="text-gray-300">As the block chain, which represents the state of all “accounts”, is kept locally at each node, it is imperative that any update to the state of accounts will be propagated to the entire network. Nodes which receive a transaction verify its validity, and send it, in turn, to all their neighbors. Similarly, nodes which receive a new block check its validity (i.e., its compatibility with all preceding blocks) and transmit it to their neighbors.</p>

    <p class="text-gray-300">The Formation and Resolution of Forks. Successive blocks are not necessarily built atop one another, and thus they form a block tree rather than a single chain (Fig. 3 illustrates such a scenario). One reason for the existence of forks is the delay in the network: it is possible for two blocks to be created at (about) the same time by far-away nodes in the network, in which case neither will point at the other as its parent, and a fork occurs.</p>

    <p class="text-gray-300">When faced with several (internally consistent) block chains each node in the network is required to adopt only one as the valid account of transactions, the “main chain”. Bitcoin’s rule is simple: pick the longest chain (or in case of ties, keep the one you received first). An important property of the longest-chain selection rule is that as time passes, all the nodes in the network will adopt the same main chain. Indeed, in order for a fork in the block tree to last, two fractions of the network need to successively create new blocks at about the same times, a series of events which becomes rarer as time develops.</p>

    <p class="text-gray-300">In addition to delays, forks can also occur due to a malicious deviation of a node from the protocol. An attacker may choose to extend any arbitrary block, and generate forks. The protocol cannot and does not deal with these forks differently than with delay-induced ones; if the attacker manages to present a longer chain of blocks, this chain will be accepted by other nodes in the network, and the previous main chain will be abandoned.</p>

    <p class="text-gray-300">Double-Spend Attacks. This method of overriding the main chain can be used by an attacker to reverse transactions, a scheme called a “double-spend attack”. The attacker may pay some merchant and then secretly create a chain of blocks without this payment that is longer than the network’s. By releasing his chain he can trigger the replacement in the ledger which effectively erases</p>

    <p class="text-gray-300">the transaction, or redirects the payment elsewhere (such an attack is illustrated in Fig. 3).</p>

    <p class="text-gray-300">The computational effort required to create each block makes this attack a difficult undertaking, since the honest nodes usually have a great deal of computational power, and the attacker must get very lucky if he is to replace long chains.</p>

    <p class="text-gray-300">However, if an attacker holds enough computational power he is able to generate blocks fast enough to bypass the main chain and override it, according to the longest-chain selection rule. This enables him to reverse any transaction that appears in the main chain at will. Specifically, if the attacker has more computational power than the rest of the network combined, he is able to generate blocks at a higher rate than the honest nodes and eventually to replace chains of arbitrary length. This stronger form of attack is known in Bitcoin jargon as "the 50% attack".[5]</p>

    <p class="text-gray-300">We model the Bitcoin network as a directed graph <span class="math">G = (V, E)</span>. Each node <span class="math">v</span> has some fraction <span class="math">p_v \\geq 0</span> of the computational power of the entire network: <span class="math">\\sum_{v \\in V} p_v = 1</span>. Each individual node <span class="math">v</span> in the network generates blocks according to a Poisson process with a rate of <span class="math">p_v \\cdot \\lambda</span>, so that the entire network combined generates blocks at a Poisson process with rate <span class="math">\\lambda</span> (the protocol's current value, <span class="math">\\lambda = \\frac{1}{600}</span>, was chosen by Satoshi at Bitcoin's inception). We assume that each edge <span class="math">e \\in E</span> has a delay <span class="math">d_e</span> associated with it, which is simply the time it takes to send a block across it.</p>

    <p class="text-gray-300">In the context of a network under attack, we will use <span class="math">\\lambda = \\lambda_h</span> as the honest network's block creation rate. The attacker's rate is denoted relative to the honest network by <span class="math">q \\cdot \\lambda_h &amp;gt; 0</span>, for some <span class="math">0 &amp;lt; q &amp;lt; 1</span>. In contrast to the honest network, we assume that the attacker is creating long chains efficiently: its blocks are always built on top of one another. See Appendix A for a more detailed consideration of the relation between the attacker and the network.</p>

    <p class="text-gray-300">For every block <span class="math">B</span>, we denote by <span class="math">time(B)</span> its (absolute) creation time. The blocks essentially form a time-developing tree structure that is rooted at the genesis block – the first block created at the moment of Bitcoin's inception; we denote the structure of this tree at time <span class="math">t</span> by <span class="math">tree(t)</span>, and by <span class="math">subtree(B)</span> the subtree rooted at <span class="math">B</span>. Finally, the depth of block <span class="math">B</span> in the tree will be denoted <span class="math">depth(B)</span>.</p>

    <p class="text-gray-300">[5] The 50% attack owes its name to Satoshi’s result showing that the main chain is secure (after sufficient waiting periods) as long as the attacker holds less than 50% of the computational power. We show in this paper that in fact networks with delays are more vulnerable and can be attacked with less computational power.</p>

    <p class="text-gray-300">[6] This essentially assumes that all computational assets held by the attacker are centralized and that blocks that it creates are transmitted instantly in its internal network.</p>

    <p class="text-gray-300">The structure of the block tree is affected by the blocks that nodes point to as their parent, and extend. Formally, we model this choice as a function <span class="math">s(\\cdot)</span> which maps a block tree <span class="math">T=(V_{T},E_{T})</span> to a block <span class="math">B\\in V_{T}</span> that is to be the parent of the next block. Every node may posses a different view of the tree (it may not have heard of all created blocks) and thus applies <span class="math">s</span> to its currently known tree.</p>

    <p class="text-gray-300">The Bitcoin protocol currently requires nodes to build new blocks at the end of the longest chain that is known to them. Accordingly, we denote by <span class="math">longest(t)</span> the deepest leaf in <span class="math">tree(t)</span>. Unless explicitly stated otherwise, we assume nodes follow this rule.</p>

    <p class="text-gray-300">The term “main chain” will correspond to the path from the genesis block to the leaf that is selected for extension (usually <span class="math">longest(t)</span>). The main chain is considered by nodes to be the single accepted version of transaction history. Its growth rate is therefore one of the core measures of the system’s performance. Formally, the time it takes the main chain to advance from length <span class="math">n-1</span> to <span class="math">n</span> is a random variable that we denote as <span class="math">\\tau_{n}</span>. We denote <span class="math">\\tau=\\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{i=1}^{n}\\tau_{n}</span>, and <span class="math">\\beta=\\frac{1}{E[\\tau]}</span>. <span class="math">\\beta</span> is the rate of block addition to the main chain, while <span class="math">\\lambda</span> is the rate of block addition to the block tree.</p>

    <p class="text-gray-300">Another parameter embedded in the protocol is the maximal block size (in KB), denoted by <span class="math">b</span>. We assume throughout the paper that there is high demand for transaction processing and that blocks are always full to the limit.</p>

    <p class="text-gray-300">Finally, we define the primary measure of Bitcoin’s scalability as the number of transactions per second (TPS) the system adds to the history (the main chain), in expectation. We denote by <span class="math">K</span> the average number of transactions per KB. The TPS is then: <span class="math">TPS(\\lambda,b):=\\beta(\\lambda,b)\\cdot b\\cdot K</span>.</p>

    <h2 id="sec-6" class="text-2xl font-bold">4 Reduced Security at High Throughput</h2>

    <p class="text-gray-300">In this section we explain why the Bitcoin protocol becomes more susceptible to double-spend attacks when its throughput is increased. Assume an attacker creates blocks at a rate of <span class="math">q\\cdot\\lambda_{h}</span>. If <span class="math">q\\cdot\\lambda_{h}</span> is greater than the growth rate of the network’s main chain, <span class="math">\\beta</span>, the attack will always be successful (given enough time), regardless of the current length of the chain it aims to bypass and replace (by The Law of Large Numbers). Conversely, if <span class="math">q&lt;\\frac{\\beta}{\\lambda_{h}}</span>, the probability of the attacker’s chain bypassing the main chain decreases exponentially as the main chain grows in length (See Theorem 10 for the formal proof). We therefore think of the ratio <span class="math">\\frac{\\beta}{\\lambda_{h}}</span> as the “security threshold” of the system.</p>

    <p class="text-gray-300">The throughput of the protocol is affected by the two elementary parameters: the block creation rate <span class="math">\\lambda</span>, and the block size <span class="math">b</span>. The difficulty of the computational problem which is required to create a valid block can be lowered in order to accelerate the block creation process. Similarly, larger blocks can be allowed to propagate if one wishes to increase the block size. A naïve attempt at increasing the throughput can be made by simply increasing both parameters. We argue</p>

    <p class="text-gray-300">that both of these modifications lead to an increased number of forks in the block tree, which in turn leads to a reduction of the security threshold of the system. In other words, attackers can perform effective attacks with less computational power once the throughput is increased. The qualitative tradeoffs between these parameters are depicted in Fig. 2.</p>

    <p class="text-gray-300">!<a href="img-0.jpeg">img-0.jpeg</a> Fig. 1. The relation between the block size and the time it took to reach  <span class="math">25\\%</span>  (red),  <span class="math">50\\%</span>  (green), and  <span class="math">75\\%</span>  (blue) of monitored nodes, based on data provided by Decker and Wattenhofer [7].</p>

    <p class="text-gray-300">!<a href="img-1.jpeg">img-1.jpeg</a> Fig.2. A general view of tradeoffs in the Bitcoin protocol. Increasing the block size or the block rate causes an increase in the TPS, but also decreases the security from double-spend attacks.</p>

    <p class="text-gray-300">Larger Blocks. Indeed, while a node has not yet learned of the latest addition to the main chain, any block that it creates will not add to that chain, but rather contribute to a less updated alternative branch. Thus as the block size is increased, blocks naturally take longer to propagate through the network, hence more forks occur. This observation is well supported by a measurement study conducted by Decker and Wattenhofer [7] who have measured block propagation delays in the Bitcoin network. Figure 1, which is based on raw data that they have generously shared with us, depicts a clear linear relation between the block size and its propagation time.</p>

    <p class="text-gray-300">Accelerated Block Creation. Similarly, if block creation is accelerated, more blocks are being created by the honest network (larger  <span class="math">\\lambda_h</span> ) while the most recent block in the main chain is propagated. Again, these blocks will often be created by nodes that are not fully up to date and will not extend the longest chain. The attacker on the other hand, also creates blocks faster (at a rate of  <span class="math">q \\cdot \\lambda_h</span> ), but does not suffer from a loss of efficiency.</p>

    <p class="text-gray-300">Reduced Security. In both cases described above, blocks that are created do not always contribute to the lengthening of the main chain, which makes it easier for an attacker to replace it.</p>

    <p class="text-gray-300">Figure 3 illustrates a scenario in which a highly forked block tree was created by the honest network. The attacker secretly creates a chain of 6 blocks (denoted 1A, 2A, ..., 6A) which is clearly longer than the network's longest chain (ending in block 5B). If block propagation was faster (in relation to the creation rate), all blocks in the honest network's tree would form a single long chain and would not be overtaken by the attacker.</p>

    <p class="text-gray-300">!<a href="img-2.jpeg">img-2.jpeg</a> Fig.3. A block tree in which the longest chain and the chain selected by GHOST differ. An attacker's chain is able to switch the longest chain, but not the one selected by GHOST.</p>

    <p class="text-gray-300">In this section we present our main contribution to the protocol: a new policy for the selection of the main chain in the block tree. The advantage of this suggested change to the protocol is that it maintains the security threshold for successful  <span class="math">50\\%</span>  attacks at 1 (rather than  <span class="math">\\frac{\\beta}{\\lambda_n}</span> ), even if the network suffers from extreme delays and the attacker does not. This allows the protocol designer to set high block creation rates and large block sizes without the fear of approaching the  <span class="math">50\\%</span>  -attack cliff edge, which in turn implies that a high transaction throughput can be securely maintained.</p>

    <p class="text-gray-300">The basic observation behind the protocol modification that we suggest, is that blocks that are off the main chain can still contribute to its weight. Consider, for example, the block tree in Fig. 3. Block 1B is supported by blocks 2B, 2C, and 2D that extend it directly, and include it in their chain. Similarly, blocks 3C, 3D, and 3E support both 1B and 2C as part of their chain. The heaviest subtree protocol we suggest makes use of this fact, and adds additional weight to blocks, helping to ensure that they will be part of the main chain.</p>

    <p class="text-gray-300">Recall our definition from Sect. 3; any node chooses the parent of its next block according to a policy  <span class="math">s(T)</span> , that maps a tree  <span class="math">T</span>  to a block in  <span class="math">T</span>  which essentially represents the main chain. Formally, our new protocol is a new parent-</p>

    <p class="text-gray-300">selection policy. This new policy redefines the main chain, which is what should be regarded as the valid branch of transaction history.</p>

    <p class="text-gray-300">For a block <span class="math">B</span> in a block tree <span class="math">T</span>, let <span class="math">subtree(B)</span> be the subtree rooted at <span class="math">B</span>, and let <span class="math">Children_{T}(B)</span> be the set of blocks directly referencing <span class="math">B</span> as their parent. Denote by <span class="math">GHOST(T)</span> the parent-selection policy we propose, defined as the output of the following algorithm.</p>

    <h6 id="sec-8" class="text-base font-medium mt-4">Algorithm 1</h6>

    <p class="text-gray-300">Greedy Heaviest-Observed Sub-Tree (<span class="math">GHOST</span>) Input: Block tree <span class="math">T</span></p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>set <span class="math">B\\leftarrow</span> Genesis Block</li>

      <li>if <span class="math">Children_{T}(B)=\\emptyset</span> then return(B) and exit</li>

    </ol>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">3. else update $B\\leftarrow\\underset{C\\in Children_{T}(B)}{\\operatorname{argmax}}\\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">subtree_{T}(C)\\right</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">^{8}$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>goto line 2</li>

    </ol>

    <p class="text-gray-300">The algorithm follows a path from the root of the tree (the genesis block) and chooses at each fork the block leading to the heaviest subtree. In the tree depicted in Fig. 3, for instance, the subtree of block 1B contains 12 blocks, whereas that of 1A contains only 6. The algorithm will thus pick 1B as belonging to the main chain, and proceed to resolve the forks inside <span class="math">subtree</span>(1B). This will result the choice of blocks 0, 1B, 2C, 3D, 4B as the main chain of the tree (and not the longest chain, ending in block 5B). This makes forks inside the subtree rooted at 1B of no consequence to the weight of block 1B itself — every addition of a block to <span class="math">subtree</span>(1B) makes it harder to omit it from the main chain. In particular, when the attacker publishes its 6-blocks long secret chain, the same blocks as before remain in the main chain.</p>

    <h3 id="sec-9" class="text-xl font-semibold mt-8">5.1 Basic Properties of GHOST</h3>

    <p class="text-gray-300">It is imperative to first show that all nodes eventually adopt the same history when following GHOST. For every block <span class="math">B</span> define by <span class="math">\\psi_{B}</span> the earliest moment at which it was either abandoned by all nodes, or adopted by them all. We call the adoption of a block by all nodes the collapse of the fork.</p>

    <h6 id="sec-10" class="text-base font-medium mt-4">Proposition 2 (The Convergence of History)</h6>

    <p class="text-gray-300"><span class="math">Pr(\\psi_{B}&lt;\\infty)=1</span>. In other words, every block is eventually either fully abandoned or fully adopted. Moreover, <span class="math">E[\\psi_{B}]&lt;\\infty</span>.</p>

    <h6 id="sec-11" class="text-base font-medium mt-4">Proof</h6>

    <p class="text-gray-300">Let <span class="math">D</span> be the delay diameter of the network. Assume that at time <span class="math">t&gt;time(B)</span> block B is neither adopted by all nodes nor abandoned by all of them. Denote by <span class="math">\\mathcal{E}_{t}</span> the event in which the next block creation in the system occurs between times <span class="math">t+D</span> and <span class="math">t+2D</span>, and then no other block is produced until time <span class="math">t+3D</span>. We argue that once such an event occurs, block B is either adopted or abandoned by all nodes. Indeed, between time <span class="math">t</span> and <span class="math">t+D</span> all nodes learn of all</p>

    <p class="text-gray-300">existing blocks (as no new ones are manufactured), and therefore each pair of leaves (of the block tree) that have nodes actively trying to extend them must have equal weight subtrees rooted at some common ancestor. A single block is then created which breaks these ties, and another <span class="math">D</span> time units allow it to propagate to all nodes, which causes them to switch to a single shared history. Notice that <span class="math">Pr(\\mathcal{E}_{t})</span> is uniformly (in <span class="math">t</span>) lower bounded by a positive number, as it doesn’t depend on <span class="math">t</span> (as the exponential distribution is memoryless). Hence the expected waiting time for the first <span class="math">\\mathcal{E}_{t}</span> event is finite (see “Awaiting the almost inevitable” in <em>[19]</em>, Chapter 10.11). Finally, the stopping time <span class="math">\\psi_{B}</span> is upper bounded, by definition, by the waiting time for the first <span class="math">\\mathcal{E}_{t}</span>, implying <span class="math">E[\\psi_{B}]&lt;\\infty</span>.</p>

    <p class="text-gray-300"><span class="math">\\Box</span></p>

    <p class="text-gray-300">We now show the main advantage of the GHOST chain selection rule, namely, that it is resilient to 50% attacks, even at high rates or with significant delays in the network: By waiting a sufficiently long period of time <span class="math">\\tau</span> after the block’s creation, the probability that its status will change from “accepted” to “abandoned” can be made arbitrarily small.</p>

    <h6 id="sec-12" class="text-base font-medium mt-4">Proposition 3 (Resilience to 50% Attacks)</h6>

    <p class="text-gray-300">Assume the attacker’s block creation rate is <span class="math">q\\cdot\\lambda_{h}</span>, and <span class="math">0\\leq q&lt;1</span>. The probability that a block <span class="math">B</span> will be off the main chain sometime after <span class="math">time(B)+\\tau</span>, given that it was in the main chain at <span class="math">time(B)+\\tau</span>, goes to zero as <span class="math">\\tau</span> goes to infinity.</p>

    <p class="text-gray-300">Contrast the statement above with the security threshold introduced in Sect. 4, where <span class="math">q&lt;\\frac{3}{\\lambda_{h}}</span> was required to guarantee resilience against 50% attacks. This proposition suggests that in any network following the GHOST rule, the security threshold is 1.</p>

    <h6 id="sec-13" class="text-base font-medium mt-4">Proof (of Proposition 3)</h6>

    <p class="text-gray-300">The event in which <span class="math">B</span> is eventually discarded from the main chain is contained in the event that a collapse has yet to occur (i.e., <span class="math">\\psi_{B}\\geq time(B)+\\tau</span>). Relying again on the finiteness of <span class="math">E[\\psi_{B}]</span> (Proposition 2), and applying Markov’s inequality, it follows that the probability that by <span class="math">time(B)+\\tau</span>, <span class="math">B</span> was either already abandoned or already adopted by all (honest) nodes goes to 1, as <span class="math">\\tau</span> goes to infinity. In the former case, the proposition holds trivially. In the latter case, blocks are now built in <span class="math">B</span>’s subtree at the rate of <span class="math">\\lambda_{h}</span>, which is higher than <span class="math">q\\lambda_{h}</span>. Thus, as <span class="math">\\tau</span> grows, the gap between the size of <span class="math">subtree(B)</span> and the attacker’s chain grows, making the probability of the attack succeeding sometime in the future arbitrarily low (The Law of Large Numbers). <span class="math">\\Box</span></p>

    <p class="text-gray-300">The Rate of Collapse in GHOST. In Subsection 5.1 we have discussed the collapse time <span class="math">\\psi_{B}</span> for any block <span class="math">B</span> and its implications to the growth and convergence of the main chain in GHOST. Long living forks imply longer waiting times until the entire network contributes confirmations to a block, and further implies long waiting times for transaction authorization. It can prove useful to further investigate how fast the collapse at <span class="math">B</span> occurs. We do this for a simple model including only two forks, each with equal contributing computational power. Even this seemingly simple case proves to be non-trivial.</p>

    <h6 id="sec-14" class="text-base font-medium mt-4">Theorem 4.</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Consider a network with two nodes, <span class="math">u</span> and <span class="math">v</span>, that equally create blocks at a rate of <span class="math">\\lambda/2</span>, which are connected by a single link with delay <span class="math">d</span>. For any block <span class="math">B</span>, <span class="math">E[n_{B}]\\leq\\frac{(d\\lambda)^{2}}{8}+\\frac{d\\lambda}{2}</span>, where $n_{B}:=</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">subtree_{T}(B)</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> for </span>T=tree(\\psi_{B})$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">The theorem gives an upper bound for the special configuration of two nodes; we conjecture, however, that it is the worst case, and that in general setups collapses occur even faster. Its proof appears in Appendix B.</p>

    <h2 id="sec-15" class="text-2xl font-bold">6 Main Chain Growth in GHOST and in Longest-Chain</h2>

    <p class="text-gray-300">In this section we begin to systematically compare the two chain selection rules. Central to this comparison is an analysis of the growth rate of the main chain (<span class="math">\\beta</span>) under each one. Since this growth rate is highly dependent on the exact topology of the network which is both unknown and extremely difficult to measure, we take a dual approach: First we bound the rates analytically from above and below. Second, we simulate networks with randomly sampled overlay topologies and measure the resulting block-trees. We then go on to discuss the implications of these results in terms of security, throughput, and resource use of each rule.</p>

    <h3 id="sec-16" class="text-xl font-semibold mt-8">6.1 A Lower Bound</h3>

    <p class="text-gray-300">We begin our analysis with the following approach: suppose that a cluster of relatively well connected nodes (with delay diameter <span class="math">D</span>) contains a fraction <span class="math">0\\leq\\alpha\\leq 1</span> of the computational power of the entire network. In this case, blocks created within this sub-network propagate internally relatively quickly, and we can bound the rate of growth of the main chain from below. The bounds are tight, both for longest-chain and for GHOST, and thus form a good basis for comparison.</p>

    <h6 id="sec-17" class="text-base font-medium mt-4">Lemma 5 (Longest-Chain & Bounded Delay).</h6>

    <p class="text-gray-300">Let G=(V,E) be a network graph (a sub-graph of the entire network) which generates blocks at a rate <span class="math">\\lambda^{\\prime}=\\alpha\\cdot\\lambda</span> with delay diameter <span class="math">D</span>. Then under the longest-chain rule, the rate at which the longest chain grows <span class="math">\\beta(\\lambda)\\geq\\frac{\\lambda^{\\prime}}{1+\\lambda^{\\prime}\\cdot D}</span>.</p>

    <h6 id="sec-18" class="text-base font-medium mt-4">Lemma 6 (GHOST & Bounded Delay).</h6>

    <p class="text-gray-300">Let G=(V,E) be a network graph (a sub-graph of the entire network) which generates blocks at a rate <span class="math">\\lambda^{\\prime}=\\alpha\\cdot\\lambda</span> with delay diameter <span class="math">D</span>. Then under the GHOST rule, the rate at which the longest chain grows <span class="math">\\beta(\\lambda)\\geq\\frac{\\lambda^{\\prime}}{1+2\\lambda^{\\prime}\\cdot D}</span>.</p>

    <p class="text-gray-300">Both Lemma 5 and Lemma 6 can be shown to be tight. The bound is achieved in a complete graph with <span class="math">n</span> nodes, <span class="math">n\\to\\infty</span>, where the delay on all edges is exactly <span class="math">D</span>, and each node has <span class="math">1/n</span>’th of the computational power. This lower bound can thus be thought of as approximating the ideal decentralized network, where the computational power is well distributed among many equidistant nodes.</p>

    <p class="text-gray-300">Lemma 5 follows, intuitively, from the fact that after some block <span class="math">U</span> at depth <span class="math">n</span> was created and sent to all nodes (<span class="math">D</span> seconds), it takes in expectation <span class="math">\\frac{1}{\\lambda}</span> seconds</p>

    <p class="text-gray-300">for the next block <span class="math">U^{\\prime}</span> to be created. As the creator of <span class="math">U^{\\prime}</span> was certainly aware of the creation of <span class="math">U</span>, its depth must be at least <span class="math">n + 1</span>. The rate is thus lower bounded by <span class="math">\\frac{1}{D + \\frac{1}{\\lambda^{\\prime}}} = \\frac{\\lambda^{\\prime}}{1 + \\lambda^{\\prime} \\cdot D}</span>. Refer to Appendix C for a formal proof.</p>

    <p class="text-gray-300">As GHOST does not select the longest chain, it can be expected that the rate of growth of its main chain will be somewhat lower than in the longest-chain rule. This is indeed the case. The loss in growth rate, however, is relatively minor, and unlike in the longest-chain rule, has no bearing on the security of GHOST. Lemma 6 follows as an immediate consequence of the following claim, which is proven in Appendix D.</p>

    <p class="text-gray-300"><strong>Claim 7.</strong> Let <span class="math">B</span> be a block in tree <span class="math">T</span> in a network as in Lemma 6, then regardless of history, the expected waiting time for the creation of the last child of <span class="math">B</span> is upper bounded by <span class="math">2D + \\frac{1}{\\lambda&#x27;}</span>.</p>

    <p class="text-gray-300"><strong>Application to Throughput (Under Longest-Chain)</strong> What recommendations should we give the designer of the system who wishes to set the protocol's parameters, given that the network's topology is unknown? We now show how some rather limited knowledge of the network's topology could be used by the designer to guarantee a certain measure of security.</p>

    <p class="text-gray-300">Assume we have managed to measure the delay diameter of some fraction of the network, namely, the maximal time <span class="math">D(b)</span> it takes a block of size <span class="math">b</span> to arrive at some fraction <span class="math">\\alpha</span> of the network. Following the results depicted in Fig. 1, we adopt a linear model of the delays; we thus assume that <span class="math">D(b)</span> is of the form <span class="math">D(b) = D_{prop} + D_{bw} \\cdot b</span>. Notice that <span class="math">D_{prop}</span> is a measure of aggregate propagation delay, and <span class="math">D_{bw}</span> is an aggregate measure of bandwidth in units of seconds per KB.</p>

    <p class="text-gray-300"><strong>Lemma 8.</strong> Assume there exists a sub-network with a block creation rate of <span class="math">\\alpha \\lambda</span> and delay diameter <span class="math">D(b)</span>, in a network following the longest-chain rule. Then for any <span class="math">x \\in \\left(0, \\frac{K}{D_{bw}}\\right)</span>, the protocol is able to achieve both a throughput of at least <span class="math">x</span> TPS and a security threshold of at least <span class="math">\\alpha \\cdot \\left(1 - \\frac{x \\cdot D_{bw}}{K}\\right)</span>, through a right choice of the parameters <span class="math">b</span> and <span class="math">\\lambda</span>.</p>

    <p class="text-gray-300"><strong>Proof.</strong> By Lemma 5, the main chain grows at a rate of at least <span class="math">\\frac{1}{\\frac{1}{\\alpha\\lambda} + D(b)}</span>. By the definition of the throughput, <span class="math">TPS = b \\cdot K \\cdot \\beta \\geq \\frac{K}{\\frac{1}{\\alpha\\lambda} + D_{prop} + D_{bw}}</span>. For any <span class="math">x \\in \\left(0, \\frac{K}{D_{bw}}\\right)</span>, there exists a large enough <span class="math">b = b_x</span> such that the RHS equals <span class="math">x</span> (fixing <span class="math">\\lambda</span>), thereby guaranteeing <span class="math">TPS \\geq x</span>. The lower bound on <span class="math">\\beta</span> then implies:</p>

    <div class="my-4 text-center"><span class="math-block">\\frac{\\beta}{\\alpha \\lambda} \\geq \\frac{1}{1 + \\alpha \\lambda (D_{prop} + b_x \\cdot D_{bw})} = 1 - \\frac{1}{\\frac{1}{\\alpha \\lambda \\cdot b_x \\cdot D_{bw}} + \\frac{D_{prop}}{b_x \\cdot D_{bw}} + 1} = 1 - \\frac{x \\cdot D_{bw}}{K}.</span></div>

    <p class="text-gray-300">□</p>

    <p class="text-gray-300">Any evaluation of the real Bitcoin network's behavior under higher throughput requires full knowledge of the topology of the network. Unfortunately, the</p>

    <p class="text-gray-300">structure is both unknown (partly because it is hard to measure, but also because miners attempt to keep their connections secret) and keeps shifting as nodes connect and disconnect. To obtain an order of magnitude estimation we apply Decker and Watenhoffer’s measurements of Bitcoin’s network to the bound from Lemma 8.</p>

    <p class="text-gray-300">The best linear fit to the results, for <span class="math">\\alpha=0.5</span>, yields a slope of <span class="math">D_{bw}=0.066</span>. This implies, for instance, an achievable throughput of 15.15 TPS, coupled with resilience to attackers with <span class="math">q</span> up to 0.25 computational power.</p>

    <h4 id="sec-19" class="text-lg font-semibold mt-6">6.1.2 Application of the Bound to GHOST (Efficiency)</h4>

    <p class="text-gray-300">We have shown in Proposition 3 that the security threshold in a network following GHOST is always 1. While this means there is no limiting security constraint (contrary to the longest-chain case), the throughput cannot grow limitlessly: the transmission of many blocks (only a fraction of which contribute to the main chain) consumes bandwidth. Therefore, the ratio <span class="math">\\frac{\\beta}{\\lambda}</span> is still of interest, not in a security context, but rather as a measure of the network’s efficiency in its resource utilization.</p>

    <p class="text-gray-300">Following the same method as previously, one can apply the linear delays model to Lemma 6 and show that the network’s efficiency under a given throughput is at least <span class="math">\\alpha\\cdot\\big{(}1-\\frac{TPS\\cdot 2\\cdot D_{bw}}{K}\\big{)}</span>. E.g., the network is able to process 9.09 transactions per second, while maintaining an efficiency of at least 0.2.</p>

    <h3 id="sec-20" class="text-xl font-semibold mt-8">6.2 An Upper Bound</h3>

    <p class="text-gray-300">We proceed now to give upper bounds on the main chain’s growth rate. The idea of the upper bound is to locate a partition of the network graph, such that blocks take at least <span class="math">d</span> time units to cross the partition (i.e., all links crossing the cut have delay at least <span class="math">d</span>). Given such a partition the network is inherently inefficient to some degree, as the communication delay between the two parts may cause forks. The following theorem formalizes this:</p>

    <h6 id="sec-21" class="text-base font-medium mt-4">Theorem 6.1</h6>

    <p class="text-gray-300">Let G=(V,E) be the network graph. Let <span class="math">S,T\\subset V</span> be a partition of the nodes such that <span class="math">\\forall s\\in S,\\forall t\\in T</span> we have <span class="math">d_{\\{s,t\\}}\\geq d</span>, and let <span class="math">p_{S},p_{T}(p_{S}\\neq p_{T})</span> be the fraction of computational power owned by nodes in <span class="math">S,T</span> correspondingly. Then both under longest-chain and under GHOST, the main chain’s growth rate is bounded from above as follows: <span class="math">\\beta(\\lambda)\\leq\\frac{(p_{S}\\lambda)^{2}e^{p_{S}\\lambda 2d}-(p_{T}\\lambda)^{2}e^{p_{T}\\lambda 2d}}{p_{S}\\lambda e^{p_{S}\\lambda 2d}-p_{T}\\lambda e^{p_{T}\\lambda 2d}}</span>.</p>

    <p class="text-gray-300">The theorem is tight – networks consisting of only two nodes add blocks to the main chain at <em>exactly</em> this rate. We defer the rather involved proof to Appendix E.</p>

    <h3 id="sec-22" class="text-xl font-semibold mt-8">6.3 Simulation Results</h3>

    <p class="text-gray-300">We simulated the growth of the main chain in networks roughly emulating the topologies of Bitcoin’s P2P overlay network for nodes adhering either to longest-chain or to GHOST. Following a behavior similar to the default in Bitcoin’s</p>

    <p class="text-gray-300">reference client, each node initiates links to 8 uniformly selected neighbors (and accepts all links others initiated). We simulate a network with 1000 nodes, and assign computational power uniformly at random. The propagation delays on the links were sampled from a normal distribution ( <span class="math">\\mu = \\sigma = 100</span>  milliseconds). Similarly, the bandwidth of each node was drawn from a normal distribution ( <span class="math">\\mu = 1, \\sigma = 0.2</span>  MB). Both values were redrawn for negative results. The system was later allowed to evolve as blocks were propagated by nodes. Figure 5 depicts the security threshold measured in the system as a function of the block creation rate. Figure 4 illustrates the resulting TPS in both cases, and shows that the loss in efficiency of network resources caused by following the GHOST rule is indeed relatively small. See further discussion in Subsection 6.1.</p>

    <p class="text-gray-300">!<a href="img-3.jpeg">img-3.jpeg</a> Fig.4.  <span class="math">TPS(\\lambda)</span></p>

    <p class="text-gray-300">!<a href="img-4.jpeg">img-4.jpeg</a> Fig. 5. Security(λ)</p>

    <p class="text-gray-300">We have so far considered only the effect that delayed block propagation has on the  <span class="math">50\\%</span>  attack. Even attackers with a modest block creation rate can still succeed in a double-spend attack if they are lucky enough to generate many blocks in a quick burst; Satoshi, in his original paper, analyzes this threat. His analysis does not apply, however, to networks with non-negligible delay, and so we revisit this question.</p>

    <p class="text-gray-300">The Acceptance Policy in Longest-Chain. The process of transaction authorization is defined by an acceptance policy chosen by the recipient of funds. Formally, the policy can be described as a function  <span class="math">n(t,r,q)</span> , where  <span class="math">r</span>  is the risk the recipient is willing to tolerate,  <span class="math">q</span>  the upper bound on the attacker's fraction of computational power, and  <span class="math">t</span>  the time that elapsed since the transaction was broadcast to the network. If the transaction receiver observes  <span class="math">n</span>  blocks ("confirmations") atop his transaction by time  <span class="math">t</span> , he approves it only if  <span class="math">n \\geq n(t,r,q)</span> , and</p>

    <p class="text-gray-300">otherwise waits for <span class="math">n</span> to increase.⁹ The policies for the GHOST and longest-chain rules differ. Notice however, that in both cases, if <span class="math">t</span> seconds have passed since the transaction was received, the probability that the attacker has completed <span class="math">k</span> blocks is <span class="math">\\zeta_k := e^{-q\\lambda_h t} \\frac{(q\\lambda_h t)^k}{k!}</span>. Thus, given some <span class="math">n, t</span> we have a probability distribution on the initial gap between the attacker and the honest network. The following theorem bounds the probability that an attacker will close this gap.</p>

    <p class="text-gray-300"><strong>Theorem 10.</strong> Consider a network <span class="math">G</span> with delays. Let <span class="math">1 / \\beta_{1}</span> be an upper-bound on the expected waiting time for the next lengthening of the main chain, for all possible states of the system. Let <span class="math">q\\lambda_{h} &amp;lt; \\beta_{1}</span> be the creation rate of the attacker (according to a Poisson process), and suppose the gap between the network's longest chain and that of the attacker is <span class="math">X_{0}</span> blocks. Then the probability that the attacker will succeed in extending its chain to be longer than the network's is at most <span class="math">\\left(\\frac{q\\lambda_h}{\\beta_1}\\right)^{X_0 + 1}</span>.</p>

    <p class="text-gray-300">The theorem is proved in Appendix F. This result justifies the following acceptance policy:</p>

    <div class="my-4 text-center"><span class="math-block">n (t, r, q) := \\min _ {n} \\left\\{\\sum_ {k = 0} ^ {n} \\zeta_ {k} \\cdot \\left(\\frac {q \\lambda_ {h}}{\\beta_ {1}}\\right) ^ {n - k + 1} + \\sum_ {k = n + 1} ^ {\\infty} \\zeta_ {k} \\leq r \\right\\}</span></div>

    <p class="text-gray-300">The first term inside the parenthesis corresponds to the chance of the attacker closing the gap (at some future time) given that at time <span class="math">t</span> he is behind by <span class="math">n - k</span> blocks. The second term aggregates the probability that its chain is long enough at the moment of acceptance.</p>

    <p class="text-gray-300"><strong>The Acceptance Policy in GHOST.</strong> In GHOST, a block <span class="math">B</span> gains confirmations from all blocks in its subtree. Once a collapse to a single subtree occurs, further confirmations are added at a full rate of <span class="math">\\lambda_h</span>. This justifies the following policy:</p>

    <div class="my-4 text-center"><span class="math-block">n (t, r, q) := \\min _ {n} \\left\\{(1 - \\eta_ {B} ^ {t}) \\cdot \\left(\\sum_ {k = 0} ^ {n} \\zeta_ {k} \\cdot \\left(\\frac {q \\lambda_ {h}}{\\lambda_ {h}}\\right) ^ {n - k + 1} + \\sum_ {k = n + 1} ^ {\\infty} \\zeta_ {k}\\right) + \\eta_ {B} ^ {t} \\leq r \\right\\}</span></div>

    <p class="text-gray-300">where <span class="math">\\eta_B^t</span> is the probability that at time <span class="math">t</span>, block <span class="math">B</span> has yet to be included in the main chain of the entire honest network. The formulation given above includes the event of a collapse. Subject to that occurrence, block <span class="math">B</span> gains confirmations at a faster pace.</p>

    <h2 id="sec-24" class="text-2xl font-bold">8 GHOST Implementation Details</h2>

    <p class="text-gray-300">Below we outline some additional details about the use and implementation of the GHOST chain selection rule.</p>

    <p class="text-gray-300"><strong>Links to Multiple Parents.</strong> As our protocol requires knowledge of off-chain blocks by all nodes, we propose that their headers (but not necessarily their</p>

    <p class="text-gray-300">⁹ Previous work, such as [12, 16], considered simpler policies that did not take elapsed time into account.</p>

    <p class="text-gray-300">entire contents) be propagated to all nodes. Information about off-chain blocks can then be embedded inside each block by simply listing the hashes of other childless blocks it is aware of.</p>

    <p class="text-gray-300">Deployment. At low block creation rates, and with small block sizes, both GHOST and the conventional longest-chain rule behave the same: all blocks will simply be on a single long chain. Differences between the two rules appear only at high throughputs. The adoption of GHOST can therefore be gradual at low transaction rates–nodes will be partially compatible with the longest-chain version as long as transaction rates do not increase (additional references to block headers can be placed inside fields that the regular protocol currently ignores, and so backward compatibility can be maintained). This point, however, is of little importance. Increasing Bitcoin’s block size or the block creation rate will require a hard fork in the protocol. Consequently, for these changes to take place a majority of the mining power needs to accept them.</p>

    <p class="text-gray-300">Retargeting (Difficulty Adjustment). Given potentially complex relations between the growth rate of the main chain and the rate of created blocks, and the fact that GHOST depends more on the total rate of block creation, we suggest a change in the way automatic difficulty adjustments to the proof-of-work are done. Instead of targeting a certain rate of growth for the longest chain, i.e., <span class="math">\\beta</span> (which is Bitcoin’s current strategy), we suggest that the total rate of block creation be kept constant (<span class="math">\\lambda</span>), which can be done, as the information on the entire block tree is available following the links to all ancestor blocks. Notice that the relation between <span class="math">\\beta</span> and the difficulty is highly complex, and so Bitcoin’s current targeting mechanism will malfunction at high rates.</p>

    <p class="text-gray-300">Fees and Minted Coins. While GHOST does make use of off-chain blocks to secure the protocol, we believe it is best to allocate minted coins only to the creators of blocks that are on the main chain, similarly to how the longest chain rule works today. The rate of minting can be adjusted independently from the block creation rate (but in a very similar way) by adjusting the amount of minted coins per block given the measured number of blocks in the recent past (e.g., in a 2 week window). A companion paper on <em>Inclusive protocols</em> <em>[10]</em> discusses the inclusion of transactions from blocks that are off the main chain (and the allocation of related fees).</p>

    <p class="text-gray-300">Preventing Amplified Denial of Service Attacks. As each block in Bitcoin is sent to the entire network by the nodes themselves, any burst of blocks may disrupt the network. Attackers are naturally limited in their ability to create <em>recent</em> blocks due to the proof-of-work requirement, but may try to create blocks off-chain that are built upon blocks in the distant past (when the difficulty level was low). This issue is handled by the current implementation using checkpoints (points in the chain before which no additional off-chain blocks are accepted). Other mechanisms that involve probabilistic proofs of combined difficulty (for large chains that go back too far in the past) have also been suggested. Both solutions can be adapted to GHOST as well.</p>

    <p class="text-gray-300">9 Additional Related Work</p>

    <p class="text-gray-300">The original security analysis done by Satoshi <em>[12]</em> has been improved in a whitepaper published by Meni Rosenfeld <em>[16]</em>. Several papers have looked at incentive concerns related to the operation of the protocol, examining issues related to transaction propagation <em>[6]</em>, selfish mining <em>[8]</em>, and the distribution of rewards within mining-pools <em>[15]</em>. Other works on Bitcoin have looked at its privacy aspects <em>[13, 5]</em>, including analysis of its transaction graph <em>[14]</em> which allows to de-anonymize some of its users. The Zerocoin protocol has been offered as a way to improve anonymity <em>[11]</em>.</p>

    <p class="text-gray-300">Our work deals, among other issues, with enabling fast confirmations for transactions in the network. A paper by Karame et. al. discusses similar issues, that relate to possible attacks on nodes that accept zero-confirmation transactions <em>[9]</em>. They suggest several countermeasures that may help avoid such attacks. Their work does not deal with an attack by an adversary with a significant block creation rate, which can compute alternative chains on its own.</p>

    <p class="text-gray-300">A paper closely related to ours is one that was published by Decker and Wattenhofer, in which they present a measurement study of message propagation times in the Bitcoin network. They associate delays with the creation of forks in the block-tree, and with an increased vulnerability to the 50% attack <em>[7]</em>. As far as we are aware, no other work addresses the issue of Bitcoin’s scalability, or its security in a network with delayed block propagation.</p>

    <h2 id="sec-25" class="text-2xl font-bold">10 Conclusion</h2>

    <p class="text-gray-300">This paper has focused primarily on the effect network delays have on Bitcoin’s security from double-spend attacks. In this context we presented GHOST, our suggestion for the modification of the protocol, which helps secure Bitcoin when processing transactions at high rates. Regarding the current state of the protocol, we have given some theoretical security guarantees that can be applicable even if limited information is known about the network topology. Our results underscore the importance of the health of the network to Bitcoin’s security and scalability.</p>

    <p class="text-gray-300">Many additional research questions should be addressed in light of our results: How should the block creation rate and block size dynamically adjust to changing network conditions? Additionally, in Bitcoin so-called Simplified Protocol Verification nodes can operate without downloading the entire block chain. If we are to increase the number of blocks per second, their job becomes harder. It is therefore of great interest to create light nodes that can, for example, verify the block chain probabilistically, without needing to download all headers. Finally, it can be shown that in networks with delay that operate at high rates, large miners get more than their fair share of the blocks, an effect that skews rewards in favor of large miners and slowly pushes the system towards a more centralized one. One way to mitigate the problem, which can be applied to GHOST as well, is presented in a companion paper on Inclusive protocols <em>[10]</em>.</p>

    <p class="text-gray-300">##</p>

    <p class="text-gray-300">The authors were supported in part by the Israel Science Foundation (Grants 616/13, and 1773/13), and by the Israel Smart Grid (ISG) Consortium.</p>

    <h2 id="sec-26" class="text-2xl font-bold">References</h2>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[1] https://bitcoinj.github.io/working-with-micropayments</li>

      <li>[2] https://blockchain.info/charts/n-transactions</li>

      <li>[3] https://gist.github.com/gavinandresen/e20c3b5a1d4b97f79ac2</li>

      <li>[4] https://www.ethereum.org/</li>

      <li>[5] Androulaki, E., Karame, G.O., Roeschlin, M., Scherer, T., Capkun, S.: Evaluating user privacy in bitcoin. In: Financial Cryptography and Data Security, pp. 34–51. Springer (2013)</li>

      <li>[6] Babaioff, M., Dobzinski, S., Oren, S., Zohar, A.: On bitcoin and red balloons. In: The 13th ACM Conference on Electronic Commerce. pp. 56–73. ACM (2012)</li>

      <li>[7] Decker, C., Wattenhofer, R.: Information propagation in the bitcoin network. In: 13th IEEE International Conference on Peer-to-Peer Computing (P2P), Trento, Italy (September 2013)</li>

      <li>[8] Eyal, I., Sirer, E.G.: Majority is not enough: Bitcoin mining is vulnerable. In: Financial Cryptography and Data Security, pp. 436–454. Springer (2014)</li>

      <li>[9] Karame, G.O., Androulaki, E., Capkun, S.: Double-spending fast payments in bitcoin. In: The 2012 ACM conference on Computer and communications security. pp. 906–917. ACM (2012)</li>

      <li>[10] Lewenberg, Y., Sompolinsky, Y., Zohar, A.: Inclusive block chain protocols. In: Financial Cryptography and Data Security. Springer (2015)</li>

      <li>[11] Miers, I., Garman, C., Green, M., Rubin, A.D.: Zerocoin: Anonymous distributed e-cash from bitcoin. In: IEEE Symposium on Security and Privacy (2013)</li>

      <li>[12] Nakamoto, S.: Bitcoin: A peer-to-peer electronic cash system (2008)</li>

      <li>[13] Reid, F., Harrigan, M.: An analysis of anonymity in the bitcoin system. In: Security and Privacy in Social Networks, pp. 197–223. Springer (2013)</li>

      <li>[14] Ron, D., Shamir, A.: Quantitative analysis of the full bitcoin transaction graph. In: Financial Cryptography and Data Security, pp. 6–24. Springer (2013)</li>

      <li>[15] Rosenfeld, M.: Analysis of bitcoin pooled mining reward systems. arXiv preprint arXiv:1112.4980 (2011)</li>

      <li>[16] Rosenfeld, M.: Analysis of hashrate-based double spending. arXiv preprint arXiv:1402.2009 (2014)</li>

      <li>[17] Serfozo, R.: Basics of applied stochastic processes. Springer (2009)</li>

      <li>[18] Sompolinsky, Y., Zohar, A.: Bitcoin’s security model revisited. arXiv preprint arXiv:1605.09193 (2016)</li>

      <li>[19] Williams, D.: Probability with martingales. Cambridge university press (1991)</li>

    </ul>

    <h2 id="sec-27" class="text-2xl font-bold">Appendix A Where Is The Attacker in Longest Chain?</h2>

    <p class="text-gray-300">From a practical perspective, we must remember that a node listening to the Bitcoin network does not really know the amount of computational power the honest nodes in the network possess. In particular, the attacker may be building</p>

    <p class="text-gray-300">blocks along with the network up until the time of the attack, or he may not. Therefore, all that is observed is some amount of computational power which triggers the reported block creation rate <span class="math">\\lambda_{rep}</span>. We now ask ourselves what is the worst case when using the longest-chain rule? An attacker who participates or one that does not? Also, what is the right security threshold in terms of <span class="math">\\lambda_{rep}</span> (rather than <span class="math">\\lambda_{h}</span> which is unknown)?</p>

    <p class="text-gray-300">We begin with the assumption that the attacker has a fraction <span class="math">q</span> of the computational power of the honest network. Denote by <span class="math">\\lambda_{a},\\lambda_{h}</span> the block creation rate of the attacker and the honest nodes respectively, and by <span class="math">\\lambda=\\lambda_{a}+\\lambda_{h}</span> their joint rate. Our assumption is <span class="math">\\lambda_{a}&lt;q\\lambda_{h}</span> as before. <span class="math">\\lambda_{rep}</span> is the observed rate of block creation in the system (before the attack), which is in the range <span class="math">[\\lambda_{h},\\lambda_{h}+\\lambda_{a}]</span>. The following proposition shows that for a given threshold <span class="math">q</span> it is enough to use <span class="math">\\lambda_{rep}</span> as a measure of the honest network’s creation rate, as the attacker would only make it harder on itself if it joined the rest of the network and generated blocks before the attack. This is quite counter-intuitive, as the attacker that adds to the rate before the attack fools the network into thinking it is stronger. In reality, it increases the number of its blocks but lowers the network’s efficiency, which is the true measure of resilience to attacks.</p>

    <h6 id="sec-28" class="text-base font-medium mt-4">Proposition 11</h6>

    <p class="text-gray-300">If the network’s observed block rate is <span class="math">\\lambda_{rep}</span>, for a given block size, and <span class="math">\\beta(\\lambda_{rep})\\geq\\frac{2\\cdot q}{1+q}\\lambda_{rep}</span>, then the network is secure against an attacker with computational power lower than <span class="math">q\\lambda_{h}</span>. Furthermore, an attacker is most effective if it does not participate in block mining before the attack.</p>

    <h6 id="sec-29" class="text-base font-medium mt-4">Proof</h6>

    <p class="text-gray-300">If a fraction <span class="math">f</span> of the attacker’s blocks were included in <span class="math">\\lambda_{rep}</span> prior to the attack, then <span class="math">\\lambda_{rep}=\\lambda_{h}+f\\cdot\\lambda_{a}</span>. I.e., <span class="math">\\lambda_{h}=\\lambda_{rep}-f\\cdot\\lambda_{a}=\\lambda_{rep}-fq\\lambda_{h}</span>.</p>

    <p class="text-gray-300">Observe that every block that the attacker publishes before the attack could only increase the length of the main chain, and that the attacker’s maximal contribution to the chain’s length is the number of blocks that he published. Therefore: <span class="math">\\beta(\\lambda_{h})\\geq\\beta(\\lambda_{rep})-f\\cdot\\lambda_{a}</span>. We obtain:</p>

    <p class="text-gray-300"><span class="math">\\beta\\left(\\lambda_{h}\\right)\\geq\\beta(\\lambda_{rep})-f\\cdot\\lambda_{a}\\geq\\frac{2\\cdot q}{1+q}\\cdot\\lambda_{rep}-f\\cdot\\lambda_{a}=</span> <span class="math">\\frac{2\\cdot q}{1+q}\\cdot\\left(\\lambda_{h}+f\\cdot\\lambda_{a}\\right)-f\\cdot\\lambda_{a}=\\frac{2\\cdot q}{1+q}\\cdot\\lambda_{h}-\\left(1-\\frac{2\\cdot q}{1+q}\\right)\\cdot f\\cdot\\lambda_{a}&gt;</span> <span class="math">\\frac{2\\cdot q}{1+q}\\cdot\\lambda_{h}-\\left(1-\\frac{2\\cdot q}{1+q}\\right)\\cdot f\\cdot q\\cdot\\lambda_{h}\\geq</span> <span class="math">\\frac{2\\cdot q}{1+q}\\cdot\\lambda_{h}-\\left(1-\\frac{2\\cdot q}{1+q}\\right)\\cdot q\\cdot\\lambda_{h}=\\left(\\frac{2\\cdot q}{1+q}-\\left(q-\\frac{2\\cdot q^{2}}{1+q}\\right)\\right)\\cdot\\lambda_{h}=</span> <span class="math">\\left(\\frac{2\\cdot q}{1+q}+\\frac{2\\cdot q^{2}}{1+q}-q\\right)\\cdot\\lambda_{h}=\\frac{2\\cdot q+2\\cdot q^{2}-q-q^{2}}{1+q}\\cdot\\lambda_{h}=q\\cdot\\lambda_{h}&gt;\\lambda_{a}.</span></p>

    <p class="text-gray-300">In conclusion, <span class="math">\\beta(\\lambda_{h})&gt;\\lambda_{a}</span>. The attacker’s chain thus grows slower than the longest chain in the honest network’s tree.</p>

    <p class="text-gray-300">The attacker is most efficient if he avoids publishing his blocks before the attack (<span class="math">f=0</span>), because these blocks can be used by him to increase the success-probability of double-spending. More on this in <em>[18]</em>. <span class="math">\\Box</span></p>

    <h2 id="sec-30" class="text-2xl font-bold">Appendix B Proof of Theorem 4</h2>

    <h6 id="sec-31" class="text-base font-medium mt-4">Theorem 4</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Consider a network with two nodes, <span class="math">u</span> and <span class="math">v</span>, and equal block creation rates <span class="math">\\lambda/2</span>, which are connected by a single link with delay <span class="math">d</span>. For any block <span class="math">B</span>, <span class="math">E[n_{B}]\\leq\\frac{(d\\lambda)^{2}}{8}+\\frac{d\\lambda}{2}</span>, where $n_{B}:=</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">subtree_{T}(B)</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> for </span>T=tree(\\psi_{B})$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h6 id="sec-32" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">We define a state <span class="math">x_{n}</span> representing the time gap between the of creation the <span class="math">n</span>’th block by each of the nodes, in favor of <span class="math">u</span>.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">It is clear that whenever $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x_{n}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">>d<span class="math">, a collapse has occurred, as this means a message from </span>u<span class="math"> about a new block has arrived at </span>v$ without the latter creating a corresponding block in time, or vice versa.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">In order to count <span class="math">n_{B}</span>, we recursively express the expected number of <em>additional</em> blocks in <span class="math">subtree(B)</span>, given the current state <span class="math">x_{n}</span>. We denote this by <span class="math">h(x_{n})</span>.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Given that the time gap <span class="math">x_{n+1}</span> is positive, its value depends on the next block creation of <span class="math">v</span>, and thus follows an exponential distribution with rate <span class="math">\\lambda/2</span>; the same argument applies to the case <span class="math">x_{n+1}&lt;0</span>. If $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x_{n+1}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><d<span class="math">, the expected addition to </span>subtree(B)<span class="math"> (conditioned on the current state) is simply </span>1+h(x_{n+1})<span class="math">, otherwise, it is exactly </span>0<span class="math">. We express </span>h()<span class="math"> as a sum of two functions </span>f(),g()<span class="math">. One for the case in which the time gap increases in favor of </span>u<span class="math"> (</span>f<span class="math">), and one for the case in which it decreases (</span>g<span class="math">). By symmetry, the probability for these events is </span>\\frac{1}{2}<span class="math">. This justifies the following equations for </span>f<span class="math">, </span>g<span class="math"> and </span>h$:</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><span class="math">f(x)</span> <span class="math">:=\\frac{1}{2}\\int\\limits_{x}^{d}\\mu e^{-\\mu(t-x)}(h(t)+1)dt=e^{\\mu x}\\int\\limits_{x}^{d}\\mu e^{-\\mu t}\\frac{h(t)+1}{2}dt</span> <span class="math">g(x)</span> <span class="math">:=\\frac{1}{2}\\int\\limits_{-d}^{x}\\mu e^{-\\mu(x-u)}(h(u)+1)du=e^{-\\mu x}\\int\\limits_{-d}^{x}\\mu e^{\\mu u}\\frac{h(t)+1}{2}du</span> <span class="math">h(x)</span> <span class="math">=f(x)+g(x).</span></p>

    <p class="text-gray-300">Differentiating these functions we obtain,</p>

    <p class="text-gray-300"><span class="math">\\frac{df}{dx}=\\mu e^{\\mu x}\\int\\limits_{x}^{d}\\mu e^{-\\mu t}\\frac{h(t)+1}{2}dt+e^{\\mu x}\\cdot-1\\cdot\\mu e^{-\\mu x}\\frac{h(x)+1}{2}=</span> <span class="math">\\mu f(x)-\\mu\\frac{h(x)+1}{2}=\\mu f(x)-\\mu\\frac{f(x)+g(x)+1}{2}=</span> <span class="math">\\frac{\\mu}{2}(f(x)-g(x)-1).</span></p>

    <p class="text-gray-300">Similarly,</p>

    <p class="text-gray-300"><span class="math">\\frac{dg}{dx}=\\frac{\\mu}{2}(f(x)-g(x)+1)</span></p>

    <p class="text-gray-300">We thus arrive at the following linear non homogeneous differential system:</p>

    <p class="text-gray-300">\\[ \\left(\\begin{matrix}f\\\\ g\\end{matrix}\\right)^{\\prime}=\\left(\\begin{matrix}\\frac{\\mu}{2}&-\\frac{\\mu}{2}\\\\ \\frac{\\mu}{2}&-\\frac{\\mu}{2}\\end{matrix}\\right)\\cdot\\left(\\begin{matrix}f\\\\ g\\end{matrix}\\right)+\\left(\\begin{matrix}-\\frac{\\mu}{2}\\\\ \\frac{\\mu}{2}\\end{matrix}\\right), \\]</p>

    <p class="text-gray-300">with the following boundary conditions:</p>

    <p class="text-gray-300"><span class="math">f(d)=0,g(-d)=0.</span></p>

    <p class="text-gray-300">Solving this system yields:</p>

    <p class="text-gray-300"><span class="math">f(x)=\\frac{1}{4}\\left((d\\mu)^{2}-(x\\mu)^{2}+2d\\mu-2x\\mu\\right)</span> <span class="math">g(x)=\\frac{1}{4}\\left((d\\mu)^{2}-(x\\mu)^{2}+2d\\mu+2x\\mu\\right)</span> <span class="math">h(x)=\\frac{1}{2}\\left((d\\mu)^{2}-(x\\mu)^{2}+2d\\mu\\right)</span></p>

    <p class="text-gray-300">As the state at which the competition begins is <span class="math">x=0</span>, by symmetry, we get that the expected number of blocks until a collapse is <span class="math">h(0)=\\frac{(d\\mu)^{2}}{2}+d\\mu</span> blocks. ∎</p>

    <h2 id="sec-33" class="text-2xl font-bold">Appendix C Proof of Lemma 5</h2>

    <h6 id="sec-34" class="text-base font-medium mt-4">Lemma 5</h6>

    <p class="text-gray-300">Let G=(V,E) be a network graph (a sub-graph of the entire network) which generates blocks at a rate <span class="math">\\lambda^{\\prime}=\\alpha\\cdot\\lambda</span> with delay diameter <span class="math">D</span>. Then under the longest-chain rule, the rate at which the longest chain grows <span class="math">\\beta(\\lambda)\\geq\\frac{\\lambda^{\\prime}}{1+\\lambda^{\\prime}\\cdot D}</span>.</p>

    <h6 id="sec-35" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">We follow a sequence of block creation events for blocks <span class="math">U_{0},U_{1},U_{2},\\ldots</span> such that each block <span class="math">U_{i+1}</span> is the first block to be created after <span class="math">D</span> seconds have passed from the creation of the previous block <span class="math">U_{i}</span> (so that there has been sufficient time to send <span class="math">U_{i}</span> to all nodes in the network), i.e., the first block <span class="math">B</span> for which <span class="math">time(B)-D&gt;time(U_{i})</span>. Let us now make the following claim.</p>

    <h6 id="sec-36" class="text-base font-medium mt-4">Claim 12</h6>

    <p class="text-gray-300">Let <span class="math">U_{0},U_{1},U_{2},\\ldots</span> be a series of blocks that were created at least <span class="math">D</span> time units apart. Then for all <span class="math">n\\in\\mathbb{N}</span>: <span class="math">Depth(U_{n})-Depth(U_{0})\\geq n</span>.</p>

    <p class="text-gray-300">The claim can be proven by induction. It is trivially true for <span class="math">n=0</span>. Now we assume that the claim is true for <span class="math">n=k</span>, and show it is true for <span class="math">n=k+1</span>. by <span class="math">time(U_{k})</span> we have <span class="math">Depth(U_{k})-Depth(U_{0})\\geq n</span>. Then, consider the time at which block <span class="math">U_{k+1}</span> is created. The node that created it has done so after hearing about block <span class="math">U_{k}</span>, it therefore has a chain that is at least of length <span class="math">k</span> (by the induction</p>

    <p class="text-gray-300">assumption and because Chains can only grow or be replaced by longer chains). Therefore <span class="math">U_{k+1}</span> is built at depth that is at least 1 more than <span class="math">U_k</span>.</p>

    <p class="text-gray-300">Now that we have established the claim, we can turn to calculating the lower-bound of <span class="math">\\beta</span>. Denote by <span class="math">X_i = \\text{time}(U_i) - \\text{time}(U_{i-1})</span> the random variable representing the time between block creations. Notice that the <span class="math">X_i</span>'s are i.i.d. random variables (because the time interval they denote is exactly <span class="math">D</span> time units for the block to spread plus an exponentially distributed waiting time for the next block's creation somewhere in the network). Also note that <span class="math">\\beta \\geq E[\\frac{1}{n}\\sum_{i=1}^{n}X_i]^{-1}</span>, as the chain grows by at least <span class="math">n</span> during the time <span class="math">\\sum_{i=1}^{n}X_i</span>. We therefore have <span class="math">\\beta \\geq \\frac{1}{E[X_1]}</span>. Additionally, we know that <span class="math">E[X_1] = D + E[Y]</span>, where <span class="math">Y</span> is a random variable with an exponential distribution with parameter <span class="math">\\lambda&#x27;</span>. As <span class="math">E[Y] = \\frac{1}{\\lambda&#x27;}</span> we have: <span class="math">\\beta \\geq \\frac{1}{D + \\frac{1}{\\lambda&#x27;}} = \\frac{\\lambda&#x27;}{1 + \\lambda&#x27; \\cdot D}</span>.</p>

    <h2 id="sec-37" class="text-2xl font-bold">D Proof of Claim 7</h2>

    <p class="text-gray-300"><strong>Claim 7:</strong></p>

    <p class="text-gray-300">Let <span class="math">B</span> be a block in tree <span class="math">T</span> in a network as in Lemma 6, then regardless of history, the expected waiting time for the creation of the last child of <span class="math">B</span> is upper bounded by <span class="math">2D + \\frac{1}{\\lambda&#x27;}</span>.</p>

    <p class="text-gray-300"><strong>Proof.</strong> Let <span class="math">C</span> be the first block created after <span class="math">D</span> seconds have passed from <span class="math">B</span>'s creation. Denote by <span class="math">\\tau</span> the time from <span class="math">B</span>'s creation until <span class="math">C</span> has been created and yet another <span class="math">D</span> seconds elapsed. We argue that <span class="math">E[\\tau] \\leq 2D + 1 / \\lambda&#x27;</span>. This is easy to see: It takes <span class="math">1 / \\lambda&#x27;</span> seconds in expectation to create block <span class="math">C</span>, an event which can only occur after <span class="math">D</span> seconds have passed from <span class="math">B</span>'s creation. Then, we deterministically wait another <span class="math">D</span> seconds to propagate <span class="math">C</span> to the entire network.</p>

    <p class="text-gray-300">We claim that after <span class="math">\\tau</span> seconds from <span class="math">B</span>'s creation, <span class="math">B</span> will have no more children. Let us examine the two possible cases:</p>

    <p class="text-gray-300"><strong>Case I:</strong> <span class="math">C</span> is a descendant of B. Once <span class="math">C</span> has been propagated to all nodes, no node considers <span class="math">B</span> a leaf, and the GHOST chain selection rule only extends leaves (in the subtree known to the extending node).</p>

    <p class="text-gray-300"><strong>Case II:</strong> <span class="math">C</span> is not a descendant of B. Because <span class="math">B</span> was propagated to all nodes before <span class="math">C</span> was created, the node that extended <span class="math">C</span> was well aware of <span class="math">B</span>, but did not extend it. It therefore had a strictly heavier subtree than <span class="math">B</span> is part of after the creation of <span class="math">C</span>. <span class="math">D</span> seconds later, block <span class="math">C</span> is known to all other nodes, along with its entire supporting subtree. In this case, <span class="math">B</span> will not be extended directly either – nodes have switched away from <span class="math">B</span> if no other children extend it, or have switched to its descendants if it does have children.</p>

    <h2 id="sec-38" class="text-2xl font-bold">E Proof of Theorem 9</h2>

    <p class="text-gray-300"><strong>Proof.</strong> In the above setting, <span class="math">v</span> and <span class="math">u</span> create blocks separately, and whenever one completes a block it sends the message with its new block through the link, to arrive at its counterpart <span class="math">d</span> seconds later; in these <span class="math">d</span> seconds the node still</p>

    <p class="text-gray-300">continues with the attempt to build new blocks and lengthen its own version of the main chain. Thus messages about blocks of the same depth (which were created by <span class="math">u</span> and <span class="math">v</span> roughly at the same time) may simultaneously be traveling in opposite directions on the link, causing a fork in the block-tree.</p>

    <p class="text-gray-300">Note that the block-tree is actually a binary-tree — at any point of time their are at most two branches not abandoned, as the two-node network contains at most two conflicting world views. Among two candidate chains, the heaviest one and the longest one coincide; the analysis below applies thus equally to a network following the “longest chain” rule and to that following GHOST.</p>

    <p class="text-gray-300">In order to count the number of blocks that fail to enter the main chain, we notice that such an event occurs precisely when two blocks of the same height have been created.</p>

    <p class="text-gray-300">Consider a block <span class="math">U</span> of node <span class="math">u</span>. We say that the <em>window</em> of <span class="math">U</span> is created <span class="math">d</span> time units before <span class="math">U</span>’s creation, and is gone <span class="math">d</span> time units after it. Notice, that a block <span class="math">U</span> is built upon any of <span class="math">v</span>’s blocks that was created before <span class="math">U</span>’s window was created, and also that block <span class="math">U</span> arrives at node <span class="math">v</span> exactly at the end of <span class="math">U</span>’s window.</p>

    <p class="text-gray-300">We say that <span class="math">U</span> is “threatened” at a given time, if <span class="math">U</span>’s window has been created, and the chain at <span class="math">v</span> is of length <span class="math">depth(U)-1</span> (this time interval is contained in <span class="math">U</span>’s window). During this period, the next block created by <span class="math">v</span> will be of the same depth as <span class="math">U</span> and one of the blocks is wasted. We define <span class="math">open</span> as the time that elapsed from <span class="math">U</span>’s window’s creation to the moment at which it becomes threatened, and define <span class="math">close</span> as the time that elapsed from its window’s creation until it ceases to be threatened.</p>

    <p class="text-gray-300">Notice that the closure of <span class="math">U</span> can occur in two ways: either 2d time has passed from the <span class="math">U</span>-window creation, and <span class="math">v</span> received a message containing <span class="math">U</span>, or <span class="math">v</span> generated a competing block of the same height before that. Therefore, the difference between the moment <span class="math">U</span> is opened and the moment it is closed is between 0 and 2d. In addition, notice that two blocks of <span class="math">u</span> cannot be simultaneously threatened (<span class="math">v</span>’s chain cannot be shorter by 1 from both their depths at the same time).</p>

    <p class="text-gray-300">Assuming block <span class="math">U_{n}</span>’s window was created at a time that we shall denote as time 0, <span class="math">open(U_{n})</span> and <span class="math">close(U_{n})</span> are random variables taking values in <span class="math">[0,2d]</span>, for whom we have <span class="math">close(U_{n})\\geq open(U_{n})</span>. The distribution of <span class="math">open(U_{n})</span> is composed of a continuous part on the region <span class="math">(0,2d]</span>, and a discrete part on the atomic event <span class="math">\\{open(U_{n})=0\\}</span>. We denote the former by <span class="math">\\alpha_{n}(x)</span>, for <span class="math">x\\in(0,2d]</span>, and the latter by <span class="math">\\alpha_{n,0}</span>. Similarly, <span class="math">close(U_{n})</span>’s probability distribution has a continuous part which we denote <span class="math">\\omega_{n}(x)</span> on <span class="math">[0,2d)</span>, and a discrete part <span class="math">\\omega_{n,2d}</span> for the atomic event <span class="math">\\{close(U_{n})=2d\\}</span>.</p>

    <p class="text-gray-300">We denote by <span class="math">f_{S}</span> and <span class="math">f_{T}</span> the pdf’s of the exponential random variables with rates <span class="math">p_{S}\\lambda</span> and <span class="math">p_{T}\\lambda</span>, respectively. We claim that the following relations hold:</p>

    <p class="text-gray-300"><span class="math">\\alpha_{n}(x)=\\int_{x}^{2d}\\omega_{n-1}(y)\\cdot f_{S}(y-x)dy+</span></p>

    <p class="text-gray-300"><span class="math">\\omega_{n-1,2d}\\cdot f_{S}(2d-x),\\quad 0&lt;x\\leq 2d</span> (1)</p>

    <p class="text-gray-300"><span class="math">\\omega_{n}(x)=\\int_{0}^{x}\\alpha_{n}(z)\\cdot f_{T}(x-z)dy+</span></p>

    <p class="text-gray-300"><span class="math">\\alpha_{n,0}\\cdot f_{T}(x),\\quad 0\\leq x&lt;2d</span> (2)</p>

    <p class="text-gray-300">Indeed, starting with 1, <span class="math">U_{n}</span> opens <span class="math">x</span> seconds after the window creation if and only if for some <span class="math">y</span>, <span class="math">U_{n-1}</span> closed <span class="math">y</span> seconds after its window creation (with probability <span class="math">\\omega_{n-1}(y)</span> for <span class="math">y&lt;2d</span> and <span class="math">\\omega_{n-1,2d}</span> for <span class="math">y=2d</span>), and the gap between their respective creations was <span class="math">y-x</span> seconds (<span class="math">f_{S}(y-x)</span>). This calculation is relevant only to <span class="math">x&gt;0</span>, as only under the assumption that <span class="math">U_{n}</span>’s window creation preceded <span class="math">U_{n-1}</span>’s closure the period between <span class="math">U_{n-1}</span>’s opening and closing (<span class="math">y</span>) contains that between <span class="math">U_{n}</span>’s window creation and opening (<span class="math">x</span>).</p>

    <p class="text-gray-300">Regarding 2, <span class="math">U_{n}</span> closes <span class="math">x</span> seconds after its window creation if and only if for some <span class="math">z</span>, <span class="math">z</span> seconds passed between <span class="math">U_{n}</span>’s window creation and its opening (with probability <span class="math">\\alpha_{n}(z)</span> for <span class="math">z&gt;0</span> and <span class="math">\\alpha_{n,0}</span> for <span class="math">z=0</span>), and <span class="math">x-z</span> seconds between the latter and its closing (<span class="math">f_{T}(x-z)</span>). That the gap between the opening and the closing of <span class="math">U_{n}</span> is controlled by <span class="math">f_{T}</span> is true only in the region <span class="math">x&lt;2d</span>.</p>

    <p class="text-gray-300">The processes <span class="math">open(U_{n})</span> and <span class="math">close(U_{n})</span> are Markovian, and we now write equations 1 and 2 applied to their limiting distributions, <span class="math">\\alpha(x),\\alpha_{0}</span> and <span class="math">\\omega(x),\\omega_{2d}</span>:</p>

    <p class="text-gray-300"><span class="math">\\alpha(x)=\\int_{x}^{2d}\\omega(y)\\cdot f_{S}(y-x)dy+</span></p>

    <p class="text-gray-300"><span class="math">\\omega_{2d}\\cdot f_{S}(2d-x),\\quad 0&lt;x\\leq 2d</span> (3)</p>

    <p class="text-gray-300"><span class="math">\\omega(x)=\\int_{0}^{x}\\alpha(z)\\cdot f_{T}(x-z)dy+</span></p>

    <p class="text-gray-300"><span class="math">\\alpha_{0}\\cdot f_{T}(x),\\quad 0\\leq x&lt;2d</span> (4)</p>

    <p class="text-gray-300">These equations resolve to a differential equation system:</p>

    <p class="text-gray-300">\\[ \\left(\\begin{matrix}\\alpha\\\\ \\omega\\end{matrix}\\right)^{\\prime}=\\begin{pmatrix}p_{S}\\lambda&-p_{S}\\lambda\\\\ p_{T}\\lambda&-p_{T}\\lambda\\end{pmatrix}\\cdot\\left(\\begin{matrix}\\alpha\\\\ \\omega\\end{matrix}\\right) \\]</p>

    <p class="text-gray-300">whose solution is:</p>

    <p class="text-gray-300"><span class="math">\\left(\\begin{matrix}\\alpha(x)\\\\ \\omega(x)\\end{matrix}\\right)=\\frac{A}{S}\\left(\\begin{matrix}p_{S}\\lambda(e^{S\\cdot x}-1)\\\\ p_{T}\\lambda(e^{S\\cdot x}-1)\\end{matrix}\\right)+\\left(\\begin{matrix}\\alpha(0)\\\\ \\omega(0)\\end{matrix}\\right)</span> (5)</p>

    <p class="text-gray-300">for <span class="math">A=\\alpha(0)-\\omega(0)</span> ; <span class="math">S=p_{S}\\lambda-p_{T}\\lambda</span>.</p>

    <p class="text-gray-300">##</p>

    <h6 id="sec-39" class="text-base font-medium mt-4">Lemma 13</h6>

    <p class="text-gray-300">Equation 5 implies</p>

    <p class="text-gray-300"><span class="math">\\omega_{2d}=\\frac{p_{S}\\lambda-p_{T}\\lambda}{p_{S}\\lambda-p_{T}\\lambda e^{-(p_{S}\\lambda-p_{T}\\lambda)2d}}.</span></p>

    <p class="text-gray-300">By the definition of <span class="math">\\omega_{2d}</span>, it is precisely the fraction of <span class="math">u</span>’s blocks that have no conflicting blocks created by <span class="math">v</span>. The blocks which contribute to the growth of the main chain are can thus be counted by considering all of <span class="math">v</span>’s blocks as valid, and adding to those all of <span class="math">u</span>’s non-conflicting blocks. Altogether, we obtain</p>

    <p class="text-gray-300"><span class="math">\\beta(\\lambda)=p_{T}\\lambda+\\omega_{2d}\\cdot p_{S}\\lambda=</span> <span class="math">p_{T}\\lambda+\\frac{p_{S}\\lambda-p_{T}\\lambda}{p_{S}\\lambda-p_{T}\\lambda e^{-(p_{S}\\lambda-p_{T}\\lambda)2d}}p_{S}\\lambda=</span> <span class="math">\\frac{(p_{S}\\lambda)^{2}e^{p_{S}\\lambda 2d}-(p_{T}\\lambda)^{2}e^{p_{T}\\lambda 2d}}{p_{S}\\lambda e^{p_{S}\\lambda 2d}-p_{T}\\lambda e^{p_{T}\\lambda 2d}}.</span></p>

    <p class="text-gray-300">This concludes the proof of Theorem 9. ∎</p>

    <p class="text-gray-300">All that remains is to prove Lemma 13</p>

    <h6 id="sec-40" class="text-base font-medium mt-4">Proof (of Lemma 13)</h6>

    <p class="text-gray-300">By equation 1, <span class="math">\\alpha(2d)=\\omega_{2d}p_{S}\\lambda</span>, and by 2, <span class="math">\\omega(0)=\\alpha_{0}p_{T}\\lambda</span>. Therefore,</p>

    <p class="text-gray-300"><span class="math">\\omega(x)=\\hat{A}p_{T}\\lambda(e^{(p_{S}\\lambda-p_{T}\\lambda)x}-1)+\\alpha_{0}p_{T}\\lambda\\text{, for }\\hat{A}:=\\frac{A}{p_{S}\\lambda-p_{T}\\lambda}\\Rightarrow</span> <span class="math">\\omega^{\\prime}(x)=p_{T}\\lambda Ae^{(p_{S}\\lambda-p_{T}\\lambda)x}.</span></p>

    <p class="text-gray-300">By E, <span class="math">\\omega^{\\prime}(x)=p_{T}\\lambda(\\alpha(x)-\\omega(x))</span>, and therefore,</p>

    <p class="text-gray-300"><span class="math">\\alpha(x)-\\omega(x)=Ae^{(p_{S}\\lambda-p_{T}\\lambda)x}\\Rightarrow\\alpha^{\\prime}(x)=p_{S}\\lambda(\\alpha(x)-\\omega(x))\\Rightarrow</span> <span class="math">\\alpha(x)=\\int\\limits_{0}^{x}p_{S}\\lambda(\\alpha(t)-\\omega(t))dt+\\alpha(0)=p_{S}\\lambda\\int\\limits_{0}^{x}Ae^{(p_{S}\\lambda-p_{T}\\lambda)t}dt+\\alpha(0)=</span> <span class="math">\\frac{p_{S}\\lambda\\cdot A}{p_{S}\\lambda-p_{T}\\lambda}(e^{(p_{S}\\lambda-p_{T}\\lambda)x}-1)+\\alpha(0).</span></p>

    <p class="text-gray-300">We obtain,</p>

    <p class="text-gray-300"><span class="math">\\alpha(0)=\\omega(0)+A=\\alpha_{0}p_{T}\\lambda+A\\Longrightarrow</span> <span class="math">\\alpha(x)=\\frac{p_{S}\\lambda\\cdot A}{p_{S}\\lambda-p_{T}\\lambda}(e^{(p_{S}\\lambda-p_{T}\\lambda)x}-1)+\\alpha_{0}p_{T}\\lambda+A\\Longrightarrow</span> <span class="math">\\alpha(2d)=\\frac{p_{S}\\lambda\\cdot A}{p_{S}\\lambda-p_{T}\\lambda}(E-1)+\\alpha_{0}p_{T}\\lambda+A,</span></p>

    <p class="text-gray-300">Therefore, for <span class="math">\\hat{A}:=\\frac{A}{p_{S}\\lambda-p_{T}\\lambda}</span> and <span class="math">E:=e^{(p_{S}\\lambda-p_{T}\\lambda)2d}</span>,</p>

    <p class="text-gray-300"><span class="math">\\hat{A}=\\frac{\\alpha(2d)-\\alpha_{0}p_{T}\\lambda}{p_{S}\\lambda E-p_{T}\\lambda}=\\frac{\\omega_{2d}p_{S}\\lambda-\\alpha_{0}p_{T}\\lambda}{p_{S}\\lambda E-p_{T}\\lambda}.</span></p>

    <p class="text-gray-300">We have thus obtained explicit expressions for <span class="math">\\alpha(x)</span> and <span class="math">\\omega(x)</span> subject to the parameters <span class="math">\\alpha_0</span> and <span class="math">\\omega_{2d}</span>:</p>

    <div class="my-4 text-center"><span class="math-block">\\alpha(x) = \\hat{A}(p_S \\lambda e^{(p_S \\lambda - p_T \\lambda) x} - p_T \\lambda) + \\alpha_0 p_T \\lambda</span></div>

    <div class="my-4 text-center"><span class="math-block">\\omega(x) = \\hat{A}(p_T \\lambda e^{(p_S \\lambda - p_T \\lambda) x} - p_T \\lambda) + \\alpha_0 p_T \\lambda</span></div>

    <p class="text-gray-300">By the definition of <span class="math">\\alpha</span> we know that <span class="math">\\alpha</span>'s integral over the range <span class="math">(0, 2d]</span> should be <span class="math">1 - \\alpha_0</span>:</p>

    <div class="my-4 text-center"><span class="math-block">1 - \\alpha_0 = \\int_0^{2d} \\left( \\hat{A}(p_S \\lambda e^{(p_S \\lambda - p_T \\lambda) t} - p_T \\lambda) + \\alpha_0 p_T \\lambda \\right) dt =</span></div>

    <div class="my-4 text-center"><span class="math-block">\\hat{A} \\left( \\frac{p_S \\lambda (E - 1)}{p_S \\lambda - p_T \\lambda} - 2d \\cdot p_T \\lambda \\right) + 2d \\cdot \\alpha_0 p_T \\lambda =</span></div>

    <div class="my-4 text-center"><span class="math-block">\\hat{A} \\left( \\hat{E} - 2d \\cdot p_T \\lambda \\right) + 2d \\cdot \\alpha_0 p_T \\lambda,</span></div>

    <p class="text-gray-300">for <span class="math">\\hat{E} := \\frac{p_S \\lambda (E - 1)}{p_S \\lambda - p_T \\lambda}</span>. Therefore,</p>

    <div class="my-4 text-center"><span class="math-block">\\alpha_0 = 1 - \\frac{\\omega_{2d} p_S \\lambda - \\alpha_0 p_T \\lambda}{p_S \\lambda E - p_T \\lambda} \\left( \\hat{E} - 2d \\cdot p_T \\lambda \\right) - 2d \\cdot \\alpha_0 p_T \\lambda.</span></div>

    <p class="text-gray-300">Similarly, the integral of <span class="math">\\omega</span> over <span class="math">[0, 2d)</span> should be <span class="math">1 - \\omega_{2d}</span>, and combining this with the relation <span class="math">\\alpha(x) - \\omega(x) = A e^{(p_S \\lambda - p_T \\lambda) x}</span> we obtain:</p>

    <div class="my-4 text-center"><span class="math-block">1 - \\alpha_0 - (1 - \\omega_{2d}) = \\int_0^{2d} A e^{(p_S \\lambda - p_T \\lambda) t} dt = \\hat{A}(E - 1) \\Longrightarrow</span></div>

    <div class="my-4 text-center"><span class="math-block">\\omega_{2d} - \\alpha_0 = \\hat{A}(E - 1) = \\frac{\\omega_{2d} p_S \\lambda - \\alpha_0 p_T \\lambda}{p_S \\lambda E - p_T \\lambda}(E - 1) \\Longrightarrow</span></div>

    <div class="my-4 text-center"><span class="math-block">\\frac{\\omega_{2d}}{\\alpha_0} - 1 = \\frac{\\frac{\\omega_{2d}}{\\alpha_0} p_S \\lambda - p_T \\lambda}{p_S \\lambda E - p_T \\lambda}(E - 1) \\Longrightarrow</span></div>

    <div class="my-4 text-center"><span class="math-block">\\left(1 - \\frac{p_S \\lambda (E - 1)}{p_S \\lambda E - p_T \\lambda}\\right) \\frac{\\omega_{2d}}{\\alpha_0} = 1 - \\frac{p_T \\lambda}{p_S \\lambda E - p_T \\lambda}(E - 1) \\Longrightarrow</span></div>

    <div class="my-4 text-center"><span class="math-block">\\frac{\\omega_{2d}}{\\alpha_0} = \\frac{1 - \\frac{p_T \\lambda}{p_S \\lambda E - p_T \\lambda}(E - 1)}{1 - \\frac{p_S \\lambda (E - 1)}{p_S \\lambda E - p_T \\lambda}} = E</span></div>

    <p class="text-gray-300">We conclude with,</p>

    <div class="my-4 text-center"><span class="math-block">\\omega_{2d} = E \\cdot \\alpha_0 = E \\left(1 - \\frac{\\omega_{2d} p_S \\lambda - \\frac{\\omega_{2d}}{E} p_T \\lambda}{p_S \\lambda E - p_T \\lambda} \\left(\\hat{E} - 2d \\cdot p_T \\lambda\\right) - 2d \\cdot \\frac{\\omega_{2d}}{E} p_T \\lambda\\right) =</span></div>

    <div class="my-4 text-center"><span class="math-block">E \\left(1 - \\frac{\\omega_{2d}}{E} \\left(\\hat{E} - 2d \\cdot p_T \\lambda\\right) - 2d \\cdot \\frac{\\omega_{2d}}{E} p_T \\lambda\\right) = E - \\omega_{2d} \\hat{E} \\Longrightarrow</span></div>

    <div class="my-4 text-center"><span class="math-block">\\omega_{2d} = \\frac{E}{1 + \\hat{E}} = \\frac{e^{(p_S \\lambda - p_T \\lambda) 2d}}{1 + \\frac{p_S \\lambda (e^{(p_S \\lambda - p_T \\lambda) 2d} - 1)}{p_S \\lambda - p_T \\lambda}} = \\frac{e^{(p_S \\lambda - p_T \\lambda) 2d}}{p_S \\lambda e^{(p_S \\lambda - p_T \\lambda) 2d} - p_T \\lambda} \\cdot (p_S \\lambda - p_T \\lambda) = \\frac{p_S \\lambda - p_T \\lambda}{p_S \\lambda - p_T \\lambda e^{-(p_S \\lambda - p_T \\lambda) 2d}}.</span></div>

    <p class="text-gray-300">□</p>

    <h2 id="sec-41" class="text-2xl font-bold">F Proof of Theorem 10</h2>

    <p class="text-gray-300"><strong>Theorem 10:</strong></p>

    <p class="text-gray-300">Consider a network <span class="math">G</span> with delays. Let <span class="math">1 / \\beta_1</span> be an upper-bound on the expected waiting time for the next lengthening of the main chain, for all possible states of the system. Let <span class="math">q\\lambda_h &amp;lt; \\beta_1</span> be the creation rate of the attacker (according to a Poisson process), and suppose the gap between the network's longest chain and that of the attacker is <span class="math">X_0</span> blocks. Then the probability that the attacker will succeed in extending its chain to be longer than the network's is at most <span class="math">\\left(\\frac{q\\lambda_h}{\\beta_1}\\right)^{X_0 + 1}</span>.</p>

    <p class="text-gray-300">The proof depends on the following two lemmas:</p>

    <p class="text-gray-300"><strong>Lemma 14.</strong> Let <span class="math">\\varsigma</span> be a nonnegative random variable with increasing hazard rate function. Then, <span class="math">\\forall k\\in \\mathbb{N}</span></p>

    <div class="my-4 text-center"><span class="math-block">E[\\varsigma^k] \\leq k! E^k[\\varsigma].</span></div>

    <p class="text-gray-300">Note that the almost inverse inequality, <span class="math">E[\\varsigma^k] \\geq E^k[\\varsigma]</span>, stems from Jensen's inequality.</p>

    <p class="text-gray-300"><strong>Lemma 15.</strong> Let <span class="math">\\varsigma</span> be as in Lemma 14, let <span class="math">f</span> be its pdf, and denote <span class="math">\\beta \\coloneqq E[\\varsigma]^{-1}</span>. Then for any constant <span class="math">0 &amp;lt; \\gamma &amp;lt; \\beta</span> the function <span class="math">H_{\\gamma,\\beta}</span> obtains a positive root <span class="math">a_0</span> smaller than <span class="math">\\frac{\\gamma}{\\beta}</span>, where</p>

    <div class="my-4 text-center"><span class="math-block">H_{\\gamma,\\beta}(a) := \\int_0^{\\infty} f(\\varsigma) e^{\\gamma \\left(\\frac{1}{a} - 1\\right) \\varsigma} d\\varsigma - \\frac{1}{a}.</span></div>

    <p class="text-gray-300"><strong>Proof (of Lemma 14).</strong> By induction on <span class="math">k</span>. The base case <span class="math">k = 0</span> is trivial. For <span class="math">k + 1</span> we have:</p>

    <div class="my-4 text-center"><span class="math-block">E[\\varsigma^{k+1}] = \\int_0^{\\infty} \\varsigma^{k+1} f(\\varsigma) = \\int_0^{\\infty} \\varsigma^{k+1} \\lambda(\\varsigma) e^{-\\Lambda(\\varsigma)} d\\varsigma =</span></div>

    <div class="my-4 text-center"><span class="math-block">[\\varsigma^{k+1} \\cdot -e^{-\\Lambda(\\varsigma)}]_0^{\\infty} + \\int_0^{\\infty} (k+1) \\varsigma^k e^{-\\Lambda(\\varsigma)} d\\varsigma = (k+1) \\int_0^{\\infty} \\frac{\\varsigma^k}{\\lambda(\\varsigma)} \\lambda(\\varsigma) e^{-\\Lambda(\\varsigma)} d\\varsigma</span></div>

    <p class="text-gray-300">On the other hand,</p>

    <div class="my-4 text-center"><span class="math-block">E [ \\varsigma ] = \\int_ {0} ^ {\\infty} \\varsigma f (\\varsigma) = \\int_ {0} ^ {\\infty} \\varsigma \\lambda (\\varsigma) e ^ {- \\varLambda (\\varsigma)} d \\varsigma =</span></div>

    <div class="my-4 text-center"><span class="math-block">[ \\varsigma^ {\\cdot} - e ^ {- \\varLambda (\\varsigma)} ] _ {0} ^ {\\infty} + \\int_ {0} ^ {\\infty} e ^ {- \\varLambda (\\varsigma)} d \\varsigma = \\int_ {0} ^ {\\infty} e ^ {- \\varLambda (\\varsigma)} d \\varsigma ,</span></div>

    <p class="text-gray-300">and therefore,</p>

    <div class="my-4 text-center"><span class="math-block">(k + 1)! E ^ {k + 1} [ \\varsigma ] = (k + 1) k! E ^ {k} [ \\varsigma ] E [ \\varsigma ] =</span></div>

    <div class="my-4 text-center"><span class="math-block">(k + 1) k! E ^ {k} [ \\varsigma ] \\int_ {0} ^ {\\infty} e ^ {- \\varLambda (\\varsigma)} d \\varsigma = (k + 1) \\int_ {0} ^ {\\infty} \\frac {k ! E ^ {k} [ \\varsigma ]}{\\lambda (\\varsigma)} \\lambda (\\varsigma) e ^ {- \\varLambda (\\varsigma)} d \\varsigma</span></div>

    <p class="text-gray-300">It is thus sufficient to prove that,</p>

    <div class="my-4 text-center"><span class="math-block">(k + 1) \\int_ {0} ^ {\\infty} \\frac {\\varsigma^ {k}}{\\lambda (\\varsigma)} \\lambda (\\varsigma) e ^ {- \\varLambda (\\varsigma)} d \\varsigma \\leq (k + 1) \\int_ {0} ^ {\\infty} \\frac {k ! E ^ {k} [ \\varsigma ]}{\\lambda (\\varsigma)} \\lambda (\\varsigma) e ^ {- \\varLambda (\\varsigma)} d \\varsigma ,</span></div>

    <p class="text-gray-300">or, equivalently, that</p>

    <div class="my-4 text-center"><span class="math-block">\\int_ {0} ^ {\\infty} \\frac {\\varsigma^ {k} - k ! E ^ {k} [ \\varsigma ]}{\\lambda (\\varsigma)} \\lambda (\\varsigma) e ^ {- \\varLambda (\\varsigma)} d \\varsigma \\leq 0.</span></div>

    <p class="text-gray-300">Using the induction hypothesis we obtain:</p>

    <div class="my-4 text-center"><span class="math-block">\\int_ {0} ^ {\\infty} \\frac {\\varsigma^ {k} - k ! E ^ {k} [ \\varsigma ]}{\\lambda (\\varsigma)} \\lambda (\\varsigma) e ^ {- \\varLambda (\\varsigma)} d \\varsigma =</span></div>

    <div class="my-4 text-center"><span class="math-block">\\int_ {0} ^ {(k! E ^ {k} [ \\varsigma ]) ^ {\\frac {1}{k}}} \\frac {\\varsigma^ {k} - k ! E ^ {k} [ \\varsigma ]}{\\lambda (\\varsigma)} \\lambda (\\varsigma) e ^ {- \\varLambda (\\varsigma)} d \\varsigma + \\int_ {(k! E ^ {k} [ \\varsigma ]) ^ {\\frac {1}{k}}} ^ {\\infty} \\frac {\\varsigma^ {k} - k ! E ^ {k} [ \\varsigma ]}{\\lambda (\\varsigma)} \\lambda (\\varsigma) e ^ {- \\varLambda (\\varsigma)} d \\varsigma</span></div>

    <div class="my-4 text-center"><span class="math-block">\\leq \\frac {1}{\\lambda ((k ! E ^ {k} [ \\varsigma ]) ^ {\\frac {1}{k}})} \\int_ {0} ^ {(k! E ^ {k} [ \\varsigma ]) ^ {\\frac {1}{k}}} (\\varsigma^ {k} - k! E ^ {k} [ \\varsigma ]) \\lambda (\\varsigma) e ^ {- \\Lambda (\\varsigma)} d \\varsigma +</span></div>

    <div class="my-4 text-center"><span class="math-block">\\frac {1}{\\lambda ((k ! E ^ {k} [ \\varsigma ]) ^ {\\frac {1}{k}})} \\int_ {(k! E ^ {k} [ \\varsigma ]) ^ {\\frac {1}{k}}} ^ {\\infty} (\\varsigma^ {k} - k! E ^ {k} [ \\varsigma ]) \\lambda (\\varsigma) e ^ {- \\varLambda (\\varsigma)} d \\varsigma =</span></div>

    <div class="my-4 text-center"><span class="math-block">\\frac {1}{\\lambda ((k ! E ^ {k} [ \\varsigma ]) ^ {\\frac {1}{k}})} \\int_ {0} ^ {\\infty} (\\varsigma^ {k} - k! E ^ {k} [ \\varsigma ]) \\lambda (\\varsigma) e ^ {- \\varLambda (\\varsigma)} d \\varsigma \\leq 0,</span></div>

    <p class="text-gray-300">where we used <span class="math">\\lambda</span>'s monotonicity in the first inequality and the induction hypothesis in the last one.</p>

    <p class="text-gray-300">Proof (of Lemma 15). We have <span class="math">H_{\\gamma, \\beta}(0) = \\infty</span>. <span class="math">H_{\\gamma, \\beta}</span> is continuous in <span class="math">a</span> and thus, by The Intermediate Value Theorem, it suffices to show that <span class="math">H_{\\gamma, \\beta}(\\frac{\\gamma}{\\beta}) \\leq 0</span>.</p>

    <p class="text-gray-300">Let <span class="math">H(\\gamma) \\coloneqq H_{\\gamma, \\beta}(\\frac{\\gamma}{\\beta})</span>. We need to show that <span class="math">H(\\gamma) \\leq 0</span>, and we do so by showing that its Taylor series elements (around <span class="math">\\beta</span>) are all (!) nonpositive. That is, we show that <span class="math">H^{(k)}(\\beta)\\frac{(\\gamma - \\beta)^k}{k!} \\leq 0</span>, and this would imply, <span class="math">H(\\gamma) = \\Sigma_{k=0}^{\\infty} H^{(k)}(\\beta)\\frac{(\\gamma - \\beta)^k}{k!} \\leq 0</span>.</p>

    <p class="text-gray-300">Indeed,</p>

    <div class="my-4 text-center"><span class="math-block">H ^ {(k)} (\\beta) = \\frac {d ^ {k}}{d \\gamma^ {k}} \\left\\{\\int_ {0} ^ {\\infty} f (t) e ^ {\\gamma (\\frac {1}{\\beta} - 1) t} d t - \\frac {1}{\\frac {\\gamma}{\\beta}} \\right\\} _ {\\gamma = \\beta} =</span></div>

    <div class="my-4 text-center"><span class="math-block">\\frac {d ^ {k}}{d \\gamma^ {k}} \\left\\{\\int_ {0} ^ {\\infty} f (t) e ^ {(\\beta - \\gamma) t} d t - \\frac {\\beta}{\\gamma} \\right\\} _ {\\gamma = \\beta} =</span></div>

    <div class="my-4 text-center"><span class="math-block">\\left\\{\\int_ {0} ^ {\\infty} (- t) ^ {k} f (t) e ^ {\\gamma \\left(\\frac {1}{a} - 1\\right) t} d t + \\frac {k ! \\beta}{(- \\gamma) ^ {k + 1}} \\right\\} _ {\\gamma = \\beta} =</span></div>

    <div class="my-4 text-center"><span class="math-block">\\int_ {0} ^ {\\infty} (- t) ^ {k} f (t) d t + k! \\beta^ {- k} (- 1) ^ {k + 1}.</span></div>

    <p class="text-gray-300">As the Taylor elements of  <span class="math">H(\\gamma)</span>  are of alternating signs (recall  <span class="math">\\gamma &amp;lt; \\beta</span> ), it suffices to show the inequalities  <span class="math">H^{(k)}(\\beta) \\leq 0</span>  and  <span class="math">H^{(k)}(\\beta) \\geq 0</span>  for even and odd  <span class="math">k</span> 's respectively. It is sufficient to show that for all  <span class="math">k</span> :  <span class="math">\\int_0^\\infty t^k f(t)dt \\leq k!\\beta^{-k}</span> , which was proven in Lemma 14.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Proof (of Theorem 10). Let  <span class="math">\\tau_{n}</span>  be the waiting time for the  <span class="math">n</span> th lengthening of the main chain. Let  $f_{\\tau_n</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\tau_{n - 1},\\dots,\\tau_1}<span class="math">  be the conditional pdf of  </span>\\tau_{n}<span class="math">  given  </span>\\tau_{n - 1},\\ldots ,\\tau_{1}<span class="math"> . Denote  </span>\\beta \\coloneqq E[\\tau_n\\mid \\tau_{n - 1},\\dots,\\tau_1]^{-1}<span class="math">  for some given history (that is, for some realization of the  </span>\\tau_{i}<span class="math"> &#x27;s up to  </span>n - 1<span class="math"> ). By our assumption,  </span>\\beta \\geq \\beta_{1}<span class="math"> , and thus  </span>\\forall k\\in \\mathbb{N},\\beta^{-k}\\leq \\beta_{1}^{-k}$  (we will make use of this inequality later).</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">The random variable  <span class="math">\\tau_{n}</span>  given a history is nonnegative with increasing hazard rate. Indeed, when a node creates a new block it broadcasts it to the network, and as more and more nodes learn about it, more computational power is contributed to the effort of creating the next one and thereby lengthening the main chain. If meanwhile a conflicting block was created elsewhere, still more computational power is working on lengthening the main chain, just on a different version of it.</p>

    <p class="text-gray-300">The attacker's chain is built according to a Poisson process in the worst case, whose rate we denoted by  <span class="math">\\gamma</span> . Let  <span class="math">N_{2}</span>  be the event-count (random variable) of this process, namely,  <span class="math">N_{2}(t) \\coloneqq \\max \\{n \\mid \\sum_{j=1}^{n} \\tau_{n} \\leq t\\}</span> . Define,  <span class="math">X_{n} \\coloneqq n - N_{2}(\\sum_{j=1}^{n} \\tau_{n})</span> , and  <span class="math">Y_{n} \\coloneqq \\left(\\frac{\\gamma}{\\beta_{1}}\\right)^{X_{n}}</span> .</p>

    <p class="text-gray-300">The process  <span class="math">X = (X_{n})</span>  represents the gap between the lengths of the attacker's chain and the (worst-case) main chain, in favor of the latter, as the  <span class="math">n</span> th lengthening of the latter occurred.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">We claim that  <span class="math">Y = (Y_{n})</span>  is a super-martingale, namely that for any history,  <span class="math">E[Y_{n + 1} \\mid Y_n, \\dots, Y_0] \\leq Y_n</span> . Indeed, while the value of  <span class="math">X_{n + 1}</span>  depends naturally on  <span class="math">\\tau_{n + 1}, \\dots, \\tau_1</span> , the increment  <span class="math">X_{n + 1} - X_n</span>  given a history  <span class="math">\\tau_n, \\dots, \\tau_1</span>  is controlled by the random variable  <span class="math">\\tau_n</span>  given this history, with the pdf  $f_{\\tau_n</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\tau_{n - 1},\\dots,\\tau_1}<span class="math">  which we abbreviate  </span>f$ . We have:</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="my-4 text-center"><span class="math-block">E \\left[ Y _ {n + 1} \\mid Y _ {n},..., Y _ {0} \\right] = E \\left[ \\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {X _ {n + 1}} \\mid \\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {X _ {n}},..., \\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {X _ {0}} \\right] = \\tag {6}</span></div>

    <div class="my-4 text-center"><span class="math-block">\\sum_ {k = 0} ^ {\\infty} \\int_ {0} ^ {\\infty} f \\left(\\tau_ {n + 1}\\right) \\frac {e ^ {- \\gamma \\tau_ {n + 1}} \\left(\\gamma \\tau_ {n + 1}\\right) ^ {k}}{k !} \\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {X _ {n} + 1 - k} d \\tau_ {n + 1} = \\tag {7}</span></div>

    <div class="my-4 text-center"><span class="math-block">\\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {X _ {n} + 1} \\cdot \\sum_ {k = 0} ^ {\\infty} \\int_ {0} ^ {\\infty} f (\\tau_ {n + 1}) \\frac {e ^ {- \\gamma \\tau_ {n + 1}} \\left(\\gamma \\tau_ {n + 1}\\right) ^ {k}}{k !} \\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {- k} d \\tau_ {n + 1} =</span></div>

    <div class="my-4 text-center"><span class="math-block">\\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {X _ {n} + 1} \\cdot \\int_ {0} ^ {\\infty} f (\\tau_ {n + 1}) e ^ {- \\gamma \\tau_ {n + 1}} \\sum_ {k = 0} ^ {\\infty} \\frac {\\left(\\frac {\\gamma \\tau_ {n + 1}}{\\beta_ {1}}\\right) ^ {k}}{k !} d \\tau_ {n + 1} =</span></div>

    <div class="my-4 text-center"><span class="math-block">\\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {X _ {n} + 1} \\cdot \\int_ {0} ^ {\\infty} f (\\tau_ {n + 1}) e ^ {- \\gamma \\tau_ {n + 1}} e ^ {\\frac {1}{\\beta_ {1}} \\gamma \\tau_ {n + 1}} d \\tau_ {n + 1} =</span></div>

    <div class="my-4 text-center"><span class="math-block">\\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {X _ {n}} \\cdot \\frac {\\gamma}{\\beta_ {1}} \\cdot \\int_ {0} ^ {\\infty} f \\left(\\tau_ {n + 1}\\right) e ^ {\\gamma \\left(\\frac {1}{\\beta_ {1}} - 1\\right) \\tau_ {n + 1}} d \\tau_ {n + 1} \\leq \\tag {8}</span></div>

    <div class="my-4 text-center"><span class="math-block">\\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {X _ {n}} = Y _ {n}.</span></div>

    <p class="text-gray-300">Equality 6 is due to the attacker's chain advancing during the waiting time  <span class="math">\\tau_{n+1}</span>  according to a Poisson process with rate  <span class="math">\\tau_{n+1} \\cdot \\gamma</span> . In 7 we made explicit the fact that  <span class="math">\\left(\\frac{\\gamma}{\\beta_1}\\right)^{X_n}</span>  is a constant in the  <span class="math">\\sigma</span> -algebra corresponding to the natural filtration (usually denoted by  <span class="math">\\sigma(X_n, \\ldots, X_1)</span> ). Finally, as a corollary of Lemma 14,  <span class="math">E[\\tau_n^k \\mid \\tau_{n-1}, \\ldots, \\tau_1] \\leq k! \\beta^{-k} \\leq k! \\beta_1^{-k}</span> . Combining this with the end of Lemma 15's proof shows that  <span class="math">H_{\\gamma, \\beta_1}(\\frac{\\gamma}{\\beta_1}) \\leq 0</span> , hence 8.</p>

    <p class="text-gray-300">Let  <span class="math">x_{1} &amp;lt; X_{0} &amp;lt; x_{2}</span>  be some fixed constants, let the stopping time  <span class="math">\\pi</span>  be defined by  <span class="math">\\pi := \\min \\{n \\mid X_{n} \\leq x_{1} \\lor X_{n} \\geq x_{2}\\}</span> , and finally, define the event  <span class="math">E_{x_{1},x_{2}} := \\{X_{\\pi} = x_{2}\\}</span>  (i.e., "  <span class="math">X</span>  reached  <span class="math">x_{2}</span>  before it reached  <span class="math">x_{1}</span>  ). By Doob's Optional Stopping Theorem (See [19], p. 100-101) applied to the super martingale  <span class="math">Y</span> , we have,</p>

    <div class="my-4 text-center"><span class="math-block">\\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {X _ {0}} = Y _ {0} \\geq E [ Y _ {\\pi} ] =</span></div>

    <div class="my-4 text-center"><span class="math-block">P r \\left(E _ {x _ {1}, x _ {2}}\\right) \\cdot \\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {x _ {2}} + P r \\left(E _ {x _ {1}, x _ {2}} ^ {c}\\right) \\cdot \\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {x _ {1}} \\Longrightarrow</span></div>

    <div class="my-4 text-center"><span class="math-block">\\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {X _ {0}} - \\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {x _ {1}} \\geq P r (E _ {x _ {1}, x _ {2}}) \\cdot \\left(\\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {x _ {2}} - \\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {x _ {1}}\\right) \\Longrightarrow</span></div>

    <div class="my-4 text-center"><span class="math-block">P r (E _ {x _ {1}, x _ {2}}) \\geq \\frac {\\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {X _ {0}} - \\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {x _ {1}}}{\\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {x _ {2}} - \\left(\\frac {\\gamma}{\\beta_ {1}}\\right) ^ {x _ {1}}}.</span></div>

    <p class="text-gray-300">Taking <span class="math">x_{1} = -1</span> and <span class="math">x_{2} \\to \\infty</span> we obtain a lower bound on the probability that the gap between the chains will never reach minus 1: <span class="math">1 - \\left(\\frac{\\gamma}{\\beta_{1}}\\right)^{X_{0} + 1}</span>. The success probability of an attack is thus upper bounded by <span class="math">\\left(\\frac{\\gamma}{\\beta_{1}}\\right)^{X_{0} + 1}</span>.</p>

    <p class="text-gray-300">Note that an almost identical method shows that if the random variables <span class="math">\\tau_{n}</span> are i.i.d then there exists an <span class="math">a_0 \\leq \\frac{\\gamma}{\\beta_1}</span> such that <span class="math">Y := a_0^X</span> is a martingale.</p>`;
---

<BaseLayout title="Accelerating Bitcoin&#x27;s Transaction Processing. Fast Money Gr... (2013/881)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2013 &middot; eprint 2013/881
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <p class="mt-4 text-xs text-gray-500">
        All content below belongs to the original authors. This page
        reproduces the paper for educational purposes. Always
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >cite the original</a>.
      </p>
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <Fragment set:html={CONTENT} />

  </article>
</BaseLayout>
