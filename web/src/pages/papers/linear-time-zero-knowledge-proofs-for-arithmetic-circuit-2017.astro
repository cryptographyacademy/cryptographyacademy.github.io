---
import BaseLayout from '../../layouts/BaseLayout.astro';
import PaperDisclaimer from '../../components/PaperDisclaimer.astro';
import PaperHistory from '../../components/PaperHistory.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2017/872';
const CRAWLER = 'marker';
const CONVERTED_DATE = '2026-02-22';
const TITLE_HTML = 'Linear-Time Zero-Knowledge Proofs for Arithmetic Circuit Satisfiability∗';
const AUTHORS_HTML = 'Jonathan Bootle&lt;sup&gt;1&lt;/sup&gt; , Andrea Cerulli&lt;sup&gt;1&lt;/sup&gt; , Essam Ghadafi2† , Jens Groth&lt;sup&gt;1&lt;/sup&gt; , Mohammad Hajiabadi3† , and Sune K. Jakobsen&lt;sup&gt;1&lt;/sup&gt;';

const CONTENT = `    <p class="text-gray-300">Jonathan Bootle&lt;sup&gt;1&lt;/sup&gt; , Andrea Cerulli&lt;sup&gt;1&lt;/sup&gt; , Essam Ghadafi2† , Jens Groth&lt;sup&gt;1&lt;/sup&gt; , Mohammad Hajiabadi3† , and Sune K. Jakobsen&lt;sup&gt;1&lt;/sup&gt;</p>

    <p class="text-gray-300">&lt;sup&gt;1&lt;/sup&gt; University College London, London, UK {jonathan.bootle.14,andrea.cerulli.13,j.groth,s.jakobsen}@ucl.ac.uk</p>

    <blockquote class="border-l-4 border-gray-600 pl-4 my-4 text-gray-400 italic">
    <p class="text-gray-300">&lt;sup&gt;2&lt;/sup&gt; University of the West of England, Bristol, UK essam.ghadafi@uwe.ac.uk</p>

    </blockquote>

    <p class="text-gray-300">&lt;sup&gt;3&lt;/sup&gt; University of California, Berkeley, CA, USA mdhajiabadi@berkeley.edu</p>

    <p class="text-gray-300">Abstract. We give computationally efficient zero-knowledge proofs of knowledge for arithmetic circuit satisfiability over a large field. For a circuit with N addition and multiplication gates, the prover only uses O(N) multiplications and the verifier only uses O(N) additions in the field. If the commitments we use are statistically binding, our zero-knowledge proofs have unconditional soundness, while if the commitments are statistically hiding we get computational soundness. Our zero-knowledge proofs also have sub-linear communication if the commitment scheme is compact.</p>

    <p class="text-gray-300">Our construction proceeds in three steps. First, we give a zero-knowledge proof for arithmetic circuit satisfiability in an ideal linear commitment model where the prover may commit to secret vectors of field elements, and the verifier can receive certified linear combinations of those vectors. Second, we show that the ideal linear commitment proof can be instantiated using error-correcting codes and non-interactive commitments. Finally, by choosing efficient instantiations of the primitives we obtain linear-time zero-knowledge proofs.</p>

    <p class="text-gray-300">Keywords. Zero-knowledge, arithmetic circuit, ideal linear commitments.</p>

    <h3 id="sec-1" class="text-xl font-semibold mt-8">1 Introduction</h3>

    <p class="text-gray-300">A zero-knowledge proof [GMR85] is a protocol between two parties: a prover and a verifier. The prover wants to convince the verifier that an instance u belongs</p>

    <p class="text-gray-300">&lt;sup&gt;∗&lt;/sup&gt;The research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Programme (FP/2007- 2013) / ERC Grant Agreement n. 307937.</p>

    <p class="text-gray-300">&lt;sup&gt;†&lt;/sup&gt;Part of the work was done while at University College London.</p>

    <p class="text-gray-300">to a specific language L&lt;sup&gt;R&lt;/sup&gt; in NP. She has a witness w such that (u, w) belongs to the NP relation R defining the language, but wants to convince the verifier that the statement u ∈ L&lt;sup&gt;R&lt;/sup&gt; is true without revealing the witness or any other confidential information.</p>

    <p class="text-gray-300">Zero-knowledge proofs are widely used in cryptography since it is often useful to verify that a party is following a protocol without requiring her to divulge secret keys or other private information. Applications range from digital signatures and public-key encryption to secure multi-party computation and verifiable cloud computing.</p>

    <p class="text-gray-300">Efficiency is crucial for large and complex statements such as those that may arise in the latter applications. Important efficiency parameters include the time complexity of the prover, the time complexity of the verifier, the amount of communication measured in bits, and the number of rounds the prover and verifier need to interact. Three decades of research on zero-knowledge proofs have gone into optimizing these efficiency parameters and many insights have been learned.</p>

    <p class="text-gray-300">For zero-knowledge proofs with unconditional soundness where it impossible for any cheating prover to convince the verifier of a false statement, it is possible to reduce communication to the witness size [IKOS09, KR08, GGI+14]. For zeroknowledge arguments where it is just computationally intractable for the prover to cheat the verifier we can do even better and get sub-linear communication complexity [Kil92].</p>

    <p class="text-gray-300">There are many constant-round zero-knowledge proofs and arguments, for instance Bellare, Jakobsson and Yung [BJY97] construct four round arguments based on one-way functions. In the common reference string model, it is even possible to give non-interactive proofs where the prover computes a convincing zero-knowledge proof directly without receiving any messages from the verifier [BFM88].</p>

    <p class="text-gray-300">The verifier computation is in general at least proportional to the instance size because the verifier must read the entire instance in order to verify it. However, the verifier computation can be sub-linear in the time it takes for the relation to verify a witness for the statement being true [GKR08], which is useful in both zero-knowledge proofs and verifiable computation.</p>

    <p class="text-gray-300">Having reduced the cost of many other efficiency parameters, today the major bottleneck is the prover's computation. Classical number-theoretic constructions for circuit satisfiability such as [CD98] require a linear number of exponentiations, i.e., the cost is O(λN) group multiplications where N is the number of gates and λ is a security parameter. Relying on different techniques and underlying cryptography [DIK10] has reduced the computational overhead further to being O(log(λ)). This leaves a tantalizing open question of whether we can come all the way down to constant overhead O(1), i.e., make the prover's cost within a constant factor of the time it takes to verify (u, w) ∈ R directly.</p>

    <h3 id="sec-2" class="text-xl font-semibold mt-8">1.1 Our Contributions</h3>

    <p class="text-gray-300">We construct zero-knowledge proofs of knowledge for the satisfiability of arithmetic circuits. An instance is an arithmetic circuits with N fan-in 2 addition and multiplication gates over a finite field F and a specification of the values of some of the wires. A witness consists of the remaining wires such that the values are consistent with the gates and the wire values specified in the instance.</p>

    <p class="text-gray-300">Our zero-knowledge proofs are highly efficient asymptotically:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>Prover time is O(N) field additions and multiplications.</li>
      <li>Verifier time is O(N) field additions.</li>
    </ul>

    <p class="text-gray-300">This is optimal up to a constant factor for both the prover and verifier. The prover only incurs a constant overhead compared to the time needed to evaluate the circuit from scratch given an instance and a witness, and for instances of size equivalent to Ω(N) field elements the verifier only incurs a constant overhead compared to the time it takes to read the instance. The constants are large, so we do not recommend implementing the zero-knowledge proof as it is, but from a theoretical perspective we consider it a big step forward to get constant overhead for both prover and verifier.</p>

    <p class="text-gray-300">Our zero-knowledge proofs have perfect completeness, i.e., when the prover knows a satisfactory witness she is always able to convince the verifier. Our constructions are proofs of knowledge, that is, not only does the prover demonstrate the statement is true but also that she knows a witness. The proofs have special honest-verifier zero-knowledge, which means that given a set of verifier challenges it is possible to simulate a proof answering the challenges without knowing a witness. The flavour of knowledge soundness and special honest-verifier zero-knowledge depends on the underlying commitment scheme we use. When instantiated with statistically binding commitment schemes, we obtain proofs (statistically knowledge sound) with computational zero-knowledge. When we use statistically hiding commitments we obtain arguments of knowledge with statistical special honest verifier zero-knowledge. The communication complexity of our proofs with unconditional soundness is only O(N) field elements, while our arguments with computational soundness have sub-linear communication of poly(λ) √ N field elements when the commitments are compact. Round complexity is also low, when we optimize for computational efficiency for prover and verifier we only use O(log log N) rounds.</p>

    <h3 id="sec-3" class="text-xl font-semibold mt-8">1.2 Construction and Techniques</h3>

    <p class="text-gray-300">Our construction is modular and consists of three steps. First, we construct a proof in a communication model we call the Ideal Linear Commitment (ILC) channel. In the ILC model, the prover can commit vectors of secret field elements to the channel. The verifier may later query openings to linear combinations of the committed vectors, which the channel will answer directly. We show that idealizing the techniques by Groth et al. [Gro09, BCC&lt;sup&gt;+&lt;/sup&gt;16] gives us efficient proofs in the ideal linear commitment model. By optimizing primarily for prover computation and secondarily for round efficiency, we get a round complexity of O(log log N) rounds, which is better than the O(log N) rounds of Bootle et al. [BCC+16] that optimized for communication complexity.</p>

    <p class="text-gray-300">Next, we compile proofs in the ILC model into proof and argument systems using non-interactive commitment schemes; however, unlike previous works we do not commit directly to the vectors. Instead, we encode the vectors as randomized codewords using a linear error-correcting code. We now consider the codewords as rows of a matrix and commit to the columns of that matrix. When the verifier asks for a linear combination of the vectors, the prover simply tells the verifier what the linear combination is. However, the verifier does not have to have blind confidence in the prover because she can ask for openings of some of the committed columns and use them to spot check that the resulting codeword is correct.</p>

    <p class="text-gray-300">Finally, we instantiate the scheme with concrete error-correcting codes and non-interactive commitment schemes. We use the error-correcting codes of Druk and Ishai [DI14], which allow the encoding of k field elements using O(k) additions in the field. Statistically hiding commitment schemes can be constructed from collision-resistant hash functions, and using the recent hash functions of Applebaum et al. [AHI+17] we can hash t field elements at a cost equivalent to O(t) field additions. Statistically binding commitment schemes on the other hand can be built from pseudorandom number generators. Using the lineartime computable pseudorandom number generators of Ishai et al. [IKOS08] we get linear-time computable statistically binding commitments. Plugging either of the commitment schemes into our construction yields zero-knowledge proofs with linear-time computation for both prover and verifier.</p>

    <h3 id="sec-4" class="text-xl font-semibold mt-8">1.3 Related Work</h3>

    <p class="text-gray-300">There is a rich body of research on zero-knowledge proofs. Early practical zeroknowledge proofs such as Schnorr [Sch91] and Guillou-Quisquater [GQ88] used number-theoretic assumptions. There have been several works extending these results to prove more general statements [CDS94, CD98, Gro09, BCC+16] with the latter giving discrete-logarithm based arguments for arithmetic circuit satisfiability with logarithmic communication complexity and a linear number of exponentiations for the prover, i.e., a computation cost of O(λN) group multiplications for λ-bit exponents and a circuit with N multiplication gates.</p>

    <p class="text-gray-300">Ishai et al. [IKOS09] showed how to use secure multi-party computation (MPC) protocols to construct zero-knowledge proofs. The intuition behind this generic construction is that the prover first executes in her head an MPC protocol for computing a circuit verifying some relation R and then commits to the views of all the virtual parties. The verifier asks the prover to open a subset of those views and then verifies their correctness and consistency with each other. Soundness and zero-knowledge follow from robustness and privacy of the MPC protocol. Applying this framework to efficient MPCs gives asymptotically efficient zero-knowledge proofs. For example, the perfectly secure MPC of [DI06] is used in [IKOS09] to obtain zero-knowledge proofs for the satisfiability of Boolean circuits with communication linear in the circuit size, O(N), and a computational cost of Ω(λN), for circuits of size N and security parameter λ. Damg˚ard et al. [DIK10] used the MPC framework to construct zero-knowledge proofs for the satisfiability of arithmetic circuits. Their construction has more balanced efficiency and achieves O(polylog(λ)N) complexity for both computation and communication.</p>

    <p class="text-gray-300">Jawurek et al. [JKO13] gave a very different approach to building zeroknowledge proofs based on garbled circuits. Their approach proved [FNO15, CGM16] to be very efficient in practice for constructing proofs for languages represented as Boolean circuits. These techniques are appealing for proving small statements as they require only a constant number of symmetric-key operations per gate, while the main bottleneck is in their communication complexity. Asymptotically, this approach yields computational and communication complexity of O(λN) bit operations and bits, respectively, when λ is the cost of a single symmetric-key operation. Recently, these techniques found applications in zero-knowledge proofs for checking the execution of RAM programs [HMR15, MRS17]. For instances that can be represented as RAM programs terminating in T steps and using memory of size M, these zero-knowledge proofs yield communication and computation with polylog(M) overhead compared to the running time T of the RAM program.</p>

    <p class="text-gray-300">Cramer et al. [CDP12] introduce zero-knowledge proofs for verifying multiplicative relations of committed values using techniques related to ours. When applied to zero-knowledge proofs for the satisfiability of Boolean circuits, the asymptotic communication and computation complexities of [CDP12] are close to [IKOS09], although the constants are smaller. Unlike [CDP12], we do not require any homomorphic property from the commitment scheme, and instead of relying on linear secret sharing schemes with product reconstruction, we use linear error-correcting codes.</p>

    <p class="text-gray-300">In past years, a lot of attention has been dedicated to the study of succinct non-interactive arguments of knowledge (SNARKs) [Gro10, BCCT12, GGPR13, BCCT13, PHGR13, BCG+13, Gro16]. These are very compact arguments offering very efficient verification time. In the most efficient cases, the arguments consist of only a constant number of group elements and verification consists of a constant number of pairings and a number of group exponentiations that is linear in the instance size but independent of the witness size. The main bottleneck of these arguments is the computational complexity of the prover which requires O(N) group exponentiations.</p>

    <p class="text-gray-300">Recently, Ben-Sasson, Chiesa and Spooner [BSCS16] proposed the notion of interactive oracle proofs (IOPs), which are interactive protocols where the prover may send a probabilisticaly checkable proof (PCP) in each round. Ben-Sasson et al. [BSCG&lt;sup&gt;+&lt;/sup&gt;16] construct a 3-round public-coin IOP (with soundness error 1/2) for Boolean circuit satisfiability with linear proof length and quasi-linear running times for both the prover and the verifier. Moreover, the constructed IOP has constant query complexity (the number of opening queries requested by the verifier), while prior PCP constructions require sub-linear query complexity. Another follow-up work by Ben-Sasson et al. [BSCGV16] gives 2-round zero-knowledge IOPs (duplex PCPs) for any language in NTIME(T(n)) with quasi-linear prover computation in n + T(n).</p>

    <p class="text-gray-300">Efficiency Comparison. All the proofs we list above have super-linear cost for the prover. This means our zero-knowledge proofs are the most efficient zero-knowledge proofs for arithmetic circuits for the prover. We also know that our verification time is optimal for an instance of size  <span class="math">\\Omega(N)</span>  field elements since the verification time is comparable to the time it takes just to read the instance.</p>

    <p class="text-gray-300">Another well-studied class of languages is Boolean circuit satisfiability but here our techniques do not fare as well since there would be an overhead in representing bits as field elements. We therefore want to make clear that our claim of high efficiency and a significant performance improvement over the state of the art relates only to arithmetic circuits. Nevertheless, we find the linear cost for arithmetic circuits a significant result in itself. This is the first time for any general class of NP-complete language that true linear cost is achieved for the prover when compared to the time it takes to evaluate the statement directly given the prover's witness.</p>

    <h3 id="sec-5" class="text-xl font-semibold mt-8">2 Preliminaries</h3>

    <h3 id="sec-6" class="text-xl font-semibold mt-8">2.1 Notation and Computational Model</h3>

    <p class="text-gray-300">We write  <span class="math">y \\leftarrow A(x)</span>  for an algorithm outputting y on input x. When the algorithm is randomized, and we wish to explicitly refer to a particular choice of random coins r chosen by the algorithm, we write  <span class="math">y \\leftarrow A(x;r)</span> . We write PPT/DPT for algorithms running in probabilistic polynomial time and deterministic polynomial time in the size of their inputs. Typically, the size of inputs and output will be polynomial in a security parameter  <span class="math">\\lambda</span> , with the intention that larger  <span class="math">\\lambda</span>  means better security. For functions  <span class="math">f,g:\\mathbb{N}\\to[0,1]</span> , we write  <span class="math">f(\\lambda)\\approx g(\\lambda)</span>  if  <span class="math">|f(\\lambda)-g(\\lambda)|=\\frac{1}{\\lambda^{\\omega(1)}}</span> . We say a function f is overwhelming if  <span class="math">f(\\lambda)\\approx 1</span>  and f is negligible if  <span class="math">f(\\lambda)\\approx 0</span> .</p>

    <p class="text-gray-300">Throughout the paper, we will be working over a finite field  <span class="math">\\mathbb F</span> . To get negligible risk of an adversary breaking our zero-knowledge proofs, we need the field to be large enough such that  <span class="math">\\log |\\mathbb F| = \\omega(\\lambda)</span> . When considering efficiency of our zero-knowledge proofs, we will assume the prover and verifier are RAM machines where operations on W-bit words have unit cost. We assume a field element is represented by  <span class="math">\\mathcal{O}(\\frac{\\log |\\mathbb F|}{W})</span>  words and that additions in  <span class="math">\\mathbb F</span>  carry a cost of  <span class="math">\\mathcal{O}\\left(\\frac{\\log |\\mathbb F|}{W}\\right)</span>  machine operations. We expect multiplications to be efficiently computable as well but at a higher cost of  <span class="math">\\omega\\left(\\frac{\\log |\\mathbb F|}{W}\\right)</span>  machine operations.</p>

    <p class="text-gray-300">For a positive integer n, [n] denotes the set  <span class="math">\\{1,\\ldots,n\\}</span> . We use bold letters such as  <span class="math">\\boldsymbol{v}</span>  for row vectors. For  <span class="math">\\boldsymbol{v} \\in \\mathbb{F}^n</span>  and a set  <span class="math">J = \\{j_1,\\ldots,j_k\\} \\subset [n]</span>  with  <span class="math">j_1 &lt; \\cdots &lt; j_k</span>  we define the vector  <span class="math">\\boldsymbol{v}|_J</span>  to be  <span class="math">(\\boldsymbol{v}_{j_1},\\ldots,\\boldsymbol{v}_{j_k})</span> . Similarly, for a matrix  <span class="math">V \\in \\mathbb{F}^{m \\times n}</span>  we let  <span class="math">V|_J \\in \\mathbb{F}^{m \\times k}</span>  be the submatrix of V restricted to the columns indicated in J.</p>

    <h4 id="sec-7" class="text-lg font-semibold mt-6">2.2 Proofs of Knowledge</h4>

    <p class="text-gray-300">A proof system is defined by a triple of stateful PPT algorithms (K,P, V), which we call the setup generator, the prover and verifier, respectively. The setup generator K creates public parameters pp that will be used by the prover and the verifier. We think of pp as being honestly generated, however, in the proofs we construct it consists of parts that are either publicly verifiable or could be generated by the verifier, so we use the public parameter model purely for simplicity and efficiency of our proofs, not for security.</p>

    <p class="text-gray-300">The prover and verifier communicate with each other through a communication channel chan ←→. When P and V interact on inputs s and t through a communication channel chan ←→ we let view&lt;sup&gt;V&lt;/sup&gt; ← hP(s) chan ←→ V(t)i be the view of the verifier in the execution, i.e., all inputs he gets including random coins and let trans&lt;sup&gt;P&lt;/sup&gt; ← hP(s) chan ←→ V(t)i denote the transcript of the communication between prover and channel. This overloads the notation ← hP(s) chan ←→ V(t)i but it will always be clear from the variable name if we get the verifier's view or the prover's transcript. At the end of the interaction the verifier accepts or rejects. We write hP(s) chan ←→ V(t)i = b depending on whether the verifier rejects (b = 0) or accepts (b = 1).</p>

    <p class="text-gray-300">In the standard channel ←→, all messages are forwarded between prover and verifier. We also consider an ideal linear commitment channel, ILC ←→, or simply ILC, described in Figure 1. When using the ILC channel, the prover can submit a commit command to commit to vectors of field elements of some fixed length k, specified in ppILC. The vectors remain secretly stored in the channel, and will not be forwarded to the verifier. Instead, the verifier only learns how many vectors the prover has committed to. The verifier can submit a send command to the ILC to send field elements to the prover. In addition, the verifier can also submit open queries to the ILC for obtaining the opening of any linear combinations of the vectors sent by the prover. We stress that the verifier can request several linear combinations within a single open query, as depicted in Figure 1.</p>

    <p class="text-gray-300">In a proof system over the ILC channel, sequences of commit, send and open queries could alternate in an arbitrary order. We call a proof system over the ILC channel non-adaptive if the verifier only makes one open query to the ILC channel before terminating his interaction with the channel, otherwise we call it adaptive. Although adaptive proof systems are allowed by the model, in this paper we will only consider non-adaptive ILC proof systems to simplify the exposition.</p>

    <p class="text-gray-300">We remark that ILC proof systems are different from linear interactive proofs considered in [BCI&lt;sup&gt;+&lt;/sup&gt;13]. In linear interactive proofs both the prover and verifier send vectors of field elements, but the prover can only send linear (or affine) transformations of the verifier's previously sent vectors. However, for our constructions it is important that the prover can compute on field elements received by the verifier and for instance evaluate polynomials.</p>

    <p class="text-gray-300">We say a proof system is public coin if the verifier's messages to the communication channel are chosen uniformly at random and independently of the</p>

    <p class="text-gray-300">    <img src="_page_7_Figure_1.jpeg" alt="" class="my-4 max-w-full" />
</p>

    <p class="text-gray-300">Fig. 1: Description of the ILC channel.</p>

    <p class="text-gray-300">actions of the prover, i.e., the verifier's messages to the prover correspond to the verifier's randomness  <span class="math">\\rho</span> .</p>

    <p class="text-gray-300">We will consider relations  <span class="math">\\mathcal{R}</span>  consisting of tuples (pp, u, w), and define  <span class="math">\\mathcal{L}_{\\mathcal{R}} = \\{(pp, u) | \\exists w : (pp, u, w) \\in \\mathcal{R}\\}</span> . We refer to u as the <em>instance</em> and w as the <em>witness</em> that  <span class="math">(pp, u) \\in \\mathcal{L}_{\\mathcal{R}}</span> . The <em>public parameter pp</em> will specify the security parameter  <span class="math">\\lambda</span> , perhaps implicitly through its length, and may also contain other parameters used for specifying the specific relation, e.g. a description of a field. Typically, pp will also contain parameters that do not influence membership of  <span class="math">\\mathcal{R}</span>  but may aid the prover and verifier, for instance, a description of an encoding function that they will use.</p>

    <p class="text-gray-300">We will construct SHVZK proofs of knowledge for the relation  <span class="math">\\mathcal{R}_{\\mathsf{AC}}</span> , where the instances are arithmetic circuits over a field  <span class="math">\\mathbb{F}</span>  specified by pp. An instance consists of many fan-in 2 addition and multiplication gates over  <span class="math">\\mathbb{F}</span> , a description of how wires in the circuit connect to the gates, and values assigned to some of the input wires. Witnesses w are the remaining inputs such that the output of the circuit is 0. For an exact definition of how we represent an arithmetic circuit, see Section 3. We would like to stress the fact that the wiring of the circuit is part of the instance and we allow a fully adaptive choice of the arithmetic circuit. This stands in contrast to pairing-based SNARKs that usually only consider circuits with fixed wires, i.e., the arithmetic circuit is partially non-adaptive, and getting full adaptivity through a universal circuit incurs an extra efficiency overhead.</p>

    <p class="text-gray-300">The protocol  <span class="math">(\\mathcal{K}, \\mathcal{P}, \\mathcal{V})</span>  is called a <em>proof of knowledge</em> over communication channel  <span class="math">\\stackrel{\\text{chan}}{\\longleftrightarrow}</span>  for relation  <span class="math">\\mathcal{R}</span>  if it has perfect completeness and computational knowledge soundness as defined below.</p>

    <p class="text-gray-300"><strong>Definition 1 (Perfect Completeness).</strong> The proof is perfectly complete if for all PPT adversaries A</p>

    <p class="text-gray-300"><span class="math">$\\Pr\\left[\\begin{array}{c} pp \\leftarrow \\mathcal{K}(1^{\\lambda}); (u,w) \\leftarrow \\mathcal{A}(pp): \\\\ (pp,u,w) \\notin \\mathcal{R} \\ \\lor \\ \\langle \\mathcal{P}(pp,u,w) \\stackrel{\\text{chan}}{\\longleftrightarrow} \\mathcal{V}(pp,u) \\rangle = 1 \\end{array}\\right] = 1.</span>$</p>

    <p class="text-gray-300"><strong>Definition 2 (Knowledge soundness).</strong> A public-coin proof system has computational (strong black-box) knowledge soundness if for all  <span class="math">DPT\\mathcal{P}^*</span>  there exists an expected PPT extractor  <span class="math">\\mathcal{E}</span>  such that for all PPT adversaries  <span class="math">\\mathcal{A}</span></p>

    <p class="text-gray-300"><span class="math">$\\Pr\\left[\\begin{array}{c} pp \\leftarrow \\mathcal{K}(1^{\\lambda}); (u,s) \\leftarrow \\mathcal{A}(pp); w \\leftarrow \\mathcal{E}^{\\langle \\mathcal{P}^*(s) \\overset{\\text{chan}}{\\longleftrightarrow} \\mathcal{V}(pp,u) \\rangle}(pp,u) : \\\\ b = 1 \\ \\land \\ (pp,u,w) \\notin \\mathcal{R} \\end{array}\\right] \\approx 0.</span>$</p>

    <p class="text-gray-300">Here the oracle  <span class="math">\\langle \\mathcal{P}^*(s) \\overset{\\text{chan}}{\\longleftrightarrow} \\mathcal{V}(pp,u) \\rangle</span>  runs a full protocol execution and if the proof is successful it returns a transcript of the prover's communication with the channel. The extractor  <span class="math">\\mathcal{E}</span>  can ask the oracle to rewind the proof to any point in a previous transcript and execute the proof again from this point on with fresh public-coin challenges from the verifier. We define  <span class="math">b \\in \\{0,1\\}</span>  to be the verifier's output in the first oracle execution, i.e., whether it accepts or not, and we think of s as the state of the prover. The definition can then be paraphrased as saying that if the prover in state s makes a convincing proof, then we can extract a witness.</p>

    <p class="text-gray-300">If the definition holds also for unbounded  <span class="math">\\mathcal{P}^*</span>  and  <span class="math">\\mathcal{A}</span>  we say the proof has statistical knowledge soundness.</p>

    <p class="text-gray-300">If the definition of knowledge soundness holds for a non-rewinding extractor, i.e., a single transcript of the prover's communication with the communication channel suffices, we say the proof system has knowledge soundness with straight-line extraction.</p>

    <p class="text-gray-300">We will construct public-coin proofs that have special honest-verifier zero-knowledge. This means that if the verifier's challenges are known, or even adversarially chosen, then it is possible to simulate the verifier's view without the witness. In other words, the simulator works for verifiers who may use adversarial coins in choosing their challenges but they follow the specification of the protocol as an honest verifier would.</p>

    <p class="text-gray-300"><strong>Definition 3 (Special Honest-Verifier Zero-Knowledge).</strong> The proof of knowledge is computationally special honest-verifier zero-knowledge (SHVZK) if there exists a PPT simulator S such that for all stateful interactive PPT adversaries A that output (u, w) such that  <span class="math">(pp, u, w) \\in R</span>  and randomness  <span class="math">\\rho</span>  for the verifier</p>

    <p class="text-gray-300"><span class="math">$\\begin{split} &amp; \\Pr \\left[ \\begin{array}{c} pp \\leftarrow \\mathcal{K}(1^{\\lambda}); (u, w, \\rho) \\leftarrow \\mathcal{A}(pp); \\\\ \\mathsf{view}_{\\mathcal{V}} \\leftarrow \\langle \\mathcal{P}(pp, u, w) \\overset{\\mathrm{chan}}{\\longleftrightarrow} \\mathcal{V}(pp, u; \\rho) \\rangle : \\mathcal{A}(\\mathsf{view}_{\\mathcal{V}}) = 1 \\end{array} \\right] \\\\ \\approx &amp; \\Pr \\left[ pp \\leftarrow \\mathcal{K}(1^{\\lambda}); (u, w, \\rho) \\leftarrow \\mathcal{A}(pp); \\mathsf{view}_{\\mathcal{V}} \\leftarrow \\mathcal{S}(pp, u, \\rho) : \\mathcal{A}(\\mathsf{view}_{\\mathcal{V}}) = 1 \\right]. \\end{split}</span>$</p>

    <p class="text-gray-300">We say the proof is statistically SHVZK if the definition holds also against unbounded adversaries, and we say the proof is perfect SHVZK if the probabilities are exactly equal.</p>

    <p class="text-gray-300">From Honest-Verifier to General Zero-Knowledge. Honest-verifier zeroknowledge only guarantees the simulator works for verifiers following the proof system specifications. It might be desirable to consider general zero-knowledge where the simulator works for arbitrary malicious verifiers that may deviate from the specification of the proof. However, honest-verifier zero-knowledge is a first natural stepping stone to get efficient zero-knowledge proofs. We recall that our proofs are public coin, which means that the verifier's messages are chosen uniformly at random and independently from the messages received from the verifier. Below we recall few options to obtain general zero-knowledge proofs from a public-coin SHVZK proof. All these transformations are very efficient in terms of computation and communication such that the efficiency properties of our special honest-verifier zero-knowledge protocols are preserved.</p>

    <p class="text-gray-300">In the Fiat-Shamir transform [FS86] the verifier's challenges are computed using a cryptographic hash function applied to the transcript up to the challenge. The Fiat-Shamir transform is more generally used to turn a public-coin proof into a non-interactive one. Since interaction with the verifier is no longer needed, general zero-knowledge is immediately achieved. If the hash function can be computed in linear time in the input size, then the Fiat-Shamir transform only incurs an additive linear overhead in computation for the prover and verifier. The drawback of the Fiat-Shamir transform is that security is usually proved in the random oracle model [BR93] where the hash function is modelled as an ideal random function.</p>

    <p class="text-gray-300">Assuming a common reference string and relying on trapdoor commitments, Damg˚ard [Dam00] gave a transformation yielding concurrently secure protocols for Σ-Protocols. The transformation can be optimized [Gro04] using the idea that for each public-coin challenge x, the prover first commits to a value x 0 , then the verifier sends a value x &lt;sup&gt;00&lt;/sup&gt;, after which the prover opens the commitment and uses the challenge x = x &lt;sup&gt;0&lt;/sup&gt; + x &lt;sup&gt;00&lt;/sup&gt;. The coin-flipping can be interleaved with the rest of the proof, which means the transformation preserves the number of rounds and only incurs a very small efficiency cost to do the coin-flipping for the challenges.</p>

    <p class="text-gray-300">If one does not wish to rely on a common reference string for security, one can use a private-coin transformation where the verifier does not reveal the random coins used to generate the challenges sent to the prover (hence the final protocol is no longer public coin). One example is the Micciancio and Petrank [MP03] transformation (yielding concurrently secure protocols) while incurring a small overhead of ω(log λ) with respect to the number of rounds as well as the computational and communication cost in each round. The transformation preserves the soundness and completeness errors of the original protocol; however, it does not preserve statistical zero-knowledge as the obtained protocol only has computational zero-knowledge.</p>

    <p class="text-gray-300">There are other public-coin transformations to general zero-knowledge e.g. Goldreich et al. [GSV98]. The transformation relies on a random-selection protocol between the prover and verifier to specify a set of messages and restricting the verifier to choose challenges from this set. This means to get negligible soundness error these transformations require  <span class="math">\\omega(1)</span>  sequential repetitions so the round complexity goes up.</p>

    <h4 id="sec-8" class="text-lg font-semibold mt-6">2.3 Linear-Time Linear Error-Correcting Codes</h4>

    <p class="text-gray-300">A code over an alphabet  <span class="math">\\Sigma</span>  is a subset  <span class="math">\\mathcal{C} \\subseteq \\Sigma^n</span> . A code  <span class="math">\\mathcal{C}</span>  is associated with an encoding function  <span class="math">E_{\\mathcal{C}}: \\Sigma^k \\to \\Sigma^n</span>  mapping messages of length k into codewords of length n. We assume there is a setup algorithm  <span class="math">\\operatorname{Gen}_{\\mathsf{E}_{\\mathcal{C}}}</span>  which takes as input a finite field  <span class="math">\\mathbb{F}</span>  and the parameter  <span class="math">k \\in \\mathbb{N}</span> , and outputs an encoding function  <span class="math">E_{\\mathcal{C}}</span> .</p>

    <p class="text-gray-300">In what follows, we restrict our attention to  <span class="math">\\mathbb{F}</span> -linear codes for which the alphabet is a finite field  <span class="math">\\mathbb{F}</span> , the code  <span class="math">\\mathcal{C}</span>  is a k-dimensional linear subspace of  <span class="math">\\mathbb{F}^n</span> , and  <span class="math">E_{\\mathcal{C}}</span>  is an  <span class="math">\\mathbb{F}</span> -linear map. The rate of the code is defined to be  <span class="math">\\frac{k}{n}</span> . The Hamming distance between two vectors  <span class="math">\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{F}^n</span>  is denoted by  <span class="math">\\mathsf{hd}(\\boldsymbol{x}, \\boldsymbol{y})</span>  and corresponds to the number of coordinates in which  <span class="math">\\boldsymbol{x}, \\boldsymbol{y}</span>  differ. The (minimum) distance of a code is defined to be the minimum Hamming distance  <span class="math">\\mathsf{hd}_{\\min}</span>  between distinct codewords in  <span class="math">\\mathcal{C}</span> . We denote by  <span class="math">[n, k, \\mathsf{hd}_{\\min}]_{\\mathbb{F}}</span>  a linear code over  <span class="math">\\mathbb{F}</span>  with length n, dimension k and minimum distance  <span class="math">\\mathsf{hd}_{\\min}</span> . The Hamming weight of a vector  <span class="math">\\boldsymbol{x}</span>  is  <span class="math">\\mathsf{wt}(\\boldsymbol{x}) = |\\{i \\in [n] : \\boldsymbol{x}_i \\neq 0\\}|</span> .</p>

    <p class="text-gray-300">In the next sections, we will use families of linear codes achieving asymptotically good parameters. More precisely, we require codes with linear length,  <span class="math">n = \\Theta(k)</span> , and linear distance,  <span class="math">\\mathsf{hd}_{\\min} = \\Theta(k)</span> , in the dimension k of the code. We recall that random linear codes achieve with high probability the best trade-off between distance and rate. However, in this work we are particularly concerned with computational efficiency of the encoding procedure and random codes are not known to be very efficient.</p>

    <p class="text-gray-300">To obtain zero-knowledge proofs and arguments with linear cost for prover and verifier, we need to use codes that can be encoded in linear time. Starting from the seminal work of Spielman [Spi95], there has been a rich stream of research [GI01, GI02, GI03, GI05, DI14, CDD <span class="math">^+</span> 16] regarding linear codes with linear-time encoding. Our construction can be instantiated, for example, with one of the families of codes presented by Druk and Ishai [DI14]. These are defined over a generic finite field  <span class="math">\\mathbb F</span>  and meets all the above requirements.</p>

    <p class="text-gray-300"><strong>Theorem 1</strong> ([DI14]). There exist constants  <span class="math">c_1 &gt; 1</span>  and  <span class="math">c_2 &gt; 0</span>  such that for every finite field  <span class="math">\\mathbb{F}</span>  there exists a family of  <span class="math">[\\lceil c_1 k \\rceil, k, \\lfloor c_2 k \\rfloor]_{\\mathbb{F}}</span>  linear codes, which can be encoded by a uniform family of linear-size arithmetic circuit of addition gates.</p>

    <h4 id="sec-9" class="text-lg font-semibold mt-6">2.4 Commitment Schemes</h4>

    <p class="text-gray-300">A non-interactive commitment scheme allows a sender to commit to a secret message and later reveal the message in a verifiable way. Here we are interested in commitment schemes that take as input an arbitrary length message so the message space is  <span class="math">\\{0,1\\}^*</span> . A commitment scheme is defined by a pair of PPT algorithms (Setup, Commit).</p>

    <p class="text-gray-300">Setup <span class="math">(1^{\\lambda}) \\to ck</span> : Given a security parameter, this returns a commitment key ck.</p>

    <p class="text-gray-300">Commit&lt;sub&gt;ck&lt;/sub&gt; <span class="math">(m) \\to c</span> : Given a message m, this picks a randomness  <span class="math">r \\leftarrow \\{0,1\\}^{\\text{poly}(\\lambda)}</span>  and computes a commitment  <span class="math">c = \\mathsf{Commit}_{ck}(m;r)</span> .</p>

    <p class="text-gray-300">A commitment scheme must be <em>binding</em> and <em>hiding</em>. The binding property means that it is infeasible to open a commitment to two different messages, whereas the hiding property means that the commitment does not reveal anything about the committed message.</p>

    <p class="text-gray-300"><strong>Definition 4 (Binding).</strong> A commitment scheme is computationally binding if for all PPT adversaries A</p>

    <p class="text-gray-300"><span class="math">$\\Pr\\left[\\begin{array}{c} ck \\leftarrow \\mathsf{Setup}(1^\\lambda); \\ (m_0, r_0, m_1, r_1) \\leftarrow \\mathcal{A}(ck): \\\\ m_0 \\neq m_1 \\ \\land \\ \\mathsf{Commit}_{ck}(m_0; r_0) = \\mathsf{Commit}_{ck}(m_1; r_1) \\end{array}\\right] \\approx 0.</span>$</p>

    <p class="text-gray-300">If this holds also for unbounded adversaries, we say the commitment scheme is statistically binding.</p>

    <p class="text-gray-300"><strong>Definition 5 (Hiding).</strong> A commitment scheme is computationally hiding if for all PPT stateful adversaries A</p>

    <p class="text-gray-300"><span class="math">$\\Pr\\left[ \\begin{matrix} ck \\leftarrow \\mathsf{Setup}(1^\\lambda); \\ (m_0, m_1) \\leftarrow \\mathcal{A}(ck); \\ b \\leftarrow \\{0, 1\\}; \\\\ c \\leftarrow \\mathsf{Commit}_{ck}(m_b): \\ \\mathcal{A}(c) = b \\end{matrix} \\right] \\approx \\frac{1}{2},</span>$</p>

    <p class="text-gray-300">where A outputs messages of equal length  <span class="math">|m_0| = |m_1|</span> . If the definition holds also for unbounded adversaries, we say the commitment scheme is statistically hiding.</p>

    <p class="text-gray-300">We will be interested in using highly efficient commitment schemes. We say a commitment scheme is linear-time if the time to compute  <span class="math">Commit_{ck}(m)</span>  is  <span class="math">\\operatorname{poly}(\\lambda) + \\mathcal{O}(|m|)</span>  bit operations, which we assume corresponds to  <span class="math">\\operatorname{poly}(\\lambda) +</span>  <span class="math">\\mathcal{O}(\\frac{|m|}{W})</span>  machine operations on our W-bit RAM machine. We will also be interested in having small size commitments. We say a commitment scheme is compact if there is a polynomial  <span class="math">\\ell(\\lambda)</span>  such that commitments have size at most  <span class="math">\\ell(\\lambda)</span>  regardless of how long the message is. We say a commitment scheme is public coin if there is a polynomial  <span class="math">\\ell(\\lambda)</span>  such that  <span class="math">\\mathsf{Setup}(1^{\\lambda})</span>  picks the commitment key uniformly at random as  <span class="math">ck \\leftarrow \\{0,1\\}^{\\ell(\\lambda)}</span> . We will now discuss some candidate linear-time commitment schemes. Applebaum et al. [AHI&lt;sup&gt;+&lt;/sup&gt;17] gave constructions of low-complexity families of collision-resistant hash functions, where it is possible to evaluate the hash function in linear time in the message size. Their construction is based on the binary shortest vector problem assumption, which is related to finding non-trivial low-weight vectors in the null space of a matrix over  <span class="math">\\mathbb{F}_2</span> . To get down to linear-time complexity, they conjecture the binary shortest vector problem is hard when the matrix is sparse, e.g., an LDPC parity check matrix [Gal62]. Halevi and Micali [HM96] show that a collision-resistant hash function gives rise to a compact statistically hiding commitment scheme. Their transformation is very efficient, so starting with a linear-time hash function, one obtains a linear-time statistically hiding compact commitment scheme. Moreover, if we instantiate the hash function with the one by Applebaum et al. [AHI+17], which is public coin, we obtain a linear-time public-coin statistically hiding commitment scheme. Ishai et al. [IKOS08] propose linear-time computable pseudorandom generators. If we have statistically binding commitment scheme this means we can commit to an arbitrary length message m by picking a seed s for the pseudorandom generator, stretch it to t = PRG(s) of length |m| and let (Commitck(s), t ⊕ m) be the commitment to m. Assuming the commitment scheme is statistically binding, this gives us a linear-time statistically binding commitment scheme for arbitrary length messages. It can also easily be seen that commitments have the same length as the messages plus an additive polynomial overhead that depends only on the security parameter. The construction also preserves the public-coin property of the seed commitment scheme.</p>

    <h2 id="sec-10" class="text-2xl font-bold">3 Zero-Knowledge Proofs for Arithmetic Circuit Satisfiability in the Ideal Linear Commitment Model</h2>

    <p class="text-gray-300">In this section, we construct a SHVZK proof of knowledge for arithmetic circuit satisfiability relations RAC in the ILC model. Our proof can be seen as an abstraction of the zero-knowledge argument of Groth [Gro09] in an idealized vector commitment setting. In the ILC model, the prover can commit to vectors in F &lt;sup&gt;k&lt;/sup&gt; by sending them to the channel. The ILC channel stores the received vectors and communicates to verifier the number of vectors it received. The verifier can send messages to the prover via the ILC channel, which in the case of Groth's and our proof system consist of field elements in F. Finally, the verifier can query the channel to open arbitrary linear combinations of the committed vectors sent by the prover. The field F and the vector length k is specified by the public parameter ppILC. It will later emerge that to get the best communication and computation complexity for arithmetic circuits with N gates, k should be approximately &lt;sup&gt;√&lt;/sup&gt; N.</p>

    <p class="text-gray-300">Consider a circuit with a total of N fan-in 2 gates, which can be either addition gates or multiplication gates over a field F. Each gate has two inputs (left and right) and one output wire, and each output wire can potentially be attached as input to several other gates. In total, we have 3N inputs and outputs to gates. Informally, the description of an arithmetic circuit consists of a set of gates, the connection of wires between gates and known values assigned to some of the inputs and outputs. A circuit is said to be satisfiable if there exists an assignment complying with all the gates, the wiring, and the known values specified in the instance.</p>

    <p class="text-gray-300">At a high level, the idea of the proof is for the prover to commit to the 3N inputs and outputs of all the gates in the circuit, and then prove that these assignments are consistent with the circuit description. This amounts to performing the following tasks:</p>

    <p class="text-gray-300">– Prove for each value specified in the instance that this is indeed the value the prover has committed to.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>Prove for each addition gate that the committed output is the sum of the committed inputs.</li>
      <li>Prove for each multiplication gate that the committed output is the product of the committed inputs.</li>
      <li>Prove for each wire that all committed values corresponding to this wire are the same</li>
    </ul>

    <p class="text-gray-300">To facilitate these proofs, we arrange the committed values into row vectors  <span class="math">v_i \\in \\mathbb{F}^k</span>  similarly to [Gro09]. Without loss of generality we assume both the number of addition gates and the number of multiplication gates are divisible by k, which can always be satisfied by adding few dummy gates to the circuit. We can then number addition gates from (1,1) to  <span class="math">(m_A,k)</span>  and multiplication gates  <span class="math">(m_A+1,1)</span>  to  <span class="math">(m_A+m_M,k)</span> . We insert assignments to left inputs, right inputs and outputs of addition gates into entries of three matrices  <span class="math">A,B,C\\in\\mathbb{F}^{m_A\\times k}</span> , respectively. We sort entries to the matrices so that wires attached to the same gate correspond to the same entry of the three matrices, as shown in Figure 2. A valid assignment to the wires then satisfies A+B=C. We proceed in a similar way for the  <span class="math">m_M \\cdot k</span>  multiplication gates to obtain three matrices  <span class="math">D, E, F \\in \\mathbb{F}^{m_M \\times k}</span>  such that  <span class="math">D \\circ E = F</span> , where  <span class="math">\\circ</span>  denotes the Hadamard (i.e. entry-wise) product of matrices. All the committed wires then constitute a matrix</p>

    <p class="text-gray-300"><span class="math">$V = \\begin{pmatrix} A \\\\ B \\\\ C \\\\ D \\\\ E \\\\ F \\end{pmatrix} \\in \\mathbb{F}^{(3m_A + 3m_M) \\times k}</span>$</p>

    <p class="text-gray-300">Without loss of generality, we also assume the gates to be sorted so that the wire values specified in the instance correspond to full rows in V. Again, this is without loss of generality because we can always add a few dummy gates to the circuit and to the instance to complete a row.</p>

    <p class="text-gray-300">With these transformations in mind, let us write the arithmetic circuit relation as follows</p>

    <p class="text-gray-300"><span class="math">$\\mathcal{R}_{\\mathsf{AC}} = \\left\\{ \\begin{array}{l} (pp, u, w) = \\left( (\\mathbb{F}, k, *) \\;,\\; (m_A, m_M, \\pi, \\{ v_i \\}_{i \\in S}) \\;,\\; (\\{ v_i \\}_{i \\in \\bar{S}}) \\right) \\;; \\\\ m = 3m_A + 3m_M \\wedge \\pi \\in \\Sigma_{[m] \\times [k]} \\\\ \\wedge \\; S \\subseteq [m] \\qquad \\wedge \\; \\bar{S} = [m] \\setminus S \\\\ \\wedge \\; A = (v_i)_{i=1}^{m_A} \\qquad \\wedge \\; D = (v_i)_{i=3m_A+1}^{3m_A+m_M} \\\\ \\wedge \\; B = (v_i)_{i=m_A+1}^{2m_A} \\quad \\wedge \\; E = (v_i)_{i=3m_A+2m_M}^{3m_A+2m_M} \\\\ \\wedge \\; C = (v_i)_{i=2m_A+1}^{3m_A} \\wedge \\; F = (v_i)_{i=3m_A+2m_M+1}^{3m_A+2m_M+1} \\\\ \\wedge \\; A + B = C \\qquad \\wedge \\; D \\circ E = F \\\\ \\wedge \\; V = (v_i)_{i=1}^{m} \\qquad \\wedge \\; V_{i,j} = V_{\\pi(i,j)} \\; \\forall \\; (i,j) \\in [m] \\times [k] \\end{array} \\right\\}</span>$</p>

    <p class="text-gray-300">The role of the permutation  <span class="math">\\pi</span>  is to specify the wiring of the arithmetic circuit. For each wire, we can write a cycle  <span class="math">((i_1, j_1), \\ldots, (i_t, j_t))</span>  that lists the location of the committed values corresponding to this wire. Then we let  <span class="math">\\pi \\in \\Sigma_{[m] \\times [k]}</span>  be the</p>

    <p class="text-gray-300">    <img src="_page_14_Figure_1.jpeg" alt="" class="my-4 max-w-full" />
</p>

    <p class="text-gray-300">Fig. 2: Representation of an arithmetic circuit and arrangements of the wires into 6 matrices.</p>

    <p class="text-gray-300">product of all these cycles, which unambiguously defines the wiring of the circuit. To give an example using the circuit in Figure 2, the output wire of the first addition gate also appears as input of the first multiplication gate and the second addition gate. Therefore, if they appear as entries (5, 1),(9, 1),(1, 2) in the matrix V defined by the rows v&lt;sup&gt;i&lt;/sup&gt; , then we would have the cycle ((5, 1),(9, 1),(1, 2)) indicating entries that must to be identical. The output of the second addition gate feeds into the third addition gate, so this might give us a cycle ((5, 2),(4, 1)) of entries that should have the same value. The permutation π is the product of all these cycles that define which entries should have the same value.</p>

    <p class="text-gray-300">In the proof for arithmetic circuit satisfiability, the prover starts by committing to all values {vi} m &lt;sup&gt;i&lt;/sup&gt;=1. She will then call suitable sub-proofs to handle the four constraints these committed values should specify. We describe all the sub-proofs after the main proof given in Figure 3 and refer to Appendix A for the detailed constructions.</p>

    <p class="text-gray-300">Here we use the convention that when vectors or matrices are written in square brackets, i.e., when we write [A] in the instance, it means that these are values that have already been committed to the ILC channel. The prover knows these values, but the verifier may not know them. The first sub-proof D &lt;sup&gt;P&lt;/sup&gt;eq ppILC, {vi}i∈S, [U] ILC ←→ Veq ppILC, {ui}i∈S, [U] E allows the verifier to check that values included in the instance are contained in the corresponding commitments the prover previously sent to the ILC channel. The second subproof D &lt;sup&gt;P&lt;/sup&gt;sum ppILC, [A], [B], [C] ILC ←→ Vsum ppILC, [A], [B], [C] E is used to prove the committed matrices A, B and C satisfy A + B = C. The sub-proof D &lt;sup&gt;P&lt;/sup&gt;prod ppILC, [D], [E], [F] ILC ←→ Vprod ppILC, [D], [E], [F] E is used to prove that the committed matrices D, E and F satisfy D ◦ E = F. The last sub-proof D &lt;sup&gt;P&lt;/sup&gt;perm ppILC, π, [A], [B] ILC ←→ Vperm ppILC, π, [A], [B] E is used to prove that A has the same entries as B except they have been permuted according to the</p>

    <pre><code class="language-text">\\mathcal{P}_{\\mathsf{ILC}}(pp_{\\mathsf{ILC}}, u, w)
                                                                                    \\mathcal{V}_{\\mathsf{ILC}}(pp_{\\mathsf{ILC}}, u)
  - Parse u = (m_A, m_M, \\pi, \\{v_i\\}_{i \\in S})
                                                                                       - Parse u = (m_A, m_M, \\pi, \\{u_i\\}_{i \\in S})
  - Parse w = (\\{\\boldsymbol{v}_i\\}_{i \\in \\bar{S}})
                                                                                       - Run \\mathcal{V}_{eq}(pp_{\\mathsf{ILC}}, (\\{\\boldsymbol{u}_i\\}_{i \\in S}, [U]))
   - Send (commit, \\{v_i\\}_{i=1}^m) to the ILC channel
                                                                                       - Run \\mathcal{V}_{\\text{sum}}(pp_{\\mathsf{ILC}},([A],[B],[C]))
  - The vectors define V \\in \\mathbb{F}^{m \\times k} and sub- - Run \\mathcal{V}_{\\text{prod}}(pp_{\\mathsf{ILC}},([D],[E],[F]))
       matrices A, B, C, D, E, F as described earlier
                                                                                      - Run \\mathcal{V}_{perm}(pp_{\\mathsf{ILC}},(\\pi,[V],[V]))
   - Let U = (\\boldsymbol{v}_i)_{i \\in S}

    Return 1 if all the sub-proofs are

   - Run \\mathcal{P}_{eq}(pp_{\\mathsf{ILC}}, (\\{\\boldsymbol{v}_i\\}_{i \\in S}, [U]))
                                                                                           accepted and 0 otherwise
  - Run \\mathcal{P}_{\\text{sum}}(pp_{\\mathsf{ILC}},([A],[B],[C]))
   - Run \\mathcal{P}_{\\text{prod}}(pp_{\\mathsf{ILC}},([D],[E],[F]))
   - Run \\mathcal{P}_{perm}(pp_{\\mathsf{ILC}},(\\pi,[V],[V]))
</code></pre>

    <p class="text-gray-300">Fig. 3: Arithmetic circuit satisfiability proof in the ILC model.</p>

    <p class="text-gray-300">permutation  <span class="math">\\pi</span> . Note that when we call the permutation sub-proof with B=A, then the statement is that A remains unchanged when we permute the entries according to  <span class="math">\\pi</span> . This in turn means that all committed values that lie on the same cycle in the permutation must be identical, i.e., the matrix A respects the wiring of the circuit.</p>

    <p class="text-gray-300"><strong>Theorem 2.</strong>  <span class="math">(\\mathcal{K}_{ILC}, \\mathcal{P}_{ILC}, \\mathcal{V}_{ILC})</span>  is a proof system for  <span class="math">\\mathcal{R}_{AC}</span>  in the ILC model with perfect completeness, statistical knowledge soundness with straight-line extraction, and perfect special honest-verifier zero-knowledge.</p>

    <p class="text-gray-300"><span class="math">{\\it Proof.}</span>  Perfect completeness follows from the perfect completeness of the subproofs.</p>

    <p class="text-gray-300">Perfect SHVZK follows from the perfect SHVZK of the sub-proofs. A simulated transcript is obtained by combining the outputs of the simulators of all the sub-proofs.</p>

    <p class="text-gray-300">Also statistical knowledge soundness follows from the knowledge soundness of the sub-proofs. The statistical knowledge soundness of the equality sub-proof guarantees that commitments to values included in the instance indeed contain the publicly known values. The correctness of the addition gates and multiplication gates follows from the statistical knowledge soundness of the respective sub-proofs. Finally, as we have argued above, the permutation sub-proof guarantees the committed values respect the wiring of the circuit.</p>

    <p class="text-gray-300">Since all sub-proofs have knowledge soundness with straight line extraction, so does the main proof.</p>

    <p class="text-gray-300">The efficiency of our arithmetic circuit satisfiability proof in the ILC model is given in Figure 4. A detailed breakdown of the costs of each sub-protocol can be found in Appendix A. The asymptotic results displayed below are obtained when the parameter k specified by  <span class="math">pp_{\\text{ILC}}</span>  is approximately  <span class="math">\\sqrt{N}</span> . The query complexity</p>

    <p class="text-gray-300">qc is the number of linear combinations the verifier queries from the ILC channel in the opening query. The verifier communication  <span class="math">C_{\\text{ILC}}</span>  is the number of messages sent from the verifier to the prover via the ILC channel and in our proof system it is proportional to the number of rounds. Let  <span class="math">\\mu</span>  be the number of rounds in the ILC proof and  <span class="math">t_1, \\ldots, t_{\\mu}</span>  be the numbers of vectors that the prover sends to the ILC channel in each round, and let  <span class="math">t = \\sum_{i=1}^{\\mu} t_i</span> .</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Prover computation</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">T_{\\mathcal{P}_{ILC}} = \\mathcal{O}(N)</span> multiplications in <span class="math">\\mathbb{F}</span></th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Verifier computation</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">T_{\\mathcal{V}_{ILC}} = \\mathcal{O}(N)</span> additions in <span class="math">\\mathbb{F}</span></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Query complexity</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">qc = 20</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Verifier communication</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">C_{ILC} = \\mathcal{O}(\\log\\log(N))</span> field elements</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Round complexity</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mu = \\mathcal{O}(\\log\\log(N))</span></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Total number of committed vectors</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">t = \\mathcal{O}\\left(\\sqrt{N}\\right) \\text{ vectors in } \\mathbb{F}^k</span></td>
          </tr>
        </tbody>
      </table>
    </div>

    <p class="text-gray-300">Fig. 4: Efficiency of our arithmetic circuit satisfiability proof in the ILC model  <span class="math">(\\mathcal{K}_{\\mathsf{ILC}}, \\mathcal{V}_{\\mathsf{ILC}})</span>  for  <span class="math">(pp, u, w) \\in \\mathcal{R}_{\\mathsf{AC}}</span> .</p>

    <h3 id="sec-11" class="text-xl font-semibold mt-8">4 Compiling Ideal Linear Commitment Proofs into Standard Proofs</h3>

    <p class="text-gray-300">In this section, we show how to compile a proof of knowledge with straight-line extraction for relation  <span class="math">\\mathcal R</span>  over the communication channel ILC into a proof of knowledge without straight-line extraction for the same relation over the standard channel. Recall that the ILC channel allows the prover to submit vectors of length k to the channel and the verifier can then query linear combinations of those vectors.</p>

    <p class="text-gray-300">The idea behind the compilation of an ILC proof is that instead of committing to vectors  <span class="math">v_{\\tau}</span>  using the channel ILC, the prover encodes each vector  <span class="math">v_{\\tau}</span>  as  <span class="math">\\mathsf{E}_{\\mathcal{C}}(v_{\\tau})</span>  using a linear error-correcting code  <span class="math">\\mathsf{E}_{\\mathcal{C}}</span> . In any given round, we can think of the codewords as rows  <span class="math">\\mathsf{E}_{\\mathcal{C}}(v_{\\tau})</span>  in a matrix  <span class="math">\\mathsf{E}_{\\mathcal{C}}(V)</span> . However, instead of committing to the rows of the matrix, the prover commits to the columns of the matrix. When the verifier wants to open a linear combination of the original vectors, he sends the coefficients  <span class="math">q=(q_1,\\ldots,q_t)</span>  of the linear combination to the prover, and the prover responds with the linear combination  <span class="math">v_{(q)}\\leftarrow qV</span> . Notice that we will use the notation  <span class="math">v_{(q)}</span> , and later on  <span class="math">v_{(\\gamma)}</span> , to denote vectors that depend on q and  <span class="math">\\gamma</span> : the q and  <span class="math">\\gamma</span>  are not indices. Now, to spot check that the prover is not giving a wrong  <span class="math">v_{(q)}</span> , the verifier may request the jth element of each committed codeword  <span class="math">e_{\\tau}</span> . This corresponds to revealing the jth column of error-corrected matrix  <span class="math">\\mathsf{E}_{\\mathcal{C}}(V)</span> . Since the code  <span class="math">\\mathsf{E}_{\\mathcal{C}}</span>  is linear, the revealed elements should satisfy  <span class="math">\\mathsf{E}_{\\mathcal{C}}(v_{(q)})_j = \\sum_{\\tau=1}^t q_{\\tau} \\mathsf{E}_{\\mathcal{C}}(v_{\\tau})_j = q(\\mathsf{E}_{\\mathcal{C}}(V)|_j)</span> . The verifier will spot check on multiple columns, so that if the code has sufficiently large minimum distance</p>

    <p class="text-gray-300">and the prover gives a wrong  <span class="math">v_{(q)}</span> , then with overwhelming probability, the verifier will open at least one column j where the above equality does not hold.</p>

    <p class="text-gray-300">Revealing entries in a codeword may leak information about the encoded vector. To get SHVZK, instead of using  <span class="math">\\mathsf{E}_{\\mathcal{C}}</span> , we use a randomized encoding  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}</span>  defined by  <span class="math">\\hat{\\mathsf{E}}_{\\mathcal{C}}(\\boldsymbol{v};\\boldsymbol{r})=(\\mathsf{E}_{\\mathcal{C}}(\\boldsymbol{v})+\\boldsymbol{r},\\boldsymbol{r})</span> . This doubles the code-length to 2n but ensures that when you reveal entry j, but not entry j+n, then the verifier only learns a random field element. The spot checking technique using  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}</span>  is illustrated in Fig. 5. In the following, we use the notation  <span class="math">\\boldsymbol{e}_{\\tau}=(\\mathsf{E}_{\\mathcal{C}}(\\boldsymbol{v}_{\\tau})+\\boldsymbol{r}_{\\tau},\\boldsymbol{r}_{\\tau})</span>  and  <span class="math">E=(\\mathsf{E}_{\\mathcal{C}}(V)+R,R)</span> . We also add a check, where the verifier sends an extra</p>

    <p class="text-gray-300"><span class="math">$\\begin{array}{ccccc} \\left( \\begin{array}{c} \\boldsymbol{v}_0 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp;</span>$</p>

    <p class="text-gray-300">Fig. 5: Vectors  <span class="math">\\mathbf{v}_{\\tau}</span>  organized in matrix V are encoded row-wise as matrix  <span class="math">E = \\tilde{\\mathsf{E}}_{\\mathcal{C}}(V;R)</span> . The vertical line in the right matrix and vector denotes concatenation of matrices respectively vectors. The prover commits to each column of E. When the prover given  <span class="math">\\mathbf{q}</span>  wants to reveal the linear combination  <span class="math">\\mathbf{v}_{(\\mathbf{q})} = \\mathbf{q}V</span>  she also reveals  <span class="math">\\mathbf{r}_{(\\mathbf{q})} = \\mathbf{q}R</span> . The verifier now asks for openings of  <span class="math">2\\lambda</span>  columns  <span class="math">J = \\{j_1, \\ldots, j_{2\\lambda}\\}</span>  in E and verifies for these columns that  <span class="math">\\mathbf{q}E|_J = \\tilde{\\mathsf{E}}_{\\mathcal{C}}(\\mathbf{v}_{(\\mathbf{q})}; \\mathbf{r}_{(\\mathbf{q})})|_J</span> . To avoid revealing any information about  <span class="math">\\mathsf{E}_{\\mathcal{C}}(V)</span> , we must ensure that  <span class="math">\\forall j \\in [n]: j \\in J \\Rightarrow j+n \\notin J</span> . If the spot checks pass, the verifier believes that  <span class="math">\\mathbf{v}_{(\\mathbf{q})} = \\mathbf{q}V</span> .</p>

    <p class="text-gray-300">random linear combination  <span class="math">\\gamma \\in \\mathbb{F}^t</span>  to ensure that if a malicious prover commits to values of  <span class="math">e_{\\tau}</span>  that are far from being codewords, the verifier will most likely reject. The reason the challenges q from the ILC proof are not enough to ensure this is that they are not chosen uniformly at random. One could, for instance, imagine that there was a vector  <span class="math">v_{\\tau}</span>  that was never queried in a non-trivial way, and hence the prover could choose it to be far from a codeword. To make sure this extra challenge  <span class="math">\\gamma</span>  does not reveal information to the verifier, the prover picks a random blinding vector  <span class="math">v_0</span> , which is added as the first row of V and will be added to the linear combination of the challenge  <span class="math">\\gamma</span> .</p>

    <h4 id="sec-12" class="text-lg font-semibold mt-6">4.1 Construction</h4>

    <p class="text-gray-300">Let  <span class="math">(\\mathcal{K}_{\\mathsf{ILC}}, \\mathcal{P}_{\\mathsf{ILC}}, \\mathcal{V}_{\\mathsf{ILC}})</span>  be a <em>non-adaptive</em>  <span class="math">\\mu</span> -round SHVZK proof of knowledge with straight-line extraction over ILC for a relation  <span class="math">\\mathcal{R}</span> . Here, non-adaptive means that</p>

    <p class="text-gray-300">the verifier waits until the last round before querying linear combinations of vectors and they are queried all at once instead of the queries depending on each other.&lt;sup&gt;4&lt;/sup&gt; Let  <span class="math">\\operatorname{Gen}_{\\mathsf{E}_{\\mathcal{C}}}</span>  be a generator that given field  <span class="math">\\mathbb{F}</span>  and length parameter k outputs a constant rate linear code  <span class="math">\\mathsf{E}_{\\mathcal{C}}</span>  that is linear-time computable given its description and has linear minumum distance. Define the  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}</span>  with code length 2n as above:  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}(v;r)=(\\mathsf{E}_{\\mathcal{C}}(v)+r,r)</span> . Finally, let (Setup, Commit) be a non-interactive commitment scheme.</p>

    <p class="text-gray-300">We now define a proof of knowledge  <span class="math">(\\mathcal{K}, \\mathcal{P}, \\mathcal{V})</span>  in Fig. 6, where we use the following notation: given matrices  <span class="math">V_1, \\ldots, V_{\\mu}, R_1, \\ldots, R_{\\mu}</span>  and  <span class="math">E_1, \\ldots, E_{\\mu}</span>  we define</p>

    <p class="text-gray-300"><span class="math">$V = \\begin{pmatrix} V_1 \\\\ \\vdots \\\\ V_{\\mu} \\end{pmatrix} \\qquad R = \\begin{pmatrix} R_1 \\\\ \\vdots \\\\ R_{\\mu} \\end{pmatrix} \\qquad E = \\begin{pmatrix} E_1 \\\\ \\vdots \\\\ E_{\\mu} \\end{pmatrix}.</span>$</p>

    <p class="text-gray-300">The matrices  <span class="math">V_1, \\ldots, V_{\\mu}</span>  are formed by the row vectors  <span class="math">\\mathcal{P}_{\\mathsf{ILC}}</span>  commits to, and we let  <span class="math">t_1, \\ldots, t_{\\mu}</span>  be the numbers of vectors in each round, i.e., for all i we have  <span class="math">V_i \\in \\mathbb{F}^{t_i \\times k}</span> .</p>

    <p class="text-gray-300">We say that a set  <span class="math">J \\subset [2n]</span>  is allowed if  <span class="math">|J \\cap [n]| = \\lambda</span>  and  <span class="math">|J \\setminus [n]| = \\lambda</span>  and there is no  <span class="math">j \\in J</span>  such that  <span class="math">j + n \\in J</span> . In the following we will always assume codewords have length  <span class="math">n \\geq 2\\lambda</span> . We use  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}(V;R)</span>  to denote the function that applies  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}</span>  row-wise. In the protocol for  <span class="math">\\mathcal{V}</span> , we are using that  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}(\\boldsymbol{v};r)|_J</span>  can be computed from just  <span class="math">\\boldsymbol{v}</span>  and  <span class="math">\\boldsymbol{r}|_{\\{j\\in[n]:j\\in J\\vee j+n\\in J\\}}</span> . We use  <span class="math">\\mathsf{Commit}(E;\\boldsymbol{s})</span>  to denote the function that applies  <span class="math">\\mathsf{Commit}</span>  column-wise on E and returns a vector  <span class="math">\\boldsymbol{c}</span>  of 2n commitments. We group all  <span class="math">\\mathcal{V}_{\\mathsf{ILC}}</span> 's queries in one matrix  <span class="math">Q \\in \\mathbb{F}^{qc \\times t}</span> , where t is the total number of vectors committed to by  <span class="math">\\mathcal{P}</span>  and qc is the query complexity of  <span class="math">\\mathcal{V}_{\\mathsf{ILC}}</span> , i.e., the total number of linear combinations  <span class="math">\\boldsymbol{q}</span>  that  <span class="math">\\mathcal{V}_{\\mathsf{ILC}}</span>  requests to be opened.</p>

    <h4 id="sec-13" class="text-lg font-semibold mt-6">4.2 Security Analysis</h4>

    <p class="text-gray-300"><strong>Theorem 3 (Completeness).</strong> If  <span class="math">(\\mathcal{K}_{\\mathsf{ILC}}, \\mathcal{P}_{\\mathsf{ILC}}, \\mathcal{V}_{\\mathsf{ILC}})</span>  is complete for relation  <span class="math">\\mathcal{R}</span>  over  <span class="math">\\mathsf{ILC}</span> , then  <span class="math">(\\mathcal{K}, \\mathcal{P}, \\mathcal{V})</span>  in Fig. 6 is complete for relation  <span class="math">\\mathcal{R}</span> .</p>

    <p class="text-gray-300">Proof. All the commitment openings are correct, so they will be accepted by the verifier. In the execution of  <span class="math">\\langle \\mathcal{P}(pp,u,w) \\longleftrightarrow \\mathcal{V}(pp,u) \\rangle</span> , the fact that  <span class="math">\\mathsf{E}_{\\mathcal{C}}</span>  is linear implies  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}</span>  is linear and hence all the linear checks will be true. If  <span class="math">(pp,u,w) \\in \\mathcal{R}</span>  then  <span class="math">(pp_{\\mathsf{ILC}},u,w) \\in \\mathcal{R}</span>  and being complete  <span class="math">\\langle \\mathcal{P}_{\\mathsf{ILC}}(pp_{\\mathsf{ILC}},u,w) \\longleftrightarrow \\mathcal{V}_{\\mathsf{ILC}}(pp_{\\mathsf{ILC}},stm) \\rangle = 1</span>  so  <span class="math">\\mathcal{V}</span> 's internal copy of  <span class="math">\\mathcal{V}_{\\mathsf{ILC}}</span>  will accept. Thus, in this case,  <span class="math">\\langle \\mathcal{P}(pp,u,w) \\longleftrightarrow \\mathcal{V}(pp,u) \\rangle = 1</span> , which proves completeness.</p>

    <p class="text-gray-300"><strong>Theorem 4 (Knowledge Soundness).</strong> If  <span class="math">(\\mathcal{K}_{ILC}, \\mathcal{P}_{ILC}, \\mathcal{V}_{ILC})</span>  is statistically knowledge sound with a straight-line extractor for relation  <span class="math">\\mathcal{R}</span>  over ILC and (Setup, Commit) is computationally (statistically) binding, then  <span class="math">(\\mathcal{K}, \\mathcal{P}, \\mathcal{V})</span>  as constructed above is computationally (statistically) knowledge sound for relation  <span class="math">\\mathcal{R}</span> .</p>

    <p class="text-gray-300">&lt;sup&gt;&amp;&lt;/sup&gt;lt;sup&gt;4&lt;/sup&gt;The construction can be easily modified to an adaptive ILC proof. For each round of queries in the ILC proof, there will one extra round in the compiled proof.</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">\\mathcal{P}(pp, u, w)</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">\\mathcal{K}(1^{\\lambda})</span></th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">P(pp, u, w)</span> - Parse input: • Parse <span class="math">pp = (pp_{ILC}, E_{\\mathcal{C}}, ck)</span> • Parse <span class="math">pp_{ILC} = (\\mathbb{F}, k)</span> • Get <span class="math">n</span> from <span class="math">E_{\\mathcal{C}}</span> - Round 1: • <span class="math">v_0 \\leftarrow \\mathbb{F}^k</span> • <span class="math">e_0 \\leftarrow \\tilde{E}_{\\mathcal{C}}(v_0; r_0)</span> • (commit, <span class="math">V_1</span> ) <span class="math">\\leftarrow \\mathcal{P}_{ILC}(pp_{ILC}, u, w)</span> • <span class="math">E_1 \\leftarrow \\tilde{E}_{\\mathcal{C}}(V_1; R_1)</span> • Let <span class="math">E_{01} = \\begin{pmatrix} e_0 \\\\ E_1 \\end{pmatrix}</span> • <span class="math">c_1 = Commit(E_{01}; s_1)</span> • Send <span class="math">(c_1, t_1)</span> to <span class="math">\\mathcal{V}</span> - Rounds <span class="math">2 \\le i \\le \\mu</span> : • Get challenge <span class="math">x_{i-1}</span> from <span class="math">\\mathcal{V}</span> • (commit, <span class="math">V_i</span> ) <span class="math">\\leftarrow \\mathcal{P}_{ILC}(x_{i-1})</span> • <span class="math">E_i \\leftarrow \\tilde{E}_{\\mathcal{C}}(V_i; R_i)</span> • <span class="math">c_i = Commit(E_i; s_i)</span> • Send <span class="math">(c_i, t_i)</span> to <span class="math">\\mathcal{V}</span> - Round <span class="math">\\mu + 1</span> : • Get <span class="math">(\\gamma, Q)</span> from <span class="math">\\mathcal{V}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{K}(1^{\\lambda})</span> <span class="math">-pp_{ILC} \\leftarrow \\mathcal{K}_{ILC}(1^{\\lambda})</span> <span class="math">- \\operatorname{Parse} pp_{ILC} = (\\mathbb{F}, k)</span> <span class="math">- \\operatorname{E}_{\\mathcal{C}} \\leftarrow \\operatorname{Gen}_{E_{\\mathcal{C}}}(\\mathbb{F}, k)</span> <span class="math">- \\operatorname{ck} \\leftarrow \\operatorname{Setup}(1^{\\lambda})</span> <span class="math">- \\operatorname{Return} pp = (pp_{ILC}, E_{\\mathcal{C}}, \\operatorname{ck})</span> <span class="math">\\boxed{\\mathcal{V}(pp, u)}</span> <span class="math">- \\operatorname{Parse} \\operatorname{input}</span> <span class="math">\\bullet \\operatorname{Parse} pp = (pp_{ILC}, E_{\\mathcal{C}}, \\operatorname{ck})</span> <span class="math">\\bullet \\operatorname{Parse} pp_{ILC} = (\\mathbb{F}, k)</span> <span class="math">\\bullet \\operatorname{Get} n \\operatorname{from} E_{\\mathcal{C}}</span> <span class="math">\\bullet \\operatorname{Give} \\operatorname{input} (pp_{ILC}, u) \\operatorname{to} \\mathcal{V}_{ILC}</span> <span class="math">- \\operatorname{Rounds} 1 \\leq i &lt; \\mu:</span> <span class="math">\\bullet \\operatorname{Receive} (c_i, t_i)</span> <span class="math">\\bullet (\\operatorname{send}, x_i) \\leftarrow \\mathcal{V}_{ILC}(t_i)</span> <span class="math">\\bullet \\operatorname{Send} x_i \\operatorname{to} \\mathcal{P}</span> <span class="math">- \\operatorname{Round} \\mu:</span> <span class="math">\\bullet \\operatorname{Receive} (c_{\\mu}, t_{\\mu})</span> <span class="math">\\bullet \\mathcal{Y} \\leftarrow \\mathbb{F}^{\\sum_{i=1}^{\\mu} t_i}</span></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">$ \\bullet \\ v_{(\\gamma)} \\leftarrow v_0 + \\gamma V $ $ \\bullet \\ r_{(\\gamma)} \\leftarrow r_0 + \\gamma R $ $ \\bullet \\ V_{(Q)} \\leftarrow QV $ $ \\bullet \\ R_{(Q)} \\leftarrow QR $ $ \\bullet \\ \\operatorname{Send} \\ (v_{(\\gamma)}, r_{(\\gamma)}, V_{(Q)}, R_{(Q)}) \\text{ to } \\mathcal{V} $ $ - \\ \\operatorname{Round} \\ \\mu + 2: $ $ \\bullet \\ \\operatorname{Get} \\ J \\subset [2n] \\ \\operatorname{from} \\ \\mathcal{V} $ $ \\bullet \\ \\operatorname{Send} \\ (E_{01} <em>J, s_1 _J, \\dots, E</em>{\\mu}, s_{\\mu} _J) \\text{ to } \\mathcal{V} $</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">• (open, <span class="math">Q</span> ) <span class="math">\\leftarrow \\mathcal{V}_{ILC}(t_{\\mu})</span>&lt;br&gt;• Send <span class="math">(\\gamma, Q)</span> to <span class="math">\\mathcal{P}</span>&lt;br&gt;- Round <span class="math">\\mu + 1</span> :&lt;br&gt;• Receive <span class="math">(v_{(\\gamma)}, r_{(\\gamma)}, V_{(Q)}, R_{(Q)})</span>&lt;br&gt;• Choose random allowed <span class="math">J \\subset [2n]</span>&lt;br&gt;• Send <span class="math">J</span> to <span class="math">\\mathcal{P}</span>&lt;br&gt;- Round <span class="math">\\mu + 2</span> :</td>
          </tr>
        </tbody>
      </table>
    </div>

    <p class="text-gray-300">Fig. 6: Construction of  <span class="math">(\\mathcal{K}, \\mathcal{P}, \\mathcal{V})</span>  from  <span class="math">(\\mathcal{K}_{ILC}, \\mathcal{P}_{ILC}, \\mathcal{V}_{ILC})</span> , commitment scheme (Setup, Commit) and error-correcting code  <span class="math">\\mathcal{C}</span> .</p>

    <p class="text-gray-300"><em>Proof.</em> We prove the computational case. The statistical case is similar.</p>

    <p class="text-gray-300">In order to argue that  <span class="math">(\\mathcal{K}, \\mathcal{P}, \\mathcal{V})</span>  is computationally knowledge sound, we will first show that for every DPT  <span class="math">\\mathcal{P}^*</span>  there exists a deterministic (but not necessarily</p>

    <p class="text-gray-300">efficient)  <span class="math">\\mathcal{P}_{\\parallel C}^*</span>  such that for all PPT  <span class="math">\\mathcal{A}</span>  we have</p>

    <p class="text-gray-300"><span class="math">$\\Pr\\left[\\begin{array}{c} pp \\leftarrow \\mathcal{K}(1^{\\lambda}); (pp_{\\mathsf{ILC}}, \\cdot) = pp; (u, s) \\leftarrow \\mathcal{A}(pp) : \\\\ \\langle \\mathcal{P}^{*}(s) \\longleftrightarrow \\mathcal{V}(pp, u; (\\rho_{\\mathsf{ILC}}, \\rho)) \\rangle = 1 \\\\ \\wedge \\langle \\mathcal{P}^{*}_{\\mathsf{ILC}}(s, pp, u) \\longleftrightarrow \\mathcal{V}_{\\mathsf{ILC}}(pp_{\\mathsf{ILC}}, u; \\rho_{\\mathsf{ILC}}) \\rangle = 0 \\end{array}\\right] \\approx 0. \\tag{1}</span>$</p>

    <p class="text-gray-300">Note that the randomness  <span class="math">\\rho_{\\mathsf{ILC}}</span>  in  <span class="math">\\mathcal{V}</span>  which comes from the internal  <span class="math">\\mathcal{V}_{\\mathsf{ILC}}</span>  in line two is the same as the randomness used by  <span class="math">\\mathcal{V}_{\\mathsf{ILC}}</span>  in line three.</p>

    <p class="text-gray-300">Our constructed  <span class="math">\\mathcal{P}^*_{\\mathsf{ILC}}</span>  will run an internal copy of  <span class="math">\\mathcal{P}^*</span> . When the internal  <span class="math">\\mathcal{P}^*</span>  in round i sends a message  <span class="math">(c_i, t_i)</span> ,  <span class="math">\\mathcal{P}^*_{\\mathsf{ILC}}</span>  will simulate  <span class="math">\\mathcal{P}^*</span>  on every possible continuation of the transcript, and for each  <span class="math">j=1,\\ldots,2n</span>  find the most frequently occurring correct opening  <span class="math">((E_i)_j,(s_i)_j)</span>  of  <span class="math">(c_i)_j</span> .  <span class="math">\\mathcal{P}^*_{\\mathsf{ILC}}</span>  will then use this to get matrices  <span class="math">E_i^*</span> . For each row  <span class="math">e_\\tau^*</span>  of these matrices,  <span class="math">\\mathcal{P}^*_{\\mathsf{ILC}}</span>  finds a vector  <span class="math">v_\\tau</span>  and randomness  <span class="math">r_\\tau</span>  such that  <span class="math">\\mathsf{hd}(\\tilde{\\mathsf{E}}_{\\mathcal{C}}(v_\\tau,r_\\tau),e_\\tau^*)&lt;\\frac{\\mathsf{hd}_{\\min}}{3}</span>  if such a vector exists. If for some  <span class="math">\\tau</span>  no such vector  <span class="math">v_\\tau</span>  exists, then  <span class="math">\\mathcal{P}^*_{\\mathsf{ILC}}</span>  aborts. Otherwise we let  <span class="math">V_i</span>  and  <span class="math">R_i</span>  denote the matrices formed by the row vectors  <span class="math">v_\\tau</span>  and  <span class="math">r_\\tau</span>  in round i and  <span class="math">\\mathcal{P}^*_{\\mathsf{ILC}}</span>  sends  <span class="math">V_i</span>  to the ILC. Notice that since the minimum distance of  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}</span>  is at least  <span class="math">\\mathsf{hd}_{\\min}</span> , there is at most one such vector  <span class="math">v_\\tau</span>  for each  <span class="math">e_\\tau^*</span> .</p>

    <p class="text-gray-300">The internal copy of  <span class="math">\\mathcal{P}^*</span>  will expect to get two extra rounds, where in the first it should receive  <span class="math">\\gamma</span>  and Q and should respond with  <span class="math">\\boldsymbol{v}_{(\\gamma)}^*, \\boldsymbol{r}_{(\\gamma)}^*, V_{(Q)}</span>  and  <span class="math">R_{(Q)}</span> , and in the second it should receive J and send  <span class="math">E_{01}|_{J}, \\boldsymbol{s}_{1}|_{J}, \\ldots, E_{\\mu}, \\boldsymbol{s}_{\\mu}|_{J}</span> . Since  <span class="math">\\mathcal{P}_{\\text{ILC}}^*</span>  does not send and receive corresponding messages,  <span class="math">\\mathcal{P}_{\\text{ILC}}^*</span>  does not have to run this part of  <span class="math">\\mathcal{P}^*</span> . Of course, for each commitment sent by  <span class="math">\\mathcal{P}^*</span> , these rounds are internally simulated many times to get the most frequent opening. Notice that a  <span class="math">\\mathcal{V}_{\\text{ILC}}</span>  communicating over ILC with our constructed  <span class="math">\\mathcal{P}_{\\text{ILC}}^*</span>  will, on challenge Q receive  <span class="math">V_{(Q)} = QV</span>  from the ILC.</p>

    <p class="text-gray-300">The verifier  <span class="math">\\mathcal{V}</span>  accepts only if its internal copy of  <span class="math">\\mathcal{V}_{\\mathsf{ILC}}</span>  accepts. Hence, the only three ways  <span class="math">\\langle \\mathcal{P}^*(s) \\longleftrightarrow \\mathcal{V}(pp,u;(\\rho_{\\mathsf{ILC}},\\rho)) \\rangle</span>  can accept without  <span class="math">\\langle \\mathcal{P}^*_{\\mathsf{ILC}}(s,pp,u) \\overset{\\mathsf{ILC}}{\\longleftrightarrow} \\mathcal{V}_{\\mathsf{ILC}}(pp_{\\mathsf{ILC}},u;\\rho_{\\mathsf{ILC}}) \\rangle</span>  being accepting are</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>if  <span class="math">\\mathcal{P}^*</span>  makes an opening of a commitment that is not its most frequent opening of that commitment, or</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>if  <span class="math">\\mathcal{P}^*_{\\mathsf{ILC}}</span>  has an error because for some  <span class="math">\\tau</span>  no  <span class="math">v_{\\tau}, r_{\\tau}</span>  with  <span class="math">\\mathsf{hd}(\\tilde{\\mathsf{E}}_{\\mathcal{C}}(v_{\\tau}, r_{\\tau}), e_{\\tau}^*) &lt; \\frac{\\mathsf{hd}_{\\min}}{2}</span>  exists, or</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>if  <span class="math">\\mathcal{P}^*</span>  sends some  <span class="math">V_{(Q)}^* \\neq V_{(Q)}</span> .</li>
    </ol></li>
    </ul>

    <p class="text-gray-300">We will now argue that for each of these three cases, the probability that they happen and V accepts is negligible.</p>

    <p class="text-gray-300">Since  <span class="math">\\mathcal{P}^*</span>  runs in polynomial time and the commitment scheme is computationally binding, there is only negligible probability that  <span class="math">\\mathcal{P}^*</span>  sends a valid opening that is not the most frequent. Since  <span class="math">\\mathcal{V}</span>  will reject any opening that is not valid, the probability of  <span class="math">\\mathcal{V}</span>  accepting in case 1 is negligible.</p>

    <p class="text-gray-300">Next, we consider the second case. To do so, define the event Err that  <span class="math">E^*</span>  is such that for some  <span class="math">\\gamma^* \\in \\mathbb{F}^t</span>  we have  <span class="math">\\mathsf{hd}(\\tilde{\\mathcal{C}}, \\gamma^* E^*) \\geq \\frac{\\mathsf{hd}_{\\min}}{3}</span> . Here  <span class="math">\\tilde{\\mathcal{C}}</span>  denotes the image of  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}</span> , i.e.  <span class="math">\\tilde{\\mathcal{C}} = \\{(c+r,r): c \\in \\mathcal{C}, r \\in \\mathbb{F}^n\\}</span> . Clearly, if  <span class="math">\\mathcal{P}^*_{\\mathsf{ILC}}</span>  returns an error because no  <span class="math">v_i, r_i</span>  with  <span class="math">\\mathsf{hd}(\\tilde{\\mathsf{E}}_{\\mathcal{C}}(v_i, r_i), e_i^*) &lt; \\frac{\\mathsf{hd}_{\\min}}{3}</span>  exist then we have Err.</p>

    <p class="text-gray-300">The proof of the following claim can be found in Appendix B.</p>

    <p class="text-gray-300">Claim. Let  <span class="math">e_0^*, \\dots, e_t^* \\in \\mathbb{F}^{2n}</span> . If Err occurs, then for uniformly chosen  <span class="math">\\gamma \\in \\mathbb{F}^t</span> , there is probability at most  <span class="math">\\frac{1}{|\\mathbb{F}|}</span>  that  <span class="math">\\mathsf{hd}(\\tilde{\\mathcal{C}}, e_0^* + \\gamma E^*) &lt; \\frac{\\mathsf{hd}_{\\min}}{6}</span> .</p>

    <p class="text-gray-300">Thus, if Err then with probability at least  <span class="math">1-\\frac{1}{|\\mathbb{F}|}</span>  the vector  <span class="math">\\gamma</span>  is going to be such that  <span class="math">\\mathsf{hd}(\\tilde{\\mathcal{C}}, e_0^* + \\gamma E^*) \\geq \\frac{\\mathsf{hd}_{\\min}}{6}</span> . If this happens, then for the vectors  <span class="math">(v_{(\\gamma)}^*, r_{(\\gamma)}^*)</span>  sent by  <span class="math">\\mathcal{P}^*</span> , we must have  <span class="math">\\mathsf{hd}(\\tilde{\\mathsf{E}}_{\\mathcal{C}}(v_{(\\gamma)}^*, r_{(\\gamma)}^*), e_0^* + \\gamma E^*) \\geq \\frac{\\mathsf{hd}_{\\min}}{6}</span> . This means that either in the first half of the codeword  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}(v_{(\\gamma)}^*, r_{(\\gamma)}^*)</span>  or in the second half, there will be at least  <span class="math">\\frac{\\mathsf{hd}_{\\min}}{12}</span>  values of j where it differs from  <span class="math">e_0^* + \\gamma E^*</span> . It is easy to see that the  <span class="math">\\lambda</span>  values of j in one half of [2n] are chosen uniformly and independently at random conditioned on being different.</p>

    <p class="text-gray-300">For each of these j, there is a probability at most  <span class="math">1 - \\frac{\\mathsf{hd}_{\\min}}{12n}</span>  that  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}(v_{(\\gamma)}, r_{(\\gamma)})_j = e_{0,j}^* + \\gamma E^*|_j</span> , and since the j's are chosen uniformly under the condition that they are distinct, given that this holds for the first i values, the probability is even smaller for the i+1'th. Hence, the probability that it holds for all j in this half is negligible. This shows that the probability that Err happens and  <span class="math">\\mathcal V</span>  accepts is negligible.</p>

    <p class="text-gray-300">Now we turn to case 3, where Err does not happen but  <span class="math">\\mathcal{P}^*</span>  sends a  <span class="math">V_{(Q)}^* \\neq V_{(Q)}</span> . In this case, for all  <span class="math">\\boldsymbol{\\gamma}^* \\in \\mathbb{F}^t</span> , we have  <span class="math">\\mathsf{hd}(\\tilde{\\mathcal{C}}, \\sum_{\\tau=1}^t \\boldsymbol{\\gamma}_\\tau^* \\boldsymbol{e}_\\tau^*) &lt; \\frac{\\mathsf{hd}_{\\min}}{3}</span> . In particular, this holds for the vector  <span class="math">\\boldsymbol{\\gamma}</span>  given by  <span class="math">\\boldsymbol{\\gamma}_\\tau = 1</span>  and  <span class="math">\\boldsymbol{\\gamma}_{\\tau&#x27;} = 0</span>  for  <span class="math">\\tau&#x27; \\neq \\tau</span> , so the  <span class="math">\\boldsymbol{v}_\\tau</span> 's are well-defined.</p>

    <p class="text-gray-300">For two matrices A and B of the same dimensions, we define their Hamming distance  <span class="math">\\mathsf{hd}_2(A,B)</span>  to be the number of j's such that the jth column of A and jth column of B are different. This agrees with the standard definition of Hamming distance, if we consider each matrix to be a string of column vectors. The proof of the following claim can be found in Appendix B.</p>

    <p class="text-gray-300">Claim. Assume  <span class="math">\\neg Err</span>  and let V and R be defined as above. Then for any  <span class="math">q \\in \\mathbb{F}^t</span>  there exists an  <span class="math">r_{(q)}</span>  with  <span class="math">\\mathsf{hd}(\\tilde{\\mathsf{E}}_{\\mathcal{C}}(qV, r_{(q)}), qE^*) &lt; \\frac{\\mathsf{hd}_{\\min}}{3}</span> .</p>

    <p class="text-gray-300">there exists an  <span class="math">r_{(q)}</span>  with  <span class="math">\\mathsf{hd}(\\tilde{\\mathsf{E}}_{\\mathcal{C}}(qV,r_{(q)}),qE^*)&lt;\\frac{\\mathsf{hd}_{\\min}}{3}</span> . In particular, for any  <span class="math">V_{(Q)}^*\\neq QV</span> , and any  <span class="math">R_{(Q)}^*</span>  we have</p>

    <p class="text-gray-300"><span class="math">$\\mathsf{hd}_2\\left(\\tilde{\\mathsf{E}}_{\\mathcal{C}}\\left(V_{(Q)}^*,R_{(Q)}^*\\right),QE^*\\right) \\geq 2\\frac{\\mathsf{hd}_{\\min}}{3}.</span>$</p>

    <p class="text-gray-300">This means that if  <span class="math">\\neg Err</span>  occurs and  <span class="math">\\mathcal{P}^*</span>  attempts to open a  <span class="math">V_{(Q)}^* \\neq V_{(Q)} = QV</span>  then</p>

    <p class="text-gray-300"><span class="math">$\\mathsf{hd}_2\\left(\\tilde{\\mathsf{E}}_{\\mathcal{C}}\\left(V_{(Q)}^*,R_{(Q)}^*\\right),QE^*\\right) \\geq 2\\frac{\\mathsf{hd}_{\\min}}{3}.</span>$</p>

    <p class="text-gray-300">As argued above, if the distance between two strings of length 2n is at least  <span class="math">\\frac{\\mathsf{hd}_{\\min}}{3}</span> , the probability that J will not contain a j such that the two strings differ in position j is negligible. Hence, the probability that  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}\\left(V_{(Q)}^*, R_{(Q)}^*\\right)|_J = QE^*|_J</span>  is negligible. Thus, the probability that  <span class="math">\\neg Err</span>  and  <span class="math">\\mathcal V</span>  accepts while  <span class="math">\\mathcal V_{\\mathsf{ILC}}</span>  does not is negligible. This proves (1).</p>

    <p class="text-gray-300">Next, we want to define a transcript extractor  <span class="math">\\mathcal{T}</span>  that given rewindable access to  <span class="math">\\langle \\mathcal{P}^*(s) \\longleftrightarrow \\mathcal{V}(pp,u) \\rangle</span>  outputs  <span class="math">\\mathsf{trans}_{\\mathcal{P}_{\\mathsf{ILC}}}</span> , which we would like to correspond to all messages sent between  <span class="math">\\mathcal{P}^*_{\\mathsf{ILC}}</span>  and the channel in  <span class="math">\\langle \\mathcal{P}^*_{\\mathsf{ILC}}(s,pp,u) \\overset{\\mathsf{ILC}}{\\longleftrightarrow}</span></p>

    <p class="text-gray-300"><span class="math">\\mathcal{V}_{\\mathsf{ILC}}(pp_{\\mathsf{ILC}}, u; \\rho_{\\mathsf{ILC}})</span> . Here  <span class="math">\\rho_{\\mathsf{ILC}}</span>  is the randomness used by the  <span class="math">\\mathcal{V}_{\\mathsf{ILC}}</span>  inside  <span class="math">\\mathcal{V}</span>  in the first execution of  <span class="math">\\mathcal{T}</span> 's oracle  <span class="math">\\langle \\mathcal{P}^*(s) \\longleftrightarrow \\mathcal{V}(pp,u) \\rangle</span> . However, we allow  <span class="math">\\mathcal{T}</span>  to fail if  <span class="math">\\mathcal{V}</span>  does not accept in this first transcript and further to fail with negligible probability. Formally, we want  <span class="math">\\mathcal T</span>  to run in expected PPT such that for all PPT  <span class="math">\\mathcal{A}</span> :</p>

    <p class="text-gray-300"><span class="math">$\\Pr\\begin{bmatrix} pp \\leftarrow \\mathcal{K}(1^{\\lambda}); (pp_{\\mathsf{ILC}}, \\cdot) = pp; (u, s) \\leftarrow \\mathcal{A}(pp); \\\\ \\operatorname{trans}_{\\mathcal{P}_{\\mathsf{ILC}}} \\leftarrow \\mathcal{T}^{\\langle \\mathcal{P}^{*}(s) \\longleftrightarrow \\mathcal{V}(pp, u) \\rangle}(pp, u); \\\\ \\operatorname{trans}_{\\mathcal{P}_{\\mathsf{ILC}}} \\leftarrow \\langle \\mathcal{P}^{*}_{\\mathsf{ILC}}(s, pp, u) \\overset{\\mathsf{ILC}}{\\longleftrightarrow} \\mathcal{V}_{\\mathsf{ILC}}(pp_{\\mathsf{ILC}}, u; \\rho_{\\mathsf{ILC}}) \\rangle : \\\\ b = 1 \\wedge \\operatorname{trans}_{\\mathcal{P}_{\\mathsf{ILC}}} \\neq \\operatorname{trans}_{\\mathcal{P}_{\\mathsf{ILC}}} \\end{cases} \\approx 0.</span>$
(2)</p>

    <p class="text-gray-300">Here b is the value output by V the first time T's oracle runs  <span class="math">\\langle \\mathcal{P}^*(s) \\longleftrightarrow</span>  <span class="math">\\mathcal{V}(pp,u)</span> , and the randomness  <span class="math">\\rho_{\\mathsf{ILC}}</span>  used by  <span class="math">\\mathcal{V}_{\\mathsf{ILC}}</span>  in the third line is identical to the random value used by the  <span class="math">\\mathcal{V}_{\\mathsf{ILC}}</span>  inside  <span class="math">\\mathcal{V}</span>  in the first transcript. On input (pp, u), the transcript extractor  <span class="math">\\mathcal{T}</span>  will first use its oracle to get a transcript of  <span class="math">\\langle \\mathcal{P}^*(s) \\longleftrightarrow \\mathcal{V}(pp, u; (\\rho_{\\mathsf{ILC}}, \\rho)) \\rangle</span> . If  <span class="math">\\mathcal{V}</span>  rejects,  <span class="math">\\mathcal{T}</span>  will just abort. If  <span class="math">\\mathcal{V}</span>  accepts,  <span class="math">\\mathcal{T}</span>  will rewind the last message of  <span class="math">\\mathcal{P}^*</span>  to get a transcript for a new random challenge J.  <span class="math">\\mathcal{T}</span>  continues this way, until it has an accepting transcript for 2nindependently chosen sets J. Notice that if there is only one choice of J that results in  <span class="math">\\mathcal{V}</span>  accepting,  <span class="math">\\mathcal{P}^*</span>  will likely have received each allowed challenge around 2n times and  <span class="math">\\mathcal{T}</span>  will get the exact same transcript 2n times before it is done rewinding. Still,  <span class="math">\\mathcal{T}</span>  runs in expected polynomial time: if a fraction p of all allowed set J results in accept, the expected number of rewindings given that the first transcripts accepts is  <span class="math">\\frac{2n-1}{p}</span> . However, the probability that the first run accepts is p, and if it does not accept,  <span class="math">\\mathcal{T}</span>  does not do any rewindings. In total, that gives  <span class="math">\\frac{(2n-1)p}{n} = 2n-1</span>  rewindings in expectation.</p>

    <p class="text-gray-300">We let  <span class="math">J_1, \\ldots, J_{2n}</span>  denote the set of challenges J in the accepting transcripts obtained by  <span class="math">\\mathcal{T}</span> . If  <span class="math">\\bigcup_{i=1}^{2n} J_i</span>  has less than  <span class="math">2n - \\frac{\\mathsf{hd}_{\\min}}{3}</span>  elements,  <span class="math">\\mathcal{T}</span>  terminates. Otherwise,  <span class="math">\\mathcal{T}</span>  is defined similarly to  <span class="math">\\mathcal{P}^*_{\\mathsf{ILC}}</span> : it uses the values of the openings to get at least  <span class="math">2n - \\frac{\\text{hd}_{\\min}}{3}</span>  columns of each  <span class="math">E_i</span> . For each of the row vectors,  <span class="math">e_{\\tau}</span> , it computes  <span class="math">v_{\\tau}</span>  and  <span class="math">r_{\\tau}</span>  such that  <span class="math">\\mathsf{E}_{\\mathcal{C}}(v_{\\tau}, r_{\\tau})</span>  agrees with  <span class="math">e_{\\tau}</span>  in all entries  <span class="math">(e_{\\tau})_j</span>  for which the j'th column have been revealed, if such v exists. Since  <span class="math">\\mathcal{T}</span>  will not correct any errors, finding such  <span class="math">v_{\\tau}</span>  and  <span class="math">r_{\\tau}</span>  corresponds to solving a linear set of equations. Notice that since the minimum distance is more than  <span class="math">2\\frac{hd_{\\min}}{2}</span>  there is at most one such  <span class="math">v_{\\tau}</span>  for each  <span class="math">\\tau \\in [t]</span> . If for some  <span class="math">\\tau</span>  there is no such  <span class="math">v_{\\tau}</span> , then Taborts, otherwise  <span class="math">\\mathcal{T}</span>  use the resulting vectors  <span class="math">\\boldsymbol{v}_{\\tau}</span>  as the prover messages to define</p>

    <p class="text-gray-300">If  <span class="math">|\\bigcup_{i=1}^{\\kappa} J_i| &lt; 2n - \\frac{\\mathsf{hd}_{\\min}}{3}</span> , there are at least  <span class="math">\\frac{\\mathsf{hd}_{\\min}}{6}</span>  numbers in  <span class="math">[n] \\setminus \\bigcup_{i=1}^{\\kappa} J_i</span>  or in  <span class="math">\\{n+1,\\ldots,2n\\} \\setminus \\bigcup_{i=1}^{\\kappa} J_i</span> . In either case, a random allowed J has negligible probability of being contained in  <span class="math">\\bigcup_{i=1}^{\\kappa} J_i</span> . Since  <span class="math">\\mathcal{T}</span>  runs in expected polynomial time, this implies by induction that there is only negligible probability that  <span class="math">|\\bigcup_{i=1}^{\\kappa} J_i| &lt; \\min(\\kappa, 2n - \\frac{\\mathsf{hd}_{\\min}}{3})</span>  and therefore  <span class="math">|\\bigcup_{i=1}^{2n} J_i| &lt; 2n - \\frac{\\mathsf{hd}_{\\min}}{3}</span> . Finally, we need to show</p>

    <p class="text-gray-300">Claim. The probability that for some  <span class="math">\\tau</span>  there are no  <span class="math">\\boldsymbol{v}_{\\tau}</span>  and  <span class="math">\\boldsymbol{r}_{\\tau}</span>  such that  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}(\\boldsymbol{v}_{\\tau}, \\boldsymbol{r}_{\\tau})</span>  agrees with  <span class="math">\\boldsymbol{e}_{\\tau}</span>  on the opened  <span class="math">j \\in \\bigcup_{i=1}^{2n} J_i</span>  and b=1 is negligible.</p>

    <p class="text-gray-300">In particular, the probability that b = 1 but  <span class="math">\\mathcal{T}</span>  does not extract the transcript of  <span class="math">\\mathcal{P}_{\\mathsf{ILC}}^*</span>  is negligible.</p>

    <p class="text-gray-300"><em>Proof.</em> Since we can ignore events that happen with negligible probability, and the expected number of rewindings is polynomial, we can assume that in all the rewindings,  <span class="math">\\mathcal{P}^*</span>  only makes openings to the most common openings. We showed that the probability that b=1 but  <span class="math">\\mathcal{P}^*</span>  sends a  <span class="math">V_{(Q)}^* \\neq V</span>  is negligible and by the same argument the probability that b=1 but  <span class="math">\\mathcal{P}^*</span>  sends  <span class="math">\\mathbf{v}_{(\\gamma)}^* \\neq \\mathbf{v}_{(\\gamma)}</span>  is negligible. Therefore, in the following, we will assume  <span class="math">\\mathbf{v}_{(\\gamma)}^* = \\mathbf{v}_{(\\gamma)}</span> .</p>

    <p class="text-gray-300">Now suppose that there is some  <span class="math">e_{\\tau}</span>  such that the opened values are inconsistent with being  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}(v_{\\tau}, r_{\\tau})</span>  for any  <span class="math">r_{\\tau}</span> . That is, there is some j such that  <span class="math">j, n+j \\in \\bigcup_{i=1}^{2n} J_i</span>  and  <span class="math">(e_{\\tau})_j - (e_{\\tau})_{n+j} \\neq \\mathsf{E}_{\\mathcal{C}}(v)_j</span> . For uniformly chosen  <span class="math">\\gamma_{\\tau} \\in \\mathbb{F}</span> , we get that  <span class="math">\\gamma_{\\tau}((e_{\\tau})_j - (e_{\\tau})_{n+j} - \\mathsf{E}_{\\mathcal{C}}(v)_j)</span>  is uniformly distributed in  <span class="math">\\mathbb{F}</span> . Hence for a random  <span class="math">\\gamma \\in \\mathbb{F}^t</span> , we have that  <span class="math">\\gamma \\cdot ((e)_j - (e)_{n+j} - \\mathsf{E}_{\\mathcal{C}}(v)_j)</span>  is uniformly distributed. When  <span class="math">\\mathcal{V}</span>  sends  <span class="math">\\gamma</span> ,  <span class="math">\\mathcal{P}^*</span>  will respond with  <span class="math">v_{(\\gamma)}^* = v_{(\\gamma)}</span>  and some  <span class="math">r_{(\\gamma)}^*</span> .  <span class="math">\\mathcal{V}</span>  will only accept on a challenge J if for all  <span class="math">j \\in J</span>  we have  <span class="math">(e_0 + \\gamma e)_j = \\tilde{\\mathsf{E}}_{\\mathcal{C}}(v_{(\\gamma)}, r_{(\\gamma)}^*)_j</span> . Since  <span class="math">j, n+j \\in \\bigcup_{i=1}^{2n} J_i</span>  we have  <span class="math">(e_0 + \\gamma e)_j = \\tilde{\\mathsf{E}}_{\\mathcal{C}}(v_{(\\gamma)}, r_{(\\gamma)}^*)_j</span>  and  <span class="math">(e_0 + \\gamma e)_{n+j} = \\tilde{\\mathsf{E}}_{\\mathcal{C}}(v_{(\\gamma)}, r_{(\\gamma)}^*)_{n+j}</span>  so</p>

    <p class="text-gray-300"><span class="math">$\\begin{aligned} (\\boldsymbol{e}_0)_j - (\\boldsymbol{e}_0)_{n+j} + \\boldsymbol{\\gamma} \\boldsymbol{e}_j - \\boldsymbol{\\gamma} \\boldsymbol{e}_{n+j} &amp;= &amp; \\tilde{\\mathsf{E}}_{\\mathcal{C}}(\\boldsymbol{v}_{(\\boldsymbol{\\gamma})}, \\boldsymbol{r}^*_{(\\boldsymbol{\\gamma})})_j - \\tilde{\\mathsf{E}}_{\\mathcal{C}}(\\boldsymbol{v}_{(\\boldsymbol{\\gamma})}, \\boldsymbol{r}^*_{(\\boldsymbol{\\gamma})})_{n+j} \\ &amp;= &amp; \\tilde{\\mathsf{E}}_{\\mathcal{C}}(\\boldsymbol{v}_{(\\boldsymbol{\\gamma})})_j \\ &amp;= &amp; \\tilde{\\mathsf{E}}_{\\mathcal{C}}(\\boldsymbol{v}_{(\\boldsymbol{\\gamma})})_j \\end{aligned}</span>$</p>

    <p class="text-gray-300">that is,</p>

    <p class="text-gray-300"><span class="math">$\\boldsymbol{\\gamma}\\boldsymbol{e}_j - \\boldsymbol{\\gamma}\\boldsymbol{e}_{n+j} - \\boldsymbol{\\gamma}\\mathsf{E}_{\\mathcal{C}}(\\boldsymbol{v})_j = \\mathsf{E}_{\\mathcal{C}}(\\boldsymbol{v}_0)_j - (\\boldsymbol{e}_0)_j + (\\boldsymbol{e}_0)_{n+j}</span>$</p>

    <p class="text-gray-300">For random  <span class="math">\\gamma</span>  the left-hand side is uniform and the right-hand side is fixed, hence equality only happens with negligible probability. That proves the claim.</p>

    <p class="text-gray-300">Since  <span class="math">\\mathcal{E}_{\\mathsf{ILC}}^{\\langle \\mathcal{P}_{\\mathsf{ILC}}^*(s,pp,u) \\overset{\\mathsf{ILC}}{\\longleftrightarrow} \\mathcal{V}_{\\mathsf{ILC}}(pp_{\\mathsf{ILC}},u) \\rangle}(pp,u)</span>  is a straight-line extractor, we can simply assume that it gets the transcript as an input, and can be written as  <span class="math">\\mathcal{E}_{\\mathsf{ILC}}(pp_{\\mathsf{ILC}},u,\\mathsf{trans}_{\\mathcal{P}_{\\mathsf{ILC}}})</span> . For any PPT  <span class="math">\\mathcal{A}</span>  consider the following experiment.</p>

    <p class="text-gray-300"><span class="math">$\\begin{bmatrix} pp \\leftarrow \\mathcal{K}(1^{\\lambda}); (pp_{\\mathsf{ILC}}, \\cdot) = pp; (u, s) \\leftarrow \\mathcal{A}(pp); \\\\ \\operatorname{trans}_{\\mathcal{P}_{\\mathsf{ILC}}} \\leftarrow \\mathcal{T}^{\\langle \\mathcal{P}^{*}(s) \\longleftrightarrow \\mathcal{V}(pp, u)}(pp, u); \\\\ \\operatorname{trans}_{\\mathcal{P}_{\\mathsf{ILC}}} \\leftarrow \\langle \\mathcal{P}^{*}_{\\mathsf{ILC}}(s, pp, u) \\overset{\\mathsf{ILC}}{\\longleftrightarrow} \\mathcal{V}_{\\mathsf{ILC}}(pp_{\\mathsf{ILC}}, u; \\rho_{\\mathsf{ILC}}) \\rangle = b_{\\mathsf{ILC}}; \\\\ w \\leftarrow \\mathcal{E}_{\\mathsf{ILC}}(pp_{\\mathsf{ILC}}, u, \\operatorname{trans}_{\\mathcal{P}_{\\mathsf{ILC}}}); \\\\ \\widetilde{w} \\leftarrow \\mathcal{E}_{\\mathsf{ILC}}(pp_{\\mathsf{ILC}}, u, \\operatorname{trans}_{\\mathcal{P}_{\\mathsf{ILC}}}); \\end{bmatrix}</span>$</p>

    <p class="text-gray-300"><span class="math">$(3)</span>$</p>

    <p class="text-gray-300">We have shown that when doing this experiment, the probability that  <span class="math">b = 1 \\wedge b_{\\mathsf{ILC}} = 0</span>  and the probability that  <span class="math">b = 1 \\wedge \\mathsf{trans}_{\\mathcal{P}_{\\mathsf{ILC}}} \\neq \\mathsf{trans}_{\\mathcal{P}_{\\mathsf{ILC}}}</span>  are both negligible. By knowledge soundness of  <span class="math">(\\mathcal{K}_{\\mathsf{ILC}}, \\mathcal{P}_{\\mathsf{ILC}}, \\mathcal{V}_{\\mathsf{ILC}})</span> , the probability that  <span class="math">b_{\\mathsf{ILC}} = 1 \\wedge (pp, u, w) \\notin \\mathcal{R}</span>  is also negligible. Finally, if  <span class="math">\\mathsf{trans}_{\\mathcal{P}_{\\mathsf{ILC}}} = \\mathsf{trans}_{\\mathcal{P}_{\\mathsf{ILC}}}</span>  then clearly  <span class="math">w = \\widetilde{w}</span> . Taken together this implies that the probability of  <span class="math">b = 1 \\wedge (pp, u, \\widetilde{w}) \\notin \\mathcal{R}</span>  is negligible. We now define  <span class="math">\\mathcal{E}^{\\langle \\mathcal{P}^*(s) \\longleftrightarrow \\mathcal{V}(pp, u) \\rangle}(pp, u)</span>  to compute  <span class="math">\\mathcal{E}_{\\mathsf{ILC}}(pp_{\\mathsf{ILC}}, u, \\mathcal{T}^{\\langle \\mathcal{P}^*(s) \\longleftrightarrow \\mathcal{V}(pp, u) \\rangle}(pp, u))</span> . The above experiment shows that  <span class="math">(\\mathcal{K}, \\mathcal{P}, \\mathcal{V})</span>  is knowledge sound with  <span class="math">\\mathcal{E}</span>  as extractor.</p>

    <p class="text-gray-300"><strong>Theorem 5 (SHVZK).</strong> If  <span class="math">(\\mathcal{K}_{\\mathsf{ILC}}, \\mathcal{P}_{\\mathsf{ILC}}, \\mathcal{V}_{\\mathsf{ILC}})</span>  is perfect SHVZK and (Setup, Commit) is computationally (statistically) hiding then  <span class="math">(\\mathcal{K}, \\mathcal{P}, \\mathcal{V})</span>  is computationally (statistically) SHVZK.</p>

    <p class="text-gray-300"><em>Proof.</em> To prove we have SHVZK we describe how the simulator  <span class="math">\\mathcal{S}(pp, u, \\rho)</span>  should simulate the view of  <span class="math">\\mathcal{V}</span> . Along the way, we will argue why, the variables output by  <span class="math">\\mathcal{S}</span>  have the correct joint distribution. To keep the proof readable, instead of saying that &quot;the joint distribution of [random variable] and all previously defined random variables is identical to the distribution in the real view of  <span class="math">\\mathcal{V}</span>  in  <span class="math">\\langle \\mathcal{P}(pp, u, w) \\longleftrightarrow \\mathcal{V}(pp, u) \\rangle</span> &quot; we will simply say that &quot;[random variable] has the correct distribution&quot;.</p>

    <p class="text-gray-300">Using the randomness  <span class="math">\\rho</span>  the simulator learns the queries  <span class="math">\\rho_{\\mathsf{ILC}} = (x_1, \\dots, x_{\\mu-1}, Q)</span>  the internal  <span class="math">\\mathcal{V}_{\\mathsf{ILC}}</span>  run by the honest  <span class="math">\\mathcal{V}</span>  will send.  <span class="math">\\mathcal{S}</span>  can therefore run  <span class="math">\\mathcal{S}_{\\mathsf{ILC}}(pp_{\\mathsf{ILC}}, u, \\rho_{\\mathsf{ILC}})</span>  to simulate the view of the internal  <span class="math">\\mathcal{V}_{\\mathsf{ILC}}</span> . This gives it  <span class="math">(t_1, \\dots, t_\\mu, V_{(Q)})</span> . By the SHVZK property of  <span class="math">(\\mathcal{K}_{\\mathsf{ILC}}, \\mathcal{P}_{\\mathsf{ILC}}, \\mathcal{V}_{\\mathsf{ILC}})</span>  these random variables will all have the correct joint distribution.</p>

    <p class="text-gray-300">Then S reads the rest of  <span class="math">\\rho</span>  to learn also the challenges  <span class="math">\\gamma</span>  and J that  <span class="math">\\mathcal V</span>  will send. The simulator picks uniformly at random  <span class="math">\\mathbf v_{(\\gamma)} \\leftarrow \\mathbb F^k</span> . Since in a real proof  <span class="math">\\mathbf v_0</span>  is chosen at random, we see that the simulated  <span class="math">\\mathbf v_{(\\gamma)}</span>  has the correct distribution. Now S picks  <span class="math">E_{01}|_J,\\ldots,E_\\mu|_J</span>  uniformly at random. Recall that we defined  <span class="math">\\tilde{\\mathsf E}_{\\mathcal C}(\\mathbf v;\\mathbf r)=(\\mathsf E_{\\mathcal C}(\\mathbf v)+\\mathbf r,\\mathbf r)</span>  and by definition of J being allowed, we have for all  <span class="math">j\\in J</span>  that  <span class="math">j+n\\notin J</span> . This means for any choice of  <span class="math">\\mathbf v_0\\in\\mathbb F^k</span>  and  <span class="math">V\\in\\mathbb F^{t\\times k}</span>  that when we choose random  <span class="math">\\mathbf v_0\\leftarrow\\mathbb F^n</span>  and  <span class="math">R\\leftarrow\\mathbb F^{t\\times n}</span>  we get uniformly random  <span class="math">\\tilde{\\mathsf E}_{\\mathcal C}(\\mathbf v_0;\\mathbf v_0)|_J</span>  and  <span class="math">\\tilde{\\mathsf E}_{\\mathcal C}(V;R)</span> . Consequently,  <span class="math">E_{01}|_J,\\ldots,E_\\mu|_J</span>  have the correct distribution.</p>

    <p class="text-gray-300">Next, the simulator picks  <span class="math">\\mathbf{r}_{(\\gamma)} \\in \\mathbb{F}^n</span>  and  <span class="math">R_{(Q)} \\in \\mathbb{F}^{t \\times n}</span>  one entry and column at a time. For all j such that  <span class="math">j \\notin J</span>  and  <span class="math">j + n \\notin J</span>  the simulator picks random  <span class="math">(\\mathbf{r}_{(\\gamma)})_j \\leftarrow \\mathbb{F}</span>  and random  <span class="math">R_j \\leftarrow \\mathbb{F}^t</span> . For all j such that  <span class="math">j \\in J</span>  or  <span class="math">j + n \\in J</span> , the simulator then computes the unique  <span class="math">(\\mathbf{r}_{(\\gamma)})_j \\in \\mathbb{F}</span>  and  <span class="math">R_j \\in \\mathbb{F}^t</span>  such that we get  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}(\\mathbf{v}_{(\\gamma)}; \\mathbf{r}_{(\\gamma)}) = \\mathbf{e}_0|_J + \\gamma E|_J</span>  and  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}(V_{(Q)}; R_{(Q)}) = QE|_J</span> .</p>

    <p class="text-gray-300">Finally, S defines  <span class="math">E_{01}|_{\\bar{J}},\\ldots,E_{\\mu}|_{\\bar{J}}</span>  to be 0 matrices. It then picks  <span class="math">s_1,\\ldots,s_{\\mu}</span>  at random and makes the commitments  <span class="math">c_1,\\ldots,c_{\\mu}</span>  as in the protocol. For  <span class="math">j\\in J</span>  we see that all the  <span class="math">c_i|_j</span>  commitments are computed as in the real execution from values that have the same distribution as in a real proof. Hence, they will have the correct distribution. The  <span class="math">c_i|_j</span> s for  <span class="math">j\\notin J</span>  are commitments to different values than in a real proof. However, by the computational (statistical) hiding property of the commitment scheme, they have a distribution that is computationally (statistically) indistinguishable from the correct distribution.</p>

    <h4 id="sec-14" class="text-lg font-semibold mt-6">4.3 Efficiency</h4>

    <p class="text-gray-300">We will now estimate the efficiency of a compiled proof of knowledge  <span class="math">(\\mathcal{K}, \\mathcal{P}, \\mathcal{V})</span>  for  <span class="math">(pp, u, w) \\in \\mathcal{R}</span> . Let  <span class="math">\\mu</span>  be the number of rounds,  <span class="math">t = \\sum_{i=1}^{\\mu} t_i</span> , k, n given in  <span class="math">\\mathsf{E}_{\\mathcal{C}}</span> , and qc the query complexity, i.e.,  <span class="math">Q \\in \\mathbb{F}^{\\mathsf{qc} \\times t}</span> . Let  <span class="math">T_{\\mathcal{P}_{\\mathsf{ILC}}}</span>  be the running time of  <span class="math">\\mathcal{P}_{\\mathsf{ILC}}(pp_{\\mathsf{ILC}}, u, w)</span> ,  <span class="math">T_{\\mathsf{E}_{\\mathcal{C}}}(k)</span>  be the encoding time for a vector in  <span class="math">\\mathbb{F}^k</span> ,  <span class="math">T_{\\mathsf{Commit}}(t_i)</span>  be</p>

    <p class="text-gray-300">the time to commit to  <span class="math">t_i</span>  field elements,  <span class="math">T_{\\text{Mmul}}(\\text{qc}, t, b)</span>  be the time it takes to multiply matrices in  <span class="math">\\mathbb{F}^{\\text{qc} \\times t}</span>  and  <span class="math">\\mathbb{F}^{t \\times b}</span> , and  <span class="math">T_{\\mathcal{V}_{\\text{ILC}}}</span>  is the running time of  <span class="math">\\mathcal{V}_{\\text{ILC}}(pp_{\\text{ILC}}, u)</span> . Let furthermore  <span class="math">C_{\\text{ILC}}</span>  be the communication from the verifier to the prover in  <span class="math">\\langle \\mathcal{P}_{\\text{ILC}} \\stackrel{\\text{ILC}}{\\longleftrightarrow} \\mathcal{V}_{\\text{ILC}} \\rangle</span> ,  <span class="math">C_{\\text{Commit}}(t_i)</span>  be the combined size of commitment and randomness for a message consisting of  <span class="math">t_i</span>  field elements. We give the dominant factors of efficiency of the compiled proof in Fig. 7. The estimates presume  <span class="math">T_{\\text{Commit}}(t_1+1)</span>  is not too far from  <span class="math">T_{\\text{Commit}}(t_1)</span> .</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Measure</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Cost</th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Prover Computation</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">T_{\\mathcal{P}_{ILC}} + t \\cdot T_{\\tilde{E}_{\\mathcal{C}}}(k) + 2n \\cdot \\sum_{i=1}^{\\mu} T_{Commit}(t_i) + T_{\\mathrm{Mmul}}(\\mathrm{qc} + 1, t, k + n)</span></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\left T_{\\mathcal{V}_{ILC}} + (\\operatorname{qc} + 1) \\cdot T_{\\tilde{E}_{\\mathcal{C}}}(k) + 2\\lambda \\cdot \\sum_{i=1}^{\\mu} T_{Commit}(t_i) + T_{\\mathrm{Mmul}}(\\operatorname{qc} + 1, t, 2\\lambda)\\right </span></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Communication</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\left C_{ILC} + 2n \\cdot \\sum_{i=1}^{\\mu} C_{Commit}(t_i) + (qc+1) \\cdot (k+n) + (qc+1) \\cdot t + 2\\lambda \\cdot t\\right </span></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Round Complexity</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mu + 2</span></td>
          </tr>
        </tbody>
      </table>
    </div>

    <p class="text-gray-300">Fig. 7: Efficiency of a compiled proof of knowledge  <span class="math">(\\mathcal{K}, \\mathcal{P}, \\mathcal{V})</span>  for  <span class="math">(pp, u, w) \\in \\mathcal{R}</span> . Communication is measured in field elements and computation in field operations.</p>

    <h3 id="sec-15" class="text-xl font-semibold mt-8">5 Instantiations and Conclusion</h3>

    <p class="text-gray-300">Putting together the sequence of proofs and sub-proofs in the ILC model, compiling into the standard model using an error-correcting code and a commitment scheme, and finally instantiating the commitment scheme yields special honest-verifier zero-knowledge proofs for arithmetic circuit satisfiability.</p>

    <p class="text-gray-300">Let us now analyze the efficiency of the compilation we get from Fig. 7. If the error-correcting code is linear-time computable, we get  <span class="math">T_{\\tilde{\\mathsf{E}}_{\\mathcal{C}}}(k) = \\mathcal{O}(k)</span>  operations in  <span class="math">\\mathbb{F}</span> , and with the code from Druk and Ishai [DI14] is will actually be  <span class="math">\\mathcal{O}(k)</span>  additions in  <span class="math">\\mathbb{F}</span> .</p>

    <p class="text-gray-300">Let us now plug in the efficiency of our ILC proof given in Fig. 4 into the efficiency formulas in Fig. 7. We use  <span class="math">k \\approx \\sqrt{N}</span> ,  <span class="math">n = \\mathcal{O}(k)</span> ,  <span class="math">t = \\mathcal{O}(\\sqrt{N})</span> ,  <span class="math">\\mu = \\mathcal{O}(\\log\\log N)</span> ,  <span class="math">\\mathrm{qc} = 20 = \\mathcal{O}(1)</span>  and assume  <span class="math">k \\gg \\lambda</span> . We then get prover computation  <span class="math">T_{\\mathcal{P}} = \\mathcal{O}(N)</span>  multiplications  <span class="math">+2n \\cdot \\sum_{i=1}^{\\mu} T_{\\mathsf{Commit}}(t_i)</span> , verifier computation  <span class="math">T_{\\mathcal{V}} = \\mathcal{O}(N)</span>  additions  <span class="math">+2\\lambda \\cdot \\sum_{i=1}^{\\mu} T_{\\mathsf{Commit}}(t_i)</span> , communication  <span class="math">C = 2n \\cdot \\sum_{i=1}^{\\mu} C_{\\mathsf{Commit}}(t_i) + \\mathcal{O}(\\lambda\\sqrt{N})</span>  field elements, and round complexity  <span class="math">\\mu = \\mathcal{O}(\\log\\log N)</span> .</p>

    <p class="text-gray-300">Instantiating with the commitment scheme from Applebaum et al. [AHI&lt;sup&gt;+&lt;/sup&gt;17] we get computational knowledge soundness and statistical SHVZK. The commitments are compact, a commitment has size  <span class="math">C_{\\mathsf{Commit}}(t_i) = \\mathrm{poly}(\\lambda)</span>  regardless of the message size, giving us sub-linear communication. The commitments can be computed in linear time at a cost of  <span class="math">T_{\\mathsf{Commit}}(t_i) = \\mathrm{poly}(\\lambda) + \\mathcal{O}(t_i)</span>  additions., giving us linear time computation for prover and verifier.</p>

    <p class="text-gray-300">Instantiating with the commitment from Ishai et al. [IKOS08] we get statistical knowledge soundness and computational SHVZK. The commitments have</p>

    <p class="text-gray-300">linear size  <span class="math">C_{\\mathsf{Commit}}(t_i) = \\mathrm{poly}\\lambda + t_i</span>  giving us linear communication overall. The commitments can be computed in linear time at a cost of  <span class="math">T_{\\mathsf{Commit}}(t_i) = \\mathrm{poly}(\\lambda) + \\mathcal{O}(t_i)</span>  additions, again giving us linear time computation for prover and verifier.</p>

    <p class="text-gray-300">We summarize the costs in Table 8 below and conclude that we now have SHVZK proof systems for arithmetic circuit satisfiability where the prover computation only has constant overhead compared to direct computation of the arithmetic circuit given the witness. Moreover, the verifier computation is a linear number of <em>additions</em>, which is proportional to the time it takes simply to read the instance.</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">\\boxed{ Measure \\backslash Instantiation }</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Using [AHI &lt;sup&gt;+&lt;/sup&gt; 17]</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Using [IKOS08]</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"></th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Prover Computation</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(N)</span> multiplications in <span class="math">\\mathbb{F}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(N)</span> multiplications in <span class="math">\\mathbb{F}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Verifier Computation</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(N)</span> additions in <span class="math">\\mathbb{F}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(N)</span> additions in <span class="math">\\mathbb{F}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Communication</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\operatorname{poly}(\\lambda)\\sqrt{N}</span> field elements</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(N)</span> field elements</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Round Complexity</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(\\log \\log N)</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(\\log \\log N)</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Completeness</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Perfect</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Perfect</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Knowledge Soundness</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Computational</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Statistical</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">SHVZK</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Statistical</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Computational</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
          </tr>
        </tbody>
      </table>
    </div>

    <p class="text-gray-300">Fig. 8: Efficiency of two instantiations of our SHVZK proofs.</p>

    <h4 id="sec-16" class="text-lg font-semibold mt-6">References</h4>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li><p class="text-gray-300">[AHI&lt;sup&gt;+&lt;/sup&gt;17] B. Applebaum, N. Haramaty, Y. Ishai, E. Kushilevitz, and V. Vaikuntanathan. Low-complexity cryptographic hash functions. Cryptology ePrint Archive, Report 2017/036, 2017. http://eprint.iacr.org/2017/036.</p></li>
      <li><p class="text-gray-300">[BCC&lt;sup&gt;+&lt;/sup&gt;16] J. Bootle, A. Cerulli, P. Chaidos, J. Groth, and C. Petit. Efficient zero-knowledge arguments for arithmetic circuits in the discrete log setting. In Advances in Cryptology–EUROCRYPT, LNCS. Springer, 2016.</p></li>
      <li><p class="text-gray-300">[BCCT12] N. Bitansky, R. Canetti, A. Chiesa, and E. Tromer. From extractable collision resistance to succinct non-interactive arguments of knowledge, and back again. In Innovations in Theoretical Computer Science Conference—ITCS. ACM, 2012.</p></li>
      <li><p class="text-gray-300">[BCCT13] N. Bitansky, R. Canetti, A. Chiesa, and E. Tromer. Recursive composition and bootstrapping for SNARKS and proof-carrying data. In ACM Symposium on Theory of Computing–STOC. ACM, 2013.</p></li>
      <li><p class="text-gray-300">[BCG&lt;sup&gt;+&lt;/sup&gt;13] E. Ben-Sasson, A. Chiesa, D. Genkin, E. Tromer, and M. Virza. <em>SNARKs</em> for C: Verifying program executions succinctly and in zero knowledge. In Advances in Cryptology–CRYPTO, LNCS. Springer, 2013.</p></li>
      <li><p class="text-gray-300">[BCI+13] N. Bitansky, A. Chiesa, Y. Ishai, R. Ostrovsky, and O. Paneth. Erratum: Succinct non-interactive arguments via linear interactive proofs. In Theory of Cryptography-TCC, LNCS. Springer, 2013.</p></li>
      <li><p class="text-gray-300">[BFM88] M. Blum, P. Feldman, and S. Micali. Non-interactive zero-knowledge and its applications. In ACM Symposium on Theory of Computing–STOC. ACM, 1988.</p></li>
      <li><p class="text-gray-300">[BJY97] M. Bellare, M. Jakobsson, and M. Yung. Round-optimal zero-knowledge arguments based on any one-way function. In Advances in Cryptology– EUROCRYPT, LNCS. Springer, 1997.</p></li>
      <li><p class="text-gray-300">[BR93] M. Bellare and P. Rogaway. Random oracles are practical: A paradigm for designing efficient protocols. In ACM Conference on Computer and Communications Security (ACM CCS). ACM, 1993.</p></li>
      <li><p class="text-gray-300">[BSCG&lt;sup&gt;+&lt;/sup&gt;16] E. Ben-Sasson, A. Chiesa, A. Gabizon, M. Riabzev, and N. Spooner. Short interactive oracle proofs with constant query complexity, via composition and sumcheck. In Electronic Colloquium on Computational Complexity (ECCC). 2016.</p></li>
      <li><p class="text-gray-300">[BSCGV16] E. Ben-Sasson, A. Chiesa, A. Gabizon, and M. Virza. Quasi-linear size zero knowledge from linear-algebraic PCPs. In Theory of Cryptography– TCC, LNCS. Springer, 2016.</p></li>
      <li><p class="text-gray-300">[BSCS16] E. Ben-Sasson, A. Chiesa, and N. Spooner. Interactive oracle proofs. In Theory of Cryptography–TCC, LNCS. Springer, 2016.</p></li>
      <li><p class="text-gray-300">[CD98] R. Cramer and I. Damg˚ard. Zero-knowledge proofs for finite field arithmetic; or: Can zero-knowledge be for free? In Advances in Cryptology– CRYPTO, LNCS. Springer, 1998.</p></li>
      <li><p class="text-gray-300">[CDD&lt;sup&gt;+&lt;/sup&gt;16] I. Cascudo, I. Damg˚ard, B. David, N. D¨ottling, and J. B. Nielsen. Rate-1, linear time and additively homomorphic UC commitments. In Advances in Cryptology–CRYPTO, LNCS. Springer, 2016.</p></li>
      <li><p class="text-gray-300">[CDP12] R. Cramer, I. Damg˚ard, and V. Pastro. On the amortized complexity of zero knowledge protocols for multiplicative relations. In Information Theoretic Security–ICITS, LNCS. Springer, 2012.</p></li>
      <li><p class="text-gray-300">[CDS94] R. Cramer, I. Damg˚ard, and B. Schoenmakers. Proofs of partial knowledge and simplified design of witness hiding protocols. In Advances in Cryptology–CRYPTO, LNCS. Springer, 1994.</p></li>
      <li><p class="text-gray-300">[CGM16] M. Chase, C. Ganesh, and P. Mohassel. Efficient zero-knowledge proof of algebraic and non-algebraic statements with applications to privacy preserving credentials. Cryptology ePrint Archive, Report 2016/583, 2016. http://eprint.iacr.org/2016/583.</p></li>
      <li><p class="text-gray-300">[Dam00] I. Damg˚ard. Efficient concurrent zero-knowledge in the auxiliary string model. In Advances in Cryptology–EUROCRYPT, LNCS. Springer, 2000.</p></li>
      <li><p class="text-gray-300">[DI06] I. Damg˚ard and Y. Ishai. Scalable secure multiparty computation. In Advances in Cryptology–CRYPTO, LNCS. Springer, 2006.</p></li>
      <li><p class="text-gray-300">[DI14] E. Druk and Y. Ishai. Linear-time encodable codes meeting the Gilbert-Varshamov bound and their cryptographic applications. In Innovations in Theoretical Computer Science Conference–ITCS. ACM, 2014.</p></li>
      <li><p class="text-gray-300">[DIK10] I. Damg˚ard, Y. Ishai, and M. Krøigaard. Perfectly secure multiparty computation and the computational overhead of cryptography. In Advances in Cryptology–EUROCRYPT, LNCS. Springer, 2010.</p></li>
      <li><p class="text-gray-300">[FNO15] T. K. Frederiksen, J. B. Nielsen, and C. Orlandi. Privacy-free garbled circuits with applications to efficient zero-knowledge. In Advances in Cryptology–EUROCRYPT, LNCS. Springer, 2015.</p></li>
      <li><p class="text-gray-300">[FS86] A. Fiat and A. Shamir. How to prove yourself: Practical solutions to identification and signature problems. In Advances in Cryptology–CRYPTO, LNCS. Springer, 1986.</p></li>
      <li><p class="text-gray-300">[Gal62] R. G. Gallager. Low-density parity-check codes. IRE Trans. Information Theory, 8(1):21, 1962.</p></li>
      <li><p class="text-gray-300">[GGI&lt;sup&gt;+&lt;/sup&gt;14] C. Gentry, J. Groth, Y. Ishai, C. Peikert, A. Sahai, and A. Smith. Using fully homomorphic hybrid encryption to minimize non-interative zeroknowledge proofs. Journal of Cryptology, 2014.</p></li>
      <li><p class="text-gray-300">[GGPR13] R. Gennaro, C. Gentry, B. Parno, and M. Raykova. Quadratic span programs and succinct NIZKs without PCPs. In Advances in Cryptology– EUROCRYPT, LNCS. Springer, 2013.</p></li>
      <li><p class="text-gray-300">[GI01] V. Guruswami and P. Indyk. Expander-based constructions of efficiently decodable codes. In Symposium on Foundations of Computer Science–FOCS. IEEE Computer Society, 2001.</p></li>
      <li><p class="text-gray-300">[GI02] V. Guruswami and P. Indyk. Near-optimal linear-time codes for unique decoding and new list-decodable codes over smaller alphabets. In ACM Symposium on Theory of Computing–STOC. ACM, 2002.</p></li>
      <li><p class="text-gray-300">[GI03] V. Guruswami and P. Indyk. Linear time encodable and list decodable codes. In ACM Symposium on Theory of Computing–STOC. ACM, 2003.</p></li>
      <li><p class="text-gray-300">[GI05] V. Guruswami and P. Indyk. Linear-time encodable/decodable codes with near-optimal rate. IEEE Trans. Information Theory, 51(10):3393, 2005.</p></li>
      <li><p class="text-gray-300">[GKR08] S. Goldwasser, Y. T. Kalai, and G. N. Rothblum. Delegating computation: interactive proofs for muggles. In ACM Symposium on Theory of Computing–STOC. ACM, 2008.</p></li>
      <li><p class="text-gray-300">[GMR85] S. Goldwasser, S. Micali, and C. Rackoff. The knowledge complexity of interactive proof-systems (extended abstract). In ACM Symposium on Theory of Computing–STOC. ACM, 1985.</p></li>
      <li><p class="text-gray-300">[GQ88] L. C. Guillou and J.-J. Quisquater. A practical zero-knowledge protocol fitted to security microprocessor minimizing both trasmission and memory. In Advances in Cryptology–EUROCRYPT, LNCS. Springer, 1988.</p></li>
      <li><p class="text-gray-300">[Gro04] J. Groth. Honest verifier zero-knowledge arguments applied. BRICS, 2004.</p></li>
      <li><p class="text-gray-300">[Gro09] J. Groth. Linear algebra with sub-linear zero-knowledge arguments. In Advances in Cryptology–CRYPTO, LNCS. Springer, 2009.</p></li>
      <li><p class="text-gray-300">[Gro10] J. Groth. Short pairing-based non-interactive zero-knowledge arguments. In Advances in Cryptology–ASIACRYPT, LNCS. Springer, 2010.</p></li>
      <li><p class="text-gray-300">[Gro16] J. Groth. On the size of pairing-based non-interactive arguments. In Advances in Cryptology–EUROCRYPT, LNCS. Springer, 2016.</p></li>
      <li><p class="text-gray-300">[GSV98] O. Goldreich, A. Sahai, and S. Vadhan. Honest-verifier statistical zeroknowledge equals general statistical zero-knowledge. In ACM Symposium on Theory of Computing–STOC. ACM, 1998.</p></li>
      <li><p class="text-gray-300">[HM96] S. Halevi and S. Micali. Practical and provably-secure commitment schemes from collision-free hashing. In Advances in Cryptology–CRYPTO, vol. 1109 of LNCS. Springer, 1996.</p></li>
      <li><p class="text-gray-300">[HMR15] Z. Hu, P. Mohassel, and M. Rosulek. Efficient zero-knowledge proofs of non-algebraic statements with sublinear amortized cost. In Advances in Cryptology–CRYPTO, LNCS. Springer, 2015.</p></li>
      <li><p class="text-gray-300">[IKOS08] Y. Ishai, E. Kushilevitz, R. Ostrovsky, and A. Sahai. Cryptography with constant computational overhead. In ACM Symposium on Theory of Computing–STOC. ACM, 2008.</p></li>
      <li><p class="text-gray-300">[IKOS09] Y. Ishai, E. Kushilevitz, R. Ostrovsky, and A. Sahai. Zero-knowledge proofs from secure multiparty computation. SIAM Journal on Computing, 39(3):1121, 2009.</p></li>
      <li><p class="text-gray-300">[JKO13] M. Jawurek, F. Kerschbaum, and C. Orlandi. Zero-knowledge using garbled circuits: how to prove non-algebraic statements efficiently. In ACM Conference on Computer and Communications Security (ACM CCS). ACM, 2013.</p></li>
      <li><p class="text-gray-300">[Kil92] J. Kilian. A note on efficient zero-knowledge proofs and arguments. In ACM Symposium on Theory of Computing–STOC. ACM, 1992.</p></li>
      <li><p class="text-gray-300">[KR08] Y. T. Kalai and R. Raz. Interactive PCP. In Automata, Languages and Programming: International Colloquium – ICALP, LNCS. Springer, 2008.</p></li>
      <li><p class="text-gray-300">[MP03] D. Micciancio and E. Petrank. Simulatable commitments and efficient concurrent zero-knowledge. In Advances in Cryptology–EUROCRYPT, LNCS. Springer, 2003.</p></li>
      <li><p class="text-gray-300">[MRS17] P. Mohassel, M. Rosulek, and A. Scafuro. Sublinear zero-knowledge arguments for RAM programs. In Advances in Cryptology–EUROCRYPT, LNCS. Springer, 2017.</p></li>
      <li><p class="text-gray-300">[PHGR13] B. Parno, J. Howell, C. Gentry, and M. Raykova. Pinocchio: Nearly practical verifiable computation. In IEEE Symposium on Security and Privacy. IEEE Computer Society, 2013.</p></li>
      <li><p class="text-gray-300">[Sch91] C.-P. Schnorr. Efficient signature generation by smart cards. Journal of Cryptology, 4(3):161, 1991.</p></li>
      <li><p class="text-gray-300">[Spi95] D. A. Spielman. Linear-time encodable and decodable error-correcting codes. In ACM Symposium on Theory of Computing–STOC. ACM, 1995.</p></li>
    </ul>

    <h3 id="sec-17" class="text-xl font-semibold mt-8">A Sub-proofs in the ILC Model</h3>

    <p class="text-gray-300">We will now give full descriptions of all the sub-proofs used in Section 3. Some of the sub-proofs require additional sub-proofs, the full structure of the arithmetic circuit satisfiability proof in the ILC model is given in Fig. 9.</p>

    <p class="text-gray-300">    <img src="_page_30_Figure_3.jpeg" alt="" class="my-4 max-w-full" />
</p>

    <p class="text-gray-300">Fig. 9: ILC proof for arithmetic circuit satisfiability decomposed into sub-proofs.</p>

    <p class="text-gray-300">Many of the sub-proofs are presented as stand-alone proofs, but when run as part of a larger arithmetic circuit protocol, it may be that certain vectors will already have been committed to the ILC channel. This is emphasized by putting square brackets around committed values. As most of the proofs involves relations over previously committed vectors, we introduce a new notation for them. A relation R consists of pairs (pp, u) for which instances u can contain committed vectors, denoted by the presence of square brackets around the vectors. Whenever the prover P gets as input a pair (pp, u), we use the bracket notation to indicate that she knows the content of committed elements in u. Differently, we use the bracket notation to denote that the verifier V does not learn what is inside the commitments, apart that these are already stored in the ILC channel and how many vectors there are.</p>

    <p class="text-gray-300">In the proof of knowledge soundness of each sub-protocol, we assume that the knowledge extractor for the sub-protocols will have access to any values that the prover has committed to as part of the larger arithmetic circuit protocol.</p>

    <h4 id="sec-18" class="text-lg font-semibold mt-6">A.1 Proof for the Correct Opening of Committed Vectors</h4>

    <p class="text-gray-300">Next, we give a proof for checking consistency of committed vectors with values in the statement. In this proof the prover Peq simply commits to a set of vectors by sending (commit,u1, . . . ,ut) to the ILC. As we use this sub-proof as a building block for more complex proofs, the vectors will be already stored in the ILC at earlier stages of the proof, thus the prover is not required any further action. Let  <span class="math">U = (u_i)_{i=1}^t</span> , we denote with [U] the commitments stored in the ILC. The description of the verifier  <span class="math">\\mathcal{V}_{eq}</span>  is given in Figure 10. The verifier asks for a random linear combination of the corresponding vectors and checks it against the values in the statement. The corresponding relation is</p>

    <p class="text-gray-300"><span class="math">$\\mathcal{R}_{eq} = \\left\\{ \\begin{array}{l} (pp_{\\mathsf{ILC}}, u) = ((\\mathbb{F}, k), (\\boldsymbol{u}_1, \\dots, \\boldsymbol{u}_t, [U])) : \\\\ \\boldsymbol{u}_1, \\dots, \\boldsymbol{u}_t \\in \\mathbb{F}^k \\quad \\land \\quad U = (\\boldsymbol{u}_i)_{i=1}^t \\end{array} \\right\\}.</span>$</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">\\mathcal{P}_{\\mathrm{eq}}((\\mathbb{F},k),(\\boldsymbol{u}_1,\\ldots,\\boldsymbol{u}_t),[U])</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">\\mathcal{P}_{\\mathrm{sum}}((\\mathbb{F},k),([A],[B],[C]))</span></th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">- Do nothing</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">- Do nothing</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\overline{\\mathcal{V}_{\\mathrm{eq}}((\\mathbb{F},k),(\\boldsymbol{u}_1,\\ldots,\\boldsymbol{u}_t),[U])}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{V}_{\\mathrm{sum}}((\\mathbb{F},k),([A],[B],[C]))</span></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">- Pick random <span class="math">x \\leftarrow \\mathbb{F}</span>&lt;br&gt;- Set <span class="math">X = (x^1, \\dots, x^m)</span>&lt;br&gt;- Query ILC on (open, <span class="math">X</span> ) and get response <span class="math">\\boldsymbol{v}</span>&lt;br&gt;- Return 1 if <span class="math">\\boldsymbol{v} = \\sum_{j=1}^m x^j \\boldsymbol{u}_j</span> , Return 0 otherwise</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">- Pick <span class="math">x \\leftarrow \\mathbb{F}</span> and set <span class="math">X = (x^1, \\dots, x^m, x^1, \\dots, x^m, -x^1, \\dots, -x^m)</span> - Query ILC on (open, <span class="math">X</span> ) and get response <span class="math">\\boldsymbol{v}</span> - Return 1 if <span class="math">\\boldsymbol{v} = \\boldsymbol{0}</span> , Return 0 otherwise</td>
          </tr>
        </tbody>
      </table>
    </div>

    <p class="text-gray-300">Fig. 10: Left hand side has equality check; right hand side has sum check.</p>

    <p class="text-gray-300"><strong>Theorem 6.</strong>  <span class="math">(\\mathcal{K}_{ILC}, \\mathcal{P}_{eq}, \\mathcal{V}_{eq})</span>  is a proof system for the relation  <span class="math">\\mathcal{R}_{eq}</span>  in the ILC model with perfect completeness, statistical knowledge soundness with straightline extraction and perfect special honest verifier zero-knowledge.</p>

    <p class="text-gray-300"><em>Proof.</em> The proof is perfectly complete, as it follows by inspection.</p>

    <p class="text-gray-300">The proof has statistical knowledge soundness with straight-line extraction. This is because the knowledge extractor already has access to the committed vectors of [U], having seen all messages sent between the prover  <span class="math">\\mathcal{P}_{eq}</span>  and the ILC. By the Schwartz-Zippel Lemma we have that if the committed vectors are not equal to the  <span class="math">u_j</span> , then they pass the consistency check with probability at most  <span class="math">\\frac{m}{|\\mathbb{F}|}</span> , which is negligible.</p>

    <p class="text-gray-300">The proof is perfect zero-knowledge as the verifier knows the values in the statement and can compute the response directly.  <span class="math">\\hfill\\Box</span></p>

    <p class="text-gray-300"><strong>Efficiency.</strong> Since the prover is inactive, the efficiency is easy to analyze. There is no communication between prover and verifier, so the round complexity is  <span class="math">\\mu = 0</span> .</p>

    <p class="text-gray-300">The verifier makes a single query to the ILC channel, so the query complexity is qc = 1. The prover has computation time  <span class="math">T_{\\mathcal{P}_{eq}} = 0</span>  and commits to t = 0 vectors during the proof (we do not count the already committed vectors that are part of the instance).</p>

    <p class="text-gray-300">The computational cost of the verifier would be around mk multiplications in  <span class="math">\\mathbb{F}</span>  if the verifier checks everything. Perhaps surprisingly, one can reduce the cost of this check to a linear number of additions  <span class="math">\\mathcal{O}(mk)</span>  in  <span class="math">\\mathbb{F}</span> . The verifier can achieve this by encoding both the response  <span class="math">\\mathbf{v}</span>  and the vectors  <span class="math">\\mathbf{u}_j</span>  using a linear error correcting code as the one in Theorem 1. Then, instead of checking the entire encoded vectors, he can perform the check only on a subset of positions of the codewords. This is made possible since the code has linear minimum distance.</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">T_{\\mathcal{P}_{\\text{eq}}}</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">T_{\\mathcal{V}_{\\mathrm{eq}}}</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">qc</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">#rounds</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">t</th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">0</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(mk)</span> additions in <span class="math">\\mathbb{F}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">1</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">0</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">0</td>
          </tr>
        </tbody>
      </table>
    </div>

    <h4 id="sec-19" class="text-lg font-semibold mt-6">A.2 Proof for the Sum of Committed Matrices</h4>

    <p class="text-gray-300">In this section, we introduce a proof of knowledge of matrices  <span class="math">A, B, C \\in \\mathbb{F}^{m \\times k}</span>  such that A+B=C. As for the previous sub-proof, we assume that the matrices have already been stored in the ILC, in which case the prover  <span class="math">\\mathcal{P}_{\\text{sum}}</span>  is not required to do any further action. We let [A], [B], [C] be the commitments to the rows of the matrices A, B, C, respectively. Moreover, we assume the prover committed the rows of the matrices in order, starting with all the rows of A, B and then C. The verifier picks a challenge x and queries the ILC on linear combinations the vectors, weighted on powers of the challenge. The idea is that entries in A, B, C are associated with the same power of x but the ones in C have opposite sign. Therefore, we expect them to sum up to zero if A+B-C=0 holds. The description of  <span class="math">\\mathcal{V}_{\\text{sum}}</span>  is given in Figure 10 and the corresponding relation is</p>

    <p class="text-gray-300"><span class="math">$\\mathcal{R}_{\\mathrm{sum}} = \\left\\{ \\begin{array}{l} (pp_{\\mathsf{ILC}}, u) = ((\\mathbb{F}, k) \\ , \\ ([A], [B], [C])) : \\\\ A, B, C \\in \\mathbb{F}^{m \\times k} \\quad \\land \\quad A + B = C \\end{array} \\right\\}.</span>$</p>

    <p class="text-gray-300"><strong>Theorem 7.</strong>  <span class="math">(\\mathcal{K}_{LC}, \\mathcal{P}_{sum}, \\mathcal{V}_{sum})</span>  is a proof system for the relation  <span class="math">\\mathcal{R}_{sum}</span>  in the ILC model with perfect completeness, statistical knowledge soundness with straight-line extraction, and perfect special honest verifier zero-knowledge.</p>

    <p class="text-gray-300"><em>Proof.</em> The proof is perfectly complete, as it follows by inspection.</p>

    <p class="text-gray-300">The proof has statistical knowledge soundness with straight-line extraction. This is because the knowledge extractor already has access to the committed vectors committed as [A], [B] and [C], having seen all messages sent between the prover and the ILC. By the Schwartz-Zippel Lemma, the probability that committed vectors not satisfying the addition gates pass the sum check is at most  <span class="math">\\frac{m}{|\\mathbb{F}|}</span> , which is negligible.</p>

    <p class="text-gray-300">The proof is perfect zero-knowledge as the verifier only sees the zero vector, which can be trivially simulated.  <span class="math">\\hfill\\Box</span></p>

    <p class="text-gray-300"><strong>Efficiency.</strong> The efficiency is easy to analyze and given in the table below. The verifier's computation is dominated by m-1 multiplications required to compute the query.</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">T_{\\mathcal{P}_{\\mathrm{sum}}}</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">T_{\\mathcal{V}_{\\text{sum}}}</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">qc</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">#rounds</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">t</th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">0</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">m-1</span> multiplications in <span class="math">\\mathbb{F}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">1</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">0</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">0</td>
          </tr>
        </tbody>
      </table>
    </div>

    <h4 id="sec-20" class="text-lg font-semibold mt-6">A.3 Proof for the Hadamard Product of Committed Matrices</h4>

    <p class="text-gray-300">We now describe a proof of knowledge of matrices  <span class="math">A,B,C\\in\\mathbb{F}^{\\mathfrak{mn}\\times k}</span>  such that  <span class="math">A\\circ B=C,</span>  where  <span class="math">A\\circ B</span>  is the Hadamard (entry-wise) product of the matrices. We assume that the prover has already committed to A,B,C which we will write with square brackets in the instance ([A],[B],[C]). The corresponding relation is</p>

    <p class="text-gray-300"><span class="math">$\\mathcal{R}_{\\mathrm{prod}} = \\left\\{ \\begin{array}{l} (pp_{\\mathsf{ILC}}, u) = ((\\mathbb{F}, k) \\;,\\; ([A], [B], [C])) : \\\\ A, B \\in \\mathbb{F}^{\\mathfrak{nm} \\times k} \\; \\wedge \\; A \\circ B = C \\end{array} \\right\\}.</span>$</p>

    <p class="text-gray-300">First, parse matrix A as a collection of  <span class="math">\\mathfrak{mn}</span>  row vectors  <span class="math">\\mathbf{a}_{i,j} \\in \\mathbb{F}^k</span>  for  <span class="math">0 \\le i \\le \\mathfrak{m} - 1</span> ,  <span class="math">1 \\le j \\le \\mathfrak{n}</span> , and similarly obtain  <span class="math">\\mathbf{b}_{i,j}</span>  and  <span class="math">\\mathbf{c}_{i,j}</span> . If  <span class="math">A \\circ B = C</span> , then  <span class="math">\\mathbf{a}_{i,j} \\circ \\mathbf{b}_{i,j} = \\mathbf{c}_{i,j}</span>  for  <span class="math">0 \\le i \\le \\mathfrak{m} - 1</span> ,  <span class="math">1 \\le j \\le \\mathfrak{n}</span> .</p>

    <p class="text-gray-300">Without loss of generality we assume  <span class="math">\\mathfrak{m}=2^{\\mu}</span>  for some integer  <span class="math">\\mu</span> . We will use  <span class="math">\\mu</span>  challenges  <span class="math">X_0,\\ldots,X_{\\mu-1}</span>  to compress  <span class="math">2\\mathfrak{m}\\mathfrak{n}</span>  vectors  <span class="math">\\mathbf{a}_{i,j},\\mathbf{b}_{i,j}</span>  of length k, corresponding to left and right inputs of multiplication gates, into  <span class="math">2\\mathfrak{m}</span>  vectors  <span class="math">\\dot{\\mathbf{a}}_j,\\dot{\\mathbf{b}}_j</span>  of the same length. The compressed vectors are computed by inserting vectors  <span class="math">\\mathbf{a}_{i,j}</span>  (resp.  <span class="math">\\mathbf{b}_{i,j}</span> ) into distinct coefficients of  <span class="math">\\mathfrak{n}</span>  multivariate polynomials in  <span class="math">X_0,\\ldots X_{\\mu-1}</span> . More precisely, vector  <span class="math">\\mathbf{a}_{i,j}</span>  is positioned in the jth polynomial as coefficient of  <span class="math">X_0^{i_0}\\cdots X_{\\mu-1}^{i_{\\mu-1}}</span> , where  <span class="math">i_{\\mu-1}i_{\\mu-2}\\ldots i_0</span>  are the digits of the binary expansion of i. The  <span class="math">2\\mathfrak{n}</span>  vectors of length k are then obtained by evaluating the multivariate polynomials into challenges  <span class="math">(x_0,x_1\\ldots,x_{\\mu-1})</span> , as shown in what follows.</p>

    <p class="text-gray-300"><span class="math">$\\dot{\\boldsymbol{a}}_{j}(x_{0},\\ldots,x_{\\mu-1}) = \\sum_{i=0}^{\\mathfrak{m}-1} \\boldsymbol{a}_{i,j} y^{i} x_{0}^{i_{0}} x_{1}^{i_{1}} \\ldots x_{\\mu-1}^{i_{\\mu-1}}</span>$</p>

    <p class="text-gray-300"><span class="math">$\\dot{\\boldsymbol{b}}_{j}(x_{0},\\ldots,x_{\\mu-1}) = \\sum_{i=0}^{\\mathfrak{m}-1} \\boldsymbol{b}_{i,j} x_{0}^{-i_{0}} x_{1}^{-i_{1}} \\ldots x_{\\mu-1}^{-i_{\\mu-1}}</span>$</p>

    <p class="text-gray-300">for  <span class="math">1 \\leq j \\leq \\mathfrak{n}</span>  and  <span class="math">(i_0, i_1, \\ldots, i_{\\mu-1}) \\in \\{0, 1\\}^{\\mu}</span>  such that  <span class="math">i = \\sum_{t=0}^{\\mu-1} 2^{i_t}</span> . These expressions can be efficiently evaluated. More details are given in the efficiency analysis following the protocol.</p>

    <p class="text-gray-300">Following a similar process as above, we then embed the above 2n vectors into the coefficients of two polynomials in X of degree n.</p>

    <p class="text-gray-300"><span class="math">$\\dot{a}(X, x_0, \\dots, x_{\\mu-1}) = \\sum_{j=1}^{n} \\dot{a}_j(x_0, \\dots, x_{\\mu-1}) y^{j\\mathfrak{m}} X^j</span>$</p>

    <p class="text-gray-300"><span class="math">$\\dot{b}(X, x_0, \\dots, x_{\\mu-1}) = \\sum_{j=1}^{n} \\dot{b}_j(x_0, \\dots, x_{\\mu-1}) X^{-j}</span>$</p>

    <p class="text-gray-300">Note that when we take the Hadamard product of the two vectors of polynomials, the Hadamard products  <span class="math">\\dot{a}_j \\circ \\dot{b}_j</span>  end up in the constant coefficient, i.e.,  <span class="math">X^0</span> . Similarly, all other waste products  <span class="math">\\dot{a}_j \\circ \\dot{b}_{j&#x27;}</span> , for  <span class="math">j \\neq j&#x27;</span> , end up in coefficients of other powers of X. The verifier will check the following polynomial expression evaluated in a random challenge x, connecting the Hadamard products of the  <span class="math">a_{i,j}</span>  and  <span class="math">b_{i,j}</span>  with the  <span class="math">c_{i,j}</span> .</p>

    <p class="text-gray-300"><span class="math">$\\dot{a} \\circ \\dot{b} = \\sum_{i=0, j=1}^{\\mathfrak{m}-1, \\mathfrak{n}} c_{i,j} y^{i+j\\mathfrak{m}} + \\sum_{t=0}^{\\mu-1} \\left( d_t^+ x_t + d_t^- x_t^{-1} \\right) + \\sum_{r=1-\\mathfrak{n}, r \\neq 0}^{\\mathfrak{n}-1} e_r x^r</span>$</p>

    <p class="text-gray-300">In this expression, the vectors  <span class="math">d_j^+, d_j^-</span>  are compression factors to make up for the lossy compression, and the  <span class="math">e_r</span>  are coefficients containing waste Hadamard products. Observe that different Hadamard products are separated by different powers of y.</p>

    <p class="text-gray-300">Finally, note that the polynomials chosen above leak information about the wire values, so we must also incorporate some random blinders into the real protocol to achieve zero-knowledge.</p>

    <p class="text-gray-300">The proof is presented as a stand-alone protocol, but when run as part of a larger arithmetic circuit proof, the vectors  <span class="math">a_{i,j}, b_{i,j}</span>  and  <span class="math">c_{i,j}</span>  will already have been sent to ILC. This is emphasized by putting square brackets around committed values.</p>

    <p class="text-gray-300"><strong>Formal Description.</strong> Next, we provide a formal description of the proof of knowledge of committed matrices satisfying Hadamard product relation  <span class="math">\\mathcal{R}_{prod}</span> .</p>

    <h4 id="sec-21" class="text-lg font-semibold mt-6"><strong>Proof:</strong></h4>

    <p class="text-gray-300">Instance: The prover has already sent  <span class="math">[a_{i,j}, b_{i,j}, c_{i,j}]_{i=0,j=1}^{\\mathfrak{m}-1,\\mathfrak{n}}</span>  to the ILC channel.</p>

    <p class="text-gray-300"><span class="math">\\mathcal{P}_{\\mathrm{prod}} \\to \\mathsf{ILC}</span> : The prover picks  <span class="math">\\dot{\\boldsymbol{a}}_0, \\dot{\\boldsymbol{b}}_0 \\leftarrow \\mathbb{F}^k</span> . The prover computes  <span class="math">\\dot{\\boldsymbol{c}}_0 = \\dot{\\boldsymbol{a}}_0 \\circ \\dot{\\boldsymbol{b}}_0</span> .</p>

    <p class="text-gray-300">The prover sends  <span class="math">\\dot{a}_0, \\dot{b}_0, \\dot{c}_0</span>  to ILC.</p>

    <p class="text-gray-300"><span class="math">\\mathsf{ILC} \\leftarrow \\mathcal{V}_{\\mathsf{prod}}</span> : Verifier sends  <span class="math">y \\leftarrow \\mathbb{F}^{\\times}</span>  to  <span class="math">\\mathsf{ILC}</span></p>

    <p class="text-gray-300"><span class="math">\\mathcal{P}_{\\text{prod}} \\to \\text{ILC:}</span>  The prover computes polynomials with vector coefficients in the variables  <span class="math">X_0, \\ldots, X_{\\mu-1}</span> , where  <span class="math">\\mathfrak{m} = 2^{\\mu}</span> . Here,  <span class="math">i_0, \\ldots, i_{\\mu-1}</span>  represent the digits of the binary expansion of i.</p>

    <p class="text-gray-300"><span class="math">$\\dot{a}_{j}(X_{0},\\ldots,X_{\\mu-1}) = \\sum_{i=0}^{\\mathfrak{m}-1} a_{i,j} y^{i} X_{0}^{i_{0}} X_{1}^{i_{1}} \\ldots X_{\\mu-1}^{i_{\\mu-1}}</span>$</p>

    <p class="text-gray-300"><span class="math">$\\dot{b}_{j}(X_{0},\\ldots,X_{\\mu-1}) = \\sum_{i=0}^{\\mathfrak{m}-1} b_{i,j} X_{0}^{-i_{0}} X_{1}^{-i_{1}} \\ldots X_{\\mu-1}^{-i_{\\mu-1}}</span>$</p>

    <p class="text-gray-300">for  <span class="math">1 \\leq j \\leq \\mathfrak{n}</span> .</p>

    <p class="text-gray-300">The prover computes the following polynomials with vector coefficients in the variable X.</p>

    <p class="text-gray-300">$$\\dot{a}(X, X_0, \\dots, X_{\\mu-1}) = \\dot{a}<em>0 + \\sum</em>{j=1}^{\\mathfrak{n}} \\dot{a}<em>j(X_0, \\dots, X</em>{\\mu-1}) y^{j\\mathfrak{m}} X^j
\\dot{b}(X, X_0, \\dots, X_{\\mu-1}) = \\dot{b}<em>0 + \\sum</em>{j=1}^{\\mathfrak{n}} \\dot{b}<em>j(X_0, \\dots, X</em>{\\mu-1}) X^{-j}$$</p>

    <p class="text-gray-300">The prover writes the Hadamard product of the above vectors of polynomials</p>

    <p class="text-gray-300"><span class="math">$\\begin{split} \\dot{\\boldsymbol{a}}(X,X_0,\\dots,X_{\\mu-1}) \\circ \\dot{\\boldsymbol{b}}(X,X_0,\\dots,X_{\\mu-1}) &amp;= \\dot{\\boldsymbol{a}}_0 \\circ \\dot{\\boldsymbol{b}}_0 \\\\ &amp;+ \\sum_{i=0,j=1}^{\\mathfrak{m}-1,\\mathfrak{n}} \\boldsymbol{a}_{i,j} \\circ \\boldsymbol{b}_{i,j} y^{i+j\\mathfrak{m}} \\\\ &amp;+ \\boldsymbol{d}_0^+ X_0 + \\boldsymbol{d}_0^- X_0^{-1} \\\\ &amp;+ \\boldsymbol{d}_1^+ (X_0) X_1 + \\boldsymbol{d}_1^- (X_0) X_1^{-1} \\\\ &amp;\\vdots \\\\ &amp;+ \\boldsymbol{d}_{\\mu-1}^+ (X_0,\\dots,X_{\\mu-2}) X_{\\mu-1} \\\\ &amp;+ \\boldsymbol{d}_{\\mu-1}^- (X_0,\\dots,X_{\\mu-2}) X_{\\mu-1}^{-1} \\\\ &amp;+ \\sum_{r=-\\mathfrak{n},r\\neq 0}^{\\mathfrak{n}} \\boldsymbol{e}_r(X_0,\\dots,X_{\\mu-1}) X^r \\end{split}</span>$</p>

    <p class="text-gray-300">The prover sends  <span class="math">d_0^+, d_0^-</span>  to ILC.</p>

    <p class="text-gray-300"><span class="math">\\mathsf{ILC} \\leftarrow \\mathcal{V}_{\\mathrm{prod}}</span>  : The verifier sends  <span class="math">x_0 \\leftarrow \\mathbb{F}^\\times</span>  to  <span class="math">\\mathsf{ILC}</span></p>

    <p class="text-gray-300"><span class="math">\\mathcal{P}_{\\text{prod}} \\to \\mathsf{ILC}</span> : The prover sends  <span class="math">\\boldsymbol{d}_1^+ = \\boldsymbol{d}_1^+(x_0), \\boldsymbol{d}_1^- = \\boldsymbol{d}_1^-(x_0)</span>  to  <span class="math">\\mathsf{ILC}</span> .</p>

    <p class="text-gray-300">For t = 1 to  <span class="math">\\mu - 2</span> :</p>

    <p class="text-gray-300">– ILC ← Vprod : The verifier sends x&lt;sup&gt;t&lt;/sup&gt; ← F &lt;sup&gt;×&lt;/sup&gt; to ILC</p>

    <p class="text-gray-300"><span class="math">$-\\mathcal{P}_{\\text{prod}} \\to \\text{ILC:}</span>$
The prover sends  <span class="math">\\boldsymbol{d}_{t+1}^+ = \\boldsymbol{d}_{t+1}^+(x_0,\\ldots,x_t)</span>  and  <span class="math">\\boldsymbol{d}_{t+1}^- = \\boldsymbol{d}_{t+1}^-(x_0,\\ldots,x_t)</span>  to ILC.</p>

    <p class="text-gray-300">ILC← Vprod : The verifier sends xµ−&lt;sup&gt;1&lt;/sup&gt; ← F &lt;sup&gt;×&lt;/sup&gt; to ILC</p>

    <p class="text-gray-300">Pprod → ILC: The prover computes e&lt;sup&gt;r&lt;/sup&gt; = er(x0, . . . , xµ−1) for −n ≤ r ≤ n, r 6= 0.</p>

    <p class="text-gray-300">The prover sends {er} n &lt;sup&gt;r&lt;/sup&gt;=−n,r6=0 to ILC.</p>

    <h3 id="sec-22" class="text-xl font-semibold mt-8">Verification:</h3>

    <p class="text-gray-300">ILC← Vprod : The verifier selects x ← F &lt;sup&gt;×&lt;/sup&gt; uniformly at random. The verifier queries the ILC channel to get</p>

    <p class="text-gray-300"><span class="math">$\\begin{split} \\dot{\\boldsymbol{a}} &amp;= \\quad \\dot{\\boldsymbol{a}}_0 + \\sum_{i=0,j=1}^{\\mathfrak{m}-1,\\mathfrak{n}} \\boldsymbol{a}_{i,j} y^{i+j\\mathfrak{m}} x_0^{i_0} x_1^{i_1} \\dots x_{\\mu-1}^{i_{\\mu-1}} x^j \\\\ \\dot{\\boldsymbol{b}} &amp;= \\quad \\dot{\\boldsymbol{b}}_0 + \\sum_{i=0,j=1}^{\\mathfrak{m}-1,\\mathfrak{n}} \\boldsymbol{b}_{i,j} x_0^{i_0} x_1^{i_1} \\dots x_{\\mu-1}^{i_{\\mu-1}} x^{-j} \\\\ \\dot{\\boldsymbol{c}} &amp;= \\quad \\dot{\\boldsymbol{c}}_0 + \\sum_{i=0,j=1}^{\\mathfrak{m}-1,\\mathfrak{n}} \\boldsymbol{c}_{i,j} y^{i+j\\mathfrak{m}} + \\sum_{t=0}^{\\mu-1} \\left( \\boldsymbol{d}_t^+ x_t + \\boldsymbol{d}_t^- x_t^{-1} \\right) + \\sum_{r=-\\mathfrak{n},r\\neq 0}^{\\mathfrak{n}} \\boldsymbol{e}_r x^r \\end{split}</span>$</p>

    <p class="text-gray-300">The verifier then checks whether the following equation holds and in that case accepts.</p>

    <p class="text-gray-300"><span class="math">$\\dot{a} \\circ \\dot{b} \\stackrel{?}{=} \\dot{c}</span>$</p>

    <p class="text-gray-300">Security Analysis. Before analysing the security of the above protocol, we prove a variation of the Schwarz-Zippel Lemma. This will be used when analysing the knowledge soundness of both the Hadamard product proof and the doubleshift proof described in the next section.</p>

    <p class="text-gray-300">Lemma 1. Let F be a field. Let P be a function of the following form, where p0,i&lt;sup&gt;0&lt;/sup&gt; are constant values, and p1,i&lt;sup&gt;1&lt;/sup&gt; (Z0), . . . , pu,i&lt;sup&gt;u&lt;/sup&gt; (Z0, . . . , Zu−1) are arbitrary functions and not necessarily polynomials.</p>

    <p class="text-gray-300"><span class="math">$P(Z_0, \\dots, Z_u) = \\sum_{i_0 = -d_0}^{d_0} p_{0,i_0} Z_0^{i_0} + \\sum_{i_1 = -d_1, i_1 \\neq 0}^{d_1} p_{1,i_1}(Z_0) Z_1^{i_1} + \\dots + \\sum_{i_u = -d_u, i_u \\neq 0}^{d_u} p_{u,i_u}(Z_0, \\dots, Z_{u-1}) Z_u^{i_u}</span>$</p>

    <p class="text-gray-300">Let S be a finite subset of F &lt;sup&gt;×&lt;/sup&gt;. Let z0, . . . , z&lt;sup&gt;u&lt;/sup&gt; be selected at random independently and uniformly from S. Let F be the event that at least one value among p0,i&lt;sup&gt;0&lt;/sup&gt; or ps,i&lt;sup&gt;s&lt;/sup&gt; (z0, . . . , zs−1) is not zero.</p>

    <p class="text-gray-300">Then</p>

    <p class="text-gray-300"><span class="math">$\\Pr\\left[\\left\\{P(z_0, \\dots, z_u) = 0\\right\\} \\land F\\right] \\le \\frac{\\sum_{t=0}^{u} (2d_t + 1)}{|S|}</span>$</p>

    <p class="text-gray-300">Proof. We prove the lemma by induction on u. The case u = 0 follows from the fact that a Laurent polynomial of degree (2d&lt;sup&gt;0&lt;/sup&gt; + 1) has at most (2d&lt;sup&gt;0&lt;/sup&gt; + 1) roots. Assume that the result holds for u − 1. We prove the lemma for u. Write</p>

    <p class="text-gray-300"><span class="math">$P(Z_0, \\dots, Z_u) = Q(Z_0, \\dots, Z_{u-1}) + \\sum_{i_u = -d_u, i_u \\neq 0}^{d_u} p_{u, i_u}(Z_0, \\dots, Z_{u-1}) Z_u^{i_u}</span>$</p>

    <p class="text-gray-300">For fixed values of z0, . . . , zu−1, this is a polynomial of degree d&lt;sup&gt;u&lt;/sup&gt; in Zu. Let G be the event that P is the zero polynomial in Zu. Let F&lt;sup&gt;P&lt;/sup&gt; and F&lt;sup&gt;Q&lt;/sup&gt; be the event F interpreted in the obvious way for P and Q.</p>

    <p class="text-gray-300"><span class="math">$\\Pr [\\{P(z_0, \\dots, z_u) = 0\\} \\land F_P] = \\Pr [\\{P(z_0, \\dots, z_u) = 0\\} \\land F_P \\land G] + \\Pr [\\{P(z_0, \\dots, z_u) = 0\\} \\land F_P \\land \\neg G]</span>$</p>

    <p class="text-gray-300">If G holds, then P is the zero polynomial in Zu, so since Q is the constant term, then Q is necessarily zero. On the other hand, if G and F&lt;sup&gt;P&lt;/sup&gt; hold simultaneously, then each value pu,i&lt;sup&gt;u&lt;/sup&gt; (z0, . . . , zu−1) must be zero, so the non-zero value must occur among p0,i&lt;sup&gt;0&lt;/sup&gt; or ps,i&lt;sup&gt;s&lt;/sup&gt; (z0, . . . , zs−1) for s &lt; u. Therefore, F&lt;sup&gt;Q&lt;/sup&gt; holds. We use these facts to bound the first probability. We bound the second probability by simply removing the event F&lt;sup&gt;P&lt;/sup&gt; .</p>

    <p class="text-gray-300"><span class="math">$\\Pr [\\{P(z_0, \\dots, z_u) = 0\\} \\land F] \\le \\Pr [\\{Q(z_0, \\dots, z_{u-1}) = 0\\} \\land F_Q] + \\Pr [\\{P(z_0, \\dots, z_u) = 0\\} \\land \\neg G]</span>$</p>

    <p class="text-gray-300">Apply the induction hypothesis to bound the first probability. To bound the second probability, observe that for any values of z0, . . . , zu−&lt;sup&gt;1&lt;/sup&gt; such that ¬G holds, P is a non-zero Laurent polynomial of degree at most (2d&lt;sup&gt;u&lt;/sup&gt; + 1) in Zu, and has at most (2d&lt;sup&gt;u&lt;/sup&gt; + 1) roots zu. The result follows. ut</p>

    <p class="text-gray-300">Theorem 8. (KILC,Pprod, Vprod) is a proof of knowledge for the relation Rprod in the ILC model with perfect completeness, statistical knowledge soundness with straight-line extraction and perfect special honest verifier zero-knowledge.</p>

    <p class="text-gray-300">Proof. Perfect completeness follows by careful inspection of the polynomial expressions computed by the prover in the above protocol.</p>

    <p class="text-gray-300">Next, we show that the proof has statistical knowledge soundness with straightline extraction. This is because the knowledge extractor already has access to</p>

    <p class="text-gray-300">the vectors committed as [A], [B] and [C], having seen all messages sent between the prover and the ILC. It remains to show that for any deterministic malicious prover  <span class="math">\\mathcal{P}_{\\text{prod}}^*</span> , if the committed vectors are not a valid witness for  <span class="math">\\mathcal{R}_{\\text{prod}}</span> , then there is negligible probability of accept. Recall that verifier queries the following values.</p>

    <p class="text-gray-300"><span class="math">$\\begin{split} \\dot{\\boldsymbol{a}} &amp;= \\dot{\\boldsymbol{a}}_0 + \\sum_{i=0,j=1}^{\\mathfrak{m}-1,\\mathfrak{n}} \\boldsymbol{a}_{i,j} y^{i+j\\mathfrak{m}} x_0^{i_0} x_1^{i_1} \\dots x_{\\mu-1}^{i_{\\mu-1}} x^j \\\\ \\dot{\\boldsymbol{b}} &amp;= \\dot{\\boldsymbol{b}}_0 + \\sum_{i=0,j=1}^{\\mathfrak{m}-1,\\mathfrak{n}} \\boldsymbol{b}_{i,j} x_0^{i_0} x_1^{i_1} \\dots x_{\\mu-1}^{i_{\\mu-1}} x^{-j} \\\\ \\dot{\\boldsymbol{c}} &amp;= \\dot{\\boldsymbol{c}}_0 + \\sum_{i=0,j=1}^{\\mathfrak{m}-1,\\mathfrak{n}} \\boldsymbol{c}_{i,j} y^{i+j\\mathfrak{m}} + \\sum_{t=0}^{\\mu-1} \\left( \\boldsymbol{d}_t^+ x_t + \\boldsymbol{d}_t^- x_t^{-1} \\right) + \\sum_{r=-\\mathfrak{n},r\\neq 0}^{\\mathfrak{n}} \\boldsymbol{e}_r x^r \\end{split}</span>$</p>

    <p class="text-gray-300">Now, substitute the expressions for  <span class="math">\\dot{a}</span>  and  <span class="math">\\dot{b}</span>  into the left-hand side of the third equality. The verifier only accepts if the equation holds. By assumption  <span class="math">\\mathcal{P}_{\\text{prod}}^*</span>  is deterministic, and we know when it made it's commitments. Hence,  <span class="math">\\dot{a}_0, \\dot{b}_0</span>  and  <span class="math">\\dot{c}_0</span>  are constants,  <span class="math">d_0^+, d_0^-</span>  are functions of  <span class="math">y, d_1^+, d_1^-</span>  are functions of y and  <span class="math">x_0, \\ldots, x_{\\mu-1}</span>  and the  <span class="math">e_r</span>  are functions of  <span class="math">y, x_0, \\ldots, x_{\\mu-1}</span> . We can now apply Lemma 1. The coefficient of  <span class="math">y^{i+j\\mathfrak{m}}</span>  on the right-hand side is  <span class="math">e_{i,j}</span>  while on the left-hand side it is  <span class="math">e_{i,j} \\circ b_{i,j}</span> . If there exist i and j such that  <span class="math">e_{i,j} \\circ b_{i,j} \\neq e_{i,j}</span> , that means we have  <span class="math">e_{i,j} \\circ b_{i,j} = e_{i,j}</span>  and  <span class="math">e_{i,j} \\circ b_{i,j} = e_{i,j}</span>  that the probability that the committed values does not satisfy the product relation but verifier accepts is negligible, which proves statistical knowledge soundness.</p>

    <p class="text-gray-300">For honest-verifier zero-knowledge, we describe how to simulate the verifier's view efficiently. Since  <span class="math">\\dot{a}_0, \\dot{b}_0 \\leftarrow \\mathbb{F}^k</span> , and these are added to  <span class="math">\\dot{a}</span>  and  <span class="math">\\dot{b}</span>  respectively in an honest transcript, we see that  <span class="math">\\dot{a}</span>  and  <span class="math">\\dot{b}</span>  are also uniformly distributed. This is trivial to simulate. In an accepting transcript the answer to the last query is always  <span class="math">\\dot{a} \\circ \\dot{b}</span>  which can be computed from  <span class="math">\\dot{a}</span>  and  <span class="math">\\dot{b}</span> , so we have special honest verifier zero knowledge.</p>

    <p class="text-gray-300"><strong>Efficiency.</strong> The verifier sends  <span class="math">\\mu + 1</span>  field elements to the prover through the ILC channel and has a computational cost dominated by  <span class="math">\\mathfrak{mn}</span>  multiplications in  <span class="math">\\mathbb{F}</span>  to compute the queries to the channel. The query complexity is qc = 3 and the prover's communication consists of commitments to  <span class="math">2\\mu + 2\\mathfrak{n} + 3</span>  vectors in  <span class="math">\\mathbb{F}^k</span> .</p>

    <p class="text-gray-300">In the protocol as written, the prover has computed on multivariate polynomials with vector coefficients. However, the prover only needs to commit to elements of  <span class="math">\\mathbb{F}^k</span> . Therefore, the prover can save considerable computational effort by computing mostly on vectors, and using challenges  <span class="math">y, x_0, \\ldots, x_{\\mu-1}</span>  as they become available to partially evaluate expressions and 'collapse' multiple vectors into fewer vectors. We analyse the prover's computation from the final round to the first round. Details follow.</p>

    <p class="text-gray-300">After receiving  <span class="math">x_{\\mu-1}</span> , and computing  <span class="math">\\dot{\\boldsymbol{a}}_j, \\dot{\\boldsymbol{b}}_j</span> , the prover must compute the values  <span class="math">\\boldsymbol{e}_r</span> . This is done by expressing  <span class="math">\\dot{\\boldsymbol{a}}, \\dot{\\boldsymbol{b}}</span>  as polynomials in X of degree  <span class="math">\\mathfrak{n}</span> , with vector coefficients  <span class="math">\\dot{\\boldsymbol{a}}_j, \\dot{\\boldsymbol{b}}_j</span> . Then, the  <span class="math">\\boldsymbol{e}_r</span>  are the coefficients of the Hadamard product polynomial. Using FFT techniques for each vector component, the cost is  <span class="math">O(k\\mathfrak{n}\\log\\mathfrak{n})</span> .</p>

    <p class="text-gray-300">Now we explain how the prover computes the values  <span class="math">d_j^+, d_j^-</span>  by computing the values of  <span class="math">\\dot{a}_j, \\dot{b}_j</span>  and  <span class="math">\\dot{a}_j \\circ \\dot{b}_j</span>  recursively. Consider the following expressions, assuming that the prover has already evaluated in all challenges preceding  <span class="math">X_{\\mu-1}</span> .</p>

    <p class="text-gray-300"><span class="math">$\\begin{split} \\dot{\\boldsymbol{a}}_{j}(x_{0},\\ldots,x_{\\mu-2},X_{\\mu-1}) &amp;= \\sum_{i=0}^{\\mathfrak{m}-1} \\boldsymbol{a}_{i,j} y^{i} x_{0}^{i_{0}} x_{1}^{i_{1}} \\ldots x_{\\mu-2}^{i_{\\mu-2}} X_{\\mu-1}^{i_{\\mu-1}} &amp;= \\boldsymbol{A}_{0,j} + \\boldsymbol{A}_{1,j} X_{\\mu-1} \\\\ \\dot{\\boldsymbol{b}}_{j}(x_{0},\\ldots,x_{\\mu-2},X_{\\mu-1}) &amp;= \\sum_{i=0}^{\\mathfrak{m}-1} \\boldsymbol{b}_{i,j} x_{0}^{-i_{0}} x_{1}^{-i_{1}} \\ldots x_{\\mu-2}^{-i_{\\mu-2}} X_{\\mu-1}^{-i_{\\mu-1}} &amp;= \\boldsymbol{B}_{0,j} + \\boldsymbol{B}_{1,j} X_{\\mu-1}^{-1} \\end{split}</span>$</p>

    <p class="text-gray-300">By assumption,  <span class="math">A_{0,j}</span> ,  <span class="math">A_{1,j}</span> ,  <span class="math">B_{0,j}</span> ,  <span class="math">B_{1,j}</span>  have already been computed at this stage. The cost of evaluating  <span class="math">\\dot{a}_j</span> ,  <span class="math">\\dot{b}_j</span>  and  <span class="math">\\dot{a}_j \\circ \\dot{b}_j</span>  and its  <span class="math">X_{\\mu-1}</span>  coefficients is then 5k multiplications to compute the necessary Hadamard products and multiply by  <span class="math">x_{\\mu-1}</span>  and its inverse, giving  <span class="math">5\\mathfrak{n}k</span>  multiplications, since we do the computation for  <span class="math">1 \\leq j \\leq \\mathfrak{n}</span> . Now,  <span class="math">d_{\\mu-1}^+ = \\sum_{j=1}^{\\mathfrak{n}} A_{1,j} \\circ B_{0,j}</span> , and  <span class="math">d_{\\mu-1}^-</span>  can be computed using a similar expression, which costs only  <span class="math">(\\mathfrak{n}-1)k</span>  additions, given that the Hadamard products, such as  <span class="math">A_{1,j} \\circ B_{0,j}</span> , were already computed in evaluating  <span class="math">\\dot{a}_j</span>  and  <span class="math">\\dot{b}_j</span> .</p>

    <p class="text-gray-300">Clearly  <span class="math">A_{0,j}, A_{1,j}, B_{0,j}, B_{1,j}</span>  have the same structure as  <span class="math">\\dot{a}_j, \\dot{b}_j</span> , but without the variable  <span class="math">X_{\\mu-1}</span> . Splitting into coefficients of  <span class="math">X_{\\mu-2}</span>  in a similar way as with  <span class="math">X_{\\mu-1}</span> , and assuming that we already have evaluations with respect to  <span class="math">X_0, \\ldots, X_{\\mu-3}</span> , we can use the same techniques as above twice to obtain  <span class="math">A_{0,j}, A_{1,j}, B_{0,j}, B_{1,j}</span> , associated Hadamard products  <span class="math">A_{0,j} \\circ B_{0,j}, A_{1,j} \\circ B_{1,j}</span> , and  <span class="math">d_{\\mu-2}^+, d_{\\mu-2}^-</span>  using  <span class="math">2 \\cdot 5\\mathfrak{n}k</span>  multiplications.</p>

    <p class="text-gray-300">By repeatedly splitting and applying this procedure  <span class="math">\\mu</span>  times, we can use the same techniques 4, then 8, up to  <span class="math">2^{\\mu-1}</span>  times. Summing up, the overall cost is dominated by  <span class="math">5k\\mathfrak{n}(2^{\\mu}-1)</span>  multiplications, which is  <span class="math">\\mathcal{O}(k\\mathfrak{n}\\mathfrak{m})</span>  multiplications.</p>

    <p class="text-gray-300">Altogether, the computational costs for the prover are  <span class="math">\\mathcal{O}(k\\mathfrak{n}\\log\\mathfrak{n}+k\\mathfrak{n}\\mathfrak{m})</span>  multiplications in  <span class="math">\\mathbb{F}</span> .</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">T_{\\mathcal{P}_{\\mathrm{prod}}}</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">T_{\\mathcal{V}_{\\mathrm{prod}}}</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">qc</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">#rounds</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">t</th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(k\\mathfrak{n}\\log\\mathfrak{n} + k\\mathfrak{m}\\mathfrak{n})</span> mult.</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(\\mathfrak{mn}+k)</span> mult.</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">3</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\log \\mathfrak{m} + 2</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">2\\mu + 2\\mathfrak{n} + 3</span></td>
          </tr>
        </tbody>
      </table>
    </div>

    <h4 id="sec-23" class="text-lg font-semibold mt-6">A.4 Proof for the Double-Shift of Committed Matrices</h4>

    <p class="text-gray-300">The following double-shift proof is used in the construction of a proof to show that the product of all entries in one matrix is equal to the product of all entries in another matrix.</p>

    <p class="text-gray-300">Consider the matrices A and B, which have  <span class="math">\\mathfrak{mn}</span>  rows, given respectively by vectors  <span class="math">\\mathbf{a}_{i,j}</span> ,  <span class="math">\\mathbf{b}_{i,j} \\in \\mathbb{F}^k</span> , with  <span class="math">0 \\le i \\le \\mathfrak{m} - 1, 1 \\le j \\le \\mathfrak{n}</span> . The top-right element of A is a 1. Columns 2 up to k of A are equal to columns 1 up to k-1 of B. Further, we can obtain the final column of B from the first column of A by deleting the first entry and appending c. In this case, A is said to be the <em>shift</em> of B.</p>

    <p class="text-gray-300"><span class="math">$\\begin{array}{c} \\mathbf{a}_{0,1} \\\\ \\mathbf{a}_{1,1} \\\\ \\vdots \\\\ \\mathbf{a}_{\\mathfrak{m}-1,\\mathfrak{n}} \\end{array} \\begin{pmatrix} 1 &amp; a_{1,2} &amp; \\cdots &amp; a_{1,k} \\\\ a_{2,1} &amp; a_{2,2} &amp; \\cdots &amp; a_{2,k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{\\mathfrak{mn},1} &amp; a_{\\mathfrak{mn},2} &amp; \\cdots &amp; a_{\\mathfrak{mn},k} \\end{pmatrix} \\quad \\begin{array}{c} \\mathbf{b}_{0,1} \\\\ \\mathbf{b}_{1,1} \\\\ \\vdots \\\\ \\mathbf{b}_{\\mathfrak{m}-1,\\mathfrak{n}} \\end{array} \\begin{pmatrix} a_{1,2} &amp; a_{1,3} &amp; \\cdots &amp; a_{1,k} &amp; a_{2,1} \\\\ a_{2,2} &amp; a_{2,3} &amp; \\cdots &amp; a_{2,k} &amp; a_{3,1} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ a_{\\mathfrak{mn},2} &amp; a_{\\mathfrak{mn},3} &amp; \\cdots &amp; a_{\\mathfrak{mn},k} &amp; c \\end{pmatrix}</span>$</p>

    <p class="text-gray-300">Here, we give an proof which allows a prover to convince a verifier in zero-knowledge that for committed matrices A, B, C and D, we have A the shift of B, C the shift of D, and B and D have the same bottom-right-most entry  <span class="math">b_{mn,k} = d_{mn,k}</span> . This is referred to as the double-shift condition. The corresponding relation is</p>

    <p class="text-gray-300"><span class="math">$\\mathcal{R}_{\\mathrm{shift}} = \\left\\{ \\begin{aligned} (pp_{\\mathsf{ILC}}, u) &amp;= ((\\mathbb{F}, k) \\ , \\ ([A], [B], [C], [D])) : \\\\ A, B, C, D \\ \\text{satisfy the double-shift condition} \\end{aligned} \\right\\}.</span>$</p>

    <p class="text-gray-300">Parsing each matrix as a collection of row vectors as above, we now describe a proof of knowledge of vectors satisfying the stated shift condition.</p>

    <p class="text-gray-300">The double-shift condition can be encoded as many linear consistency constraints between the entries of A, B, C and D. For example, for the double shift condition to hold, it is necessary that  <span class="math">(\\mathbf{a}_{0,1})_2 - (\\mathbf{b}_{0,1})_1 = 0</span> . We will use a random challenge y to embed all linear consistency constraints into one, with each individual constraint embedded with a different power of y.</p>

    <p class="text-gray-300">Similarly to the Hadamard product proof, we use challenges  <span class="math">X_0, \\ldots, X_{\\mu-1}</span> , where  <span class="math">\\mathfrak{m} = 2^{\\mu}</span> , for compression, and reduce the number of vectors from  <span class="math">4\\mathfrak{m}\\mathfrak{n}</span>  to  <span class="math">4\\mathfrak{n}</span> . Vectors are compressed as follows</p>

    <p class="text-gray-300"><span class="math">$\\hat{a}_j(x_0,\\ldots,x_{\\mu-1}) = \\sum_{i=0}^{\\mathfrak{m}-1} a_{i,j} x_0^{i_0} x_1^{i_1} \\ldots x_{\\mu-1}^{i_{\\mu-1}}</span>$</p>

    <p class="text-gray-300">with similar expressions when a is replaced by b, c and d.</p>

    <p class="text-gray-300">We then embed the compressed vectors into polynomials in X, again with similar expressions for a replaced by b, c and d.</p>

    <p class="text-gray-300"><span class="math">$\\hat{\\boldsymbol{a}}(X, x_0, \\dots, x_{\\mu-1}) = \\sum_{j=1}^{n} \\hat{\\boldsymbol{a}}_j(x_0, \\dots, x_{\\mu-1}) X^j</span>$</p>

    <p class="text-gray-300">We embed all linear consistency constraints into vectors  <span class="math">\\hat{\\boldsymbol{w}}_a</span> ,  <span class="math">\\hat{\\boldsymbol{w}}_b</span> ,  <span class="math">\\hat{\\boldsymbol{w}}_c</span>  and  <span class="math">\\hat{\\boldsymbol{w}}_d</span>  as follows. Set  <span class="math">\\boldsymbol{y} = (1, y, \\dots, y^{k-1})</span> . Set</p>

    <p class="text-gray-300"><span class="math">$\\hat{\\boldsymbol{w}}_{a}(y, X, x_{0}, \\dots, x_{\\mu}) = \\boldsymbol{y} \\sum_{i=0, j=1}^{\\mathfrak{m}-1, \\mathfrak{n}} y^{k(i+(j-1)\\mathfrak{m})} x_{0}^{-i_{0}} \\dots x_{\\mu-1}^{-i_{\\mu-1}} X^{-j}</span>$</p>

    <p class="text-gray-300"><span class="math">$\\hat{\\boldsymbol{w}}_b(y, X, x_0, \\dots, x_{\\mu}) = -y \\; \\hat{\\boldsymbol{w}}_a(y, X, x_0, \\dots, x_{\\mu})</span>$</p>

    <p class="text-gray-300"><span class="math">$\\hat{\\boldsymbol{w}}_c(y, X, x_0, \\dots, x_{\\mu}) = -y^{2N} \\hat{\\boldsymbol{w}}_a(y^{-1}, X, x_0, \\dots, x_{\\mu})</span>$</p>

    <p class="text-gray-300"><span class="math">$\\hat{\\boldsymbol{w}}_d(y, X, x_0, \\dots, x_{\\mu}) = y^{2N-1} \\hat{\\boldsymbol{w}}_a(y^{-1}, X, x_0, \\dots, x_{\\mu})</span>$</p>

    <p class="text-gray-300">To explain our choice of linear consistency constraint vectors, consider computing the scalar product of  <span class="math">\\hat{a}(y,X,x_0,\\ldots,x_\\mu)</span>  and  <span class="math">\\hat{w}_a(y,X,x_0,\\ldots,x_\\mu)</span> . Focussing on the constant term in  <span class="math">X,x_0,\\ldots,x_\\mu</span> , we see that the only contributions come from perfect cancellation of a monomial  <span class="math">x_0^{i_0}x_1^{i_1}\\ldots x_{\\mu-1}^{i_{\\mu-1}}X^j</span>  in  <span class="math">\\hat{a}</span>  with a corresponding monomial  <span class="math">x_0^{-i_0}\\ldots x_{\\mu-1}^{-i_{\\mu-1}}X^{-j}</span>  in  <span class="math">\\hat{w}_a</span> . In addition, each monomial in  <span class="math">\\hat{w}_a</span>  is multiplied by a unique power of  <span class="math">y^k</span> . Since  <span class="math">\\hat{w}_a</span>  also contains y as a factor, the constant term of the expression is a sum of all elements of the matrix A, each separated by a unique power of y. Substituting  <span class="math">y^{-1}</span>  for y produces the elements in the opposite order, and we can also multiply by powers of y to move all elements of the matrix to different powers of y. It is then straightforward to see that we can encode the double shift condition for A, B, C and D by using these two tricks.</p>

    <p class="text-gray-300">Careful calculation shows that</p>

    <p class="text-gray-300"><span class="math">$\\hat{a} \\cdot \\hat{w}_{a}(y, X, x_{0}, \\dots, x_{\\mu-1}) + \\hat{b} \\cdot \\hat{w}_{b}(y, X, x_{0}, \\dots, x_{\\mu-1}) + \\hat{c} \\cdot \\hat{w}_{c}(y, X, x_{0}, \\dots, x_{\\mu-1}) + \\hat{d} \\cdot \\hat{w}_{d}(y, X, x_{0}, \\dots, x_{\\mu-1})</span>$</p>

    <p class="text-gray-300">has constant term in  <span class="math">X, x_0, \\ldots, x_{\\mu}</span>  equal to  <span class="math">1 - y^{2N}</span>  if and only if A, B, C and D satisfy the double-shift condition. This happens because when we take the scalar products of the vectors of polynomials, all of the the linear consistency constraints end up in the constant term in X, separated by different powers of y. All other waste terms end up in other coefficients.</p>

    <p class="text-gray-300">The verifier will check the following polynomial expression evaluated in x.</p>

    <p class="text-gray-300"><span class="math">$\\hat{\\boldsymbol{a}} \\cdot \\hat{\\boldsymbol{w}}_a(X, x_0, \\dots, x_{\\mu-1}) \\\\ + \\hat{\\boldsymbol{b}} \\cdot \\hat{\\boldsymbol{w}}_b(X, x_0, \\dots, x_{\\mu-1}) \\\\ + \\hat{\\boldsymbol{c}} \\cdot \\hat{\\boldsymbol{w}}_c(X, x_0, \\dots, x_{\\mu-1}) \\\\ + \\hat{\\boldsymbol{d}} \\cdot \\hat{\\boldsymbol{w}}_d(X, x_0, \\dots, x_{\\mu-1}) = 1 - y^{2N} + \\sum_{t=0}^{\\mu-1} (f_t^+ x_t + f_t^- x_t^{-1}) + \\sum_{r=1-\\mathfrak{n}, r \\neq 0}^{\\mathfrak{n}-1} g_r X^r</span>$</p>

    <p class="text-gray-300">In this expression, the values  <span class="math">f_j^+, f_j^-</span>  can be seen as compression factors to make up for the lossy compression, and the  <span class="math">g_r</span>  are coefficients containing waste values.</p>

    <p class="text-gray-300">As part of this protocol, the prover is required to send single values to ILC rather than vectors, but this is easily incorporated into the model by padding, and has no impact on the asymptotic efficiency.</p>

    <p class="text-gray-300">Note that the polynomials chosen above leak information about the wire values, so we must also incorporate some random blinders  <span class="math">\\hat{a}_0</span> ,  <span class="math">\\hat{b}_0</span> ,  <span class="math">\\hat{c}_0</span>  and  <span class="math">\\hat{d}_0</span>  into the real protocol to achieve zero-knowledge.</p>

    <p class="text-gray-300"><strong>Formal Description.</strong> Next, we provide a formal description of the proof of knowledge of committed matrices satisfying the double-shift relation  <span class="math">\\mathcal{R}_{\\text{shift}}</span> .</p>

    <h4 id="sec-24" class="text-lg font-semibold mt-6"><strong>Proof:</strong></h4>

    <p class="text-gray-300">Instance: The prover has already sent  <span class="math">[a_{i,j}, b_{i,j}, c_{i,j}, d_{i,j}]_{i=0,j=1}^{\\mathfrak{m}-1,\\mathfrak{n}}</span>  to the ILC channel.</p>

    <p class="text-gray-300"><span class="math">\\mathcal{P}_{\\mathrm{shift}} \\to \\mathsf{ILC}</span> : The prover randomly selects  <span class="math">\\hat{\\boldsymbol{a}}_0, \\hat{\\boldsymbol{b}}_0, \\hat{\\boldsymbol{c}}_0, \\hat{\\boldsymbol{d}}_0 \\leftarrow \\mathbb{F}^k</span> . The prover sends  <span class="math">\\hat{\\boldsymbol{a}}_0, \\hat{\\boldsymbol{b}}_0, \\hat{\\boldsymbol{c}}_0</span>  and  <span class="math">\\hat{\\boldsymbol{d}}_0</span>  to  <span class="math">\\mathsf{ILC}</span> .</p>

    <p class="text-gray-300"><span class="math">\\mathsf{ILC} \\leftarrow \\mathcal{V}_{\\mathsf{shift}}</span> : Verifier sends  <span class="math">y \\leftarrow \\mathbb{F}^{\\times}</span>  to  <span class="math">\\mathsf{ILC}</span> .</p>

    <p class="text-gray-300"><span class="math">\\mathcal{P}_{\\text{shift}} \\to \\mathsf{ILC}</span> : The prover computes the following polynomial with vector coefficients in the variables  <span class="math">X_0, \\ldots, X_{\\mu-1}</span> , where  <span class="math">\\mathfrak{m} = 2^{\\mu}</span> , and similar polynomials with a replaced by b, c and d. Here,  <span class="math">i_0, \\ldots, i_{\\mu-1}</span>  represent the digits of the binary expansion of i.</p>

    <p class="text-gray-300"><span class="math">$\\hat{\\boldsymbol{a}}_{j}(X_{0},\\ldots,X_{\\mu-1}) = \\sum_{i=0}^{\\mathfrak{m}-1} \\boldsymbol{a}_{i,j} X_{0}^{i_{0}} X_{1}^{i_{1}} \\ldots X_{\\mu-1}^{i_{\\mu-1}}</span>$</p>

    <p class="text-gray-300">The prover computes the following polynomials with vector coefficients in the variable X, and similarly for b, c and d.</p>

    <p class="text-gray-300"><span class="math">$\\hat{a}(X, X_0, \\dots, X_{\\mu-1}) = \\hat{a}_0 + \\sum_{j=1}^{n} \\hat{a}_j(X_0, \\dots, X_{\\mu-1})X^j</span>$</p>

    <p class="text-gray-300"><span class="math">$\\hat{\\boldsymbol{w}}_{a}(y, X, x_{0}, \\dots, x_{\\mu}) = \\boldsymbol{y} \\sum_{i=0, j=1}^{\\mathfrak{m}-1, \\mathfrak{n}} y^{k(i+(j-1)\\mathfrak{m})} x_{0}^{-i_{0}} \\dots x_{\\mu-1}^{-i_{\\mu-1}} X^{-j}</span>$</p>

    <p class="text-gray-300">The prover finally takes the scalar product of the previous vectors of polynomials</p>

    <p class="text-gray-300"><span class="math">$\\begin{split} \\hat{a}(X,X_0,\\dots,X_{\\mu-1})\\cdot \\hat{w}_a(X,X_0,\\dots,X_{\\mu-1}) \\\\ + \\hat{b}(X,X_0,\\dots,X_{\\mu-1})\\cdot \\hat{w}_b(X,X_0,\\dots,X_{\\mu-1}) \\\\ + \\hat{c}(X,X_0,\\dots,X_{\\mu-1})\\cdot \\hat{w}_c(X,X_0,\\dots,X_{\\mu-1}) \\\\ + \\hat{d}(X,X_0,\\dots,X_{\\mu-1})\\cdot \\hat{w}_d(X,X_0,\\dots,X_{\\mu-1}) &amp;= 1-y^{2N}+f_0^+X_0+f_0^-X_0^{-1} \\\\ &amp;\\qquad \\qquad + f_1^+(X_0)X_1+f_1^-(X_0)X_1^{-1} \\\\ &amp;\\qquad \\qquad \\vdots \\\\ &amp;\\qquad \\qquad + f_{\\mu-1}^+(X_0,\\dots,X_{\\mu-2})X_{\\mu-1} \\\\ &amp;\\qquad \\qquad + f_{\\mu-1}^-(X_0,\\dots,X_{\\mu-2})X_{\\mu-1}^{-1} \\\\ &amp;\\qquad \\qquad + \\sum_{r=-\\mathfrak{n},r\\neq 0}^{\\mathfrak{n}-1}g_r(X_0,\\dots,X_{\\mu-1})X^r \\end{split}</span>$</p>

    <p class="text-gray-300">The prover sends  <span class="math">f_0^+, f_0^-</span>  to ILC.</p>

    <p class="text-gray-300">ILC <span class="math">\\leftarrow \\mathcal{V}_{\\text{shift}}</span> : The verifier sends  <span class="math">x_0 \\leftarrow \\mathbb{F}^{\\times}</span>  to ILC.</p>

    <p class="text-gray-300"><span class="math">\\mathcal{P}_{\\text{shift}} \\to \\text{ILC: The prover sends } f_1^+ = f_1^+(x_0), f_1^- = f_1^-(x_0) \\text{ to ILC.}</span></p>

    <p class="text-gray-300">For t = 1 to  <span class="math">\\mu - 2</span> :</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>ILC <span class="math">\\leftarrow \\mathcal{V}_{\\text{shift}}</span> : The verifier sends  <span class="math">x_t \\leftarrow \\mathbb{F}^{\\times}</span>  to ILC</li>
      <li><span class="math">\\mathcal{P}_{\\text{shift}} \\to \\text{ILC: The prover sends } f_{t+1}^+ = f_{t+1}^+(x_0, \\dots, x_t), f_{t+1}^- = f_{t+1}^-(x_0, \\dots, x_t)</span>  to ILC.</li>
    </ul>

    <p class="text-gray-300"><span class="math">\\mathsf{ILC} \\leftarrow \\mathcal{V}_{\\mathsf{shift}}</span> : The verifier sends  <span class="math">x_{\\mu-1} \\leftarrow \\mathbb{F}^{\\times}</span>  to  <span class="math">\\mathsf{ILC}</span> .</p>

    <p class="text-gray-300"><span class="math">\\mathcal{P}_{\\text{shift}} \\to \\text{ILC:}</span>  The prover computes  <span class="math">g_r = g_r(x_0, \\dots, x_{\\mu-1})</span>  for  <span class="math">-\\mathfrak{n} \\leq r \\leq \\mathfrak{n} - 1, r \\neq 0</span> .</p>

    <p class="text-gray-300">The prover sends  <span class="math">\\{g_r\\}_{r=-\\mathfrak{n},r\\neq 0}^{\\mathfrak{n}-1}</span>  to ILC.</p>

    <p class="text-gray-300"><strong>Verification:</strong> The verifier selects  <span class="math">x \\leftarrow \\mathbb{F}^{\\times}</span>  uniformly at random. The verifier queries the ILC channel to get</p>

    <p class="text-gray-300"><span class="math">$\\hat{a} = \\hat{a}_0 + \\sum_{i=0,j=1}^{\\mathfrak{m}-1,\\mathfrak{n}} a_{i,j} x_0^{i_0} x_1^{i_1} \\dots x_{\\mu-1}^{i_{\\mu-1}} x^j</span>$</p>

    <p class="text-gray-300">and similarly for b, c and d. The verifier also queries the ILC channel to get</p>

    <p class="text-gray-300"><span class="math">$\\hat{e} = \\sum_{t=0}^{\\mu-1} \\left( f_t^+ x_t + f_t^- x_t^{-1} \\right) + \\sum_{r=-\\mathfrak{n}, r \\neq 0}^{\\mathfrak{n}-1} g_r x^r</span>$</p>

    <p class="text-gray-300">The verifier then checks whether the following equation holds and in that case accepts.</p>

    <p class="text-gray-300">$$\\hat{\\boldsymbol{a}} \\cdot \\hat{\\boldsymbol{w}}<em>{a}(x, x</em>{0}, \\dots, x_{\\mu-1})</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>\\hat{\\boldsymbol{b}} \\cdot \\hat{\\boldsymbol{w}}<em>{b}(x, x</em>{0}, \\dots, x_{\\mu-1})</li>
      <li>\\hat{\\boldsymbol{c}} \\cdot \\hat{\\boldsymbol{w}}<em>{c}(x, x</em>{0}, \\dots, x_{\\mu-1})</li>
      <li>\\hat{\\boldsymbol{d}} \\cdot \\hat{\\boldsymbol{w}}<em>{d}(x, x</em>{0}, \\dots, x_{\\mu-1}) \\stackrel{?}{=} 1 - y^{2N} + \\hat{\\boldsymbol{e}}$$</li>
    </ul>

    <p class="text-gray-300">Security Analysis.</p>

    <p class="text-gray-300"><strong>Theorem 9.</strong>  <span class="math">(\\mathcal{K}_{\\text{ILC}}, \\mathcal{P}_{\\text{shift}}, \\mathcal{V}_{\\text{shift}})</span>  is a proof system for the relation  <span class="math">\\mathcal{R}_{\\text{shift}}</span>  in the ILC model with perfect completeness, statistical knowledge soundness with straight-line extraction, and perfect special honest verifier zero-knowledge.</p>

    <p class="text-gray-300"><em>Proof.</em> Perfect completeness follows by careful inspection of the protocol and considering the various polynomial expressions computed by the prover.</p>

    <p class="text-gray-300">Next, we show that the proof has statistical knowledge soundness with straight-line extraction. This is because the knowledge extractor already has access to the committed matrices [A], [B], [C] and [D], having seen all messages sent between the prover and the ILC. It remains to show that for any deterministic malicious prover  <span class="math">\\mathcal{P}_{\\mathrm{shift}}^*</span> , if the committed vectors are not a valid witness for  <span class="math">\\mathcal{R}_{\\mathrm{shift}}</span> , then there is negligible probability of accept. Recall that verifier queries to get the right-hand side of the following equations.</p>

    <p class="text-gray-300"><span class="math">$\\begin{split} \\hat{a} &amp;= \\hat{a}_0 + \\sum_{i=0,j=1}^{\\mathfrak{m}-1,\\mathfrak{n}} a_{i,j} x_0^{i_0} x_1^{i_1} \\dots x_{\\mu-1}^{i_{\\mu-1}} x^j \\\\ \\hat{b} &amp;= \\hat{b}_0 + \\sum_{i=0,j=1}^{\\mathfrak{m}-1,\\mathfrak{n}} b_{i,j} x_0^{i_0} x_1^{i_1} \\dots x_{\\mu-1}^{i_{\\mu-1}} x^j \\\\ \\hat{c} &amp;= \\hat{c}_0 + \\sum_{i=0,j=1}^{\\mathfrak{m}-1,\\mathfrak{n}} c_{i,j} x_0^{i_0} x_1^{i_1} \\dots x_{\\mu-1}^{i_{\\mu-1}} x^j \\\\ \\hat{d} &amp;= \\hat{d}_0 + \\sum_{i=0,j=1}^{\\mathfrak{m}-1,\\mathfrak{n}} d_{i,j} x_0^{i_0} x_1^{i_1} \\dots x_{\\mu-1}^{i_{\\mu-1}} x^j \\end{split}</span>$</p>

    <p class="text-gray-300"><span class="math">$\\hat{\\boldsymbol{a}} \\cdot \\hat{\\boldsymbol{w}}_{a}(x, x_{0}, \\dots, x_{\\mu-1}) + \\hat{\\boldsymbol{b}} \\cdot \\hat{\\boldsymbol{w}}_{b}(x, x_{0}, \\dots, x_{\\mu-1}) + \\hat{\\boldsymbol{c}} \\cdot \\hat{\\boldsymbol{w}}_{c}(x, x_{0}, \\dots, x_{\\mu-1}) + \\hat{\\boldsymbol{d}} \\cdot \\hat{\\boldsymbol{w}}_{d}(x, x_{0}, \\dots, x_{\\mu-1}) = 1 - y^{2N} + \\sum_{t=1}^{\\mu-1} \\left( f_{t}^{+} x_{t} + f_{t}^{-} x_{t}^{-1} \\right) + \\sum_{r=-n}^{n-1} g_{r} x^{r}.</span>$</p>

    <p class="text-gray-300">Now, substitute in the expressions for  <span class="math">\\hat{\\boldsymbol{a}}</span> ,  <span class="math">\\hat{\\boldsymbol{b}}</span> ,  <span class="math">\\hat{\\boldsymbol{c}}</span>  and  <span class="math">\\hat{\\boldsymbol{d}}</span>  into the left-hand side of the final equality. The verifier only accepts if the last equation holds. By assumption,  <span class="math">\\mathcal{P}_{\\text{shift}}^*</span>  is deterministic, and we know when it made its commitments. Hence,  <span class="math">\\hat{\\boldsymbol{a}}_0, \\dots \\hat{\\boldsymbol{d}}_0</span>  are constants,  <span class="math">f_0^+, f_0^-</span>  are functions of y, and  <span class="math">f_1^+, f_1^-</span>  are functions of y and  <span class="math">x_0, \\dots, x_{\\mu-1}</span> , and the  <span class="math">g_r</span>  are functions of  <span class="math">y, x_0, \\dots, x_{\\mu-1}</span> . We can now apply Lemma 1. Let  <span class="math">\\boldsymbol{A}</span>  denote the concatenation of all the  <span class="math">\\boldsymbol{a}</span>  vectors, indexed from 0 so that  <span class="math">\\boldsymbol{A}_{l-1+k(i+(j-1)\\mathfrak{m})} = (\\boldsymbol{a}_{i,j})_l</span> , and similarly for b, c and d. Suppose the committed vectors  <span class="math">\\boldsymbol{a}_{i,j}, \\boldsymbol{b}_{i,j}, \\boldsymbol{c}_{i,j}, \\boldsymbol{d}_{i,j}</span>  do not satisfy the double-shift relation. This can happen in five ways:</p>

    <pre><code class="language-text">1. A_0 \\neq 1

2. A_i \\neq B_{i-1} for some i \\in [N-1]

3. B_{N-1} \\neq D_{N-1}

4. C_i \\neq D_{i-1} for some i \\in [N-1]

5. C_0 \\neq 1
</code></pre>

    <p class="text-gray-300">Consider the coefficients of the powers of y in the equation obtained when substituting the expressions for  <span class="math">\\hat{a}</span> ,  <span class="math">\\hat{b}</span> ,  <span class="math">\\hat{c}</span>  and  <span class="math">\\hat{d}</span>  into the left-hand side of the above equation. The constant is  <span class="math">A_0</span>  on the left hand side and 1 on the right-hand side. Hence, in the first of the five cases, the constant term would be different and we have the event F. For i from 1 to N-1, we see that the coefficient of  <span class="math">y^i</span>  is  <span class="math">A_i - B_{i-1}</span>  on the left-hand side and 0 on the right-hand side, so in the second case, we will also have event F. The coefficient of  <span class="math">y^N</span>  is  <span class="math">D_{N-1} - B_{N-1}</span>  on the left-hand side and 0 on the right-hand side, so the third case also implies F. The coefficients of  <span class="math">y^{N+1}, y^{N+2}, \\ldots, y^{2N}</span>  show that the fourth and fifth cases also imply F. So if the input does not satisfy the double-shift relation, we have event F. Now Lemma 1 implies that there is negligible probability that the equation will be satisfied, and hence negligible probability that verifier will accept.</p>

    <p class="text-gray-300">Finally, we show that the proof is honest-verifier zero-knowledge. We describe how to simulate the verifier's view efficiently, given values  <span class="math">y, x_0, \\ldots, x_{\\mu-1}, x \\leftarrow \\mathbb{F}^{\\times}</span>  for the random challenges used in the protocol. In an honest transcript,  <span class="math">\\hat{a}_0</span>  is chosen uniformly at random and added to something independent of  <span class="math">\\hat{a}_0</span>  to obtain  <span class="math">\\hat{a}</span> . Hence  <span class="math">\\hat{a}</span>  is uniformly distributed and can easily be simulated. Similarly for b, c and d. The final value to simulate is  <span class="math">\\hat{e}</span> , but for an accepting transcript this is uniquely determined and easy to compute given  <span class="math">\\hat{a}, \\hat{b}, \\hat{c}</span>  and  <span class="math">\\hat{d}</span> . Therefore, we can simulate the transcript and the proof system has special honest verifier zero knowledge.</p>

    <p class="text-gray-300"><strong>Efficiency.</strong> The verifier has a query complexity of 5 and sends  <span class="math">\\mu + 1</span>  field elements to the prover. The prover commits to a total of  <span class="math">2\\mu + 2\\mathfrak{n} + 3</span>  vectors in  <span class="math">\\mathbb{F}^k</span> .</p>

    <p class="text-gray-300">Most of the terms in this proof have a similar structure to those in the Hadamard product proof. The prover must on compute more compressed vectors than before as part of the double-shift proof, but this does not change the asymptotic costs of the protocol. The major differences are that scalar products are computed instead of Hadamard products, and that both prover and verifier must compute  <span class="math">\\hat{\\boldsymbol{w}}_a(y,x,x_0,\\ldots,x_{\\mu-1})</span> , and similarly for b,c and d, in terms of the random challenges. We have that  <span class="math">\\boldsymbol{y}=(1,y,\\ldots,y^{k-1})</span>  and</p>

    <p class="text-gray-300"><span class="math">$\\hat{\\boldsymbol{w}}_{a}(y,x,x_{0},\\ldots,x_{\\mu}) = \\boldsymbol{y} \\sum_{i=0,j=1}^{\\mathfrak{m}-1,\\mathfrak{n}} y^{k(i+(j-1)\\mathfrak{m})} x_{0}^{-i_{0}} \\ldots x_{\\mu-1}^{-i_{\\mu-1}} x^{-j}</span>$</p>

    <p class="text-gray-300">This can be done using  <span class="math">\\mathcal{O}(\\mathfrak{mn}+k)</span>  multiplications in  <span class="math">\\mathbb{F}</span> , since  <span class="math">\\boldsymbol{y}</span>  requires  <span class="math">\\mathcal{O}(k)</span>  multiplications to compute, and the sum requires  <span class="math">\\mathcal{O}(\\mathfrak{mn})</span> . Aside from that, the dominant costs of the other parts of the protocol are the same, resulting in a cost of  <span class="math">\\mathcal{O}(k\\mathfrak{n}\\log\\mathfrak{n}+k\\mathfrak{mn})</span>  multiplications in  <span class="math">\\mathbb{F}</span>  for the prover, and  <span class="math">\\mathcal{O}(\\mathfrak{mn}+k)</span>  multiplications in  <span class="math">\\mathbb{F}</span>  for the verifier.</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">T_{\\mathcal{P}_{\\mathrm{shift}}}</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">T_{\\mathcal{V}_{\\text{shift}}}</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">qc</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">#rounds</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">t</th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(k\\mathfrak{n}\\log\\mathfrak{n} + k\\mathfrak{m}\\mathfrak{n})</span> mult.</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(\\mathfrak{mn}+k)</span> mult.</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">5</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\log \\mathfrak{m} + 2</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">2\\mu + 2\\mathfrak{n} + 3</span></td>
          </tr>
        </tbody>
      </table>
    </div>

    <h4 id="sec-25" class="text-lg font-semibold mt-6">A.5 Proof for the Same-Product of Matrices</h4>

    <p class="text-gray-300">Now that we have an proof for the double-shift condition, and a Hadamard-product proof, it is easy to construct an proof which shows that the product of all entries in a matrix A is the same of the product of all entries of a matrix B. The corresponding relation is</p>

    <p class="text-gray-300"><span class="math">$\\mathcal{R}_{\\text{same-prod}} = \\left\\{ A, B \\in \\mathbb{F}^{m \\times k} \\land A = (a_{i,j}) \\land B = (b_{i,j}) \\\\ \\land \\prod_{i,j} a_{i,j} = \\prod_{i,j} b_{i,j} \\right\\}.</span>$</p>

    <p class="text-gray-300">This is achieved by computing the partial products of entries of the matrix A, beginning with 1, and storing them in a matrix  <span class="math">A_1</span>  with the same dimensions as A. The partial products, ending with the product of all elements of A, are stored in another matrix  <span class="math">A_2</span> , and similarly for  <span class="math">B, B_1, B_2</span> . Now,  <span class="math">A_2 = A \\circ A_1</span> , and  <span class="math">B_2 = B \\circ B_1</span>  by design. Note that the product of all entries in A is the same as the product of the entries in B if and only if  <span class="math">A_1, A_2, B_1</span>  and  <span class="math">B_2</span>  satisfy the double shift condition. This gives rise to the Same-Product proof shown in Fig. 11. An example follows.</p>

    <p class="text-gray-300"><span class="math">$A = \\begin{pmatrix} a_{1,1} &amp; a_{1,2} \\\\ a_{2,1} &amp; a_{2,2} \\end{pmatrix}, \\qquad B = \\begin{pmatrix} b_{1,1} &amp; b_{1,2} \\\\ b_{2,1} &amp; b_{2,2} \\end{pmatrix},</span>$</p>

    <p class="text-gray-300"><span class="math">$A_{1} = \\begin{pmatrix} 1 &amp; a_{1,1} \\\\ a_{1,1}a_{1,2} &amp; a_{1,1}a_{1,2}a_{2,1} \\end{pmatrix}, \\qquad B_{1} = \\begin{pmatrix} 1 &amp; b_{1,1} \\\\ b_{1,1}b_{1,2} &amp; b_{1,1}b_{1,2}b_{2,1} \\end{pmatrix},</span>$</p>

    <p class="text-gray-300"><span class="math">$A_{2} = \\begin{pmatrix} a_{1,1} &amp; a_{1,1}a_{1,2} \\\\ a_{1,1}a_{1,2}a_{2,1} &amp; a_{1,1}a_{1,2}a_{2,1}a_{2,2} \\end{pmatrix}, \\qquad B_{2} = \\begin{pmatrix} b_{1,1} &amp; b_{1,1}b_{1,2} \\\\ b_{1,1}b_{1,2}b_{2,1} &amp; b_{1,1}b_{1,2}b_{2,1}b_{2,2} \\end{pmatrix},</span>$</p>

    <pre><code class="language-text"> \\begin{array}{|c|c|c|c|c|} \\hline \\mathcal{P}_{\\text{same-prod}}(pp_{\\mathsf{ILC}},([A],[B])) &amp; \\hline \\\\ \\hline &amp; \\mathcal{V}_{\\text{same-prod}}(pp_{\\mathsf{ILC}},([A],[B])) \\\\ \\hline \\\\ \\hline \\\\ \\hline \\\\ \\hline \\\\ \\hline \\\\ \\hline \\\\ \\hline \\\\ \\hline \\\\ \\hline \\\\
</code></pre>

    <p class="text-gray-300">Fig. 11: Same-Product Argument for two committed matrices.</p>

    <p class="text-gray-300"><strong>Theorem 10.</strong>  <span class="math">(\\mathcal{K}_{LC}, \\mathcal{P}_{same-prod}, \\mathcal{V}_{same-prod})</span>  is a proof system for the relation  <span class="math">\\mathcal{R}_{same-prod}</span>  in the LC model with perfect completeness, statistical knowledge soundness with straight-line extraction, and perfect special honest verifier zero-knowledge.</p>

    <p class="text-gray-300"><em>Proof.</em> Perfect completeness follows by inspection.</p>

    <p class="text-gray-300">For statistical knowledge soundness with straight-line extraction, let matrices A and B be given. If there exists  <span class="math">A_1, A_2, B_1, B_2</span>  such that  <span class="math">((\\mathbb{F}, k), (A, A_1, A_2)) \\in \\mathcal{R}_{prod}</span> ,  <span class="math">((\\mathbb{F}, k), (B, B_1, B_2)) \\in \\mathcal{R}_{prod}</span>  and  <span class="math">((\\mathbb{F}, k), (A_1, A_2, B_1, B_2)) \\in \\mathcal{R}_{shift}</span>  then  <span class="math">((\\mathbb{F}, k), (A, B)) \\in \\mathcal{R}_{same-prod}</span> . So by the soundness property of the underlying protocols, if  <span class="math">((\\mathbb{F}, k), (A, B)) \\notin \\mathcal{R}_{same-prod}</span> , one of the sub-protocol will have negligible probability of accept. Since the extractor can read the committed values, we have statistical knowledge soundness with straight-line extractions.</p>

    <p class="text-gray-300">To see we have perfect special honest verifier zero-knowledge, simulate that the verifier receives commitments to two matrices in  <span class="math">\\mathbb{F}^{m\\times k}</span> , commit to random matrices  <span class="math">A_1,A_2,B_1</span>  and  <span class="math">B_2</span>  in in  <span class="math">\\mathbb{F}^{m\\times k}</span>  and run the perfect special honest verifier zero-knowledge simulators on the product and double-shift proofs.</p>

    <p class="text-gray-300"><strong>Efficiency.</strong> The same-product proof involves running two product proofs and one double-shift proof. Asymptotically, these two sub-protocols have the same</p>

    <p class="text-gray-300">communication costs. Therefore, the cost of the same-product proof is asymptotically the same.</p>

    <p class="text-gray-300">The verifier has query complexity qc = 11 and the total number of committed vectors is  <span class="math">O(\\mathfrak{mn})</span> .</p>

    <p class="text-gray-300">The protocol has computational costs of  <span class="math">O(k\\mathfrak{n}\\log\\mathfrak{n} + k\\mathfrak{m}\\mathfrak{n})</span>  multiplications in  <span class="math">\\mathbb{F}</span>  for the prover, and  <span class="math">O(\\mathfrak{m}\\mathfrak{n} + k)</span>  multiplications in  <span class="math">\\mathbb{F}</span>  for the verifier.</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">T_{\\mathcal{P}_{\\mathrm{same-prod}}}</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">T_{\\mathcal{V}_{\\mathrm{same-prod}}}</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">qc</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">#rounds</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">t</th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(k\\mathfrak{n}\\log\\mathfrak{n} + k\\mathfrak{m}\\mathfrak{n})</span> mult.</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(\\mathfrak{mn}+k)</span> mult.</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">11</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\log \\mathfrak{m} + 2</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(\\mathfrak{mn})</span></td>
          </tr>
        </tbody>
      </table>
    </div>

    <h4 id="sec-26" class="text-lg font-semibold mt-6">A.6 Proof for Known Permutation of Matrices</h4>

    <p class="text-gray-300">We will now give a known permutation proof for matrices. We deviate from [Gro09] here, since Groth's known permutation argument relies on computing powers of a challenge, which would cause the verifier to use a linear number of multiplications, where we get a verifier that uses a linear number of additions. Suppose the prover has committed to the rows of two matrices  <span class="math">A, B \\in \\mathbb{F}^{m \\times k}</span> , which we will write with square brackets and let the instance contain a permutation  <span class="math">\\pi \\in \\Sigma_{[m] \\times [k]}</span> , i.e.  <span class="math">u = (\\pi, [A], [B])</span> . Then the claim is that the committed matrices satisfy  <span class="math">B = A^{\\pi}</span> , where the notation  <span class="math">A^{\\pi}</span>  means the matrix with entries  <span class="math">a_{i,j}^{\\pi} = a_{\\pi(i,j)}</span> . The corresponding relation is</p>

    <p class="text-gray-300"><span class="math">$\\mathcal{R}_{\\mathrm{perm}} = \\left\\{ \\begin{aligned} &amp;(pp_{\\mathsf{ILC}}, u) = ((\\mathbb{F}, k) \\ , \\ (\\pi, [A], [B])) : \\\\ &amp;A, B \\in \\mathbb{F}^{m \\times k} \\ \\land \\quad \\pi \\in \\varSigma_{[m] \\times [k]} \\quad \\text{and} \\quad A = B^{\\pi} \\end{aligned} \\right\\}.</span>$</p>

    <p class="text-gray-300">Given a permutation  <span class="math">\\pi \\in \\Sigma_{[m] \\times [k]}</span> , implicitly using an equivalence  <span class="math">(i, j) \\leftrightarrow (i-1)m+j</span> , we define matrices</p>

    <p class="text-gray-300"><span class="math">$V = \\begin{pmatrix} 1 &amp; 2 &amp; \\cdots &amp; k \\\\ k+1 &amp; k+2 &amp; \\cdots &amp; 2k \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; \\cdots &amp; mk \\end{pmatrix} \\qquad V^{\\pi} = \\begin{pmatrix} \\pi(1) &amp; \\pi(2) &amp; \\cdots &amp; \\pi(k) \\\\ \\pi(k+1) &amp; \\pi(k+2) &amp; \\cdots &amp; \\pi(2k) \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; \\cdots &amp; \\pi(mk) \\end{pmatrix}</span>$</p>

    <p class="text-gray-300">Assuming integers in [mk] are mapped injectively into  <span class="math">\\mathbb{F}</span>  we can think of these matrices as belonging to  <span class="math">\\mathbb{F}^{m\\times k}</span> . Let us also define  <span class="math">J\\in\\mathbb{F}^{m\\times k}</span>  to be the matrix</p>

    <p class="text-gray-300">that has 1 in all entries, i.e.,
<span class="math">$J = \\begin{pmatrix} 1 &amp; 1 \\\\ &amp; \\ddots \\\\ 1 &amp; 1 \\end{pmatrix}</span>$
.</p>

    <p class="text-gray-300">We give the permutation proof for  <span class="math">\\mathcal{R}_{perm}</span>  in Fig. 12. The idea behind the construction is to let the verifier pick random challenges x, y and let the prover commit to A + yV - xJ and  <span class="math">B + yV^{\\pi} - xJ</span> . Notice that if  <span class="math">B = A^{\\pi}</span>  then  <span class="math">B + yV^{\\pi}</span>  contains a permutation of the entries in A + yV, however, if  <span class="math">B \\neq A^{\\pi}</span>  then with overwhelming probability over y there will be entries in  <span class="math">B + yV^{\\pi}</span>  that do not</p>

    <p class="text-gray-300">appear anywhere in A + yV. The prover will now convince the verifier that the product of the entries in A + yV - xJ is equal to the product of the entries in  <span class="math">B + yV^{\\pi} - xJ</span> . What happens if the prover is trying to cheat and  <span class="math">B \\neq A^{\\pi}</span> ? Writing out the products of entries, we then get that to cheat the prover must have</p>

    <p class="text-gray-300"><span class="math">$\\prod_{i,j} (a_{i,j} + yv_{i,j} - x) = \\prod_{i,j} (b_{i,j} + y\\pi(v_{i,j}) - x).</span>$</p>

    <p class="text-gray-300">By the Schwartz-Zippel lemma this is unlikely to hold over the random choice of x unless indeed  <span class="math">B + yV^{\\pi}</span>  contains a permutation of the entries in A + yV.</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">\\mathcal{P}_{\\mathrm{perm}}(pp_{ILC},(\\pi,[A],[B])</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">\\mathcal{V}_{ILC}(pp_{ILC},(\\pi,[A],[B]))</span></th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">- Get challenges <span class="math">x, y \\in \\mathbb{F}</span> from ILC&lt;br&gt;- Let <span class="math">U = yV - xJ</span> and <span class="math">U^{\\pi} = yV^{\\pi} - xJ</span>&lt;br&gt;- Let <span class="math">A&#x27; = A + yV - xJ</span> and <span class="math">B&#x27; = B + yV^{\\pi} - xJ</span>&lt;br&gt;- Commit to the rows in <span class="math">U, U^{\\pi}, A&#x27;, B&#x27;</span>&lt;br&gt;- Run <span class="math">\\mathcal{P}_{eq}(pp_{\\text{ILC}}, (U, [U]))</span>&lt;br&gt;- Run <span class="math">\\mathcal{P}_{eq}(pp_{\\text{ILC}}, (U^{\\pi}, [U^{\\pi}]))</span>&lt;br&gt;- Run <span class="math">\\mathcal{P}_{sum}(pp_{\\text{ILC}}, ([A], [U], [A&#x27;]))</span>&lt;br&gt;- Run <span class="math">\\mathcal{P}_{sum}(pp_{\\text{ILC}}, ([B], [U^{\\pi}], [B&#x27;]))</span>&lt;br&gt;- Run <span class="math">\\mathcal{P}_{same-prod}(pp_{\\text{ILC}}, ([A&#x27;], [B&#x27;]))</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">- Pick <span class="math">x, y \\leftarrow \\mathbb{F}</span> and send them to <span class="math">\\mathcal{P}_{perm}</span>&lt;br&gt;- Compute <span class="math">U = yV - xJ</span> and <span class="math">U^{\\pi} = yV^{\\pi} - xJ</span>&lt;br&gt;- Run <span class="math">\\mathcal{V}_{eq}(pp_{\\mathbb{ILC}}, (U, [U]))</span>&lt;br&gt;- Run <span class="math">\\mathcal{V}_{eq}(pp_{\\mathbb{ILC}}, (U^{\\pi}, [U^{\\pi}]))</span>&lt;br&gt;- Run <span class="math">\\mathcal{V}_{sum}(pp_{\\mathbb{ILC}}, ([A], [U], [A&#x27;])</span>&lt;br&gt;- Run <span class="math">\\mathcal{V}_{sum}(pp_{\\mathbb{ILC}}, ([B], [U^{\\pi}], [B&#x27;])</span>&lt;br&gt;- Run <span class="math">\\mathcal{V}_{same-prod}(pp_{\\mathbb{ILC}}, ([A&#x27;], [B&#x27;]))</span>&lt;br&gt;- Return 1 if all proofs accept, Return 0 otherwise</td>
          </tr>
        </tbody>
      </table>
    </div>

    <p class="text-gray-300">Fig. 12: Known permutation proof for two committed matrices.</p>

    <p class="text-gray-300"><strong>Theorem 11.</strong>  <span class="math">(\\mathcal{K}_{ILC}, \\mathcal{P}_{perm}, \\mathcal{V}_{perm})</span>  is a proof system for the relation  <span class="math">\\mathcal{R}_{perm}</span>  in the ILC model with perfect completeness, statistical knowledge soundness with straight-line extraction, and perfect special honest verifier zero-knowledge.</p>

    <p class="text-gray-300"><em>Proof.</em> Perfect completeness follows by inspection.</p>

    <p class="text-gray-300">For statistical soundness, note that by the knowledge soundness of the equality and sum proofs we know the prover has indeed committed correctly to A+yV-xJ and  <span class="math">B+yV^\\pi-xJ</span>  and can extract these committed values. By the knowledge soundness of the same product proof, we get  <span class="math">\\prod_{i,j}(a_{i,j}+yv_{i,j}-x)=\\prod_{i,j}(b_{i,j}+y\\pi(v_{i,j})-x)</span> . The Schwartz-Zippel Lemma tells us that if A+yV and  <span class="math">B+yV^\\pi</span>  have different entries, then the probability over the random choice of  <span class="math">x\\leftarrow\\mathbb{F}</span>  of this equality to hold is at most  <span class="math">\\frac{mk}{|\\mathbb{F}|}</span> , which is negligible. If B is not equal to  <span class="math">A^\\pi</span>  then we have with probability  <span class="math">\\frac{mk}{|\\mathbb{F}|}</span>  over the choice of  <span class="math">y\\leftarrow\\mathbb{F}</span>  that one of the entries in  <span class="math">B+yV^\\pi</span>  does not appear as an entry in A+yV. Finally, note that each sub-protocol has straight-line extraction.</p>

    <p class="text-gray-300">To see we have perfect special honest verifier zero-knowledge, simulate that the verifier receives commitments to four matrices in  <span class="math">\\mathbb{F}^{m\\times k}</span>  and run the perfect</p>

    <p class="text-gray-300">special honest verifier zero-knowledge simulators on the equality, sum and same product proofs.  <span class="math">\\Box</span></p>

    <p class="text-gray-300"><strong>Efficiency.</strong> The efficiency of the proof system is given in the table below, where  <span class="math">\\mathfrak{m}, \\mathfrak{n}</span>  are chosen to get good performance in the sub-proofs on the condition  <span class="math">m = \\mathfrak{mn}</span> . The computational cost for the verifier includes  <span class="math">\\mathcal{O}(\\mathfrak{mn} + k)</span>  multiplications for the verifier, but for sufficiently large parameters this is dwarfed by  <span class="math">\\mathcal{O}(k\\mathfrak{mn}) = \\mathcal{O}(mk)</span>  additions used in the equivalence sub-proofs.</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">T_{\\mathcal{P}_{\\mathrm{perm}}}</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">T_{\\mathcal{V}_{\\mathrm{perm}}}</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">qc</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">#rounds</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">t</th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(k\\mathfrak{n}\\log\\mathfrak{n} + k\\mathfrak{m}\\mathfrak{n})</span> mult.</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(k\\mathfrak{m}\\mathfrak{n})</span> add.</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">15</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\log \\mathfrak{m} + 2</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(\\mathfrak{mn})</span></td>
          </tr>
        </tbody>
      </table>
    </div>

    <h4 id="sec-27" class="text-lg font-semibold mt-6">A.7 Efficiency of the Proof for Arithmetic Circuit Satisfiability</h4>

    <pre><code class="language-text">\\mathcal{P}_{\\mathsf{ILC}}(pp_{\\mathsf{ILC}}, u, w)
                                                                                      \\mathcal{V}_{\\mathsf{ILC}}(pp_{\\mathsf{ILC}}, u)
                                                                                         - Parse u = (m_A, m_M, \\pi, \\{\\boldsymbol{u}_i\\}_{i \\in S})
  - Parse u = (m_A, m_M, \\pi, \\{v_i\\}_{i \\in S})
  - Parse w = (\\{\\boldsymbol{v}_i\\}_{i \\in \\bar{S}})
                                                                                         - Run \\mathcal{V}_{eq}(pp_{\\mathsf{ILC}}, (\\{\\boldsymbol{u}_i\\}_{i \\in S}, [U]))
  - Send (commit, \\{v_i\\}_{i=1}^m) to the ILC
                                                                                         - Run \\mathcal{V}_{\\text{sum}}(pp_{\\mathsf{ILC}},([A],[B],[C]))
  – The vectors define V \\in \\mathbb{F}^{m \\times k}
                                                                     and sub- - Run \\mathcal{V}_{\\text{prod}}(pp_{\\mathsf{ILC}},([D],[E],[F]))
       matrices A, B, C, D, E, F as described earlier - \\text{Run } \\mathcal{V}_{perm}(pp_{\\mathsf{ILC}}, (\\pi, [V], [V]))
                                                                                         - Return 1 if all the proofs accept
   - Let U = (\\boldsymbol{v}_i)_{i \\in S}
   - Run \\mathcal{P}_{eq}(pp_{\\mathsf{ILC}}, (\\{\\boldsymbol{v}_i\\}_{i \\in S}, [U]))
                                                                                              Return 0 otherwise
  - Run \\mathcal{P}_{\\text{sum}}(pp_{\\mathsf{ILC}},([A],[B],[C]))
   - Run \\mathcal{P}_{\\text{prod}}(pp_{\\mathsf{ILC}},([D],[E],[F]))
  - Run \\mathcal{P}_{perm}(pp_{\\mathsf{ILC}},(\\pi,[V],[V]))
</code></pre>

    <p class="text-gray-300">Fig. 3: Arithmetic circuit satisfiability proof in the ILC model.</p>

    <p class="text-gray-300">By observing the efficiency of all sub-protocols used in the main protocol, shown in Figure 3 and repeated above for convenience, we get the efficiency table below. The computational cost is dominated by the permutation proof, where the matrices have higher dimensions, so we choose  <span class="math">\\mathfrak{m},\\mathfrak{n}</span>  such that  <span class="math">m=3m_A+3m_M=\\mathfrak{m}\\mathfrak{n}</span> . The total number of gates is  <span class="math">N=\\frac{k\\mathfrak{m}\\mathfrak{n}}{3}</span> . Setting  <span class="math">\\mathfrak{m}=\\mathcal{O}(\\log N)</span>  and  <span class="math">k\\approx \\sqrt{N}</span>  we get the asymptotic complexities indicated in the table.</p>

    <h4 id="sec-28" class="text-lg font-semibold mt-6">A.8 Amortized Sub-linear Verification Time</h4>

    <p class="text-gray-300">We have given proof systems for arbitrary adaptively chosen arithmetic circuits with N gates, where the verifier's cost is  <span class="math">\\mathcal{O}(N)</span>  additions. Field additions can</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">T_{\\mathcal{P}_ILC}</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">T_{\\mathcal{V}_ILC}</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">qc</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">#rounds</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">t</th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(k\\mathfrak{n}\\log\\mathfrak{n} + k\\mathfrak{m}\\mathfrak{n})</span> mult.</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(k\\mathfrak{m}\\mathfrak{n})</span> add.</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">20</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\log \\mathfrak{m} + 2</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(\\mathfrak{mn})</span></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(N)</span> multiplications</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(N)</span> additions</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">20</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(\\log \\log N)</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\mathcal{O}(\\sqrt{N})</span></td>
          </tr>
        </tbody>
      </table>
    </div>

    <p class="text-gray-300">be implemented in  <span class="math">\\mathcal{O}(\\frac{\\log |\\mathbb{F}|}{W})</span>  operations on a RAM machine with W-bit words, so the computational cost is proportional to the word-length of the circuit description. The verifier complexity is therefore optimal up to a constant factor, since it will take  <span class="math">\\Omega(N\\frac{\\log |\\mathbb{F}|}{W})</span>  operations just to read the entire instance unless it is represented in more compact form.</p>

    <p class="text-gray-300">However, if we consider a non-adaptive setting where the same circuit wiring is used many times, then it is possible to amortize the verifier's computational cost. The place where the verifier's pays a linear computational cost is when encoding the wiring of the circuit into matrices V and  <span class="math">V^{\\pi}</span>  in the known permutation proof, and when testing the correct constants  <span class="math">\\{v_i\\}_{i\\in S}</span>  have been committed to using an equality proof. But if the wiring is fixed, we do not have to re-compute the encoding, and if the instance is small, i.e., |S| is small, then these costs diminish. In this special setting, which is commonly used in the SNARK world, we get verifier computation that is sub-linear in the size of the arithmetic circuit.</p>

    <h4 id="sec-29" class="text-lg font-semibold mt-6">Proof of Claims in Section 4 В</h4>

    <p class="text-gray-300">Claim. Let  <span class="math">e_0^*, \\dots e_t^* \\in \\mathbb{F}^{\\nu}</span> . If Err occurs, then for uniformly chosen  <span class="math">\\gamma \\in \\mathbb{F}^t</span> , there is probability at most  <span class="math">\\frac{1}{|\\mathbb{F}|}</span>  that  <span class="math">\\mathsf{hd}(\\tilde{\\mathcal{C}}, e_0^* + \\gamma E^*) &lt; \\frac{\\mathsf{hd}_{\\min}}{6}</span> .</p>

    <p class="text-gray-300"><em>Proof.</em> Assume Err, that is, there exist  <span class="math">\\gamma^* \\in \\mathbb{F}^t</span>  with  <span class="math">\\mathsf{hd}\\left(\\tilde{\\mathcal{C}}, \\gamma^* E^*\\right) \\geq \\frac{\\mathsf{hd}_{\\min}}{3}</span> . We will show that for any  <span class="math">r \\in \\mathbb{F}^{\\times}</span>  we have</p>

    <p class="text-gray-300"><span class="math">$\\operatorname{hd}\\left(\\tilde{\\mathcal{C}},\\boldsymbol{e}_{0}^{*}+\\boldsymbol{\\gamma}\\boldsymbol{E}^{*}\\right)+\\operatorname{hd}\\left(\\tilde{\\mathcal{C}},\\boldsymbol{e}_{0}^{*}+(\\boldsymbol{\\gamma}+r\\boldsymbol{\\gamma}^{*})\\boldsymbol{E}^{*}\\right)\\geq\\operatorname{hd}\\left(\\tilde{\\mathcal{C}},\\boldsymbol{\\gamma}^{*}\\boldsymbol{E}^{*}\\right)\\geq\\frac{\\operatorname{hd}_{\\min}}{3}.\\tag{4}</span>$</p>

    <p class="text-gray-300">This implies that at most one of  <span class="math">e_0^* + \\gamma E^*</span>  and  <span class="math">e_0^* + (\\gamma + r\\gamma^*)E^*</span>  can have distance less than  <span class="math">\\frac{hd_{\\min}}{6}</span>  to  <span class="math">\\tilde{\\mathcal{C}}</span> . That is, for at most one  <span class="math">\\gamma \\in \\mathbb{F}^t</span>  in each equivalence class in  <span class="math">\\mathbb{F}^t/\\gamma^*\\mathbb{F}</span>  can  <span class="math">e_0^* + \\gamma E^*</span>  have distance less than  <span class="math">\\frac{\\mathsf{hd}_{\\min}}{6}</span>  to  <span class="math">\\tilde{\\mathcal{C}}</span> . Since each such equivalence class contains  <span class="math">|\\mathbb{F}|</span>  elements, there is probability at most  <span class="math">\\frac{1}{|\\mathbb{F}|}</span>  that a random  <span class="math">\\gamma \\in \\mathbb{F}^t</span>  satisfies  <span class="math">\\operatorname{hd}\\left(\\tilde{\\mathcal{C}}, e_0^* + \\gamma E^*\\right) &lt; \\frac{\\operatorname{hd_{\\min}}}{6}</span> .</p>

    <p class="text-gray-300">To finish the proof, we need to prove (4). Write  <span class="math">e_0^* + \\gamma E^* = c_1 + v_1</span>  and  <span class="math">e_0^* + c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* + v_0^* = c_0^* = c_0^* + v_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* = c_0^* =</span></p>

    <p class="text-gray-300"><span class="math">(\\gamma + r\\gamma^*)E^* = c_2 + v_2 \\text{ with } c_1, c_2 \\in \\tilde{\\mathcal{C}} \\text{ and } \\mathsf{wt}(v_1) = \\mathsf{hd}\\left(\\tilde{\\mathcal{C}}, e_0^* + \\gamma E^*\\right), \\mathsf{wt}(v_2) = c_1 + c_2 + c_3 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c_4 + c</span>  <span class="math">\\operatorname{hd}\\left(\\tilde{\\mathcal{C}},e_0^*+(\\boldsymbol{\\gamma}+r\\boldsymbol{\\gamma}^*)E^*\\right)</span> . Now</p>

    <p class="text-gray-300"><span class="math">$\\gamma^* E^* = (e_0^* + (\\gamma + r\\gamma^*) E^* - (e_0^* + \\gamma E^*)) r^{-1}</span>$</p>

    <p class="text-gray-300"><span class="math">$= (c_2 + v_2 - c_1 - v_1) r^{-1}</span>$</p>

    <p class="text-gray-300"><span class="math">$= (c_2 - c_1) r^{-1} + (v_2 - v_1) r^{-1}</span>$</p>

    <p class="text-gray-300">Here  <span class="math">(c_2-c_1)r^{-1} \\in \\tilde{\\mathcal{C}}</span>  and  <span class="math">(v_2-v_1)r^{-1}</span>  has at most</p>

    <p class="text-gray-300"><span class="math">$\\mathsf{wt}(v_1) + \\mathsf{wt}(v_2) = \\mathsf{hd}\\left(\\tilde{\\mathcal{C}}, \\boldsymbol{e}_0^* + \\boldsymbol{\\gamma} E^*\\right) + \\mathsf{hd}\\left(\\tilde{\\mathcal{C}}, e_0^* + (\\boldsymbol{\\gamma} + r\\boldsymbol{\\gamma}^*) E^*\\right)</span>$</p>

    <p class="text-gray-300">non-zero elements. This proves inequality (4), and hence the claim.</p>

    <p class="text-gray-300">Claim. Assume that  <span class="math">\\neg Err</span>  and let V and R be defined as above. Then for any  <span class="math">\\bm{q} \\in \\mathbb{F}^t</span>  there exists a  <span class="math">\\bm{r}_{(\\bm{q})}</span>  with  <span class="math">\\mathsf{hd}(\\tilde{\\mathsf{E}}_{\\mathcal{C}}(\\bm{q}V, \\bm{r}_{(\\bm{q})}), \\bm{q}E^*) &lt; \\frac{\\mathsf{hd}_{\\min}}{3}</span> . In particular, for any  <span class="math">V_{(Q)}^* \\neq QV</span> , and any  <span class="math">R&#x27;^*</span>  we have</p>

    <p class="text-gray-300"><span class="math">$\\mathsf{hd}_2\\left(\\tilde{\\mathsf{E}}_{\\mathcal{C}}\\left(V_{(Q)}^*,R_{(Q)}^*\\right),QE^*\\right) \\geq 2\\frac{\\mathsf{hd}_{\\min}}{3}.</span>$</p>

    <p class="text-gray-300"><em>Proof.</em> Assume that  <span class="math">\\neg Err</span> , that is for all  <span class="math">\\mathbf{q} \\in \\mathbb{F}^t</span>  we have  <span class="math">\\mathsf{hd}(\\tilde{\\mathcal{C}}, \\mathbf{q}E^*) &lt; \\frac{\\mathsf{hd}_{\\min}}{3}</span> . Informally, we need to strengthen this by showing that the elements in  <span class="math">\\tilde{\\mathcal{C}}</span>  that are close to each  <span class="math">qE^*</span> , are themselves linear in q.</p>

    <p class="text-gray-300">We have chosen  <span class="math">v_{\\tau}</span> 's and  <span class="math">r_{\\tau}</span> 's such that  <span class="math">\\mathsf{hd}(\\tilde{\\mathsf{E}}_{\\mathcal{C}}(v_{\\tau}, r_{\\tau}), e_{\\tau}^*) &lt; \\frac{\\mathsf{hd}_{\\min}}{3}</span> , and Vis the matrix where the  <span class="math">\\tau</span> th row is  <span class="math">v_{\\tau}</span> . We will show by induction on number of non-zero elements  <span class="math">\\mathsf{wt}(q)</span>  in q that there exists  <span class="math">r_{(q)}</span>  with  <span class="math">\\mathsf{hd}(\\mathsf{E}_{\\mathcal{C}}(qV, r_{(q)}), qE^*) &lt; 0</span>  <span class="math">\\frac{\\mathsf{hd}_{\\min}}{3}</span> .</p>

    <p class="text-gray-300">This is trivially true for  <span class="math">\\mathsf{wt}(q) = 0</span> . For  <span class="math">\\mathsf{wt}(q) = 1</span>  it follows from our choice of  <span class="math">v_\\tau</span> . Assume for induction that it is true for all q with  <span class="math">\\mathsf{wt}(q) \\le \\kappa</span>  and consider a q with  <span class="math">\\mathsf{wt}(q) \\le 2\\kappa</span> . We can now write q = q' + q'' where  <span class="math">\\mathsf{wt}(q&#x27;), \\mathsf{wt}(q&#x27;&#x27;) \\le \\kappa</span> . By the induction hypothesis, there exists  <span class="math">r_q</span>  such that  <span class="math">\\mathsf{hd}(\\tilde{\\mathsf{E}}_{\\mathcal{C}}(q&#x27;V, r_{(q&#x27;)}), q&#x27;E^*) &lt; \\frac{\\mathsf{hd}_{\\min}}{3}</span>  and similar for q''. Since q = q' + q'' this implies</p>

    <p class="text-gray-300"><span class="math">$\\begin{split} &amp;\\operatorname{hd}\\left(\\tilde{\\operatorname{E}}_{\\mathcal{C}}\\left(qV, r_{(q&#x27;)} + r_{(q&#x27;&#x27;)}\\right), qE^*\\right) \\\\ &amp;= \\operatorname{hd}\\left(\\tilde{\\operatorname{E}}_{\\mathcal{C}}\\left((q&#x27; + q&#x27;&#x27;)V, r_{(q&#x27;)} + r_{(q&#x27;&#x27;)}\\right), (q&#x27; + q&#x27;&#x27;)E^*\\right) \\\\ &amp;\\leq \\operatorname{hd}\\left(\\tilde{\\operatorname{E}}_{\\mathcal{C}}\\left(q&#x27;V, r_{(q&#x27;)}\\right), q&#x27;E^*\\right) + \\operatorname{hd}\\left(\\tilde{\\operatorname{E}}_{\\mathcal{C}}\\left(q&#x27;&#x27;V, r_{(q&#x27;&#x27;)}\\right), q&#x27;&#x27;E^*\\right) \\\\ &amp;&lt; 2\\frac{\\operatorname{hd}_{\\min}}{3}. \\end{split}</span>$</p>

    <p class="text-gray-300">Since we assume  <span class="math">\\neg Err</span> , we know that there exist <em>some</em>  <span class="math">v_{(q)}</span>  and  <span class="math">r_{(q)}</span>  such that  <span class="math">\\mathsf{hd}(\\tilde{\\mathsf{E}}_{\\mathcal{C}}(v_{(q)},r_{(q)}),qE^*)&lt;\\frac{\\mathsf{hd}_{\\min}}{3}</span> . Now, by the triangle inequality for Hamming distance, this implies</p>

    <p class="text-gray-300"><span class="math">$\\begin{split} &amp; \\operatorname{hd}\\left(\\tilde{\\operatorname{E}}_{\\mathcal{C}}\\left(\\boldsymbol{v}_{(\\boldsymbol{q})},\\boldsymbol{r}_{(\\boldsymbol{q})}\\right),\\tilde{\\operatorname{E}}_{\\mathcal{C}}\\left(\\boldsymbol{q}\\boldsymbol{V},\\boldsymbol{r}_{(\\boldsymbol{q}&#x27;)}+\\boldsymbol{r}_{(\\boldsymbol{q}&#x27;&#x27;)}\\right)\\right) \\\\ &amp; \\leq \\operatorname{hd}\\left(\\tilde{\\operatorname{E}}_{\\mathcal{C}}\\left(\\boldsymbol{v}_{(\\boldsymbol{q})},\\boldsymbol{r}_{(\\boldsymbol{q})}\\right),\\boldsymbol{q}\\boldsymbol{E}^*\\right) + \\operatorname{hd}\\left(\\boldsymbol{q}\\boldsymbol{E}^*,\\tilde{\\operatorname{E}}_{\\mathcal{C}}\\left(\\boldsymbol{q}\\boldsymbol{V},\\boldsymbol{r}_{(\\boldsymbol{q}&#x27;)}+\\boldsymbol{r}_{(\\boldsymbol{q}&#x27;&#x27;)}\\right)\\right) \\\\ &amp; &lt; \\frac{\\operatorname{hd}_{\\min}}{3} + 2\\frac{\\operatorname{hd}_{\\min}}{3} = \\operatorname{hd}_{\\min} \\end{split}</span>$</p>

    <p class="text-gray-300">Since  <span class="math">\\mathsf{hd}_{\\min}</span>  is the minimum distance of  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}</span> , we must have  <span class="math">v_{(q)} = qV</span> , and hence  <span class="math">\\mathsf{hd}(\\tilde{\\mathsf{E}}_{\\mathcal{C}}(qV,r_{(q)}),qE^*) &lt; \\frac{\\mathsf{hd}_{\\min}}{3}</span> . This finishes the induction argument. The triangle inequality for Hamming distance shows that for any  <span class="math">(v_{(q)}^*,r_{(q)}^*)</span></p>

    <p class="text-gray-300">The triangle inequality for Hamming distance shows that for any  <span class="math">(v_{(q)}^*, r_{(q)}^*)</span>  with  <span class="math">v_{(q)}^* \\neq qV</span>  we have  <span class="math">\\operatorname{hd}(\\tilde{\\mathsf{E}}_{\\mathcal{C}}(v_{(q)}^*, r_{(q)}^*), qE^*) \\geq 2\\frac{\\operatorname{hd_{\\min}}}{3}</span> . Now for any  <span class="math">V_{(Q)}^* \\neq QV</span>  there is a row  <span class="math">\\tau</span>  where the two matrices differ. Let q be the  <span class="math">\\tau</span> th row of Q. Then  <span class="math">\\operatorname{hd}(\\tilde{\\mathsf{E}}_{\\mathcal{C}}(v_{(q)}^*, r_{(q)}^*), qE^*) \\geq 2\\frac{\\operatorname{hd_{\\min}}}{3}</span>  tells us that the  <span class="math">\\tau</span> th row of  <span class="math">\\tilde{\\mathsf{E}}_{\\mathcal{C}}(V_{(Q)}^*, R_{(Q)}^*)</span>  and  <span class="math">\\tau</span> th row of  <span class="math">QE^*</span>  differs in at least  <span class="math">2\\frac{\\operatorname{hd_{\\min}}}{3}</span>  positions. In particular,  <span class="math">\\operatorname{hd}_2\\left(\\tilde{\\mathsf{E}}_{\\mathcal{C}}\\left(V_{(Q)}^*, R_{(Q)}^*\\right), QE^*\\right) \\geq 2\\frac{\\operatorname{hd_{\\min}}}{3}</span> .</p>

`;
---

<BaseLayout title="Linear-Time Zero-Knowledge Proofs for Arithmetic Circuit Sat... (2017/872)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2017 &middot; eprint 2017/872
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <PaperDisclaimer eprintUrl={EPRINT_URL} />
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <Fragment set:html={CONTENT} />

    <PaperHistory slug="linear-time-zero-knowledge-proofs-for-arithmetic-circuit-2017" />
  </article>
</BaseLayout>
