---
import BaseLayout from '../../layouts/BaseLayout.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2017/1033';
const CRAWLER = 'mistral';
const CONVERTED_DATE = '2026-02-16';
const TITLE_HTML = 'Foundations of Differentially Oblivious Algorithms';
const AUTHORS_HTML = 'T-H.  Hubert Chan, Kai-Min Chung, Bruce Maggs, Elaine Shi';

const CONTENT = `    <p class="text-gray-300">T-H. Hubert Chan Kai-Min Chung Bruce Maggs Elaine Shi</p>

    <h6 id="sec-2" class="text-base font-medium mt-4">Abstract</h6>

    <p class="text-gray-300">It is well-known that a program’s memory access pattern can leak information about its input. To thwart such leakage, most existing works adopt the technique of oblivious RAM (ORAM) simulation. Such an obliviousness notion has stimulated much debate. Although ORAM techniques have significantly improved over the past few years, the concrete overheads are arguably still undesirable for real-world systems — part of this overhead is in fact inherent due to a well-known logarithmic ORAM lower bound by Goldreich and Ostrovsky. To make matters worse, when the program’s runtime or output length depend on secret inputs, it may be necessary to perform worst-case padding to achieve full obliviousness and thus incur possibly super-linear overheads.</p>

    <p class="text-gray-300">Inspired by the elegant notion of differential privacy, we initiate the study of a new notion of access pattern privacy, which we call “<span class="math">(\\epsilon,\\delta)</span>-differential obliviousness”. We separate the notion of <span class="math">(\\epsilon,\\delta)</span>-differential obliviousness from classical obliviousness by considering several fundamental algorithmic abstractions including sorting small-length keys, merging two sorted lists, and range query data structures (akin to binary search trees). We show that by adopting differential obliviousness with reasonable choices of <span class="math">\\epsilon</span> and <span class="math">\\delta</span>, not only can one circumvent several impossibilities pertaining to full obliviousness, one can also, in several cases, obtain meaningful privacy with little overhead relative to the non-private baselines (i.e., having privacy “almost for free”). On the other hand, we show that for very demanding choices of <span class="math">\\epsilon</span> and <span class="math">\\delta</span>, the same lower bounds for oblivious algorithms would be preserved for <span class="math">(\\epsilon,\\delta)</span>-differential obliviousness.</p>

    <h2 id="sec-3" class="text-2xl font-bold">1 Introduction</h2>

    <p class="text-gray-300">Suppose that there is a database consisting of sensitive user records (e.g., medical records), and one would like to perform data analytics or queries over this dataset in a way that respects individual users’ privacy. More concretely, we imagine the following two scenarios:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>The database is encrypted and outsourced to an untrusted cloud server that is equipped with a trusted secure processor such as Intel’s SGX <em>[2, 36]</em>, such that only the secure processor can decrypt and compute over the data.</li>

      <li>The database is horizontally partitioned across multiple nodes, e.g., each hospital holds records for their own patients.</li>

    </ol>

    <p class="text-gray-300">To provide formal and mathematical guarantees of users’ privacy, one natural approach is to require that any information about the dataset that is disclosed during the computation must satisfy differential privacy (DP). Specifically, differential privacy is a well-established notion first proposed in the ground-breaking work by Dwork et al. <em>[14]</em>. Naturally, in the former scenario, we can have the secure processor compute differentially private statistics to be released or differentially private answers to analysts’ queries. In the latter scenario, since the data is distributed, we can rely on multi-party computation (MPC) <em>[19, 48]</em> to emulate a secure CPU, and compute a differentially</p>

    <p class="text-gray-300">private mechanism securely (i.e., revealing only the differentially private answer but nothing else). The above approaches (assuming that the program is executed in the RAM-model) indeed ensure that the statistics computed by the secure processor or the MPC protocol are safe to release. However, this is not sufficient for privacy: specifically, the program’s execution behavior (in particular, memory access patterns) can nonetheless leak sensitive information.</p>

    <h4 id="sec-4" class="text-lg font-semibold mt-6">Classical notion of access pattern privacy: full obliviousness.</h4>

    <p class="text-gray-300">To defeat access pattern leakage, a line of work has focused on oblivious algorithms <em>[17, 24, 34]</em> and Oblivious RAM (ORAM) constructions <em>[18, 20]</em>. These works adopt “full obliviousness” as a privacy notion, i.e., the program’s memory access patterns (including the length of the access sequence) must be indistinguishable regardless of the secret database or inputs to the program. Such a full obliviousness notion has at least the following drawbacks:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>First, to achieve full obliviousness, a generic approach is to apply an Oblivious RAM (ORAM) compiler, an elegant algorithmic technique originally proposed by Goldreich and Ostrovsky <em>[18, 20]</em>. Although ORAM constructions have significantly improved over the past few years <em>[41, 42, 46]</em>, their concrete performance is still somewhat undesirable — and some of this overhead is, in fact, inherent due to the well-known logarithmic ORAM lower bound by Goldreich and Ostrovsky <em>[18, 20]</em>.</li>

      <li>Second, to make matters worse, in cases where the program’s output length or runtime also depends on the secret input, it may be necessary to pad the program’s output length and runtime to the maximum possible to achieve full obliviousness. Such padding can sometimes incur even super-linear overhead, e.g., see our range query database example later in the paper.</li>

    </ol>

    <h4 id="sec-5" class="text-lg font-semibold mt-6">Our new notion: differential obliviousness.</h4>

    <p class="text-gray-300">Recall that our final goal is to achieve a notion of “end-to-end differential privacy”, that is, any information disclosed (including any statistics explicitly released as well as the program’s execution behavior) must be differentially private. Although securely executing an oblivious DP-mechanism would indeed achieve this goal, the full obliviousness notion appears to be an overkill. In this paper, we formulate a new notion of access pattern privacy called differential obliviousness. Differential obliviousness requires that if the memory access patterns of a program are viewed as a form of statistics disclosed, then such “statistics” must satisfy differential privacy too. Note that applying standard composition theorems of DP <em>[16]</em>, the combination of statistics disclosed and access patterns would jointly be DP too (and thus achieving the aforementioned “end-to-end DP” goal).</p>

    <p class="text-gray-300">Our differential obliviousness notion can be viewed as a relaxation of full obliviousness (when both are defined with information theoretic security). Clearly, such a relaxation is only interesting if it allows for significantly smaller overheads than full obliviousness. Indeed, with this new notion, we can hope to overcome both drawbacks for full obliviousness mentioned above. First, it might seem natural that with differential obliviousness, we can avoid worst-case padding which can be prohibitive. Second, even when padding is a non-issue (i.e., when the program’s runtime and output length are fixed), an exciting question remains:</p>

    <p class="text-gray-300">Can we asymptotically outperform full obliviousness with this new notion? In other words, can we achieve differential obliviousness without relying on full obliviousness as a stepping stone?</p>

    <p class="text-gray-300">The answer to this question seems technically challenging. In the classical DP literature, we typically achieve differential privacy by adding noise to intermediate or output statistics <em>[14]</em>. To apply the same techniques here would require adding noise to a program’s memory access patterns</p>

    <p class="text-gray-300">— this seems counter-intuitive at first sight since access patterns arise almost as a side effect of a program’s execution.</p>

    <h4 id="sec-6" class="text-lg font-semibold mt-6">Our results and contributions.</h4>

    <p class="text-gray-300">Our paper is the first to show non-trivial lower- and upper-bound results establishing that differential obliviousness is an interesting and meaningful notion of access pattern privacy, and can significantly outperform full obliviousness (even when padding is a non-issue). We show results of the following nature:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><em>New lower bounds on full obliviousness.</em> On one hand, we show that for several fundamental algorithmic building blocks (such as sorting, merging and range query data structures), any oblivious simulation must incur at least <span class="math">\\Omega(\\log N)</span> overhead where <span class="math">N</span> is the data size. Our oblivious algorithm lower bounds can be viewed as a strengthening of Goldreich and Ostrovsky’s ORAM lower bounds <em>[18, 20]</em>. Since the logarithmic ORAM lower bounds do not imply a logarithmic lower bound for any specific algorithm, our lower bounds (for specific algorithms) are necessary to show a separation between differential obliviousness and full obliviousness.</li>

      <li><em>Almost-for-free differentially oblivious algorithms.</em> On the other hand, excitingly we show for the first time that for the same tasks mentioned above, differentially oblivious algorithms exist which incur only <span class="math">O(\\log\\log N)</span> overhead (we sometimes refer to these algorithms as “almost-for-free”).</li>

      <li><em>Separations between various definitional variants.</em> We explore various ways of defining differential obliviousness and theoretical separations between these notions. For example, we show an intriguing separation between <span class="math">\\epsilon</span>-differential obliviousness and <span class="math">(\\epsilon,\\delta)</span>-differential obliviousness. Specifically, just like <span class="math">\\epsilon</span>-DP and <span class="math">(\\epsilon,\\delta)</span>-DP, a non-zero <span class="math">\\delta</span> term allows for a (negligibly) small probability of privacy failure. We show that interestingly, permitting a non-zero but negligibly small failure probability (i.e., a non-zero <span class="math">\\delta</span>) turns out to be crucial if we would like to outperform classical full obliviousness! Indeed, our “almost-for-free” differential oblivious algorithms critically make use of this non-zero <span class="math">\\delta</span> term. Perhaps more surprisingly, most of our logarithmic full obliviousness lower bounds would still apply had we required <span class="math">\\epsilon</span>-differential obliviousness for arbitrarily large <span class="math">\\epsilon</span> (even though intuitively, very little privacy is preserved for large values of <span class="math">\\epsilon</span>)!</li>

    </ol>

    <p class="text-gray-300">Both our lower bounds and upper bounds require novel techniques. Our lower bounds draw connections to the complexity of shifting graphs <em>[39]</em> that were extensively studied in the classical algorithms literature. For upper bounds, to the best of our knowledge, our algorithms show for the first time how to combine oblivious algorithms techniques and differential privacy techniques in non-blackbox manners to achieve non-trivial results. Our upper bounds also demonstrate a new algorithmic paradigm for constructing differentially oblivious algorithms: first we show how to make certain DP mechanisms oblivious and we rely on these oblivious DP mechanisms to compute a set of intermediate DP-statistics. Then, we design algorithms whose memory access patterns are “simulatable” with knowledge of these intermediate DP statistics — and here again, we make use of oblivious algorithm building blocks.</p>

    <h3 id="sec-7" class="text-xl font-semibold mt-8">1.1 Differential Obliviousness</h3>

    <p class="text-gray-300">We formulate differential obliviousness for random access machines (RAMs) where a trusted CPU with <span class="math">O(1)</span> registers interacts with an untrusted memory and performs computation. We assume</p>

    <p class="text-gray-300">that the adversary is able to observe the memory addresses the CPU reads and writes, but is unable to observe the contents of the data (e.g., the data is encrypted or secret-shared by multiple parties). This abstraction applies to both of the motivating scenarios described at the beginning of our paper.</p>

    <p class="text-gray-300">Differential obliviousness can be intuitively interpreted as differential privacy <em>[14, 44]</em>, but now the observables are access patterns. Informally, we would like to guarantee that an adversary, after having observed access patterns to (encrypted) dataset stored on the server, learns approximately the same amount of information about an individual or an event as if this individual or event were not present in the dataset.</p>

    <h4 id="sec-8" class="text-lg font-semibold mt-6">Basic definition of differential obliviousness.</h4>

    <p class="text-gray-300">Let <span class="math">M</span> be an algorithm that is expressed as a RAM program. We say that two input databases <span class="math">I</span> and <span class="math">I^{\\prime}</span> are neighboring iff they differ only in one entry. The algorithm <span class="math">M</span> is said to be <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious, iff for any two neighboring input databases <span class="math">I</span> and <span class="math">I^{\\prime}</span>, for any set <span class="math">S</span> of access patterns, it holds that</p>

    <p class="text-gray-300"><span class="math">\\Pr[\\mathbf{Accesses}^{M}(I)\\in S]\\leq e^{\\epsilon}\\cdot\\Pr[\\mathbf{Accesses}^{M}(I^{\\prime})\\in S]+\\delta,</span> (1)</p>

    <p class="text-gray-300">where <span class="math">\\mathbf{Accesses}^{M}(I)</span> denotes the ordered sequence of memory accesses made by the algorithm <span class="math">M</span> upon receiving the input <span class="math">I</span>. Therefore, <span class="math">(\\epsilon,\\delta)</span>-differential obliviousness can be thought of as <span class="math">(\\epsilon,\\delta)</span>-DP but where the observables are the access patterns.</p>

    <p class="text-gray-300">The term <span class="math">\\delta</span> can be thought of as a small probability of privacy failure that we are willing to tolerate. For all of our upper bounds, we typically require that <span class="math">\\delta</span> be negligibly small in some security parameter <span class="math">\\lambda</span>. When <span class="math">\\delta=0</span>, we also say that <span class="math">M</span> satisfies <span class="math">\\epsilon</span>-differential obliviousness.</p>

    <h4 id="sec-9" class="text-lg font-semibold mt-6">Comparison with full obliviousness.</h4>

    <p class="text-gray-300">It is interesting to contrast the notion of differential obliviousness with the classical notion of full obliviousness <em>[18, 20]</em>. An algorithm <span class="math">M</span> (expressed as a RAM program) is said to be (statistically) <span class="math">\\delta</span>-oblivious iff for any input databases <span class="math">I</span> and <span class="math">I^{\\prime}</span> of equal length, it holds that <span class="math">\\mathbf{Accesses}^{M}(I)\\stackrel{{\\scriptstyle\\delta}}{{=}}\\mathbf{Accesses}^{M}(I^{\\prime})</span> where <span class="math">\\stackrel{{\\scriptstyle\\delta}}{{=}}</span> denotes that the two distributions have statistical distance at most <span class="math">\\delta</span>. When <span class="math">\\delta=0</span>, we say that the algorithm <span class="math">M</span> satisfies perfect obliviousness. Note that to satisfy the above definition requires that the length of the access sequence be identically distributed or statistically close for any input of a fixed length — as mentioned earlier, one way to achieve this is to pad the length/runtime to the worst case.</p>

    <p class="text-gray-300">It is not difficult to observe that <span class="math">(\\epsilon,\\delta)</span>-differential obliviousness is a relaxation of <span class="math">\\delta</span>-obliviousness; and likewise <span class="math">\\epsilon</span>-differential obliviousness is a relaxation of perfect obliviousness. Technically the relaxation arises from the following aspects:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>First, differential obliviousness requires that the access patterns be close only for neighboring inputs; as the inputs become more dissimilar, the access patterns they induce are also allowed to be more dissimilar. By contrast, full obliviousness requires that the access patterns be close for any input of a fixed length.</li>

      <li>Differential obliviousness permits a multiplicative <span class="math">e^{\\epsilon}</span> difference in the distribution of the access patterns incurred by neighboring inputs (besides the <span class="math">\\delta</span> failure probability); whereas full obliviousness does not permit this <span class="math">e^{\\epsilon}</span> relaxation.</li>

    </ol>

    <p class="text-gray-300">Later in the paper, we shall see that although <span class="math">\\epsilon</span>-differential obliviousness seems much weaker than obliviousness, surprisingly the same logarithmic lower bounds pertaining to full obliviousness</p>

    <p class="text-gray-300">carry over to <span class="math">\\epsilon</span>-differential obliviousness for several algorithmic abstractions we are concerned with. However, by additionally permitting a non-zero (but negligibly small) failure probability <span class="math">\\delta</span>, we can achieve almost-for-free differentially oblivious algorithms.</p>

    <p class="text-gray-300">Definition of differential obliviousness for stateful algorithms. We will also be concerned about stateful algorithms where the memory stores persistent state in between multiple invocations of the algorithm. Concretely, we will consider range-query data structures (akin to binary search trees), where the entries of a database can be inserted dynamically over time, and range queries can be made in between these insertions. In such a dynamic database setting, we will define an adaptive notion of differential obliviousness where the adversary is allowed to adaptively choose both the entries inserted into the database, as well as the queries — and yet we require that the access patterns induced be “close” by Equation (1) for any two neighboring databases (inserted dynamically). Our notion of adaptive differential obliviousness is akin to the standard adaptive DP notion for dynamic databases <em>[16]</em>, but again our observables are now memory access patterns rather than released statistics. We defer the full definition to later technical sections.</p>

    <h3 id="sec-10" class="text-xl font-semibold mt-8">1.2 Our Results</h3>

    <p class="text-gray-300">Equipped with the new differentially oblivious notion, we will now try to understand the following questions: 1) does differential obliviousness permit asymptotically faster algorithms than full obliviousness? 2) how do the choices of <span class="math">\\epsilon</span> and <span class="math">\\delta</span> affect the asymptotical performance of differentially oblivious algorithms? To this end, we consider a few fundamental algorithmic abstractions including sorting, merging, and data structures — these algorithmic abstractions were not only extensively studied in the algorithms literature, but also heavily studied in the ORAM and oblivious algorithms literature as important building blocks.</p>

    <h4 id="sec-11" class="text-lg font-semibold mt-6">1.2.1 Sorting</h4>

    <p class="text-gray-300">We consider (possibly non-comparison-based) sorting in the balls-and-bins model: imagine that there are <span class="math">N</span> balls (i.e., records) each tagged with a <span class="math">k</span>-bit key. We would like to sort the balls based on the relative ordering of their keys. If how an algorithm moves elements is based only on the relative order (with respect to the keys) of the input elements, we say that the algorithm is comparison-based; otherwise it is said to be non-comparison-based. Unlike the keys, the balls are assumed to be opaque — they can only be moved around but cannot be computed upon. A sorting algorithm is said to be stable if for any two balls with identical keys, their relative order in the output respects that in the input.</p>

    <p class="text-gray-300">First, even without privacy requirements, it is understood that 1) any comparison-based sorting algorithm must incur at least <span class="math">\\Omega(N\\log N)</span> comparison operations — even for sorting 1-bit keys due to the well-known 0-1 principle; and 2) for special scenarios, non-comparison-based sorting techniques can achieve linear running time (e.g., radix sort, counting sort, and others <em>[3, 26, 27, 31, 43]</em>) — and a subset of these techniques apply to the balls-and-bins model. A recent manuscript by Lin, Shi, and Xie <em>[33]</em> showed that interesting barriers arise if we require full obliviousness for sorting:</p>

    <h6 id="sec-12" class="text-base font-medium mt-4">Fact 1.1 (Barriers for oblivious sorting <em>[33]</em>).</h6>

    <p class="text-gray-300">Any oblivious <span class="math">1</span>-bit stable sorting algorithm in the balls-and-bins model, even non-comparison-based ones, must incur at least <span class="math">\\Omega(N\\log N)</span> runtime</p>

    <p class="text-gray-300">(even when allowing a constant probability of security or correctness failure). As a direct corollary, any general oblivious sorting algorithm in the balls-and-bins model, even non-comparison-based ones, must incur at least <span class="math">\\Omega(N\\log N)</span> runtime.</p>

    <p class="text-gray-300">We stress that the above oblivious sorting barrier is applicable only in the balls-and-bins model (otherwise without the balls-and-bins constraint, the feasibility or infeasibility of <span class="math">o(n\\log n)</span>-size circuits for sorting remains open <em>[7]</em>). Further, as Lin, Shi, and Xie showed <em>[33]</em>, for small-length keys, the barrier also goes away if the stability requirement is removed (see Section 1.3).</p>

    <h5 id="sec-13" class="text-base font-semibold mt-4">Differentially oblivious sorting.</h5>

    <p class="text-gray-300">Can we use the differential obliviousness relaxation to overcome the above oblivious sorting barrier (in the balls-and-bins model)? We show both upper and lower bounds. For upper bounds, we show that for choices of <span class="math">\\epsilon</span> and <span class="math">\\delta</span> that give reasonable privacy, one can indeed sort small-length keys in <span class="math">o(N\\log N)</span> time and attain <span class="math">(\\epsilon,\\delta)</span>-differential obliviousness. As a typical parameter choice, for <span class="math">\\epsilon=\\Theta(1)</span> and <span class="math">\\delta</span> being a suitable negligible function in <span class="math">N</span>, we can stably sort <span class="math">N</span> balls tagged with 1-bit keys in <span class="math">O(N\\log\\log N)</span> time. Note that in this case, the best non-private algorithm takes linear time, and thus we show that privacy is attained “almost for free” for 1-bit stable sorting. More generally, for any <span class="math">k=o(\\log N/\\log\\log N)</span>, we can stably sort <span class="math">k</span>-bit keys in <span class="math">o(N\\log N)</span> time — in other words, for small-length keys we overcome the <span class="math">\\Omega(N\\log N)</span> barrier of oblivious sorting.</p>

    <p class="text-gray-300">We state our result more formally and for generalized parameters:</p>

    <h6 id="sec-14" class="text-base font-medium mt-4">Theorem 1.2 (<span class="math">(\\epsilon,\\delta)</span>-differentially oblivious stable <span class="math">k</span>-bit sorting).</h6>

    <p class="text-gray-300">For any <span class="math">\\epsilon&gt;0</span> and any <span class="math">0&lt;\\delta&lt;1</span>, there exists an <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious <span class="math">k</span>-bit stable sorting algorithm that completes in <span class="math">O(kN(\\log\\frac{k}{\\epsilon}+\\log\\log N+\\log\\log\\frac{1}{\\delta}))</span> runtime. As a special case, for <span class="math">\\epsilon=\\Theta(1)</span>, there exists an <span class="math">(\\epsilon,\\mathsf{negl}(N))</span>-differentially oblivious stable <span class="math">1</span>-bit sorting algorithm that completes in <span class="math">O(N\\log\\log N)</span> runtime for some suitable negligible function <span class="math">\\mathsf{negl}(\\cdot)</span>, say, <span class="math">\\mathsf{negl}(N):=\\exp(-\\log^{2}N)</span>.</p>

    <p class="text-gray-300">Note that the above upper bound statement allows for general choices of <span class="math">\\epsilon</span> and <span class="math">\\delta</span>. Interestingly, for the typical parameters <span class="math">\\epsilon=\\Theta(1)</span> and <span class="math">\\delta&lt;1/N</span>, our 1-bit stable sorting algorithm is optimal in light of the following lower bound. We present our lower bound statement for general parameters first, and then highlight several particularly interesting parameter choices and discuss their implications. Note that our lower bound below is applicable even to non-comparison-based sorting:</p>

    <h6 id="sec-15" class="text-base font-medium mt-4">Theorem 1.3 (Limits of <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious stable sorting in the balls-and-bins model).</h6>

    <p class="text-gray-300">For any <span class="math">0&lt;s\\leq\\sqrt{N}</span>, any <span class="math">\\epsilon&gt;0</span>, <span class="math">0&lt;\\beta&lt;1</span> and <span class="math">0\\leq\\delta\\leq\\beta\\cdot\\frac{\\epsilon}{s}\\cdot e^{-2\\epsilon s}</span>, any <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious stable <span class="math">1</span>-bit sorting algorithm in the balls-and-bins model must incur, on some input, at least <span class="math">\\Omega(N\\log s)</span> memory accesses with probability at least <span class="math">1-\\beta</span>.</p>

    <p class="text-gray-300">As a corollary, under the same parameters, any <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious <span class="math">\\Omega(\\log N)</span>-bit-key balls-and-bins sorting algorithm, even a non-stable one, must incur, on some input, at least <span class="math">\\Omega(N\\log s)</span> memory accesses with high probability.</p>

    <p class="text-gray-300">Note that the lower bound allows a tradeoff between <span class="math">\\epsilon</span> and <span class="math">\\delta</span>. For example, if <span class="math">\\epsilon=\\Theta(\\frac{1}{\\sqrt{N}})</span>, then we rule out <span class="math">o(N\\log N)</span> stable 1-bit sorting for even <span class="math">\\delta=\\exp(-\\Omega(\\log^{2}N))</span>.</p>

    <p class="text-gray-300">The case of <span class="math">\\delta=0</span> is more interesting: if <span class="math">\\delta</span> is required to be <span class="math">0</span>, then even when <span class="math">\\epsilon</span> may be arbitrarily large, any <span class="math">\\epsilon</span>-differentially oblivious stable sorting algorithm must suffer from the same lower bounds as oblivious sorting (in the balls-and-bins model)! This is a surprising conclusion because in some sense, very little privacy (or almost no privacy) is attained for large choices of <span class="math">\\epsilon</span> — and yet if <span class="math">\\delta</span> must be <span class="math">0</span>, the same barrier for full obliviousness carries over!</p>

    <p class="text-gray-300">##</p>

    <p class="text-gray-300">7</p>

    <h2 id="sec-16" class="text-2xl font-bold">1.2.2 Merging Two Sorted Lists</h2>

    <p class="text-gray-300">Merging is also a classical abstraction and has been studied extensively in the algorithms literature (e.g., [32]). Merging in the balls-and-bins model is the following task: given two input sorted arrays (by the keys) which together contain <span class="math">N</span> balls, output a merged array containing balls from both input arrays ordered by their keys. Without privacy requirements, clearly merging can be accomplished in <span class="math">O(N)</span> time. Interestingly, Pippenger and Valiant [39] proved that any oblivious algorithm must (in expectation) incur at least <span class="math">\\Omega(N \\log N)</span> ball movements to merge two arrays of length <span class="math">N</span> — even when <span class="math">O(1)</span> correctness or security failure is allowed<span class="math">^{5}</span>.</p>

    <p class="text-gray-300"><strong>Differentially oblivious merging.</strong> Since merging requires that the input arrays be sorted, we clarify the most natural notion of "neighboring": by the most natural definition, two inputs <span class="math">(I_0, I_1)</span> and <span class="math">(I_0&#x27;, I_1&#x27;)</span> are considered neighboring if for each <span class="math">b \\in \\{0, 1\\}</span>, <span class="math">\\mathsf{set}(I_b)</span> and <span class="math">\\mathsf{set}(I_b&#x27;)</span> differ in exactly one record. Given this technical notion of neighboring, differential obliviousness is defined for merging in the same manner as before.</p>

    <p class="text-gray-300">We show similar results for merging as those for 1-bit stable sorting as stated in the following informal theorems.</p>

    <p class="text-gray-300"><strong>Theorem 1.4</strong> (Limits of <span class="math">(\\epsilon, \\delta)</span>-differentially oblivious merging in the balls-and-bins model). For any <span class="math">0 &amp;lt; s \\leq \\sqrt{N}</span>, any <span class="math">\\epsilon &amp;gt; 0</span>, <span class="math">0 &amp;lt; \\beta &amp;lt; 1</span> and <span class="math">0 \\leq \\delta \\leq \\beta \\cdot \\frac{\\epsilon}{s} \\cdot e^{-\\epsilon s}</span>, any <span class="math">(\\epsilon, \\delta)</span>-differentially oblivious merging algorithm in the balls-and-bins model must incur, on some input, at least <span class="math">\\Omega(N \\log s)</span> memory accesses with probability at least <span class="math">1 - \\beta</span>.</p>

    <p class="text-gray-300"><strong>Theorem 1.5</strong> (<span class="math">(\\epsilon, \\delta)</span>-differentially oblivious merging). For any <span class="math">\\epsilon &amp;gt; 0</span> and any <span class="math">0 &amp;lt; \\delta &amp;lt; 1</span>, there exists an <span class="math">(\\epsilon, \\delta)</span>-differentially oblivious merging algorithm that completes in <span class="math">O(N(\\log \\frac{1}{\\epsilon} + \\log \\log N + \\log \\log \\frac{1}{\\delta}))</span> runtime. As a special case, for <span class="math">\\epsilon = \\Theta(1)</span>, there exists an <span class="math">(\\epsilon, \\mathsf{negl}(N))</span>-differentially oblivious merging algorithm that completes in <span class="math">O(N \\log \\log N)</span> runtime for some suitable negligible function <span class="math">\\mathsf{negl}(\\cdot)</span>.</p>

    <p class="text-gray-300">The above theorems are stated for general choices of <span class="math">\\epsilon</span> and <span class="math">\\delta</span>, below we point out several notable special cases:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>First, assuming <span class="math">\\epsilon = \\Theta(1)</span>, if <span class="math">\\delta</span> must be subexponentially small, then the same lower bound for oblivious merging will be preserved for <span class="math">(\\epsilon, \\delta)</span>-differentially oblivious merging.</li>

      <li>Second, for <span class="math">\\epsilon = \\Theta(1)</span> and <span class="math">\\delta</span> negligibly small (but not subexponentially small), we can achieve <span class="math">(\\epsilon, \\delta)</span>-differentially oblivious merging in <span class="math">O(N \\log \\log N)</span> time — yet another example of having privacy "almost-for-free".</li>

      <li>Third, just like the case of 1-bit stable sorting, both our upper and lower bounds are (almost) tight for a wide parameter range that is of interest.</li>

      <li>Finally, when <span class="math">\\delta = 0</span>, surprisingly, the <span class="math">\\Omega(N \\log N)</span> barrier for oblivious merging will be preserved no matter how large <span class="math">\\epsilon</span> is (and how little privacy we get from such a large <span class="math">\\epsilon</span>).</li>

    </ol>

    <p class="text-gray-300"><span class="math">^{5}</span> Pippenger and Valiant’s proof [39] is in fact in a balls-and-bins circuit model, but it is not too difficult, using the access pattern graph approach in our paper, to translate their lower bound to the RAM setting.</p>

    <p class="text-gray-300">1.2.3 Data Structures</p>

    <p class="text-gray-300">Data structures are <em>stateful</em> algorithms, where memory states persist across multiple invocations. Data structures are also of fundamental importance to computer science. We thus investigate the feasibilities and infeasibilities of efficient, differentially oblivious data structures. Our upper bounds work for <em>range query</em> data structures: in such a data structure, one can make insertion and range queries over time, where each insertion specifies a record tagged with a numerical key, and each range query specifies a range and should return all records whose keys fall within the range. Our lower bounds work for <em>point query</em> data structures that are basically the same as range query data structures but each range query must be “equality to a specific key” (note that restricting the queries makes our lower bounds stronger).</p>

    <p class="text-gray-300">The technical definition of differential obliviousness for stateful algorithms is similar to the earlier notion for stateless algorithms. We shall define a <em>static</em> notion and an <em>adaptive</em> notion — the static notion is used in our lower bounds and the adaptive notion is for our upper bounds (this makes both our lower and upper bounds stronger):</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><em>Static notion</em>: here we assume that the adversary commits to an insertion and query sequence upfront;</li>

      <li><em>Adaptive notion</em>: here we assume that the adversary can adaptively choose insertions and range queries over time after having observed previous access pattern to the data structure. Our adaptive notion is equivalent to the standard adaptive DP notion for dynamic datasets <em>[16]</em> except that in our case, the observables are memory access patterns.</li>

    </ul>

    <p class="text-gray-300">We defer the full definitions to the main technical sections. We note that for both the static and adaptive versions, as in the standard DP literature, we assume that the data records are private and need to be protected but the queries are public (in particular the standard DP literature considers the queries as part of the DP mechanisms <em>[16]</em>).</p>

    <h5 id="sec-17" class="text-base font-semibold mt-4">The issue of length leakage and comparison with oblivious data structures.</h5>

    <p class="text-gray-300">Recall for the earlier sorting and merging abstractions, the output length is always fixed (assuming the input length is fixed). For range query data structures, however, an additional issue arises, i.e., the number of records returned can depend on the query and the database itself. Such length disclosure can leak secret information about the data records.</p>

    <p class="text-gray-300">In the earlier line of work on oblivious data structures <em>[30, 37, 47]</em> and ORAM <em>[18, 20, 23, 42, 46]</em>, this length leakage issue is somewhat shoved under the rug. It is understood that to achieve full obliviousness, we need to pad the number of records returned to the maximum possible, i.e., as large as the database size — but this will be prohibitive in practice. Many earlier works that considered oblivious data structures <em>[18, 20, 23, 30, 37, 42, 46, 47]</em> instead allow length leakage to avoid worst-case padding.</p>

    <p class="text-gray-300">In comparison, in some sense our differential obliviousness notion gives a way to reason about such length leakage. By adopting our notion, one can achieve meaningful privacy by adding (small) noise to the output length, and without resorting to worst-case padding that can cause linear blowup.</p>

    <h5 id="sec-18" class="text-base font-semibold mt-4">Upper bound results.</h5>

    <p class="text-gray-300">As mentioned, our upper bounds work for range query data structures that support <em>insertion</em> and <em>range queries</em>. Besides the standard overhead metrics, here we also consider an additional performance metric, that is, <em>locality</em> of the data accesses. Specifically we will use the number of discontiguous memory regions required by each query to characterize the locality of the data structure, a metric frequently adopted by recent works <em>[4, 5, 9]</em>.</p>

    <p class="text-gray-300">As a baseline, without any privacy requirement, such a range query data structure can be realized with a standard binary search tree, where each insertion incurs <span class="math">O(\\log N)</span> time where <span class="math">N</span> is an upper bound on the total records inserted; and each range query can be served in <span class="math">O(\\log N+L)</span> time and accessing only <span class="math">O(\\log N)</span> discontiguous memory regions where <span class="math">L</span> denotes the number of matching records. We show the following results (stated informally).</p>

    <h6 id="sec-19" class="text-base font-medium mt-4">Theorem 1.6 (<span class="math">(\\epsilon,\\delta)</span>-differentially oblivious data structures).</h6>

    <p class="text-gray-300">Suppose that <span class="math">\\epsilon=\\Theta(1)</span> and that <span class="math">\\mathsf{negl}(\\cdot)</span> is a suitable negligible function. There is an <span class="math">(\\epsilon,\\mathsf{negl}(N))</span>-differentially oblivious data structure supporting insertions and range queries, where each of the <span class="math">N</span> insertions incurs amortized <span class="math">O(\\log N\\log\\log N)</span> runtime, and each query costs <span class="math">O(\\mathsf{poly}\\log N+L)</span> runtime where <span class="math">L</span> denotes the number of matching records, and requires accessing only <span class="math">O(\\log N)</span> discontiguous memory regions regardless of <span class="math">L</span>.</p>

    <p class="text-gray-300">The best way to understand our upper bound results is to contrast with oblivious data structures <em>[37, 30, 47]</em> and the non-private baseline:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>We asymptotically outperform known oblivious data structures that have logarithmic (multiplicative) overheads <em>[37, 30, 47]</em> (even when length leakage is permitted). Our algorithms are again “almost-for-free” in comparison with the non-private baseline mentioned earlier for both insertions and for queries that match sufficiently many records, i.e., when <span class="math">L\\geq\\mathsf{poly}\\log N</span>.</li>

      <li>We address the issue of length leakage effectively by adding polylogarithmic noise to the number of matching records; whereas full obliviousness would have required padding to the maximum (and thus incurring linear overhead).</li>

      <li>Our constructions achieve logarithmic locality for range queries whereas almost all known oblivious data structures or ORAM techniques require accessing <span class="math">\\Omega(L)</span> discontiguous regions of memory if the answer is of size <span class="math">L</span>.</li>

      <li>Finally, although not explicitly stated in the above theorem, it will be obvious later that our constructions are also non-interactive when applied to a client-server setting (assuming that the server is capable of performing computation). By contrast, we do not know of any oblivious data structure construction that achieves statistical security and non-interactivity at the same time.</li>

    </ol>

    <p class="text-gray-300">In our detailed technical sections we will also discuss applications of our differentially oblivious data structures in designated-client and public-client settings.</p>

    <h4 id="sec-20" class="text-lg font-semibold mt-6">Lower bounds.</h4>

    <p class="text-gray-300">In the context of data structures, we also prove lower bounds to demonstrate the price of differential obliviousness. As mentioned, for our lower bounds, we consider point queries which is a special case of range queries; further, we consider static rather than adaptive differential obliviousness — these make our lower bound stronger. We prove the following theorem.</p>

    <h6 id="sec-21" class="text-base font-medium mt-4">Theorem 1.7 (Limits of <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious data structures).</h6>

    <p class="text-gray-300">Suppose that <span class="math">N=\\mathsf{poly}(\\lambda)</span> for some fixed polynomial <span class="math">\\mathsf{poly}(\\cdot)</span>. Let the integers <span class="math">r&lt;s\\leq\\sqrt{n}</span> be such that <span class="math">r</span> divides <span class="math">s</span>; furthermore, let <span class="math">\\epsilon&gt;0</span>, <span class="math">0&lt;\\beta&lt;1</span> and <span class="math">0\\leq\\delta\\leq\\beta\\cdot\\frac{\\epsilon}{N}\\cdot e^{-\\epsilon s}</span>. Suppose that <span class="math">\\mathbb{DS}</span> is a perfectly correct and <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious data structure supporting point queries. Then, there exists an operational sequence with <span class="math">N</span> insertion and <span class="math">k:=\\frac{N}{r}</span> query operations interleaved, where each of <span class="math">k</span> distinct keys from the domain <span class="math">\\{0,1,\\ldots,k-1\\}</span> is inserted <span class="math">r</span> times, such that the total number of accesses <span class="math">\\mathbb{DS}</span> makes for serving this sequence is <span class="math">\\Omega(N\\log\\frac{s}{r})</span> with probability at least <span class="math">1-\\beta</span>.</p>

    <p class="text-gray-300">Hence, one immediate observation we can draw is that our earlier range query upper bound (Theorem 1.6) is optimal assuming that the number of records matching the query is at least polylogarithmic in size and assuming the typical parameters <span class="math">\\epsilon=\\Theta(1)</span> and <span class="math">\\delta&lt;1/N</span>. Moreover, the parameters <span class="math">r</span> and <span class="math">k</span> also reflect the intuition that the difficult case should be when the number <span class="math">k</span> of distinct keys is large; in the extreme case when <span class="math">r=s=\\sqrt{N}</span>, we only have the trivial lower bound <span class="math">\\Omega(N)</span>. We defer more detailed technical discussions and proofs to the subsequent formal sections.</p>

    <h3 id="sec-22" class="text-xl font-semibold mt-8">1.3 Closely Related Work</h3>

    <p class="text-gray-300">We are inspired by the recent work of <em>Kellaris et al. [29]</em>. They also consider differential privacy for access patterns for range query databases. In comparison, our work is novel in the following respects:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><em>Kellaris et al. [29]</em> present a computational differential privacy definition for the specific application of statically outsourced databases in a client-server setting.</li>

    </ul>

    <p class="text-gray-300">In comparison, our differential obliviousness is more general and is defined for any (stateless and stateful) algorithms in the RAM model; and for stateful algorithms, we define an adaptive notion of differential obliviousness. Although <em>Kellaris et al. [29]</em> also describe a construction for dynamic databases, they lack formal definitions for this case, and they implicitly assume that the client can store an unbounded amount of data and that metadata operations are for free — in our model where metadata storage and retrieval is no longer for free, their dynamic database scheme would incur on average <span class="math">\\Omega(N)</span> cost per query, where <span class="math">N</span> is the database size.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Second, to support a dynamic range (or point) query database, <em>Kellaris et al. [29]</em>, where they proposed a notion of differentially private ORAM — in their notion, neighboring is defined over the sequence of logical memory requests over time for a generic RAM program. <em>Wagh et al. [15]</em> can rely on composition theorems to support small distances in memory access due to neighboring changes to the input. Their main algorithm changes the way <em>Path ORAM [42]</em> assigns blocks to random paths: they propose to make such assignments using non-uniform distributions to reduce the stash — and thus their approach can only achieve constant-factor savings in comparison with Path ORAM. In comparison, our notion compares the access patterns of a RAM program on neighboring inputs — this notion is more natural but the downside is that the notion makes sense only for databases where entries correspond to individuals, events, or other reasonable privacy units.</li>

    </ul>

    <p class="text-gray-300"><em>Lin et al. [33]</em> recently showed that <span class="math">N</span> balls each tagged with a <span class="math">k</span>-bit key can be obliviously sorted in <span class="math">O(kN\\log\\log N/\\log k)</span> time using non-comparison-based techniques — but their algorithm is not stable, and as Theorem 1.1 explains, this is inevitable for oblivious sort. Our results for sorting small-length keys differentially obliviously match <em>Lin et al. [33]</em> in asymptotical performance (up to <span class="math">\\log\\log</span> factors) but we additionally achieve stability, and thus circumventing known barriers pertaining to oblivious sort.</p>

    <p class="text-gray-300"><em>Mazloom and Gordon [35]</em> introduce a notion of secure multi-party computation allowing differentially private leakage (including differentially-private access pattern leakage). They then show how to design an efficient protocol, under this notion, for graph-parallel computations. At the time of our writing, <em>Mazloom and Gordon [35]</em> had results that achieved constant-factor improvement over the prior work <em>GraphSC [38]</em> that achieved full security. Subsequently, they improved their</p>

    <p class="text-gray-300">result to obtain asymptotical gains (see their latest version online <em>[38]</em>). Although the two papers investigate related notions, the definitions are technically incomparable since theirs focuses on defining security for multi-party computation allowing differentially private leakage (part of which can be access pattern leakage). Their work also considers parallelism in the computation model whereas our paper focuses on a sequential model of computation.</p>

    <h2 id="sec-23" class="text-2xl font-bold">2 Definitions</h2>

    <h3 id="sec-24" class="text-xl font-semibold mt-8">2.1 Model of Computation</h3>

    <p class="text-gray-300">Abstractly, we consider a standard Random-Access-Machine (RAM) model of computation that involves a CPU and a memory. We assume that the memory allows the CPU to perform two types of operations: 1) read a value from a specified physical address; and 2) write a value to a specified physical address. In a cloud outsourcing scenario, one can think of the CPU as a client and the memory as the server (which provides only storage but no computation); therefore, in the remainder of the paper, we often refer to the CPU as the client and the memory as the server.</p>

    <p class="text-gray-300">A (possibly stateful) program in the RAM model makes a sequence of memory accesses during its execution. We define a (possibly stateful) program’s access patterns to include the ordered sequence of physical addresses accessed by the program as well as whether each access is a read or write operation.</p>

    <h4 id="sec-25" class="text-lg font-semibold mt-6">2.1.1 Algorithms in the Balls-and-Bins Model</h4>

    <p class="text-gray-300">In this paper, we consider a set of classical algorithms and data structures in the balls-and-bins model (note that data structures are stateful algorithms.) The inputs to the (possibly stateful) algorithm consist of a sequence of balls each tagged with a key. Throughout the paper, we assume that arbitrary computation can be performed on the keys, but the balls are opaque and can only be moved around. Each ball tagged with its key is often referred to as an element or a record whenever convenient. For example, a record can represent a patient’s medical record or an event collected by a temperature sensor.</p>

    <p class="text-gray-300">Unless otherwise noted, we assume that the RAM’s word size is large enough to store its own address as well as a record (including the ball and its key). Sometimes when we present our algorithms, we may assume that the RAM can operate on real numbers and sample from certain distributions at unit cost — but in all cases these assumptions can eventually be removed and we can simulate real number arithmetic on a finite-word-width RAM preserving the same asymptotic performance (and absorbing the loss in precision into the <span class="math">\\delta</span> term of <span class="math">(\\epsilon,\\delta)</span>-differential obliviousness). We defer discussions on simulating real arithmetic on a finite-word-width RAM to the Appendices.</p>

    <h4 id="sec-26" class="text-lg font-semibold mt-6">2.1.2 Additional Assumptions</h4>

    <p class="text-gray-300">We make the following additional assumptions:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>We consider possibly randomized RAM programs — we assume that whenever needed, the CPU has access to private random coins that are unobservable by the adversary. Throughout the</li>

    </ul>

    <p class="text-gray-300">paper, unless otherwise noted, for any randomized algorithm we require perfect correctness<span class="math">^7</span>.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Henceforth in this paper, we assume that the CPU can store <span class="math">O(1)</span> records in its private cache.</li>

    </ul>

    <h2 id="sec-27" class="text-2xl font-bold">2.2 Differentially Oblivious Algorithms and Oblivious Algorithms</h2>

    <p class="text-gray-300">We first define differential obliviousness for stateless algorithms. Suppose that <span class="math">M(\\lambda, I)</span> is a stateless algorithm expressed as a RAM program. Further, <span class="math">M</span> takes in two inputs, a security parameter <span class="math">\\lambda</span> and an input array (or database) denoted <span class="math">I</span>. We say that two input arrays <span class="math">I</span> and <span class="math">I&#x27;</span> are neighboring iff they are of the same length and differ in exactly one entry.</p>

    <p class="text-gray-300"><strong>Definition 2.1 (Differentially oblivious (stateless) algorithms).</strong> Let <span class="math">\\epsilon, \\delta</span> be functions in a security parameter <span class="math">\\lambda</span>. We say that the stateless algorithm <span class="math">M</span> satisfies <span class="math">(\\epsilon, \\delta)</span>-differential obliviousness, iff for any neighboring inputs <span class="math">I</span> and <span class="math">I&#x27;</span>, for any <span class="math">\\lambda</span>, for any set <span class="math">S</span> of access patterns, it holds that</p>

    <div class="my-4 text-center"><span class="math-block">\\Pr[\\mathbf{Accesses}^M(\\lambda, I) \\in S] \\leq e^{\\epsilon(\\lambda)} \\cdot \\Pr[\\mathbf{Accesses}^M(\\lambda, I&#x27;) \\in S] + \\delta(\\lambda)</span></div>

    <p class="text-gray-300">where <span class="math">\\mathbf{Accesses}^M(\\lambda, I)</span> is a random variable denoting the ordered sequence of memory accesses the algorithm <span class="math">M</span> makes upon receiving the input <span class="math">\\lambda</span> and <span class="math">I</span>.</p>

    <p class="text-gray-300">In the above, the term <span class="math">\\delta</span> behaves somewhat like a failure probability, i.e., the probability of privacy failure for any individual's record or any event. For our upper bounds subsequently, we typically would like <span class="math">\\delta</span> to be a negligible function in the security parameter <span class="math">\\lambda</span>, i.e., every individual can rest assured that as long as <span class="math">\\lambda</span> is sufficiently large, its own privacy is unlikely to be harmed. On the other hand, we would like <span class="math">\\epsilon</span> not to grow w.r.t. <span class="math">\\lambda</span>, and thus a desirable choice for <span class="math">\\epsilon</span> is <span class="math">\\epsilon(\\lambda) = O(1)</span> — e.g., we may want that <span class="math">\\epsilon = 1</span> or <span class="math">\\epsilon = \\frac{1}{\\log\\lambda}</span>.</p>

    <p class="text-gray-300">We also present the classical notion of oblivious algorithms since we will later be concerned about showing separations between differential obliviousness and classical obliviousness.</p>

    <p class="text-gray-300"><strong>Definition 2.2 (Oblivious (stateless) algorithms).</strong> We say that the stateless algorithm <span class="math">M</span> satisfies <span class="math">\\delta</span>-statistical obliviousness, iff for any inputs <span class="math">I</span> and <span class="math">I&#x27;</span> of equal length, for any <span class="math">\\lambda</span>, it holds that <span class="math">\\mathbf{Accesses}^M(\\lambda, I) \\stackrel{\\delta(\\lambda)}{\\equiv} \\mathbf{Accesses}^M(\\lambda, I&#x27;)</span> where <span class="math">\\stackrel{\\delta(\\lambda)}{\\equiv}</span> denotes that the two distributions have at most <span class="math">\\delta(\\lambda)</span> statistical distance. For the <span class="math">\\delta = 0</span> special case, we say that <span class="math">M</span> is perfectly oblivious.</p>

    <p class="text-gray-300">It is not hard to see that if an algorithm <span class="math">M</span> is <span class="math">\\delta</span>-statistically oblivious, it must also be <span class="math">(\\epsilon, \\delta)</span>-differentially oblivious. In other words, <span class="math">(\\epsilon, \\delta)</span>-differentially obliviousness is a strict relaxation of <span class="math">\\delta</span>-statistical obliviousness. Technically speaking, the relaxation comes from two aspects: 1) differential obliviousness requires that the access patterns be close in distribution only for neighboring inputs; and the access patterns for inputs that are dissimilar are allowed to be more dissimilar too; and 2) differential obliviousness additionally allows the access pattern distributions induced by neighboring inputs to differ by an <span class="math">e^{\\epsilon}</span> multiplicative factor.</p>

    <p class="text-gray-300"><strong>Definitions for stateful algorithms.</strong> So far, our definitions for differential obliviousness and obliviousness focus on stateless algorithms. Later in our paper, we will also be interested in differentially oblivious data structures. Data structures are stateful algorithms where memory states persist in between multiple invocations. The definition of differential obliviousness is somewhat</p>

    <p class="text-gray-300"><span class="math">^7</span>Jumping ahead, given an <span class="math">(\\epsilon, \\delta)</span>-differentially oblivious algorithm that incurs <span class="math">\\delta&#x27;</span> correctness error, as long as the algorithm can detect its own error during computation, it can be converted into an algorithm that is perfectly correct and <span class="math">(\\epsilon, \\delta + \\delta&#x27;)</span>-differentially oblivious: specifically, if an error is encountered, the algorithm simply computes and outputs a non-private answer.</p>

    <p class="text-gray-300">12</p>

    <p class="text-gray-300">more subtle for data structures, especially when the adversary can adaptively choose the entries to insert into the data structure, and adaptively choose the queries as well. For readability, we defer defining differentially oblivious data structures (i.e., stateful algorithms) to later technical sections.</p>

    <h2 id="sec-28" class="text-2xl font-bold">3 Differentially Oblivious Sorting: Upper Bounds</h2>

    <p class="text-gray-300">We consider sorting in the balls-and-bins model: given an input array containing <span class="math">N</span> opaque balls each tagged with a key from a known domain <span class="math">[K]</span>, output an array that is a permutation of the input such that all balls are ordered by their keys. If the sorting algorithm relies only on comparisons of keys, it is said to be comparison-based. Otherwise, if the algorithm is allowed to perform arbitrary computations on the keys, it is said to be non-comparison-based.</p>

    <p class="text-gray-300">As is well-known, comparison-based sorting must suffer from <span class="math">\\Omega(N\\log N)</span> runtime (even without privacy requirements) and there are matching <span class="math">O(N\\log N)</span> oblivious sorting algorithms <em>[1, 21]</em>. On the other hand, non-private, non-comparison-based sorting algorithms can sort <span class="math">N</span> elements (having keys in a universe of cardinality <span class="math">O(N)</span>) in linear time (e.g., counting sort).</p>

    <p class="text-gray-300">In this section, we will show that for certain cases of sorting, the notions of differential obliviousness and obliviousness result in a separation in performance.</p>

    <h3 id="sec-29" class="text-xl font-semibold mt-8">3.1 Stably Sorting <span class="math">1</span>-Bit Keys</h3>

    <p class="text-gray-300">We start with stably sorting <span class="math">1</span>-bit keys and later extend to more bits. Stable <span class="math">1</span>-bit-key sorting is the following problem: given an input array containing <span class="math">N</span> balls each tagged with a key from <span class="math">\\{0,1\\}</span>, output a stably sorted permutation of the input array. Specifically, stability requires that if two balls have the same key, their relative ordering in the output must respect their ordering in the input.</p>

    <p class="text-gray-300">We choose to start with this special case because interestingly, stable <span class="math">1</span>-bit-key sorting in the balls-and-bins model has a <span class="math">\\Omega(N\\log N)</span> lower bound due to the recent work by Lin, Shi, and Xie <em>[33]</em> — and the lower bound holds even for non-comparison-based sorting algorithms that can perform arbitrary computation on keys. More specifically, they showed that for any constant <span class="math">0&lt;\\delta&lt;1</span> any <span class="math">\\delta</span>-oblivious stable <span class="math">1</span>-bit-key sorting algorithm must in expectation perform at least <span class="math">\\Omega(N\\log N)</span> ball movements.</p>

    <p class="text-gray-300">In this section, we will show that by adopting our more relaxed differential obliviousness notion, we can circumvent the lower bound for oblivious <span class="math">1</span>-bit-key stable (balls-and-bins) sorting. Specifically, for a suitable negligible function <span class="math">\\delta</span> and for <span class="math">\\epsilon=\\Theta(1)</span>, we can accomplish <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious <span class="math">1</span>-bit-key stable sorting in <span class="math">O(N\\log\\log N)</span> time. Unsurprisingly, our algorithm is non-comparison-based, since due to the <span class="math">0</span>-<span class="math">1</span> principle, any comparison-based sorting algorithm, even for <span class="math">1</span>-bit keys, must make at least <span class="math">\\Omega(N\\log N)</span> comparisons.</p>

    <h4 id="sec-30" class="text-lg font-semibold mt-6">3.1.1 A Closely Related Abstraction: Tight Stable Compaction</h4>

    <p class="text-gray-300">Instead of constructing stable <span class="math">1</span>-bit-key sorting algorithm directly, we first construct a tight stable compaction algorithm: given some input array, tight stable compaction outputs an array containing only the <span class="math">1</span>-balls contained in the input, padded with dummies to the input array’s size. Further, we require that the relative order of appearance of the <span class="math">1</span>-balls in the output respect the order in the input.</p>

    <p class="text-gray-300">Given a tight stable compaction algorithm running in time <span class="math">t(N)</span>, we can easily realize a stable <span class="math">1</span>-bit-key sorting algorithm that completes in time <span class="math">O(t(N)+N)</span> in the following way:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Run tight stable compaction to stably move all 0-balls to the front of an output array — let <span class="math">X</span> be the resulting array;</li>

      <li>Run tight stable compaction to stably move all 1-balls to the end of an output array — let <span class="math">Y</span> be the resulting array (note that this can be done by running tight stable compaction on the reversed input array, and then reversing the result again);</li>

      <li>In one synchronized scan of <span class="math">X</span> and <span class="math">Y</span>, select an element at each position from either <span class="math">X</span> or <span class="math">Y</span> and write it into an output array.</li>

    </ol>

    <p class="text-gray-300">Moreover, if each instance of tight stable compaction is <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious, then the resulting 1-bit-key stable sorting algorithm is <span class="math">(2\\epsilon,2\\delta)</span>-differentially oblivious.</p>

    <h4 id="sec-31" class="text-lg font-semibold mt-6">3.1.2 Intuition</h4>

    <p class="text-gray-300">Absent privacy requirements, clearly tight stable compaction can be accomplished in linear time, by making one scan of the input array, and writing a ball out whenever a real element (i.e., the 1-balls) is encountered. In this algorithm, there are two pointers pointing to the input array and the output array respectively. Observing how fast these pointers advance allows the adversary to gain sensitive information about the input, specifically, whether each element is real or dummy. Our main idea is to approximately simulate this non-private algorithm, but obfuscate how fast each pointer advances just enough to obtain differential obliviousness. To achieve this we need to combine oblivious algorithms building blocks and differential privacy mechanisms.</p>

    <p class="text-gray-300">First, we rely on batching: we repeatedly read a small batch of <span class="math">s</span> elements into a working buffer (of size <span class="math">O(s)</span>), obliviously sort the buffer to move all dummies to the end, and then emit some number of elements into the output. Note that the pointers to the input and output array could still reveal information about the number of non-dummy elements in the batchs read so far. Thus, the challenge is to determine how many elements must be output when the input scan reaches position <span class="math">i</span>. Now, suppose that we have a building block that allows us to differentially privately estimate how many real elements have been encountered till position <span class="math">i</span> in the input for every such <span class="math">i</span> — earlier works on differentially private mechanisms have shown how to achieve this <em>[11, 12, 15]</em>. For example, suppose we know that the number of real elements till position <span class="math">i</span> is in between <span class="math">[C_{i}-s,C_{i}+s]</span> with high probability, then our algorithm will know to output exactly <span class="math">C_{i}-s</span> elements when the input array’s pointer reaches position <span class="math">i</span>. Furthermore, at this moment, at most <span class="math">2s</span> real elements will have been scanned but have not been output — and these elements will remain in the working buffer. We can now rely on oblivious sorting again to truncate the working buffer and remove dummies, such that the working buffer’s size will never grow too large — note that this is important since otherwise obliviously sorting the working buffer will become too expensive. Below we elaborate on how to make this idea fully work.</p>

    <h4 id="sec-32" class="text-lg font-semibold mt-6">3.1.3 Preliminary: Differentially Private Prefix Sum</h4>

    <p class="text-gray-300">Dwork et al. <em>[15]</em> and Chan et al. <em>[11, 12]</em> proposed a differentially private algorithm for computing all <span class="math">N</span> prefix sums of an input stream containing <span class="math">N</span> elements where each element is from <span class="math">\\{0,1\\}</span>. In our setting, we will need to group the inputs into bins and then adapt their prefix sum algorithm to work on the granularity of bins.</p>

    <h6 id="sec-33" class="text-base font-medium mt-4">Theorem 3.1 (Differentially private prefix sum <em>[11, 12]</em>).</h6>

    <p class="text-gray-300">For any <span class="math">\\epsilon,\\delta&gt;0</span>, there exists an <span class="math">(\\epsilon,\\delta)</span>-differentially private algorithm, such that given a stream in <span class="math">\\mathbb{Z}_{+}^{N}</span> (where neigboring streams differs in at most one position with difference at most <span class="math">1</span>), the algorithm outputs the vector of all <span class="math">N</span> prefix sums, such that</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Any prefix sum that is outputted by the algorithm has only <span class="math">O(\\frac{1}{\\epsilon}\\cdot(\\log N)^{1.5}\\cdot\\log\\frac{1}{\\delta})</span> additive error (with probability <span class="math">1</span>).</li>

      <li>The algorithm is oblivious and completes in <span class="math">O(N)</span> runtime.</li>

    </ul>

    <p class="text-gray-300">We remark that the original results in <em>[11, 12]</em> is an <span class="math">(\\epsilon,0)</span>-differentially private algorithm such that the outputted prefix sum has at most <span class="math">O(\\frac{1}{\\epsilon}\\cdot(\\log N)^{1.5}\\cdot\\log\\frac{1}{\\delta})</span> addititive error with probability at least <span class="math">1-\\delta</span>, which clearly implies the above theorem (by just outputting the non-private prefix-sum when the error in the output is too large). We choose to state Theorem 3.1 since bounded error is needed for our differentially oblivious algorithms to achieve perfect correctness.</p>

    <h4 id="sec-34" class="text-lg font-semibold mt-6">3.1.4 Detailed Algorithm</h4>

    <p class="text-gray-300">We first describe a tight stable compaction algorithm that stably compacts an input array <span class="math">I</span> given a privacy parameter <span class="math">\\epsilon</span> and a batch size <span class="math">s</span>.</p>

    <p class="text-gray-300">TightStableCompact(<span class="math">I,\\epsilon,s</span>):</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Invoke an instance of the differentially private prefix sum algorithm with the privacy budget <span class="math">\\epsilon</span> to estimate for every <span class="math">i\\in[N]</span>, the total number of <span class="math">1</span>-balls in the input stream <span class="math">I</span> up till position <span class="math">i</span> — henceforth we use the notation <span class="math">\\widehat{Y}_{i}</span> to denote the <span class="math">i</span>-th prefix sum estimated by the differentially private prefix sum algorithm.</li>

      <li>Imagine there is a working buffer initialized to be empty. We now repeat the following until there are no more bins left in the input.</li>

    </ul>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Fetch the next <span class="math">s</span> balls from the input stream into the working buffer.</li>

      <li>Obliviously sort the working buffer such that all <span class="math">1</span>-balls are moved to the front, and all <span class="math">0</span>-balls moved to the end; we use the ball’s index in the input array to break ties for stability.</li>

      <li>Suppose that <span class="math">k</span> balls from the input have been operated on so far. If there are fewer than <span class="math">\\widehat{Y}_{k}-s</span> balls in the output array, pop the head of the working buffer and append to the output array until there are <span class="math">\\widehat{Y}_{k}-s</span> balls in the output array.</li>

      <li>If the working buffer (after popping) is longer than <span class="math">2s</span>, truncate from the end such that the working buffer is of size <span class="math">2s</span>.</li>

    </ol>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Finally, at the end, if the output is shorter than <span class="math">N</span>, then obliviously sort the working buffer (using the same relative ordering function as before) and write an appropriate number of balls from the head into the output such that the output buffer is of length <span class="math">N</span>.</li>

    </ul>

    <p class="text-gray-300">Finally, as mentioned, we can construct stable <span class="math">1</span>-bit-key sorting by running two instances of tight stable compaction and then in <span class="math">O(N)</span> time combining the two output arrays into the final outcome.</p>

    <h6 id="sec-35" class="text-base font-medium mt-4">Theorem 3.2 (Tight stable compaction).</h6>

    <p class="text-gray-300">For any <span class="math">\\epsilon,\\delta&gt;0</span>, for any input array <span class="math">I</span> containing <span class="math">N</span> elements, let <span class="math">s=\\Theta(\\frac{1}{\\epsilon}\\cdot\\log^{1.5}N\\cdot\\log\\frac{1}{\\delta})</span>, then the algorithm TightStableCompact(<span class="math">I,\\epsilon,s</span>) satisfies <span class="math">(\\epsilon,\\delta)</span>-differential obliviousness and perfect correctness. Further, the algorithm completes in <span class="math">O(N\\log s)</span> runtime. As a special case, for any <span class="math">\\epsilon=\\Theta(1)</span> and <span class="math">s=\\log^{3}N</span>, the algorithm satisfies <span class="math">(\\epsilon,\\delta)</span>-differential obliviousness with perfect correctness and negligible <span class="math">\\delta</span>.</p>

    <p class="text-gray-300">##</p>

    <p class="text-gray-300">Proof.</p>

    <p class="text-gray-300">Notice that the access patterns of the algorithm is uniquely determined by the set of prefix sums computed. Thus it suffices to prove that the set of prefix sums resulting from the prefix sum algorithm satisfies <span class="math">(\\epsilon,\\delta)</span>-differential privacy. This follows in a straightforward manner from Theorem 3.1. Perfect correctness of the algorithm is guaranteed since the prefix sum has at most <span class="math">s</span> additive error, thus perfect correctness also follows from Theorem 3.1. The runtime of the algorithm is dominated by <span class="math">O(N/s)</span> number of oblivious sortings of the working buffer whose size, by construction, is at most <span class="math">O(s)</span>. Thus the runtime claims follows naturally. ∎</p>

    <h6 id="sec-36" class="text-base font-medium mt-4">Corollary 3.3 (Stable 1-bit sorting).</h6>

    <p class="text-gray-300">For any <span class="math">\\epsilon&gt;0</span> and any <span class="math">0&lt;\\delta&lt;1</span>, there exists an <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious algorithm such that for any input array with <span class="math">N</span> balls each tagged with a <span class="math">1</span>-bit key, the algorithm completes in <span class="math">O(N\\log(\\frac{1}{\\epsilon}\\log^{1.5}N\\log\\frac{1}{\\delta}))</span> runtime and stably sorts the balls with perfect correctness. As a special case, for <span class="math">\\epsilon=\\Theta(1)</span>, there exists an <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious stable <span class="math">1</span>-bit sorting algorithm such that it completes in <span class="math">O(N\\log\\log N)</span> runtime and has negligible <span class="math">\\delta</span>.</p>

    <h6 id="sec-37" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">As mentioned, we can construct stable 1-bit sorting by running two instances of tight stable compaction and then in <span class="math">O(N)</span> time combining the two output arrays into the final outcome. Thus the corollary follows in a straightforward fashion from Theorem 3.2. ∎</p>

    <h4 id="sec-38" class="text-lg font-semibold mt-6">Optimality.</h4>

    <p class="text-gray-300">In light of our lower bound to be presented in the next section (Theorem 4.7), our 1-bit-key stable sorting algorithm is in fact optimal for the typical parameters <span class="math">\\epsilon=\\Theta(1)</span> and <span class="math">\\delta&lt;1/N</span> — note that this includes most parameter ranges one might care about. For the special case of <span class="math">\\epsilon=\\Theta(1)</span>, our upper bound is <span class="math">\\widetilde{O}(N)</span> runtime for <span class="math">\\delta=e^{-\\mathsf{poly}\\log N}</span> and <span class="math">\\widetilde{O}(N\\log N)</span> runtime for <span class="math">\\delta=e^{-N^{0.1}}</span> where <span class="math">\\widetilde{O}</span> hides a <span class="math">\\log\\log</span> factor — both cases match our lower bound.</p>

    <h3 id="sec-39" class="text-xl font-semibold mt-8">3.2 Sorting More Bits</h3>

    <p class="text-gray-300">Given an algorithm for stably sorting 1-bit keys, we can easily derive an algorithm for stably sorting <span class="math">k</span>-bit keys simply using the well-known approach of Radix Sort: we sort the input bit by bit starting from the lowest-order bit. Clearly, if the stable 1-bit-key sorting building block satisfies <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious, then resulting <span class="math">k</span>-bit-key stable sorting algorithm satisfies <span class="math">(k\\epsilon,k\\delta)</span>-differentially oblivious. This gives rise to the following corollary.</p>

    <h6 id="sec-40" class="text-base font-medium mt-4">Corollary 3.4 (Stable <span class="math">k</span>-bit-key sorting).</h6>

    <p class="text-gray-300">For any <span class="math">\\epsilon,\\delta&gt;0</span>, there exists an <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious algorithm such that for any input array with <span class="math">N</span> balls each tagged with a <span class="math">k</span>-bit key, the algorithm completes in <span class="math">O(kN\\log(\\frac{k}{\\epsilon}\\log^{1.5}N\\log\\frac{1}{k\\delta}))</span> runtime and stably sorts the balls with perfect correctness.</p>

    <p class="text-gray-300">As a special case, for <span class="math">\\epsilon=\\Theta(1)</span>, there exists an <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious stable <span class="math">k</span>-bit-key sorting algorithm that completes in <span class="math">O(kN\\log\\log N)</span> runtime and has negligible <span class="math">\\delta</span>.</p>

    <p class="text-gray-300">We point out that if <span class="math">k=o(\\log N/\\log\\log N)</span>, we obtain a stable <span class="math">k</span>-bit-key sorting algorithm that overcomes the <span class="math">\\Omega(N\\log N)</span> barrier for stable <span class="math">\\delta</span>-oblivious sort in the balls-and-bins model — recall that Lin, Shi, and Xie <em>[33]</em> show that for even <span class="math">\\delta=O(1)</span>, any (possibly non-comparison-based) stable 1-bit-key <span class="math">\\delta</span>-oblivious sorting algorithm in the balls-and-bins model must incur <span class="math">\\Omega(N\\log N)</span> runtime. We stress that our algorithm is non-comparison-based, since otherwise due to the 0-1 principle, any comparison-based sorting algorithm — even without privacy requirements and even for 1-bit keys — must incur at least <span class="math">\\Omega(N\\log N)</span> runtime.</p>

    <p class="text-gray-300">###</p>

    <p class="text-gray-300">4 Limits of Differentially Oblivious Sorting</p>

    <p class="text-gray-300">We showed earlier that for a suitably and negligibly small <span class="math">\\delta</span> and <span class="math">\\epsilon=\\Theta(1)</span>, by adopting the weaker notion of <span class="math">(\\epsilon,\\delta)</span>-differential obliviousness, we can overcome the <span class="math">\\Omega(N\\log N)</span> barrier for oblivious stable sorting for small keys (in the balls-and-bins model). In this section, we show that if <span class="math">\\delta</span> must be subexponentially small (including the special case of requiring <span class="math">\\delta=0</span>), then <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious 1-bit stable sorting would suffer from the same lower bound as the oblivious case. Without loss of generality, we may assume that <em>the CPU has a single register</em> and can store a single record (containing a ball and an associated key) and its address — since any <span class="math">O(1)</span> number of registers can be simulated by a trivial ORAM with <span class="math">O(1)</span> blowup.</p>

    <h3 id="sec-41" class="text-xl font-semibold mt-8">4.1 Definitions and Preliminaries</h3>

    <p class="text-gray-300">We begin by presenting some new notions and preliminaries that are necessary for our lower bound.</p>

    <h4 id="sec-42" class="text-lg font-semibold mt-6">4.1.1 Plausibility of Access Patterns among Neighboring Inputs</h4>

    <p class="text-gray-300">In order to derive our lower bounds for differentially oblivious sorting, merging, and data structures, we show that for a differentially oblivious algorithm, with high probability, the access pattern produced for some input <span class="math">I</span> is “plausible” for many inputs that are “close” to <span class="math">I</span>.</p>

    <h6 id="sec-43" class="text-base font-medium mt-4">Definition 4.1 (<span class="math">r</span>-neighbors).</h6>

    <p class="text-gray-300">Two inputs are <span class="math">r</span>-neighboring, if they differ in at most <span class="math">r</span> positions.</p>

    <h6 id="sec-44" class="text-base font-medium mt-4">Definition 4.2 (Plausible access pattern).</h6>

    <p class="text-gray-300">An access pattern <span class="math">A</span> produced by a mechanism <span class="math">M</span> is plausible for an input <span class="math">I</span>, if <span class="math">\\Pr[\\mathbf{Accesses}^{M}(\\lambda,I)=A]&gt;0</span>; if <span class="math">\\Pr[\\mathbf{Accesses}^{M}(\\lambda,I)=A]=0</span>, we say that <span class="math">A</span> is implausible for <span class="math">I</span>.</p>

    <h6 id="sec-45" class="text-base font-medium mt-4">Lemma 4.3.</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Suppose <span class="math">I_{0}</span> is some input for a mechanism <span class="math">M</span> that is <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious, and <span class="math">\\mathcal{C}</span> is a collection of inputs that are <span class="math">r</span>-neighbors of <span class="math">I_{0}</span>. Then, the probability that <span class="math">\\mathbf{Accesses}^{M}(\\lambda,I_{0})</span> is plausible for all inputs in <span class="math">\\mathcal{C}</span> is at least <span class="math">1-\\eta</span>, where $\\eta:=</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathcal{C}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\cdot\\frac{e^{i\\tau}-1}{e^{\\epsilon}-1}\\cdot\\delta$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h6 id="sec-46" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">The proof is deferred to the Appendices (Section C.1). ∎</p>

    <h4 id="sec-47" class="text-lg font-semibold mt-6">4.1.2 Access Pattern Graphs under the Balls-and-Bins Model</h4>

    <p class="text-gray-300">Recall that we assume a balls-and-bins model and without loss of generality we may assume that the CPU has a single register and can store a single ball and its key.</p>

    <h5 id="sec-48" class="text-base font-semibold mt-4">Access pattern graph.</h5>

    <p class="text-gray-300">We model consecutive <span class="math">t</span> memory accesses by an access pattern graph defined as follows. Let <span class="math">N</span> index the CPU register together with the memory locations accessed by the CPU in those <span class="math">t</span> accesses. The <span class="math">t</span> memory accesses are represented by <span class="math">t+1</span> layers of nodes, where the layers are indexed from <span class="math">i=0</span> to <span class="math">t</span>. The nodes and edges of the access pattern graph are defined precisely as follows.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[label=()]</li>

      <li><em>Nodes.</em> For each <span class="math">0\\leq i\\leq t</span>, layer <span class="math">i</span> consists of nodes of the form <span class="math">(i,u)</span>, where <span class="math">u\\in N</span> represents either the CPU or a memory location. Intuitively, the node <span class="math">(i,u)</span> represents the opaque ball stored at <span class="math">u</span> after the <span class="math">i</span>-th memory access.</li>

      <li><em>Edges.</em> Each edge is directed and points from a node in layer <span class="math">i-1</span> to one in layer <span class="math">i</span> for some <span class="math">i\\geq 1</span>. For <span class="math">u\\in N</span>, there is a directed edge from its copy <span class="math">(i-1,u)</span> in layer <span class="math">i-1</span> to <span class="math">(i,u)</span> in layer <span class="math">i</span>. This reflects the observation that if a ball is stored at <span class="math">u</span> before the <span class="math">i</span>-th access, then it is plausible that the same ball is still stored at <span class="math">u</span> after the <span class="math">i</span>-th access.</li>

    </ol>

    <p class="text-gray-300">Suppose the CPU accesses memory location <span class="math">\\ell</span> in the <span class="math">i</span>-th access. Then, we add two directed edges <span class="math">((i-1,CPU),(i,\\ell))</span> and <span class="math">((i-1,\\ell),(i,CPU))</span>. This reflects the balls stored in the CPU and location <span class="math">\\ell</span> can possibly move between those two places.</p>

    <h4 id="sec-49" class="text-lg font-semibold mt-6">Compact access pattern graph (compact graph).</h4>

    <p class="text-gray-300">Observe that in each layer <span class="math">i</span>, any node that corresponds to a location not involved in the <span class="math">i</span>-th access has in-degree and out-degree being 1. Whenever there is such a node <span class="math">x</span> with the in-coming edge <span class="math">(u,x)</span> and the out-going edge <span class="math">(x,v)</span>, we remove the node <span class="math">x</span> and add the directed edge <span class="math">(u,v)</span>. This is repeated until there is no node with both in-degree and out-degree being 1. We call the resulting graph the <em>compact access pattern graph</em>, or simply the <em>compact graph</em>. The following lemma relates the number of memory accesses to the number of edges in the compact graph.</p>

    <h6 id="sec-50" class="text-base font-medium mt-4">Lemma 4.4 (Number of edges in a compact graph).</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Suppose <span class="math">N</span> is the set indexing the CPU together with the memory location accessed by the CPU in consecutive <span class="math">t</span> accesses. Then, the compact graph corresponding to these <span class="math">t</span> accesses has $4t+</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">N</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">-2\\leq 5t$ edges.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h6 id="sec-51" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">The proof is deferred to the Appendices (Section C.1). ∎</p>

    <h4 id="sec-52" class="text-lg font-semibold mt-6">4.1.3 Preliminaries on Routing Graph Complexity</h4>

    <p class="text-gray-300">We consider a routing graph. Let <span class="math">I</span> and <span class="math">O</span> denote a set of <span class="math">n</span> input nodes and <span class="math">m\\geq n</span> output nodes respectively. We say that <span class="math">A</span> is an assignment from <span class="math">I</span> to <span class="math">O</span> if <span class="math">A</span> is an injection from nodes in <span class="math">I</span> to nodes <span class="math">O</span>. A routing graph <span class="math">G</span> is a directed graph, and we say that <span class="math">G</span> implements the assignment <span class="math">A</span> if there exist <span class="math">n</span> <em>vertex-disjoint</em> paths from <span class="math">I</span> to <span class="math">O</span> respecting the assignment <span class="math">A</span>.</p>

    <p class="text-gray-300">Let <span class="math">\\mathbf{A}:=(A_{1},A_{2},\\ldots,A_{s})</span> denote a set of assignments from <span class="math">I</span> to <span class="math">O</span>. We say <span class="math">\\mathbf{A}</span> is non-overlapping if for every input <span class="math">x\\in I</span>, the assignements map <span class="math">x</span> to distinct outputs, i.e., <span class="math">A_{i}(x)\\neq A_{j}(x)</span> for every <span class="math">i\\neq j\\in[s]</span>. Pippenger and Valiant proved the following useful result <em>[39]</em>.</p>

    <h6 id="sec-53" class="text-base font-medium mt-4">Fact 4.5 (Pippenger and Valiant <em>[39]</em>).</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Let <span class="math">\\mathbf{A}:=(A_{1},A_{2},\\ldots,A_{s})</span> denote a set of assignments from <span class="math">I</span> to <span class="math">O</span> where $n=</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">I</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\leq</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">O</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> Let </span>G<span class="math"> be a graph that implements every </span>A_{i}<span class="math"> for </span>i\\in[s]<span class="math">. If </span>\\mathbf{A}<span class="math"> is non-overlapping, then the number of edges in </span>G<span class="math"> must be at least </span>3n\\log_{3}s$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">In our lower bound proofs, we shall make use of Fact 4.5 together with Lemma 4.4 to show that the number of memory location accesses is large in each relevant scenario. A useful set of non-overlapping assignments are shift assuments, defined as follows.</p>

    <h6 id="sec-54" class="text-base font-medium mt-4">Definition 4.6 (Shift assignment).</h6>

    <p class="text-gray-300">We say that <span class="math">A</span> is a shift assignment for the input nodes <span class="math">I=\\{x_{0},x_{1},\\ldots,x_{n-1}\\}</span> and output nodes <span class="math">O=\\{y_{0},y_{1},\\ldots,y_{n-1}\\}</span> iff there is some <span class="math">s</span> such that for any <span class="math">i\\in\\{0,1,\\ldots,n-1\\}</span>, <span class="math">x_{i}</span> is mapped to <span class="math">y_{j}</span> where <span class="math">j=(i+s)\\mod n</span> — we also refer to <span class="math">s</span> as the shift offset.</p>

    <h3 id="sec-55" class="text-xl font-semibold mt-8">4.2 Lower Bounds for Differentially Oblivious Sorting</h3>

    <h4 id="sec-56" class="text-lg font-semibold mt-6">Warmup and intuition.</h4>

    <p class="text-gray-300">As a warmup, we consider a simple lower bound proof for the case <span class="math">\\delta=0</span> and for general sorting (where the input can contain arbitrary keys not just 1-bit keys). Suppose there is some <span class="math">\\epsilon</span>-differentially oblivious balls-and-bins sorting algorithm denoted sort. Now, given a specific input array <span class="math">I</span>, let <span class="math">G</span> be such a compact graph encountered with non-zero probability <span class="math">p</span>. By the requirement of <span class="math">\\epsilon</span>-differential obliviousness, it must be that for any input array <span class="math">I^{\\prime}</span>, the probability of encountering <span class="math">G</span> must be at least <span class="math">p\\cdot e^{-\\epsilon N}&gt;0</span>. This means <span class="math">G</span> must also be able to explain any other input array <span class="math">I^{\\prime}</span>. In other words, for any input <span class="math">I^{\\prime}</span> there must exist a feasible method</p>

    <p class="text-gray-300">for routing the balls contained in the input <span class="math">I^{\\prime}</span> to their correct location in the output locations in <span class="math">G</span>. Recall that in the compact graph <span class="math">G</span>, every node <span class="math">(t,i)</span> can receive a ball from either of its two incoming edges: either from the parent <span class="math">(t^{\\prime},i)</span> for some <span class="math">t^{\\prime}&lt;t</span>, from the parent <span class="math">(t-1,CPU)</span>. Let <span class="math">T</span> be the total number of nodes in <span class="math">G</span>, by construction, it holds that the number of edges in <span class="math">G=\\Theta(T)</span>. Now due to a single counting argument, since the graph must be able to explain all <span class="math">N!</span> possible input permutations, we have <span class="math">2^{T}\\geq N!</span>. By taking logarithm on both sides, we conclude that <span class="math">T\\geq\\Omega(N\\log N)</span>.</p>

    <p class="text-gray-300">The more interesting question arises for <span class="math">\\delta\\neq 0</span>. We will now prove such a lower bound for <span class="math">\\delta\\neq 0</span>. Instead of directly tackling a general sorting lower bound, we start by considering stably sorting balls with 1-bit keys, where stability requires that any two balls with the same key must appear in the output in the same order as in the input. Note that given any general sorting algorithm, we can realize 1-bit-key stable sorting in a blackbox manner: every ball’s 1-bit key is appended with its index in the input array to break ties, and then we simply sort this array. Clearly, if the general sorting algorithm attains <span class="math">(\\epsilon,\\delta)</span>-differential obliviousness, so does the resulting 1-bit-key stable sorting algorithm. Thus, a lower bound for 1-bit-key stable sorting is stronger than a lower bound for general sorting (parameters being equal).</p>

    <h6 id="sec-57" class="text-base font-medium mt-4">Theorem 4.7 (Limits of differentially oblivious 1-bit-key stable sorting).</h6>

    <p class="text-gray-300">Let <span class="math">0&lt;s\\leq\\sqrt{N}</span> be an integer. Suppose <span class="math">\\epsilon&gt;0</span>, <span class="math">0&lt;\\beta&lt;1</span> and <span class="math">0\\leq\\delta\\leq\\beta\\cdot\\frac{\\epsilon}{s}\\cdot e^{-2\\epsilon s}</span>. Then, any (randomized) stable <span class="math">1</span>-bit-key sorting algorithm (in the balls-and-bins model) that is <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious must have some input, on which it incurs at least <span class="math">\\Omega(N\\log s)</span> memory accesses with probability at least <span class="math">1-\\beta</span>.</p>

    <h6 id="sec-58" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">We assume that the input is given in <span class="math">N</span> specific memory locations <span class="math">\\textsf{Input}[0..N-1]</span>, and the stable sorting algorithm <span class="math">M</span> must write the output in another <span class="math">N</span> specific memory locations <span class="math">\\textsf{Output}[0..N-1]</span>.</p>

    <p class="text-gray-300">For each <span class="math">0\\leq i\\leq s</span>, we define the input scenario <span class="math">I_{i}</span> as follows, such that in each scenario, there are exactly <span class="math">s</span> elements with key value <span class="math">0</span> and <span class="math">N-s</span> elements with key value <span class="math">1</span>. Specifically, in scenario <span class="math">I_{i}</span>, the first <span class="math">s-i</span> and the last <span class="math">i</span> elements in <span class="math">\\textsf{Input}[0..N-1]</span> have key value <span class="math">0</span>, while all other elements have key value <span class="math">1</span>. It can be checked that any two scenarios are <span class="math">2s</span>-neighboring.</p>

    <p class="text-gray-300">Moreover, observe that for <span class="math">0\\leq i\\leq s</span>, in scenario <span class="math">I_{i}</span>, any ball with non-zero key in <span class="math">\\textsf{Input}[j]</span> is supposed to go to <span class="math">\\textsf{Output}[j+i]</span> (where addition <span class="math">j+i</span> is performed modulo <span class="math">N</span>) after the stable sorting algorithm is run.</p>

    <p class="text-gray-300">Observe that a stable sorting algorithm can only guarantee that all the elements with key <span class="math">0</span> will appear at the prefix of <span class="math">\\textsf{Output}</span> according to their original input order. However, after running the stable sorting algorithm, we can use an extra oblivious sorting network on the first <span class="math">s</span> elements to ensure that in the input scenario <span class="math">I_{i}</span>, any element with key <span class="math">0</span> in <span class="math">\\textsf{Input}[j]</span> originally will end up finally at <span class="math">\\textsf{Output}[j+i]</span>. Therefore, the resulting algorithm is still <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious.</p>

    <p class="text-gray-300">Therefore, by Lemma 4.3, with probability at least <span class="math">1-\\eta</span> (where <span class="math">\\eta:=s\\cdot\\frac{e^{\\epsilon\\cdot 2s}-1}{e^{\\epsilon}-1}\\cdot\\delta\\leq\\beta</span>), running the algorithm <span class="math">M</span> on input <span class="math">I_{0}</span> produces an access pattern <span class="math">A</span> that is plausible for <span class="math">I_{i}</span> for all <span class="math">1\\leq i\\leq s</span>. Let <span class="math">G</span> be the compact graph (defined Section 4.1.2) corresponding to <span class="math">A</span>.</p>

    <p class="text-gray-300">Observe that <span class="math">A</span> is plausible for <span class="math">I_{i}</span> implies that <span class="math">G</span> contains <span class="math">N</span> vertex-disjoint paths, where for <span class="math">0\\leq j&lt;N</span>, there is such a path from the node corresponding to the initial memory location <span class="math">\\textsf{Input}[j]</span> to the node corresponding to the final memory location <span class="math">\\textsf{Output}[j+i]</span>.</p>

    <p class="text-gray-300">Then, Fact 4.5 implies that <span class="math">G</span> has at least <span class="math">\\Omega(N\\log s)</span> edges. Hence, Lemma 4.4 implies that the access pattern <span class="math">A</span> makes at least <span class="math">\\Omega(N\\log s)</span> memory accesses. Since our extra sorting network takes at most <span class="math">O(s\\log s)</span> memory accesses, it follows that the original sorting algorithm makes at least <span class="math">\\Omega(N\\log s)</span> accesses. ∎</p>

    <p class="text-gray-300">Notice that given any general sorting algorithm (not just for 1-bit keys), one can construct</p>

    <p class="text-gray-300">1-bit-key stable sorting easily by using the index as low-order tie-breaking bits. Thus our lower bound for stable 1-bit-key sorting also implies a lower bound for general sorting as stated in the following corollary.</p>

    <h6 id="sec-59" class="text-base font-medium mt-4">Corollary 4.8.</h6>

    <p class="text-gray-300">Let <span class="math">0&lt;s\\leq\\sqrt{n}</span> be an integer. Suppose <span class="math">\\epsilon&gt;0</span>, <span class="math">0&lt;\\beta&lt;1</span> and <span class="math">0\\leq\\delta\\leq\\beta\\cdot\\frac{\\epsilon}{s}\\cdot e^{-2\\epsilon s}</span>. Then, any (randomized) sorting algorithm that is <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious must have some input, on which it incurs at least <span class="math">\\Omega(N\\log s)</span> memory accesses with probability at least <span class="math">1-\\beta</span>.</p>

    <p class="text-gray-300">Finally, just like our upper bounds, our lower bounds here assume that the algorithm must be perfectly correct. In the Appendices (Section B), we show how to generalize the lower bound to work for algorithms that can make mistakes with a small probability.</p>

    <h2 id="sec-60" class="text-2xl font-bold">5 Differentially Oblivious Merging: Upper Bounds</h2>

    <p class="text-gray-300">Merging in the balls-and-bins model is the following abstraction: given two input arrays each of which contains at most <span class="math">N</span> balls sorted by their tagged keys, merge them into a single sorted array. Pippenger and Valiant <em>[39]</em> showed that any oblivious merging algorithm in the balls-and-bins model must incur at least <span class="math">\\Omega(N\\log N)</span> movements of balls.</p>

    <p class="text-gray-300">In this section, we show that when <span class="math">\\epsilon=O(1)</span> and <span class="math">\\delta</span> is negligibly small (but not be subexponentially small), we can accomplish <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious merging in <span class="math">O(N\\log\\log N)</span> time! This is yet another separation between obliviousness and our new notion of differential obliviousness.</p>

    <h4 id="sec-61" class="text-lg font-semibold mt-6">Clarifications: definition of neighboring inputs for merging.</h4>

    <p class="text-gray-300">In merging, both input arrays must be sorted. As a result, to define the notion of neighboring inputs, it does not make sense to take an input array and flip a position to an arbitrarily value — since obviously this would break the sortedness requirement. Instead, we say that two inputs <span class="math">(I_{0},I_{1})</span> and <span class="math">(I^{\\prime}_{0},I^{\\prime}_{1})</span> are neighboring iff for each of <span class="math">b\\in\\{0,1\\}</span>, the two (multi-)sets <span class="math">\\mathsf{set}(I_{b})</span> and <span class="math">\\mathsf{set}(I^{\\prime}_{b})</span> differ in exactly one record. Based on this notion of neighboring, <span class="math">(\\epsilon,\\delta)</span>-differentially obliviousness for merging is defined in the same manner as in Section 2.2.</p>

    <h3 id="sec-62" class="text-xl font-semibold mt-8">5.1 Intuition</h3>

    <p class="text-gray-300">The naïve non-private merging algorithm keeps track of the head pointer of each array, and performs merging in linear time. However, how fast each head pointer advances leaks the relative order of elements in the two input arrays. Oblivious merging hides this information completely but as mentioned, must incur <span class="math">\\Omega(N\\log N)</span> runtime in the balls-and-bins model. Since our requirement is differential obliviousness, this means that we can reveal some noisy aggregate statistics about the two input arrays. We next highlight our techniques for achieving better runtimes.</p>

    <h4 id="sec-63" class="text-lg font-semibold mt-6">Noisy-boundary binning and interior points.</h4>

    <p class="text-gray-300">Inspired by Bun et al. <em>[8]</em>, we divide each sorted input array into <span class="math">\\mathsf{poly}\\log\\lambda</span>-sized bins (where <span class="math">\\lambda</span> is the security parameter). To help our merging algorithm decide how fast to advance the head pointer, a differentially private mechanism by Bun et al. <em>[8]</em> is used to return an interior point of each bin, where an interior point is defined to be any value that is (inclusively) between the minimum and the maximum elements of the bin. Technically, the following components are important for our proofs to work.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Random bin loads and localization: each bin must contain a random number of real elements padded with dummies to the bin’s maximum capacity <span class="math">Z=\\mathsf{poly}\\log\\lambda</span> — this is inspired by Bun et al. <em>[8]</em>. The randomization in bin load allows a “localization” technique in our proofs,</li>

    </ol>

    <p class="text-gray-300">since inserting one element into the input array can be obfuscated by local noise and will not significantly affect the distribution of the loads of too many bins.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Secret bin load. For privacy, it is important that the actual bin loads be kept private from the adversary. This raises a technical challenge: since the adversary can observe the access patterns when the bins are constructed, how can we make sure that the access patterns do not reveal the bins’ loads? One naïve approach is to resort to oblivious algorithms — but oblivious sorting in the balls-and-bins model has a well-known <span class="math">\\Omega(N\\log N)</span> lower bound <em>[33]</em> and thus would be too expensive.</li>

    </ol>

    <h4 id="sec-64" class="text-lg font-semibold mt-6">Creating the bins privately.</h4>

    <p class="text-gray-300">To answer the above question of how to construct the bins securely without disclosing the bins’ actual loads, we again rely on a batching and queuing technique similiar in spirit to our tight stable compaction algorithm. At a high level, for every iteration <span class="math">i</span> : 1) we shall read a small, poly-logarithmically sized batch of elements from the input stream into a small, poly-logarithmically sized working buffer; 2) we rely on oblivious algorithms to construct the <span class="math">i</span>-th bin containing the smallest <span class="math">R_{i}</span> elements in the buffer padded with dummies, where the load <span class="math">R_{i}</span> has been sampled from an appropriate distribution. These elements will then be removed from the working buffer.</p>

    <p class="text-gray-300">The key to making this algorithm work is to ensure that at any time, the number of elements remaining in the buffer is at most polylogarithmic (in the security parameter). This way, running oblivious algorithms (e.g., oblivious sorting) on this small buffer would incur only <span class="math">\\log\\log</span> overheads. To this end, we again rely on a differentially private prefix sum mechanism (which must be made oblivious first) to estimate how many real elements will be placed in the first <span class="math">i</span> bins for every choice of <span class="math">i</span>. Suppose that the number of real elements in the first <span class="math">i</span> bins is in the range <span class="math">[C_{i},C^{\\prime}_{i}]</span> (except with negligible probability); then when constructing the <span class="math">i</span>-th bin, it suffices to read the input stream upto position <span class="math">C^{\\prime}_{i}</span>.</p>

    <p class="text-gray-300">It would seem like the above idea still leaks some information about each bin’s actual load — but we will prove that this leakage is safe. Concretely, in our Appendices, we will prove a binning composition theorem, showing that with our noisy-boundary binning, it is safe to release any statistic that is differentially private with respect to the binning outcome — the resulting statistic would also be differentially private with respect to the original input.</p>

    <p class="text-gray-300">Putting the above together, we devise an almost linear-time, differentially oblivious procedure for dividing input elements into bins with random bin loads, where each bin is tagged with a differentially private interior point — henceforth we call this list of bins tagged with interior points thresh-bins.</p>

    <h4 id="sec-65" class="text-lg font-semibold mt-6">Merging lists of thresh-bins.</h4>

    <p class="text-gray-300">Once we have converted each input array to a list of thresh-bins, the idea is to perform merging by reading bins from the two input arrays, and using each bin’s interior point to inform the merging algorithm which head pointer to advance. Since each bin’s load is a random variable, it is actually not clear how many elements to emit after reading each bin. Here again, we rely on a differentially private prefix sum mechanism to estimate how many elements to emit, and store all the remaining elements in a poly-logarithmically sized working buffer. In this manner, we can apply oblivious algorithm techniques to the small working buffer incurring only <span class="math">\\log\\log</span> blowup in performance.</p>

    <p class="text-gray-300">##</p>

    <p class="text-gray-300">5.2 Preliminaries</p>

    <h4 id="sec-66" class="text-lg font-semibold mt-6">Oblivious bin placement.</h4>

    <p class="text-gray-300">Oblivious bin placement is the following abstraction: given an input array <span class="math">X</span>, and a vector <span class="math">V</span> where <span class="math">V[i]</span> denotes the intended load of bin <span class="math">i</span>, the goal is to place the first <span class="math">V[1]</span> elements of <span class="math">X</span> into bin <span class="math">1</span>, place the next <span class="math">V[2]</span> elements of <span class="math">X</span> into bin <span class="math">2</span>, and so on. All output bins are padded with dummies to a maximum capacity <span class="math">Z</span>. Once the input <span class="math">X</span> is fully consumed, all remaining bins will contain solely dummies.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">We construct an oblivious algorithm for solving the bin placement problem. Our algorithm invokes building blocks such as oblivious sorting and oblivious propagation constant number of times, and thus it completes in <span class="math">O(n\\log n)</span> runtime where $n=\\max(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">X</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">,Z\\cdot</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">V</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)$. We present the theorem statement for this building block and defer the details to the Appendices.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h6 id="sec-67" class="text-base font-medium mt-4">Theorem 5.1 (Oblivious bin placement).</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">There exists a deterministic, oblivious algorithm that realizes the aforementioned bin placement abstraction and completes in time <span class="math">O(n\\log n)</span> where $n=\\max(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">X</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">,Z\\cdot</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">V</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h4 id="sec-68" class="text-lg font-semibold mt-6">Truncated geometric distribution.</h4>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Let <span class="math">Z&gt;\\mu</span> be a positive integer, and <span class="math">\\alpha\\geq 1</span>. The truncated geometric distribution <span class="math">\\mathsf{Geom}^{Z}(\\mu,\\alpha)</span> has support with the integers in <span class="math">[0..Z]</span> such that its probability mass function at <span class="math">x\\in[0,Z]</span> is proportional to $\\alpha^{-</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mu-x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}<span class="math">. We consider the special case </span>\\mu=\\frac{Z}{2}<span class="math"> (where </span>Z<span class="math"> is even) and use the shorthand </span>\\mathsf{Geom}^{Z}(\\alpha):=\\mathsf{Geom}^{Z}(\\frac{Z}{2},\\alpha)<span class="math">. In this case, the probability mass function at </span>x\\in[0..Z]<span class="math"> is </span>\\frac{\\alpha-1}{\\alpha+1-2\\alpha^{-\\frac{Z}{2}}}\\cdot\\alpha^{-</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\frac{Z}{2}-i</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h3 id="sec-69" class="text-xl font-semibold mt-8">5.3 Subroutine: Differentially Oblivious Interior Point Mechanism</h3>

    <p class="text-gray-300"><em>Bun et al. [8]</em> propose a differentially private interior point algorithm: given an array <span class="math">I</span> containing sufficient samples, they show how to release an interior point that is between <span class="math">[\\min(I),\\max(I)]</span> in a differentially private manner. Unfortunately, their algorithm does not offer access pattern privacy if executed in a naïve manner. In the Appendices, we show how to design an oblivious algorithm that efficiently realizes the interior point mechanism — our approach makes use of oblivious algorithm techniques (e.g., oblivious sorting and oblivious aggregation) that were adopted in the design of ORAM and OPRAM schemes <em>[6, 10, 18, 20, 23, 38]</em>. Importantly, since our main algorithm will call this oblivious interior point mechanism on bins containing dummy elements, we also need to make sure that our oblivious algorithm is compatible with the existence of dummy elements and not disclose how many dummy elements there are. We present the following theorem while deferring its detailed proof to the Appendices. In the Appendices, we also discuss how to realize the oblivious interior point mechanism on finite-word-length RAMs without assuming arbitrary-precision real arithmetic.</p>

    <h6 id="sec-70" class="text-base font-medium mt-4">Theorem 5.2 (Differentially private interior point).</h6>

    <p class="text-gray-300">For any <span class="math">\\epsilon,\\delta&gt;0</span>, there exists an algorithm such that given any input bin of capacity <span class="math">Z</span> consisting of <span class="math">n</span> real elements, whose real elements have keys from a finite universe <span class="math">[0..U-1]</span> and <span class="math">n\\geq\\frac{18500}{\\epsilon}\\cdot 2^{\\log^{<em>}U}\\cdot\\log^{</em>}U\\cdot\\ln\\frac{4\\log^{*}U}{\\epsilon\\delta}</span>, the algorithm</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>completes consuming only <span class="math">O(Z\\log Z)</span> time and number of memory accesses.</li>

      <li>the algorithm produces an outcome that is <span class="math">(\\epsilon,\\delta)</span>-differentially private;</li>

      <li>the algorithm has perfect correctness, i.e., the outcome is an interior point of the input bin with probability <span class="math">1</span>; and</li>

      <li>the algorithm’s memory access pattern depends only on <span class="math">Z</span>, and in particular, is independent of the number of real elements the bin contains.</li>

    </ul>

    <p class="text-gray-300">5.4 Subroutine: Creating Thresh-Bins</p>

    <p class="text-gray-300">In the ThreshBins subroutine, we aim to place elements in an input array <span class="math">X</span> into bins where each bin contains a random number of real elements (following a truncated geometric distribution), and each bin is padded with dummies to the maximum capacity <span class="math">Z</span>. The ThreshBins will emit exactly <span class="math">B</span> bins. Later when we call ThreshBins we guarantee that <span class="math">B</span> bins will almost surely consume all elements in <span class="math">X</span>. Logically, one may imagine that <span class="math">X</span> is followed by infinitely many <span class="math">\\infty</span> elements such that there are always more elements to draw from the input stream when creating the bins. Note that <span class="math">\\infty</span>’s are treated as filler elements with maximum key and not treated as dummies (and this is important for the interior point mechanism to work).</p>

    <p class="text-gray-300">ThreshBins<span class="math">(\\lambda,X,B,\\epsilon_{0})</span>:</p>

    <p class="text-gray-300">Assume:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">B\\leq\\textsf{poly}(\\lambda)</span> for some fixed polynomial <span class="math">\\textsf{poly}(\\cdot)</span>.</li>

      <li><span class="math">\\epsilon_{0}&lt;c</span> for some constant <span class="math">c</span> that is independent of <span class="math">\\lambda</span>.</li>

      <li>The keys of all elements are chosen from a finite universe denoted <span class="math">[0..U-1]</span>, where <span class="math">\\log^{*}U\\leq\\log\\log\\lambda</span> (note that this is a very weak assumption).</li>

      <li>Let the bin capacity <span class="math">Z:=\\frac{1}{\\epsilon_{0}}\\log^{8}\\lambda</span>, and <span class="math">s=\\frac{1}{\\epsilon_{0}}\\cdot\\log^{3}\\lambda</span></li>

    </ol>

    <p class="text-gray-300">Algorithm:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Recall that the elements in <span class="math">X</span> are sorted; if the length of the input <span class="math">X</span> is too small, append an appropriate number of elements with key <span class="math">\\infty</span> at the end such that it has length at least <span class="math">2BZ</span>.</li>

    </ul>

    <p class="text-gray-300">This makes sure that the real elements in the input stream do not deplete prematurely in process below.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>For <span class="math">i=1</span> to <span class="math">B</span>, let <span class="math">R_{i}=\\textsf{Geom}^{Z}(\\exp(\\epsilon_{0}))</span> be independently sampled truncated geometric random variables. Denote the vector <span class="math">R:=(R_{1},R_{2},\\ldots,R_{B})</span>.</li>

      <li>Call <span class="math">D:=\\textsf{PrefixSum}(\\lambda,R,\\frac{\\epsilon_{0}}{4},\\delta_{0})\\in Z_{+}^{B}</span>, which is the <span class="math">(\\frac{\\epsilon_{0}}{4},\\delta_{0})</span>-differentially private subroutine in Theorem 3.1 that privately estimates prefix sums, where <span class="math">\\delta_{0}</span> is set so that the additive error is at most <span class="math">s</span>. We use the convention that <span class="math">D[0]=0</span>.</li>

      <li>Let <span class="math">\\mathsf{Buf}</span> be a buffer with capacity <span class="math">Z+s=O(Z)</span>. Initially, we place the first <span class="math">s</span> elements of <span class="math">X</span> in <span class="math">\\mathsf{Buf}</span>.</li>

      <li>For <span class="math">i=1</span> to <span class="math">B</span>:</li>

    </ul>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Read the next batch of elements from the input stream <span class="math">X</span> with indices from <span class="math">D[i-1]+s+1</span> to <span class="math">D[i]+s</span>, and add these elements to the buffer <span class="math">\\mathsf{Buf}</span>.</li>

    </ul>

    <p class="text-gray-300">This is done by temporarily increasing the capacity of <span class="math">\\mathsf{Buf}</span> by appending these elements at the end. Then, oblivious sorting can be used to move any dummy elements to the end, after which we can truncate <span class="math">\\mathsf{Buf}</span> back to its original capacity.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Call ObliviousBinPlace(<span class="math">\\mathsf{Buf},(R_{i}),Z</span>) to place the first <span class="math">R_{i}</span> elements in <span class="math">\\mathsf{Buf}</span> into the next bin and the bin is padded with dummies to the maximum capacity <span class="math">Z</span>.</li>

      <li>Mark every element in <span class="math">\\mathsf{Buf}</span> at position <span class="math">R_{i}</span> or smaller as dummy. (This is done by a linear scan so that the access pattern hides <span class="math">R_{i}</span> and effectively removes the first <span class="math">R_{i}</span> elements in <span class="math">\\mathsf{Buf}</span> in the next oblivious sort.)</li>

    </ul>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Tag each bin with its estimated prefix sum from vector <span class="math">D</span>. Moreover, we use the <span class="math">(\\frac{\\epsilon_0}{4},\\delta)</span>-differentially oblivious interior point mechanism in Section 5.3 to tag each bin with an interior point, denoted by a vector <span class="math">P = (P_{1},\\ldots ,P_{B})</span>, where <span class="math">\\delta \\coloneqq \\frac{1}{4}\\exp (-0.1\\log^2\\lambda)</span>.</li>

      <li>Output the <span class="math">B</span> bins.</li>

    </ul>

    <h2 id="sec-71" class="text-2xl font-bold">5.5 Subroutine: Merging Two Lists of Thresh-Bins</h2>

    <p class="text-gray-300">We next describe an algorithm to merge two lists of thresh-bins. Recall that the elements in a list of thresh-bins are sorted, where each bin is tagged with an interior point and also an estimate of the prefix sum of the number of real elements up to that bin.</p>

    <p class="text-gray-300"><strong>MergeThreshBins</strong> <span class="math">(\\lambda, T_0, T_1, \\epsilon_0)</span>:</p>

    <p class="text-gray-300"><strong>Assume:</strong></p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">1. The input is <span class="math">T_0</span> and <span class="math">T_1</span>, each of which is a list of thresh-bins, where each bin has capacity <span class="math">Z = \\frac{1}{\\epsilon_0} \\log^8 \\lambda</span> size. For <span class="math">b \\in \\{0, 1\\}</span>, let $B_b =</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">T_b</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> be the number of bins in </span>T_b<span class="math">, and </span>B := B_0 + B_1<span class="math"> is the total number of bins. Recall that the bins in </span>T_0<span class="math"> and </span>T_1<span class="math"> are tagged with interior points </span>P_0<span class="math"> and </span>P_1<span class="math"> and estimated prefix sums </span>D_0<span class="math"> and </span>D_1$, respectively.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>The output is an array of sorted elements from <span class="math">T_0</span> and <span class="math">T_1</span>, where any dummy elements appear at the end of the array. The length of the array is <span class="math">M := BZ</span>.</li>

    </ol>

    <p class="text-gray-300"><strong>Algorithm:</strong></p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Let <span class="math">s = \\frac{1}{\\epsilon_0} \\log^3 \\lambda</span>.</li>

      <li>Initialize an empty array Output[0..M-1] of length <span class="math">M := BZ</span>.</li>

    </ul>

    <p class="text-gray-300">Initialize count := 0, the number of elements already delivered to Output.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Initialize indices <span class="math">j_0 = j_1 = 0</span> and a buffer Buf with capacity <span class="math">K := 6(Z + s) = O(Z)</span>. Add elemnts in <span class="math">T_0[1]</span> and <span class="math">T_1[1]</span> to Buf.</li>

      <li>Let <span class="math">\\mathcal{L}</span> be the list of sorted bins from <span class="math">T_0</span> and <span class="math">T_1</span> according to the tagged interior points. (Observe that we do not need oblivious sort in this step.) We will use this list to decide which bins to add to Buf.</li>

      <li>For <span class="math">i = 1</span> to <span class="math">B</span>:</li>

    </ul>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Update the indices <span class="math">j_0, j_1</span>: if the bin <span class="math">\\mathcal{L}[i]</span> belongs to <span class="math">T_b</span>, update <span class="math">j_b \\gets j_b + 1</span>. (This maintains that <span class="math">\\mathcal{L}[i] = T_b[j_b]</span>.)</li>

      <li>Add elements in bin <span class="math">T_b[j_b + 1]</span> (if exists) to Buf. This is done by appending elements in <span class="math">T_b[j_b + 1]</span> at the end of Buf to temporarily increase the size of Buf, and then use oblivious sorting followed by truncation to restore its capacity. (Note that <span class="math">T_b[j_b + 1]</span> may not be the next bin in the list <span class="math">\\mathcal{L}</span>.) Note that the elements in Buf are always sorted.</li>

      <li>Determine safe bins <span class="math">k_0, k_1</span>: For <span class="math">b \\in \\{0, 1\\}</span>, let <span class="math">k_b</span> be the maximal index <span class="math">k</span> such that the following holds: (i) <span class="math">T_b[k]</span> is inserted in Buf, (ii) there exists some bin <span class="math">T_{1-b}[u]</span> from <span class="math">T_{1-b}</span> that has been inserted into Buf and whose interior point is at least that of <span class="math">T_b[k + 1]</span>, i.e., <span class="math">P_{1-b}[u] \\geq P_b[k + 1]</span>. (Observe that any element with key smaller than that of an element in a safe bin has already been put into the buffer.) If there is no such index, set <span class="math">k_b = 0</span>. Note that the last bin <span class="math">B_b</span> cannot be safe.</li>

    </ul>

    <p class="text-gray-300">24</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Remove safe bins from <span class="math">\\mathsf{Buf}</span>: Set <span class="math">\\mathsf{newcount} := D_0[k_0] + D_1[k_1] - 2s</span>. Remove the first (newcount-count) elements from the <span class="math">\\mathsf{Buf}</span> and copy them into the next available slots in the Output array. Then update <span class="math">\\mathsf{count} \\gets \\mathsf{newcount}</span>.</li>

      <li>Output the remaining elements: Let <span class="math">\\mathsf{newcount} = \\min \\{D_0[B_0] + D_1[B_1] + 2s, BZ\\}</span>. Copy the first (newcount-count) into the next available slots in the Output array.</li>

    </ul>

    <h2 id="sec-72" class="text-2xl font-bold">5.6 Full Merging Algorithm</h2>

    <p class="text-gray-300">Finally, the full merging algorithm involves taking the two input arrays, creating thresh-bins out of them using ThreshBins, and then calling Merge to merge the two lists of thresh-bins. We defer concrete parameters of the full scheme and proofs to the Appendices.</p>

    <p class="text-gray-300"><span class="math">\\mathsf{Merge}(\\lambda, I_0, I_1, \\epsilon)</span>:</p>

    <p class="text-gray-300"><strong>Assume:</strong></p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>The input is two sorted arrays <span class="math">I_0</span> and <span class="math">I_1</span>.</li>

    </ol>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">2. We suppose that <span class="math">\\epsilon &amp;lt; c</span> for some constant <span class="math">c</span>, <span class="math">\\log^* U \\leq \\log \\log \\lambda</span>, and $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">I_0</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\leq \\mathsf{poly}_0(\\lambda)<span class="math"> and </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">I_1</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\leq \\mathsf{poly}_1(\\lambda)<span class="math"> for some fixed polynomials </span>\\mathsf{poly}_0(\\cdot)<span class="math"> and </span>\\mathsf{poly}_1(\\cdot)$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><strong>Algorithm:</strong></p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">1. First, for <span class="math">b \\in \\{0, 1\\}</span>, let $B_b := \\lceil \\frac{2</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">I_b</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}{Z} (1 + \\frac{2}{\\log^2 \\lambda}) \\rceil<span class="math">, call ThreshBins</span>(\\lambda, I_b, B_b, 0.1\\epsilon)<span class="math"> to transform each input array into a list of thresh-bins — let </span>T_0<span class="math"> and </span>T_1$ denote the outcomes respectively.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Do a linear scan on <span class="math">T, I_0, I_1</span> to check if <span class="math">T</span> contains the same number of non-dummy elements as in the input <span class="math">(I_0, I_1)</span>. If so, output <span class="math">T</span>. Otherwise (this can happen when the bin load in the thresh-bins are too small so that some elements are dropped), perform a non-private merge to output a correct merged array.</li>

    </ol>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><strong>Theorem 5.3 (Differentially oblivious merging).</strong> The <span class="math">\\mathsf{Merge}(\\lambda, I_0, I_1, \\epsilon)</span> algorithm is <span class="math">(\\epsilon, \\delta)</span>-differentially oblivious, where <span class="math">\\delta = \\exp(-\\Theta(\\log^2 \\lambda))</span>. Moreover, its running time is $O((</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">I_0</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">+</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">I_1</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)(\\log \\frac{1}{\\epsilon} + \\log \\log \\lambda))$ and it has perfect correctness.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">We defer the proofs of the above theorem to the Appendices.</p>

    <h2 id="sec-73" class="text-2xl font-bold">5.7 Limits of Differentially Oblivious Merging</h2>

    <p class="text-gray-300">In this section, we prove a lower bound regarding the performance of differentially oblivious merging.</p>

    <p class="text-gray-300"><strong>Theorem 5.4 (Limits of <span class="math">(\\epsilon, \\delta)</span>-differentially oblivious merging).</strong> Consider the merging problem, in which the input is two sorted lists of elements and the output is the merging of the two input lists into a single sorted list.</p>

    <p class="text-gray-300">Let <span class="math">0 &amp;lt; s \\leq \\sqrt{N}</span> be an integer. Suppose <span class="math">\\epsilon &amp;gt; 0</span>, <span class="math">0 &amp;lt; \\beta &amp;lt; 1</span> and <span class="math">0 \\leq \\delta \\leq \\beta \\cdot \\frac{\\epsilon}{s} \\cdot e^{-\\epsilon s}</span>. Then, any merging algorithm that is <span class="math">(\\epsilon, \\delta)</span>-differentially oblivious must have some input consisting of two sorted lists each of length <span class="math">N</span>, on which it incurs at least <span class="math">\\Omega(N \\log s)</span> memory accesses with probability at least <span class="math">1 - \\beta</span>.</p>

    <p class="text-gray-300">25</p>

    <h6 id="sec-74" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">We consider two input lists. The first list <span class="math">\\mathsf{Input}_{1}[0..N-1]</span> is always the same such that <span class="math">\\mathsf{Input}_{1}[j]</span> holds an element with key value <span class="math">j+1</span>.</p>

    <p class="text-gray-300">We consider <span class="math">s+1</span> scenarios for the second list. For <span class="math">0\\leq i\\leq s</span>, in scenario <span class="math">I_{i}</span>, <span class="math">\\mathsf{Input}_{2}[0..N-1]</span> contains <span class="math">i</span> elements with key value <span class="math">0</span> and <span class="math">N-i</span> elements with key value <span class="math">N+1</span>. It follows that any two such scenarios are <span class="math">s</span>-neighboring.</p>

    <p class="text-gray-300">By Lemma 4.3, on input scenario <span class="math">I_{0}</span>, any merging algorithm that is <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious produces an access pattern <span class="math">A</span> that is plausible for all <span class="math">I_{i}</span>’s (<span class="math">1\\leq i\\leq s</span>) with all but probability of <span class="math">s\\cdot\\frac{e^{\\epsilon s}-1}{e^{\\epsilon}-1}\\cdot\\delta\\leq\\beta</span>.</p>

    <p class="text-gray-300">We assume that the merging algorithm writes the merged list into the memory locations <span class="math">\\mathsf{Output}[0..2N-1]</span>. Hence, for all <span class="math">0\\leq i\\leq s</span>, in scenario <span class="math">I_{i}</span>, for all <span class="math">0\\leq j&lt;N</span>, the element initially stored at <span class="math">\\mathsf{Input}_{1}[j]</span> will finally appear at <span class="math">\\mathsf{Output}[i+j]</span>.</p>

    <p class="text-gray-300">Therefore, any access pattern <span class="math">A</span> that is plausible for <span class="math">I_{i}</span> must correspond to a compact graph <span class="math">G</span> that contains <span class="math">N</span> vertex-disjoint paths, each of which goes from the node representing the initial <span class="math">\\mathsf{Input}_{1}[j]</span> to the node representing the final <span class="math">\\mathsf{Output}[i+j]</span>, for <span class="math">0\\leq j&lt;N</span>.</p>

    <p class="text-gray-300">Hence, Lemma 4.5 implies that if <span class="math">A</span> is plausible for all scenarios <span class="math">I_{i}</span>’s, then the corresponding compact <span class="math">G</span> has <span class="math">\\Omega(N\\log s)</span> edges, which by Lemma 4.4 implies that the access pattern <span class="math">A</span> must make at least <span class="math">\\Omega(N\\log s)</span> memory accesses. ∎</p>

    <h2 id="sec-75" class="text-2xl font-bold">6 Differentially Oblivious Range Query Data Structure</h2>

    <h3 id="sec-76" class="text-xl font-semibold mt-8">6.1 Data Structures</h3>

    <p class="text-gray-300">A data structure in the RAM model is a possibly randomized stateful algorithm which, upon receiving requests, updates the state in memory and optionally outputs an answer to the request — without loss of generality we may assume that the answer is written down in memory addresses <span class="math">[0..L-1]</span>, where <span class="math">L</span> is the length of the answer.</p>

    <p class="text-gray-300">As mentioned, we consider data structures in the balls-and-bins model where every record (e.g., patient or event record) may be considered as an opaque ball tagged with a key. Algorithms are allowed to perform arbitrary computations on the keys but the balls can only be moved around.</p>

    <p class="text-gray-300">We start by considering data structures that support two types of operations, <em>insertions</em> and <em>queries</em>. Each insertion inserts an additional record into the database and each query comes from some query family <span class="math">\\mathcal{Q}</span>. We consider two important query families: 1) for our lower bounds, we consider point queries where each query wants to request all records that match a specified key; 2) for our upper bounds, we consider range queries where each query wants to request all records whose keys fall within a specified range <span class="math">[s,t]</span>.</p>

    <h4 id="sec-77" class="text-lg font-semibold mt-6">Correctness notion under obfuscated lengths.</h4>

    <p class="text-gray-300">As <em>Kellaris et al. [28]</em> show, leaking the number of records matching each query can, in some settings, cause entire databases to be reconstructed. Our differential obliviousness definitions below will protect such length leakage. As a result, more than the exact number of matching records may be returned with each query. Thus, we require only a relaxed correctness notion: for each query, suppose that <span class="math">L</span> records are returned — we require that all matching records must be found within the <span class="math">L</span> records returned. For example, in a client-server setting, the client can retrieve the answer-set (one by one or altogether), and then prune the non-matching records locally.</p>

    <h4 id="sec-78" class="text-lg font-semibold mt-6">Performance metrics: runtime and locality.</h4>

    <p class="text-gray-300">For our data structure construction, besides the classical <em>runtime</em> metric that we have adopted throughout the paper, we consider an additional</p>

    <p class="text-gray-300">locality metric which was commonly adopted in recent works on searchable encryption <em>[5, 9]</em> and Oblivious RAM constructions <em>[4]</em>. Real-life storage systems including memory and disks are optimized for programs that exhibit locality in its accesses — in particular, sequential accesses are typically much cheaper than random accesses. We measure a data structure’s locality by counting how many discontiguous memory regions it must access to serve each operation.</p>

    <h3 id="sec-79" class="text-xl font-semibold mt-8">6.2 Defining Differentially Oblivious Data Structures</h3>

    <p class="text-gray-300">We define two notions of differential obliviousness for data structures, static and adaptive security. Static security assumes that the data structure’s operational sequences are chosen statically independent of the answers to previous queries; whereas adaptive security assumes that the data structure’s operational sequences are chosen adaptively, possibly dependent on the answers to previous queries. Notice that this implies that both the queries and the database’s contents (which are determined by the insertion operations over time) can be chosen adaptively.</p>

    <p class="text-gray-300">As we argue later, adaptive differential obliviousness is strictly stronger than the static notion. We will use the static notion for our lower bounds and the adaptive notion for our upper bounds — this makes both our lower- and upper-bounds stronger.</p>

    <h4 id="sec-80" class="text-lg font-semibold mt-6">6.2.1 Static Differential Obliviousness for Data Structures</h4>

    <p class="text-gray-300">We now define differential obliviousness for data structures. Our privacy notion captures the following intuition: for any two neighboring databases that differ only in one record (where the database is determined by the insertion operations over time), the access patterns incurred for insertions or queries must be close in distribution. Such a notion protects the privacy of individual records in the database (or of individual events), but does not protect the privacy of the queries. Thus our notion is suitable for a scenario where the data is of a sensitive nature (e.g., hospital records) and the queries are non-sensitive (e.g., queries by a clinical researcher). In fact we will later show that if one must additionally protect the privacy of the queries, then it would be inevitable to incur <span class="math">\\Omega(N)</span> blowup in cost on at least some operational sequences. This observation also partly motivates our definition, which requires meaningful and non-trivial privacy guarantees, and importantly, does not rule out efficient solutions.</p>

    <p class="text-gray-300">We say that two operational sequences <span class="math">\\mathsf{ops}_{0}</span> and <span class="math">\\mathsf{ops}_{1}</span> (consisting of insertions and queries) are query-consistent neighboring, if the two sequences differ in exactly position <span class="math">i</span>, and moreover both <span class="math">\\mathsf{ops}_{0}[i]</span> and <span class="math">\\mathsf{ops}_{1}[i]</span> must be insertion operations.</p>

    <h6 id="sec-81" class="text-base font-medium mt-4">Definition 6.1 (Static differential obliviousness for data structures).</h6>

    <p class="text-gray-300">Let <span class="math">\\epsilon(\\cdot)</span> and <span class="math">\\delta(\\cdot)</span> be functions of a security parameter <span class="math">\\lambda</span>. We say that a data structure scheme <span class="math">\\mathbb{DS}</span> preserves static <span class="math">(\\epsilon,\\delta)</span>-differential obliviousness, if for any two query-consistent neighboring operational sequences <span class="math">\\mathsf{ops}_{0}</span> and <span class="math">\\mathsf{ops}_{1}</span>, for any <span class="math">\\lambda</span>, for any set <span class="math">S</span> of access patterns,</p>

    <p class="text-gray-300"><span class="math">\\Pr[\\mathbf{Accesses}^{\\mathbb{DS}}(\\lambda,\\mathsf{ops}_{0})\\in S]\\leq e^{\\epsilon(\\lambda)}\\cdot\\Pr[\\mathbf{Accesses}^{\\mathbb{DS}}(\\lambda,\\mathsf{ops}_{1})\\in S]+\\delta(\\lambda)</span> (2)</p>

    <p class="text-gray-300">where the random variable <span class="math">\\mathbf{Accesses}^{\\mathbb{DS}}(\\lambda,\\mathsf{ops})</span> denotes the access patterns incurred by the data structure upon receiving the security parameter <span class="math">\\lambda</span> and operational sequence <span class="math">\\mathsf{ops}</span>.</p>

    <h5 id="sec-82" class="text-base font-semibold mt-4">Discussions on alternative notions.</h5>

    <p class="text-gray-300">It is interesting to consider a stronger notion where the queries must be protected too. We consider one natural strengthening where we want to protect the queries as well as insertions, but the fact whether each operation is an insertion or query is considered non-sensitive. To formalize such a notion, one may simply redefine the notion of</p>

    <p class="text-gray-300">“neighboring” in the above definition, such that any two operational sequences that are type-consistent (i.e., they agree in the type of every operation) and differ in exactly one position are considered neighboring — and this differing position can either be query or insertion. It would not be too difficult to show that such a strong notion would rule out efficient solutions: for example, consider a sequence of operations such that some keys match <span class="math">\\Omega(N)</span> records and others match only one record. In this case, to hide each single query, it becomes inevitable that each query must access <span class="math">\\Omega(N)</span> elements even when the query is requesting the key with only one occurrence.</p>

    <h4 id="sec-83" class="text-lg font-semibold mt-6">6.2.2 Adaptive Differential Obliviousness for Data Structures</h4>

    <p class="text-gray-300">We will prove our lower bounds using the above, static notion of differential obliviousness. However, our data structure upper bounds in fact satisfies a stronger, adaptive and composable notion of security as we formally specify below. Here we allow the adversary to adaptively choose the database (i.e., insertions) as well as the queries.</p>

    <h6 id="sec-84" class="text-base font-medium mt-4">Definition 6.2 (Adaptive differential obliviousness for data structures).</h6>

    <p class="text-gray-300">We say that a data structure <span class="math">\\mathbb{DS}</span> satisfies adaptive <span class="math">(\\epsilon,\\delta)</span>-differential obliviousness iff for any (possibly unbounded) stateful algorithm <span class="math">\\mathcal{A}</span> that is query-consistent neighbor-respecting (to be defined below), for any <span class="math">N</span>, <span class="math">\\mathcal{A}</span>’s view in the following two experiments <span class="math">\\mathsf{Expt}^{0}_{\\mathcal{A}}(\\lambda,N)</span> and <span class="math">\\mathsf{Expt}^{1}_{\\mathcal{A}}(\\lambda,N)</span> satisfy the following equation:</p>

    <p class="text-gray-300"><span class="math">\\Pr[\\mathsf{Expt}^{0}_{\\mathcal{A}}(\\lambda,N)=1]\\leq e^{\\epsilon(\\lambda)}\\cdot\\Pr[\\mathsf{Expt}^{1}_{\\mathcal{A}}(\\lambda,N)=1]+\\delta(\\lambda)</span></p>

    <p class="text-gray-300">⬇ <span class="math">\\mathsf{Expt}^{b}_{\\mathcal{A}}(\\lambda,N)</span>: <span class="math">\\mathsf{addresses}_{0}=\\bot</span> <span class="math">\\text{For }t=1,2,\\ldots,N</span>: <span class="math">(\\mathsf{op}^{0}_{t},\\mathsf{op}^{1}_{t})\\leftarrow\\mathcal{A}(N,\\mathsf{addresses}_{t-1}),\\ \\ \\mathsf{addresses}_{t}\\leftarrow\\mathbb{DS}(\\lambda,\\mathsf{op}^{b}_{t})</span> <span class="math">b^{\\prime}\\leftarrow\\mathcal{A}</span>, and output <span class="math">b^{\\prime}</span></p>

    <p class="text-gray-300">In the above, <span class="math">\\mathsf{addresses}_{t}</span> denotes the ordered sequence of physical memory locations accessed for the <span class="math">t</span>-th operation <span class="math">\\mathsf{op}_{t}</span> (including whether each access is read or write).</p>

    <h5 id="sec-85" class="text-base font-semibold mt-4">Neighbor-respecting.</h5>

    <p class="text-gray-300">We say that <span class="math">\\mathcal{A}</span> is query-consistent neighbor-respecting w.r.t. <span class="math">\\mathbb{DS}</span> iff for every <span class="math">\\lambda</span> and every <span class="math">N</span>, for either <span class="math">b\\in\\{0,1\\}</span>, with probability <span class="math">1</span> in the above experiment <span class="math">\\mathsf{Expt}^{b}_{\\mathcal{A}}(\\lambda,N)</span>, <span class="math">\\mathcal{A}</span> outputs <span class="math">\\mathsf{op}^{0}_{t}=\\mathsf{op}^{1}_{t}</span> for all but one time step <span class="math">t\\in[N]</span>; and moreover for this differing time step <span class="math">t</span>, <span class="math">\\mathsf{op}^{0}_{t}</span> and <span class="math">\\mathsf{op}^{1}_{t}</span> must both be insertion operations.</p>

    <h3 id="sec-86" class="text-xl font-semibold mt-8">6.3 Warmup: Range Query from Thresh-Bins</h3>

    <p class="text-gray-300">We show that using the differentially oblivious algorithmic building blocks introduced in earlier parts of the paper, we can design an efficient differentially oblivious data structure for range queries.</p>

    <p class="text-gray-300">We first explain how the thresh-bins structure introduced for our merging algorithm can also be leveraged for range queries. Recall that a thresh-bins structure contains a list of bins in which all the real elements are sorted in increasing order, and each bin is tagged with an interior point. Given a list of thresh-bins, one can answer a range query simply by returning all bins whose interior point fall in the queried range, as well as the two bins immediately before and after (if they exist).</p>

    <h5 id="sec-87" class="text-base font-semibold mt-4">Range queries <span class="math">\\mathsf{Query}(T,[s,t])</span>.</h5>

    <p class="text-gray-300">Let <span class="math">T:=\\{\\mathsf{Bin}_{i}\\}_{i\\in[B]}</span> be a list of thresh-bins where <span class="math">\\mathsf{Bin}_{i}</span>’s interior point is <span class="math">M_{i}</span>. To query a range <span class="math">[s,t]</span>, we can proceed in the following steps:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Find a smallest set of consecutive bins <span class="math">i,i+1,i+2,\\ldots,j</span> such that <span class="math">M_{i}\\leq s\\leq t\\leq M_{j}</span> — for example, this can be accomplished through binary search. To handle boundary conditions, we may simply assume that there is an imaginery bin before the first bin with the interior point <span class="math">-\\infty</span> and there is an imaginery bin at the end with the interior point <span class="math">\\infty</span>.</li>

      <li>Now, read all bins <span class="math">\\mathsf{Bin}_{i},\\mathsf{Bin}_{i+1},\\ldots\\mathsf{Bin}_{j}</span> and output the concatenation of these bins.</li>

    </ol>

    <h3 id="sec-88" class="text-xl font-semibold mt-8">6.4 Range Query Data Structure Construction</h3>

    <p class="text-gray-300">When records are inserted over time one by one, we may maintain a hierarchy of thresh-bins, where level <span class="math">i</span> of the hierarchy is a list of thresh-bins containing in total <span class="math">2^{i}\\cdot Z</span> elements. Interestingly, our use of a hierarchical data structure is in fact inspired by hierarchical ORAM constructions <em>[18, 20, 23]</em> — however, in hierarchical ORAMs <em>[18, 20, 23]</em>, rebuilding a level of capacity <span class="math">n</span> in the hierarchical structure requires <span class="math">O(n\\log n)</span> time, but we will accomplish such rebuilding in almost linear time by using the <span class="math">\\mathsf{MergeThreshBins}</span> procedure described earlier.</p>

    <p class="text-gray-300">We now describe our data structure construction supporting insertions and range queries. The algorithm is parametrized by a privacy parameter <span class="math">\\epsilon</span>.</p>

    <h5 id="sec-89" class="text-base font-semibold mt-4">In-memory data structure.</h5>

    <p class="text-gray-300">Let <span class="math">N</span> denote the total number of insertions so far. The in-memory data structure consists of the following:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>A recent buffer denoted <span class="math">\\mathsf{Buf}</span> of capacity <span class="math">Z</span> to store the most recently inserted items, where <span class="math">Z:=\\frac{1}{\\epsilon}\\log^{8}\\lambda</span>.</li>

      <li>A total of <span class="math">\\log N</span> search structures henceforth denoted <span class="math">T_{0},T_{1},\\ldots,T_{L}</span> for <span class="math">L:=\\lceil\\log N\\rceil</span> where <span class="math">T_{i}</span> contains <span class="math">2^{i}\\cdot Z</span> real records and <span class="math">N</span> denotes the total number of insertions over all time.</li>

    </ul>

    <h5 id="sec-90" class="text-base font-semibold mt-4">Algorithm for insertion.</h5>

    <p class="text-gray-300">To insert some record, enter it into <span class="math">\\mathsf{Buf}</span> and if <span class="math">\\mathsf{Buf}</span> now contains <span class="math">Z</span> elements, we use <span class="math">\\widetilde{T}_{0}:=\\mathsf{ThreshBins}(\\lambda,\\mathsf{Buf},4,\\epsilon)</span> to put the elements of <span class="math">\\mathsf{Buf}</span> into <span class="math">4</span> bins, and empty <span class="math">\\mathsf{Buf}</span>. Now repeat the following starting at <span class="math">i=0</span>:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>If <span class="math">T_{i}</span> is empty, let <span class="math">T_{i}:=\\widetilde{T}_{i}</span> and return (i.e., terminate the procedure).</li>

      <li>Else call <span class="math">Y:=\\mathsf{MergeThreshBins}(\\lambda,T_{i},\\widetilde{T}_{i},\\epsilon)</span>; and let <span class="math">\\widetilde{T}_{i+1}=\\mathsf{ThreshBins}(\\lambda,Y,4\\cdot 2^{i+1},\\epsilon)</span>, let <span class="math">i\\leftarrow i+1</span> and repeat if <span class="math">i\\leq L</span>.</li>

      <li>For each call of <span class="math">\\mathsf{ThreshBins}</span>, check if <span class="math">\\mathsf{ThreshBins}</span> produces correct answers by counting the non-dummy elements using a linear scan (to make sure no non-dummy elements are dropped due to too small bin loads <span class="math">R</span>). If <span class="math">\\mathsf{ThreshBins}</span> produce incorrect answers, compute a non-private answer with bin load size <span class="math">R_{i}=Z/2</span> for all <span class="math">i</span>.</li>

    </ul>

    <h5 id="sec-91" class="text-base font-semibold mt-4">Algorithm for range query.</h5>

    <p class="text-gray-300">To query for some range <span class="math">[s,t]</span>, let <span class="math">T_{0},T_{1},\\ldots,T_{L}</span> be the search structures in memory. For <span class="math">i\\in[0,1,\\ldots,L]</span>, call <span class="math">\\mathsf{Query}(T_{i},[s,t])</span>. Now, concatenate all these outcomes as well as <span class="math">\\mathsf{Buf}</span>, and copy the concatenated result to a designated location in memory. To further speed up the query, we can maintain the interior points of all active levels in the hierarchical data structure in a single binary search tree.</p>

    <h6 id="sec-92" class="text-base font-medium mt-4">Theorem 6.3 (Differential obliviousness).</h6>

    <p class="text-gray-300">Let <span class="math">N</span> be the total number of insertion operations over time, let <span class="math">\\epsilon=O(1)</span>, and suppose that the universe of key satisfies <span class="math">\\log^{*}U\\leq\\log\\log\\lambda</span>. Then, there exists a negligible function <span class="math">\\delta(\\cdot)</span> such that the above scheme satisfies adaptive <span class="math">(4\\epsilon\\log N,\\delta)</span>-differential obliviousness. Furthermore, the above scheme achieves perfect correctness.</p>

    <h6 id="sec-93" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">We first prove perfect correctness. In Lemma A.7 in the appendices, we show that ThreshBins 0-obliviously realize <span class="math">\\mathcal{F}_{\\text{thresbins}}</span>, which implies that ThreshBins always enter input elements in the bins according to its bin load <span class="math">R</span> correctly. Also in the proof of Lemma A.7, we show that MergeThreshBins always produce correct merged results. Therefore, the only errror is when the bin load <span class="math">R</span> is too small to store all input elements, which happens with probability at most <span class="math">O(\\exp(-\\Theta(\\log^{2}\\lambda)))</span> by Lemma C.4. When this happens, the scheme detects the error and outputs a correct non-private answer. Therefore, the scheme achieves perfect correctness at the cost of increasing the privacy error by <span class="math">O(\\exp(-\\Theta(\\log^{2}\\lambda)))</span>.</p>

    <p class="text-gray-300">Now, differential obliviousness follows in a straightforward manner by adaptive <span class="math">\\log N</span>-fold composition of differential privacy <em>[16]</em>, by observing that every element is involved in only <span class="math">\\log N</span> instances of ThreshBins and MergeThreshBins (and also account for the above <span class="math">O(\\exp(-\\Theta(\\log^{2}\\lambda)))</span> error). For adaptive security, notice that the adaptive composition theorem works for adaptively generated database entries as well as adaptive queries <em>[16]</em>. ∎</p>

    <h6 id="sec-94" class="text-base font-medium mt-4">Theorem 6.4 (Performance).</h6>

    <p class="text-gray-300">Let <span class="math">N=\\mathsf{poly}(\\lambda)</span> be the total number of insertion operations over time where <span class="math">\\mathsf{poly}(\\cdot)</span> is some fixed polynomial. The above range query data structure achieves the following performance:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Each insertion operation consumes amortized <span class="math">O(\\log N\\log\\log N)</span> runtime;</li>

      <li>Each range query whose result set contains <span class="math">L</span> records consumes <span class="math">O(Z\\log N+L)</span> runtime (and number of accesses) and accesses only <span class="math">O(\\log N)</span> discontiguous regions in memory no matter how large <span class="math">L</span> is, i.e., the locality is independent of the number of matching records <span class="math">L</span>.</li>

    </ul>

    <h6 id="sec-95" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">The insertion cost is dominated by the cost for merging the search structures. In our construction <span class="math">T_{i}</span> and <span class="math">\\widehat{T}_{i}</span> contain <span class="math">Z\\cdot 2^{i}</span> real elements and every <span class="math">2^{i}/Z</span> operations, we must merge <span class="math">T_{i}</span> and <span class="math">\\widehat{T}_{i}</span> once incurring <span class="math">Z2^{i}\\log\\log\\lambda</span> time. It is easy to see that the total amortized cost is <span class="math">O(\\log N\\log\\log\\lambda)</span> — note that <span class="math">\\log N=O(\\lambda)</span> assuming <span class="math">N=\\mathsf{poly}(\\lambda)</span>. The runtime and locality claims for each range query follow in a straighforward manner by observing that we can build a single, standard binary search tree data structure (called the index tree) to store all the interior points of all currently active search structures, where leaves are stored from small to large in a consecutive memory region. During insertion, a level containing <span class="math">n=2^{i}Z</span> elements has only <span class="math">O(2^{i})</span> interior points, and thus inserting or deleting all of them from the index tree takes <span class="math">o(n)</span> time. For query, it takes at most <span class="math">O(\\log N+L/Z)</span> accesses into the index tree to identify all the bins that match the query; and the number of discontiguous regions accessed when searching this index tree is upper bounded by <span class="math">O(\\log N)</span>. ∎</p>

    <p class="text-gray-300">We stress that even absent privacy requirements, one of the best known approaches to build a range query data structure is through a binary search tree where each insertion costs <span class="math">O(\\log N)</span> and each query matching <span class="math">L</span> records costs <span class="math">O(\\log N+L)</span> and requires accessing <span class="math">O(\\log N)</span> discontiguous memory regions. In comparison, our solution achieves differential obliviousness almost for free in many cases: each insertion incurs only a <span class="math">O(\\log\\log N)</span> blowup, and each query incurs no asymptotical blowup if there are at least polylogarithmically many matching records and the locality loss is also <span class="math">O(1)</span>.</p>

    <p class="text-gray-300">##</p>

    <p class="text-gray-300">6.5 Applications in the Designated-Client and Public-Client Settings</p>

    <p class="text-gray-300">In a designated client setting, the data owner who performs insertions is simultaneously the querier. In this case, all records can be encrypted by the data owner’s private key. In a public client setting, the data owner performs insertions of records, whereas queries are performed by other third parties. In this case, the data owner can encrypt the data records using Attribute-Based Encryption (ABE) <em>[25, 40]</em>, and then it can issue policy-binding decryption keys to third parties to permit them to query and decrypt records that satisfy the policy predicates. In either case, we stress that our scheme can support queries non-interactively. In particular, the differentially private interior points can be released in the clear to the server, and the server can simply find the matching bins on behalf of the client and return all relevant bins to the client in a single round-trip.</p>

    <p class="text-gray-300">We stress that if the notion of obliviousness were required (say, we would like that any two operational sequences that are query-consistent and length-consistent be indistinguishable in access patterns), then we are not aware of any existing solution that simultaneously achieves statistical security, non-interactiveness, and non-trivial efficiency, even for the designated-client setting. One interesting point of comparison is ORAMs <em>[42, 46]</em> and oblivious data structures <em>[47]</em> which can achieve statistical security, but 1) they work only for the designated-client setting but not the public-client setting; 2) in general they incur logarithmically many rounds and <span class="math">O(L\\log^{2}N)</span> cost per query (absent large block-size assumptions); and 3) except for the recent work of Asharov et al. <em>[4]</em> which incurs polylogarithmic locality blowup regardless of <span class="math">L</span>, all other known solutions would suffer from (super-)linear in <span class="math">L</span> locality blowup.</p>

    <h3 id="sec-96" class="text-xl font-semibold mt-8">6.6 Lower Bounds for Differentially Oblivious Data Structures</h3>

    <p class="text-gray-300">For lower bounds, we first focus on point queries — a special case of the range queries considered in our upper bounds.</p>

    <h5 id="sec-97" class="text-base font-semibold mt-4">Non-private baseline.</h5>

    <p class="text-gray-300">To put our results in perspective and clearly illustrate the cost of privacy, we first point out that absent any privacy requirements, we can build a data structure that support point queries (in the balls-and-bins model) such that except with negligible probability, each insertion completes in <span class="math">O(1)</span> time; each point query completes in <span class="math">O(L)</span> time and accessing only <span class="math">O(1)</span> discontiguous memory regions where <span class="math">L</span> is the number of matching records <em>[22]</em>.</p>

    <h5 id="sec-98" class="text-base font-semibold mt-4">Limits of differential oblivious data structures.</h5>

    <p class="text-gray-300">We now prove lower bounds showing that assuming <span class="math">\\epsilon=O(1)</span>, if one desires sub-exponentially small <span class="math">\\delta</span>, then any <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious data structure must on some sequences of length <span class="math">N</span>, incur at least <span class="math">\\Omega(N\\log N)</span> ball movements. We prove lower bounds for the case of distinct keys and repeated keys separately: in the former case, each key has multiplicity <span class="math">1</span> and upon query only <span class="math">1</span> record is returned; in the latter, each key has more general multiplicity.</p>

    <h6 id="sec-99" class="text-base font-medium mt-4">Theorem 6.5 (Limits of <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious data structures: distinct keys).</h6>

    <p class="text-gray-300">Suppose that <span class="math">N=\\mathsf{poly}(\\lambda)</span> for some fixed polynomial <span class="math">\\mathsf{poly}(\\cdot)</span> and <span class="math">0&lt;s\\leq\\sqrt{n}</span> are integers. Let <span class="math">\\epsilon&gt;0</span>, <span class="math">0&lt;\\beta&lt;1</span> and <span class="math">0\\leq\\delta\\leq\\beta\\cdot\\frac{\\epsilon}{N}\\cdot e^{-\\epsilon s}</span>. Suppose that <span class="math">\\mathbb{DS}</span> is a perfectly correct and <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious data structure supporting point queries.</p>

    <p class="text-gray-300">Then, there exists an operational sequence with <span class="math">N</span> insertion and <span class="math">N</span> query operations interleaved together, where the <span class="math">N</span> keys inserted are distinct and are from the domain <span class="math">\\{0,1,\\ldots,N\\}</span> such that the total number of accesses <span class="math">\\mathbb{DS}</span> makes for serving this sequence is <span class="math">\\Omega(N\\log s)</span> with probability at least <span class="math">1-\\beta</span>.</p>

    <h6 id="sec-100" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">Define <span class="math">T:=\\lfloor\\frac{N}{s}\\rfloor</span>. For <span class="math">1\\leq i\\leq T</span>, define the sub-domain <span class="math">X_{i}:=\\{(i-1)s+j:0\\leq j&lt;s\\}</span> of keys. Each of the operational sequences we consider in the lower bound can be partitioned into <span class="math">T</span> epochs. For <span class="math">1\\leq i\\leq T</span>, the <span class="math">i</span>-th epoch consists of the following operations:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">s</span> insertion operations: the <span class="math">s</span> keys in <span class="math">X_{i}</span> are inserted one by one. The order in which the keys in <span class="math">X_{i}</span> are inserted is private. In this lower bound, it suffices to consider <span class="math">s</span> cyclic shifts of the keys in <span class="math">X_{i}</span>.</li>

      <li><span class="math">s</span> query operations: this is done in the (publicly-known) increasing order of keys in <span class="math">X_{i}</span>.</li>

    </ol>

    <p class="text-gray-300">Observe that the keys involved between different epochs are disjoint. It suffices to prove that the number of memory accesses made in each epoch is at least <span class="math">\\Omega(s\\log s)</span> with probability at least <span class="math">1-\\frac{\\beta}{T}</span>; by the union bound, this immediately implies the result.</p>

    <p class="text-gray-300">Fix some epoch <span class="math">i</span>, and consider the <span class="math">s</span> different cyclic shift orders of <span class="math">X_{i}</span> in which the keys are inserted. For <span class="math">0\\leq j&lt;s</span>, let <span class="math">I_{j}</span> be the input scenario where ordering of the keys in <span class="math">X_{i}</span> is shifted with offset <span class="math">j</span>.</p>

    <p class="text-gray-300">Observe that if we only change the insertion operations in epoch <span class="math">i</span> and keep all operations in other epochs unchanged, we have input scenarios that are <span class="math">s</span>-neighbors. Therefore, by Lemma 4.3, with probability at least <span class="math">1-\\eta</span> (where <span class="math">\\eta:=s\\cdot\\frac{e^{r\\cdot s}-1}{e^{s}-1}\\cdot\\delta\\leq\\beta\\cdot\\frac{s}{N}</span>), the input scenario <span class="math">I_{0}</span> in epoch <span class="math">i</span> produces an access pattern <span class="math">A</span> that is plausible for <span class="math">I_{j}</span> for all <span class="math">1\\leq j&lt;s</span>. Let <span class="math">G</span> be the compact graph (defined Section 4.1.2) corresponding to <span class="math">A</span>.</p>

    <p class="text-gray-300">Since we know that in every input scenario <span class="math">I_{j}</span>, each key in <span class="math">X_{i}</span> is inserted exactly once, we can assume that the <span class="math">s</span> insertions in epoch <span class="math">i</span> correspond to some memory locations <span class="math">\\mathsf{Input}[0..s-1]</span>. Even though the result of each query can contain dummy elements, because we know that exactly one of the returned elements must be a real element, in this case, by a final linear scan on the returned elements, we can assume that the <span class="math">s</span> queries correspond to some memory locations <span class="math">\\mathsf{Output}[0..s-1]</span>, where <span class="math">\\mathsf{Output}[k]</span> is supposed to return the element with key <span class="math">(i-1)s+k</span>.</p>

    <p class="text-gray-300">Moreover, observe that for <span class="math">0\\leq j&lt;s</span>, in scenario <span class="math">I_{j}</span>, the element inserted at <span class="math">\\mathsf{Input}[k]</span> is supposed to be returned at <span class="math">\\mathsf{Output}[k+j]</span> (where addition <span class="math">j+i</span> is performed modulo <span class="math">s</span>) during qeury.</p>

    <p class="text-gray-300">Observe that an access pattern <span class="math">A</span> is plausible for <span class="math">I_{j}</span> implies that <span class="math">G</span> contains <span class="math">s</span> vertex-disjoint paths, where for <span class="math">0\\leq k&lt;s</span>, there is such a path from the node corresponding to the initial memory location <span class="math">\\mathsf{Input}[k]</span> to the node corresponding to the final memory location <span class="math">\\mathsf{Output}[k+j]</span>.</p>

    <p class="text-gray-300">Then, Fact 4.5 implies that if <span class="math">G</span> is the compact graph of an access pattern <span class="math">A</span> that is plausible for all <span class="math">I_{j}</span>’s, then <span class="math">G</span> has at least <span class="math">\\Omega(s\\log s)</span> edges. Hence, Lemma 4.4 implies that the access pattern <span class="math">A</span> makes at least <span class="math">\\Omega(s\\log s)</span> memory accesses. This completes the lower bound proof for the number of memory accesses in one epoch, which, as mentioned above, implies the required result. ∎</p>

    <p class="text-gray-300">The following theorem is a generalization of the earlier Theorem 6.5 where each key is allowed to have multiplicity.</p>

    <h6 id="sec-101" class="text-base font-medium mt-4">Theorem 6.6 (Limits of <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious data structures: repeated keys).</h6>

    <p class="text-gray-300">Suppose that <span class="math">N=\\mathsf{poly}(\\lambda)</span> for some fixed polynomial <span class="math">\\mathsf{poly}(\\cdot)</span>. Let the integers <span class="math">r&lt;s\\leq\\sqrt{N}</span> be such that <span class="math">r</span> divides <span class="math">s</span>; furthermore, let <span class="math">\\epsilon&gt;0</span>, <span class="math">0&lt;\\beta&lt;1</span> and <span class="math">0\\leq\\delta\\leq\\beta\\cdot\\frac{\\epsilon}{N}\\cdot e^{-\\epsilon s}</span>. Suppose that <span class="math">\\mathbb{DS}</span> is a perfectly correct and <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious data structure supporting point queries.</p>

    <p class="text-gray-300">Then, there exists an operational sequence with <span class="math">N</span> insertion and <span class="math">\\frac{N}{r}</span> query operations interleaved together, where each of <span class="math">\\frac{N}{r}</span> distinct keys from the domain <span class="math">\\{0,1,\\ldots,\\frac{N}{r}-1\\}</span> is inserted <span class="math">r</span> times, such that the total number of accesses <span class="math">\\mathbb{DS}</span> makes for serving this sequence is <span class="math">\\Omega(N\\log\\frac{s}{r})</span> with probability at least <span class="math">1-\\beta</span>.</p>

    <p class="text-gray-300">Proof.</p>

    <p class="text-gray-300">The proof structure follows that of Theorem 6.5, in which there are <span class="math">T:=\\lfloor\\frac{N}{s}\\rfloor</span> epochs. For <span class="math">1\\leq i\\leq T</span>, the <span class="math">i</span>-th epoch is defined as follows:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">s</span> insertion operations: the <span class="math">s</span> keys are from the sub-domain <span class="math">X_{i}:=\\{\\frac{s}{r}\\cdot(i-1)+j:0\\leq j&lt;\\frac{s}{r}\\}</span>, where each distinct key is inserted <span class="math">r</span> times in a batch. The order in which the distinct keys in <span class="math">X_{i}</span> are batch-inserted is private. In this lower bound, we consider <span class="math">\\frac{s}{r}</span> different cyclic shifts of the <span class="math">\\frac{s}{r}</span> batches.</li>

      <li><span class="math">\\frac{s}{r}</span> query operations: this is done in the (publicly-known) increasing order of keys in <span class="math">X_{i}</span>, where each query should return <span class="math">r</span> repeated keys.</li>

    </ol>

    <p class="text-gray-300">As in Theorem 6.5, since the keys involved between different epochs are disjoint, it suffices to prove that the number of memory accesses made in each epoch is at least <span class="math">\\Omega(s\\log\\frac{s}{r})</span> with probability at least <span class="math">1-\\frac{\\beta}{T}</span>; this immediately implies the result by the union bound over all epochs.</p>

    <p class="text-gray-300">Fix some epoch <span class="math">i</span> and observe that if we only change the insertion operations in epoch <span class="math">i</span> and keep all operations in other epochs unchanged, we have input scenarios that are <span class="math">s</span>-neighbors.</p>

    <p class="text-gray-300">We consider the <span class="math">\\frac{s}{r}</span> different cyclic shift orders of <span class="math">X_{i}</span> in which the keys are inserted. For <span class="math">0\\leq j&lt;\\frac{s}{r}</span>, let <span class="math">I_{j}</span> be the input scenario where ordering of the keys in <span class="math">X_{i}</span> is shifted with offset <span class="math">j</span>. Therefore, by Lemma 4.3, with probability at least <span class="math">1-\\eta</span> (where <span class="math">\\eta:=s\\cdot\\frac{e^{r\\cdot s}-1}{e^{s}-1}\\cdot\\delta\\leq\\beta\\cdot\\frac{s}{N}</span>), the input scenario <span class="math">I_{0}</span> in epoch <span class="math">i</span> produces an access pattern <span class="math">A</span> that is plausible for <span class="math">I_{j}</span> for all <span class="math">1\\leq j&lt;\\frac{s}{r}</span>. Let <span class="math">G</span> be the compact graph (defined Section 4.1.2) corresponding to <span class="math">A</span>.</p>

    <p class="text-gray-300">Since we know that in every input scenario <span class="math">I_{j}</span>, each of the <span class="math">\\frac{s}{r}</span> keys in <span class="math">X_{i}</span> is inserted exactly <span class="math">r</span> times, we can assume that the <span class="math">s</span> insertions in epoch <span class="math">i</span> correspond to some memory locations <span class="math">\\mathsf{Input}[0..s-1]</span>.</p>

    <p class="text-gray-300">Moreover, each of the <span class="math">\\frac{s}{r}</span> queries returns exactly <span class="math">r</span> records with the same key, maybe together with some dummy elements. Hence, for <span class="math">0\\leq k&lt;\\frac{s}{r}</span>, we can assume that the result of the query for the key <span class="math">(i-1)\\frac{s}{r}+k</span> is returned in some array <span class="math">\\mathsf{Output}_{k}</span>, whose length is at least <span class="math">r</span> (and can contain dummy elements).</p>

    <p class="text-gray-300">Moreover, observe that for <span class="math">0\\leq j&lt;\\frac{s}{r}</span>, in scenario <span class="math">I_{j}</span>, the element inserted at <span class="math">\\mathsf{Input}[\\ell]</span> is supposed to be returned in the array <span class="math">\\mathsf{Output}_{k}</span>, where <span class="math">k=\\lfloor\\frac{\\ell}{r}\\rfloor+j\\mod\\frac{s}{r}</span>. The important point is that the element in <span class="math">\\mathsf{Input}[\\ell]</span> will be returned at different locations in different scenarios <span class="math">I_{j}</span>’s.</p>

    <p class="text-gray-300">Observe that an access pattern <span class="math">A</span> is plausible for <span class="math">I_{j}</span> implies that <span class="math">G</span> contains <span class="math">s</span> vertex-disjoint paths, where for <span class="math">0\\leq\\ell&lt;s</span>, there is such a path from the node corresponding to the initial memory location <span class="math">\\mathsf{Input}[\\ell]</span> to the node corresponding to some final memory location inside the array <span class="math">\\mathsf{Output}_{k}</span>, where <span class="math">k=\\lfloor\\frac{\\ell}{r}\\rfloor+j\\mod\\frac{s}{r}</span>.</p>

    <p class="text-gray-300">Then, Fact 4.5 implies that if <span class="math">G</span> is the compact graph of an access pattern <span class="math">A</span> that is plausible for all <span class="math">I_{j}</span>’s, then <span class="math">G</span> has at least <span class="math">\\Omega(s\\log\\frac{s}{r})</span> edges. Hence, Lemma 4.4 implies that the access pattern <span class="math">A</span> makes at least <span class="math">\\Omega(s\\log\\frac{s}{r})</span> memory accesses. Hence, it follows that with all but <span class="math">\\frac{\\beta}{T}</span> probability, epoch <span class="math">i</span> takes <span class="math">\\Omega(s\\log\\frac{s}{r})</span> memory accesses, as required.</p>

    <p class="text-gray-300">This completes the lower bound proof for the number of memory accesses in one epoch, which, as mentioned above, implies the required result. ∎</p>

    <h2 id="sec-102" class="text-2xl font-bold">Acknowledgments</h2>

    <p class="text-gray-300">Elaine Shi is extremely grateful to Dov Gordon for multiple helpful discussions about relaxing the notion of oblivious data accesses over the past several years, including back at Maryland and recently — these discussions partly inspired the present work. She is also grateful to Abhradeep Guha Thakurta for numerous discussions about differential privacy over the past many years including</p>

    <p class="text-gray-300">during the preparation of the present paper. We are grateful to Wei-Kai Lin and Tiancheng Xie for numerous inspiring discussions especially regarding the Pippenger-Valiant result <em>[39]</em>. We thank Kobbi Nissim, George Kellaris, and Rafael Pass for helpful discussions and suggestions. We thank Kartik Nayak and Paul Grubbs for helpful feedback that helped improve the paper.</p>

    <p class="text-gray-300">This work is supported in part by NSF grants CNS-1314857, CNS-1514261, CNS-1544613, CNS-1561209, CNS-1601879, CNS-1617676, an Office of Naval Research Young Investigator Program Award, a Packard Fellowship, a DARPA Safeware grant (subcontractor under IBM), a Sloan Fellowship, Google Faculty Research Awards, a Baidu Research Award, and a VMWare Research Award.</p>

    <h2 id="sec-103" class="text-2xl font-bold">References</h2>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[1] M. Ajtai, J. Komlós, and E. Szemerédi. An <span class="math">O(N\\log N)</span> sorting network. In STOC, 1983.</li>

      <li>[2] I. Anati, S. Gueron, S. P. Johnson, and V. R. Scarlata. Innovative technology for cpu based attestation and sealing. In HASP, 2013.</li>

      <li>[3] A. Andersson, T. Hagerup, S. Nilsson, and R. Raman. Sorting in linear time? J. Comput. Syst. Sci., 57(1):74–93, Aug. 1998.</li>

      <li>[4] G. Asharov, T.-H. H. Chan, K. Nayak, R. Pass, L. Ren, and E. Shi. Oblivious computation with data locality. Cryptology ePrint Archive 2017/772, 2017.</li>

      <li>[5] G. Asharov, M. Naor, G. Segev, and I. Shahaf. Searchable symmetric encryption: optimal locality in linear space via two-dimensional balanced allocations. In STOC, 2016.</li>

      <li>[6] E. Boyle, K. Chung, and R. Pass. Oblivious parallel RAM and applications. In TCC, 2016.</li>

      <li>[7] E. Boyle and M. Naor. Is there an oblivious RAM lower bound? In ITCS, 2016.</li>

      <li>[8] M. Bun, K. Nissim, U. Stemmer, and S. P. Vadhan. Differentially private release and learning of threshold functions. In FOCS, 2015.</li>

      <li>[9] D. Cash and S. Tessaro. The locality of searchable symmetric encryption. In Eurocrypt, 2014.</li>

      <li>[10] T.-H. H. Chan and E. Shi. Circuit OPRAM: Unifying statistically and computationally secure ORAMs and OPRAMs. In TCC, 2017.</li>

      <li>[11] T.-H. H. Chan, E. Shi, and D. Song. Private and continual release of statistics. In ICALP, 2010.</li>

      <li>[12] T.-H. H. Chan, E. Shi, and D. Song. Private and continual release of statistics. TISSEC, 2011.</li>

      <li>[13] T.-H. H. Chan, E. Shi, and D. Song. Privacy-preserving stream aggregation with fault tolerance. In FC, 2012.</li>

      <li>[14] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography Conference (TCC), 2006.</li>

      <li>[15] C. Dwork, M. Naor, T. Pitassi, and G. N. Rothblum. Differential privacy under continual observation. In STOC, 2010.</li>

    </ul>

    <p class="text-gray-300">[16] C. Dwork, G. N. Rothblum, and S. P. Vadhan. Boosting and differential privacy. In FOCS, pages 51–60. IEEE Computer Society, 2010.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[17] D. Eppstein, M. T. Goodrich, and R. Tamassia. Privacy-preserving data-oblivious geometric algorithms for geographic data. In GIS, pages 13–22, 2010.</li>

      <li>[18] O. Goldreich. Towards a theory of software protection and simulation by oblivious RAMs. In ACM Symposium on Theory of Computing (STOC), 1987.</li>

      <li>[19] O. Goldreich, S. Micali, and A. Wigderson. How to play any mental game. In ACM symposium on Theory of computing (STOC), 1987.</li>

      <li>[20] O. Goldreich and R. Ostrovsky. Software protection and simulation on oblivious RAMs. J. ACM, 1996.</li>

      <li>[21] M. T. Goodrich. Zig-zag sort: A simple deterministic data-oblivious sorting algorithm running in o(n log n) time. In STOC, 2014.</li>

      <li>[22] M. T. Goodrich, D. S. Hirschberg, M. Mitzenmacher, and J. Thaler. Cache-oblivious dictionaries and multimaps with negligible failure probability. In Design and Analysis of Algorithms - MedAlg, 2012.</li>

      <li>[23] M. T. Goodrich and M. Mitzenmacher. Privacy-preserving access of outsourced data via oblivious RAM simulation. In ICALP, 2011.</li>

      <li>[24] M. T. Goodrich, O. Ohrimenko, and R. Tamassia. Data-oblivious graph drawing model and algorithms. CoRR, abs/1209.0756, 2012.</li>

      <li>[25] S. Gorbunov, V. Vaikuntanathan, and H. Wee. Attribute-based encryption for circuits. In submission to STOC 2013.</li>

      <li>[26] Y. Han. Deterministic sorting in o(<em>n</em>loglog<em>n</em>) time and linear space. J. Algorithms, 50(1):96–105, 2004.</li>

      <li>[27] Y. Han and M. Thorup. Integer sorting in 0(n sqrt (log log n)) expected time and linear space. In FOCS, 2002.</li>

      <li>[28] G. Kellaris, G. Kollios, K. Nissim, and A. O’Neill. Generic attacks on secure outsourced databases. In CCS, 2016.</li>

      <li>[29] G. Kellaris, G. Kollios, K. Nissim, and A. O’Neill. Accessing data while preserving privacy. CoRR, abs/1706.01552, 2017.</li>

      <li>[30] M. Keller and P. Scholl. Efficient, oblivious data structures for mpc. In Asiacrypt, 2014.</li>

      <li>[31] D. G. Kirkpatrick and S. Reisch. Upper bounds for sorting integers on random access machines. Technical report, 1981.</li>

      <li>[32] D. E. Knuth. The Art of Computer Programming, Volume 3: (2Nd Ed.) Sorting and Searching. 1998.</li>

      <li>[33] W.-K. Lin, E. Shi, and T. Xie. Can we overcome the <span class="math">n\\log n</span> barrier for oblivious sort? Manuscript, 2017.</li>

    </ul>

    <p class="text-gray-300">[34] C. Liu, X. S. Wang, K. Nayak, Y. Huang, and E. Shi. ObliVM: A programming framework for secure computation. In S&P, 2015.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[35] S. Mazloom and S. D. Gordon. Differentially private access patterns in secure computation. Cryptology ePrint Archive, Report 2017/1016, 2017. https://eprint.iacr.org/2017/1016.</li>

      <li>[36] F. McKeen, I. Alexandrovich, A. Berenzon, C. V. Rozas, H. Shafi, V. Shanbhogue, and U. R. Savagaonkar. Innovative instructions and software model for isolated execution. HASP, 13:10, 2013.</li>

      <li>[37] J. C. Mitchell and J. Zimmerman. Data-oblivious data structures. In STACS, pages 554–565, 2014.</li>

      <li>[38] K. Nayak, X. S. Wang, S. Ioannidis, U. Weinsberg, N. Taft, and E. Shi. GraphSC: Parallel Secure Computation Made Easy. In IEEE S & P, 2015.</li>

      <li>[39] N. Pippenger and L. G. Valiant. Shifting graphs and their applications. J. ACM, 23(3):423–432, July 1976.</li>

      <li>[40] A. Sahai and B. Waters. Fuzzy identity-based encryption. In EUROCRYPT, 2005.</li>

      <li>[41] E. Shi, T.-H. H. Chan, E. Stefanov, and M. Li. Oblivious RAM with <span class="math">O((\\log N)^{3})</span> worst-case cost. In ASIACRYPT, pages 197–214, 2011.</li>

      <li>[42] E. Stefanov, M. van Dijk, E. Shi, C. Fletcher, L. Ren, X. Yu, and S. Devadas. Path ORAM – an extremely simple oblivious ram protocol. In CCS, 2013.</li>

      <li>[43] M. Thorup. Randomized sorting in <span class="math">o(nloglogn)</span> time and linear space using addition, shift, and bit-wise boolean operations. J. Algorithms, 42(2):205–230, 2002.</li>

      <li>[44] S. Vahdan. The Complexity of Differential Privacy.</li>

      <li>[45] S. Wagh, P. Cuff, and P. Mittal. Root ORAM: A tunable differentially private oblivious RAM. CoRR, abs/1601.03378, 2016.</li>

      <li>[46] X. S. Wang, T.-H. H. Chan, and E. Shi. Circuit ORAM: On Tightness of the Goldreich-Ostrovsky Lower Bound. In CCS, 2015.</li>

      <li>[47] X. S. Wang, K. Nayak, C. Liu, T.-H. H. Chan, E. Shi, E. Stefanov, and Y. Huang. Oblivious data structures. In CCS, 2014.</li>

      <li>[48] A. C.-C. Yao. How to generate and exchange secrets. In FOCS, 1986.</li>

    </ul>

    <p class="text-gray-300">Appendices</p>

    <h2 id="sec-104" class="text-2xl font-bold">Appendix A Additional Details and Analysis of the Merging Algorithm</h2>

    <p class="text-gray-300">This section is devoted to 1) proving Theorem 5.3. We start by introducing the notion of oblivious realization of an ideal functionality with (differentially private) leakage; and 2) supplying additional details of our differentially oblivious merging algorithm, particularly, details of the oblivious interior point mechanism.</p>

    <h3 id="sec-105" class="text-xl font-semibold mt-8">A.1 Oblivious Realization of Ideal Functionalities with Differentially Private Leakage</h3>

    <h6 id="sec-106" class="text-base font-medium mt-4">Definition A.1.</h6>

    <p class="text-gray-300">Given a (possibly randomized) functionality <span class="math">\\mathcal{F}</span>, we say that some (possibly randomized) algorithm <span class="math">\\mathsf{Alg}</span> <span class="math">\\delta</span>-obliviously realizes <span class="math">\\mathcal{F}</span> with leakage <span class="math">\\mathcal{L}</span>, if there exists a simulator <span class="math">\\mathsf{Sim}</span> (that produces simulated access pattern) such that for any <span class="math">\\lambda</span>, for any input <span class="math">I</span>, define the following executions:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Ideal execution: choose all random bits <span class="math">\\rho</span> needed by <span class="math">\\mathcal{F}</span>, and let <span class="math">O_{\\mathrm{ideal}}\\leftarrow\\mathcal{F}(\\lambda,I,\\rho)</span>, let <span class="math">L_{\\mathrm{ideal}}\\leftarrow\\mathcal{L}(\\lambda,I,\\rho)</span>. Note that the leakage function <span class="math">\\mathcal{L}</span> also obtains the same randomness as <span class="math">\\mathcal{F}</span>, and may use additional internal randomness.</li>

      <li>Real execution: let <span class="math">(O_{\\mathrm{real}},L_{\\mathrm{real}},\\mathsf{addresses})\\leftarrow\\mathsf{Alg}(\\lambda,I)</span>.</li>

    </ul>

    <p class="text-gray-300">Then, it must hold that the following distributions are <span class="math">\\delta(\\lambda)</span>-statistically close, i.e., their statistical distance is at most <span class="math">\\delta(\\lambda)</span>:</p>

    <p class="text-gray-300"><span class="math">(O_{\\mathrm{ideal}},L_{\\mathrm{ideal}},\\mathsf{Sim}(\\lambda,L_{\\mathrm{ideal}}))^{\\ \\ \\delta(\\lambda)}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (O_{\\mathrm{real}},L_{\\mathrm{real}},\\mathsf{addresses}).</span></p>

    <h6 id="sec-107" class="text-base font-medium mt-4">Definition A.2.</h6>

    <p class="text-gray-300">We say that the leakage function <span class="math">\\mathcal{L}</span> is <span class="math">(\\epsilon,\\delta)</span>-differentially private (with respect to the input) iff for every <span class="math">\\lambda</span>, for every neighboring inputs <span class="math">I</span> and <span class="math">I^{\\prime}</span> and every set <span class="math">S</span>, it holds that</p>

    <p class="text-gray-300"><span class="math">\\Pr_{\\rho,\\mathcal{L}}[\\mathcal{L}(\\lambda,I,\\rho)\\in S]\\leq e^{\\epsilon}\\Pr_{\\rho,\\mathcal{L}}[\\mathcal{L}(\\lambda,I^{\\prime},\\rho)\\in S]+\\delta</span></p>

    <p class="text-gray-300">In the above, the notation <span class="math">\\Pr_{\\rho,\\mathcal{L}}</span> means that the randomness comes from the random choice of <span class="math">\\rho</span> as well as the internal coins of <span class="math">\\mathcal{L}</span>.</p>

    <h6 id="sec-108" class="text-base font-medium mt-4">Definition A.3.</h6>

    <p class="text-gray-300">Consider some special leakage function <span class="math">\\mathcal{L}</span> that is fully determined by the output of <span class="math">\\mathcal{F}</span>, i.e., <span class="math">\\mathcal{L}(\\lambda,I,\\rho):=\\mathcal{L}(T)</span> where <span class="math">T:=\\mathcal{F}(\\lambda,I,\\rho)</span>. We say that <span class="math">\\mathcal{L}</span> is <span class="math">(\\epsilon,\\delta)</span>-differentially private with respect to the output of <span class="math">\\mathcal{F}</span> (or <span class="math">(\\epsilon,\\delta)</span>-differentially private with respect to <span class="math">T</span>), iff for every <span class="math">\\lambda</span>, for every neighboring <span class="math">T</span> and <span class="math">T^{\\prime}</span> and every set <span class="math">S</span>, it holds that <span class="math">\\Pr[\\mathcal{L}(T)\\in S]\\leq e^{\\epsilon}\\Pr[\\mathcal{L}(T^{\\prime})\\in S]+\\delta</span>, where the randomness in the probability comes from the random coins of <span class="math">\\mathcal{L}</span>.</p>

    <p class="text-gray-300">The following fact is immediate from the definition.</p>

    <h6 id="sec-109" class="text-base font-medium mt-4">Fact A.4.</h6>

    <p class="text-gray-300">If some algorithm <span class="math">\\mathsf{Alg}</span> obliviously realizes some functionality <span class="math">\\mathcal{F}</span> with leakage <span class="math">\\mathcal{L}</span>, where <span class="math">\\mathcal{L}</span> is <span class="math">(\\epsilon,\\delta)</span>-differentially private with respect to the input, then <span class="math">\\mathsf{Alg}</span> satisfies <span class="math">(\\epsilon,\\delta)</span>-differential obliviousness.</p>

    <h3 id="sec-110" class="text-xl font-semibold mt-8">A.2 Ideal <span class="math">\\mathcal{F}_{\\text{threshbins}}</span> Functionality</h3>

    <p class="text-gray-300">We describe a logical thresh-bin functionality <span class="math">\\mathcal{F}_{\\text{threshbins}}</span> that the ThreshBins subroutine obliviously realizes, and prove a lemma that formalize the main property that ThreshBins achieves.</p>

    <p class="text-gray-300">Given an input sorted array <span class="math">X</span> containing real elements (which can take a special key value <span class="math">\\infty</span>), a target bin number <span class="math">B</span>, and a parameter <span class="math">\\epsilon_{0}</span>, the ideal thresh-bin functionality <span class="math">\\mathcal{F}_{\\text{threshbins}}</span> outputs an ordered list of <span class="math">B</span> bins where each bin contains a random number of real elements padded with dummies to the bin’s capacity <span class="math">Z=\\frac{1}{\\epsilon_{0}}\\log^{8}\\lambda</span>; all real elements occur in sorted order. Moreover, each bin is tagged with an interior point. Furthermore, each bin is also tagged with an estimate of the cumulative sum, i.e., the number of real elements in the prefix up to and including this bin.</p>

    <p class="text-gray-300">If the input <span class="math">X</span> contains too many real elements, only a prefix of them may appear in the output bins; if the input <span class="math">X</span> contains too few elements, the functionality automatically appends elements with key <span class="math">\\infty</span> at the end such that there are enough elements to draw from the input. More concretely, the functionality <span class="math">\\mathcal{F}_{\\text{threshbins}}</span> is specified below:</p>

    <h6 id="sec-111" class="text-base font-medium mt-4">Assume:</h6>

    <p class="text-gray-300">The same setting as ThreshBins.</p>

    <p class="text-gray-300">Functionality:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>For <span class="math">i=1</span> to <span class="math">B</span>:</li>

    </ul>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Sample <span class="math">R_{i}\\text{</span>\\leftarrow<span class="math">}_{\\text{\\</span>}}$Geom^{Z}(\\exp(\\epsilon_{0})).</li>

      <li>Draw the next <span class="math">R_{i}</span> elements (denoted <span class="math">S_{i}</span>) from <span class="math">X</span>.</li>

      <li>Place these <span class="math">R_{i}</span> elements in order in a new bin and append with an appropriate number of dummies to reach the bin’s capacity <span class="math">Z</span>.</li>

    </ul>

    <p class="text-gray-300">Let <span class="math">T</span> denote the list of <span class="math">B</span> bins in order, and <span class="math">R=(R_{1},\\ldots,R_{B})</span> be the bin load vector.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Call <span class="math">D:=\\text{\\$$\\text{\\</span>}\\text{refixSum}}(\\lambda,C,\\frac{\\epsilon_{0}}{4})\\in Z_{+}^{B}<span class="math">, which is the </span>\\frac{\\epsilon_{0}}{4}<span class="math">-differentially private subroutine in Theorem 3.1 that privately estimates prefix sums, each of which has additive error at most </span>s<span class="math"> with all but </span>\\exp(-\\Theta(\\log^{2}\\lambda))<span class="math"> probability. We tag each bin with its estimated prefix sum from vector </span>D$.</li>

      <li>Moreover, we use the <span class="math">(\\frac{\\epsilon_{0}}{4},\\delta)</span>-differentially oblivious interior point mechanism in Section 5.3 to tag each bin with an interior point, denoted by a vector <span class="math">P=(P_{1},\\ldots,P_{B})</span>, where <span class="math">\\delta:=\\frac{1}{4}\\exp(-0.1\\log^{2}\\lambda)</span>;</li>

      <li>Output the thresh-bins <span class="math">T</span> (which is tagged with the interior points <span class="math">P</span> and the estimated prefix sums <span class="math">D</span>).</li>

    </ul>

    <p class="text-gray-300">The following lemma states that for <span class="math">\\mathcal{F}_{\\text{threshbins}}</span>, if a leakage function <span class="math">\\mathcal{L}</span> is differentially private with respect to the output <span class="math">T</span>, then <span class="math">\\mathcal{L}</span> is also differentially private with respect to the input <span class="math">X</span>. Here, two thresh-bins <span class="math">T^{0}</span> and <span class="math">T^{1}</span> are neighboring if they have the same number of bins, which, except for at most one pair of corresponding bins (from <span class="math">T^{0}</span> and <span class="math">T^{1}</span>), contain exactly the same elements; for the pair of bins that may differ, their symmetric difference contains only one element.</p>

    <h6 id="sec-112" class="text-base font-medium mt-4">Lemma A.5.</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Consider the ideal thresh-bins functionality <span class="math">\\mathcal{F}_{\\text{threshbins}}</span> and a leakage function <span class="math">\\mathcal{L}(\\lambda,T,\\epsilon_{0})</span>. If the input satisfies $B\\geq\\lceil\\frac{2</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">X</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}{Z}\\cdot(1+\\frac{2}{\\log^{2}\\lambda})\\rceil<span class="math"> and the leakage function </span>\\mathcal{L}<span class="math"> is </span>(\\epsilon,\\delta)<span class="math">-differentially private with respect to the output </span>T<span class="math">, then </span>\\mathcal{L}<span class="math"> is </span>(2\\epsilon_{0}+4\\epsilon,\\delta_{\\text{\\<span class="math-block">\\text{\\<span class="math">}\\text{bad}}</span>}+4\\delta)<span class="math">-differentially private with respect to </span>X<span class="math">, where </span>\\delta_{\\text{\\</span>\\text{\\<span class="math">}\\text{bad}}</span>}\\leq O(\\exp(-\\Theta(\\log^{2}\\lambda)))$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">######</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">To prove Lemma A.5, we start with some notations. Given an input array <span class="math">X</span> and a bin load vector <span class="math">R = (R_{1},\\ldots ,R_{B})\\in [Z]^{B}</span>, we let <span class="math">T(X,R)</span> denote the resulting thresh-bins. We say two thresh-bins <span class="math">T,T^{\\prime}</span> are <span class="math">k</span>-neighboring if there exists <span class="math">T_{1} = T,T_{2},\\dots ,T_{k},T_{k + 1} = T^{\\prime}</span> such that <span class="math">T_{i},T_{i + 1}</span> are neighboring. We partition the domain <span class="math">[Z]^B</span> of the bin load vectors into <span class="math">\\mathsf{good}\\cup \\mathsf{bad}</span>, where $\\mathsf{good} = \\{R:\\sum_{i = 1}^{B - 1}R_i\\geq</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">X</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\wedge \\forall i:0 &lt; R_i &lt; Z\\}<span class="math"> and </span>\\mathsf{bad} = [Z]^B\\backslash \\mathsf{good}<span class="math">. Let </span>\\delta_{\\mathsf{bad}}<span class="math"> be the probability that </span>R<span class="math"> is in </span>\\mathsf{bad}<span class="math"> when </span>R\\gets_{\\S}(\\mathsf{Geom}^Z (\\exp (\\epsilon_0)))^B<span class="math">. By Lemma C.4, </span>\\delta_{\\mathsf{bad}}\\leq \\exp (-\\log^2\\lambda)$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">We need the following technical lemma about the ideal thresh-bins functionality.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><strong>Lemma A.6.</strong> Consider two neighboring input arrays <span class="math">X^0, X^1</span> and parameter <span class="math">B</span> such that $B \\geq \\lceil \\frac{2</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">X_0</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}{Z} \\cdot (1 + \\frac{2}{\\log^2 \\lambda}) \\rceil<span class="math">. There exists an injective function </span>f: \\mathsf{good} \\to [Z]^B<span class="math"> such that the following holds. For every </span>R^0 \\in \\mathsf{good}<span class="math">, let </span>R^1 = f(R^0)<span class="math">, </span>T^0 = T(X^0, R^0)<span class="math">, and </span>T^1 = T(X^1, R^1)<span class="math">. We have (i) </span>\\operatorname<em>{Pr}[R^0] \\leq e^{2\\epsilon_0} \\operatorname</em>{Pr}[R^1]<span class="math"> where the probability is drawn from </span>(\\mathsf{Geom}^Z(\\exp(\\epsilon_0)))^B<span class="math">, and (ii) </span>T^0<span class="math"> and </span>T^1$ are 4- neighboring.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><strong>Proof.</strong> Recall that <span class="math">X^0, X^1</span> are neighboring means they have equal length and differ by one element. Thus, we can view <span class="math">X^1</span> as obtained by removing some <span class="math">x^0</span> from <span class="math">X^0</span> and then inserting some <span class="math">x^1</span> to it. Let <span class="math">X&#x27;^0</span> denote <span class="math">X^0 \\setminus \\{x^0\\}</span>. Let <span class="math">i</span> denote the location of <span class="math">x^0</span> in <span class="math">X^0</span>, and <span class="math">i&#x27;</span> denote the location of <span class="math">x^1</span> in <span class="math">X&#x27;^0</span>. We define <span class="math">f</span> in two corresponding steps.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>We first define <span class="math">f^0</span>. On input <span class="math">R^0</span>, let <span class="math">\\ell</span> denote the bin in <span class="math">T(X^0, R^0)</span> that contains <span class="math">x^0</span>. We define <span class="math">f^0(R^0) = R&#x27;^0</span> where <span class="math">R&#x27;^0</span> is identical to <span class="math">R^0</span> except that with the <span class="math">(\\ell + 1)</span>-st coordinate is decreased by 1, i.e., <span class="math">R_{\\ell + 1}^{\\prime 0} = R_{\\ell + 1}^0 - 1</span> and <span class="math">R_i^{\\prime 0} = R_i^0</span> for all <span class="math">i \\neq \\ell + 1</span>.</li>

      <li>We then define <span class="math">f^1</span>, which takes input <span class="math">R&#x27;^0</span>. Let <span class="math">\\ell&#x27;</span> denote the bin in <span class="math">T(X^0, R&#x27;^0)</span> that <span class="math">x^1</span> should be inserted in; if it is possible to insert <span class="math">x^1</span> into one of two neighboring bins, take <span class="math">\\ell&#x27;</span> to be the larger index of the two.</li>

    </ul>

    <p class="text-gray-300">We define <span class="math">f^{1}(R^{\\prime 0}) = R^{1}</span> where <span class="math">R^1</span> is identical to <span class="math">R^{\\prime 0}</span> except that with the <span class="math">(\\ell&#x27; + 1)</span>-st coordinate is increased by 1, i.e., <span class="math">R_{\\ell&#x27; + 1}^{1} = R_{\\ell&#x27; + 1}^{\\prime 0} + 1</span> and <span class="math">R_{i}^{1} = R_{i}^{\\prime 0}</span> for all <span class="math">i \\neq \\ell&#x27; + 1</span>.</p>

    <p class="text-gray-300">We define <span class="math">f = f^{1} \\circ f^{0}</span>. Note that by the definition of the good set, <span class="math">\\ell, \\ell&#x27; &amp;lt; B</span> so <span class="math">f</span> is well-defined.</p>

    <p class="text-gray-300">We now verify the properties of <span class="math">f</span>. For the injective property, let's first argue that <span class="math">f^0</span> is injective by showing that it is invertible. The key observation is that give <span class="math">X^0, X^1</span> and the output <span class="math">R&#x27;^0</span>, the bin <span class="math">\\ell</span> that <span class="math">x_0</span> belongs to is uniquely defined. Thus, we can compute <span class="math">(f^0)^{-1}(R&#x27;^0)</span> by increasing the <span class="math">\\ell + 1</span>-th coordinate by 1. The same argument shows that <span class="math">f^1</span> is injective, and hence <span class="math">f</span> is injective.</p>

    <p class="text-gray-300">The property that <span class="math">\\operatorname<em>{Pr}[R^0] \\leq e^{2\\epsilon_0} \\operatorname</em>{Pr}[R^1]</span> follows by the definition of truncated geometric and the fact that <span class="math">R^0</span> and <span class="math">R^1</span> only differ in two coordinates by 1. For property (ii), observe that <span class="math">T(X^0, R^0)</span> and <span class="math">T(X&#x27;^0, R&#x27;^0)</span> can only differ in the <span class="math">\\ell</span>-th and <span class="math">\\ell + 1</span>-st bins by at most one element for each bin, which means that <span class="math">T(X^0, R^0)</span> and <span class="math">T(X&#x27;^0, R&#x27;^0)</span> are 2- neighboring. Similarly, <span class="math">T(X&#x27;^0, R&#x27;^0)</span> and <span class="math">T(X^1, R^1)</span> are 2- neighboring by the same observation. Hence, <span class="math">T^0</span> and <span class="math">T^1</span> are 4- neighboring.</p>

    <p class="text-gray-300">With the above lemma, we are ready to prove Lemma A.5.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><strong>Proof of</strong> Lemma A.5. Consider two neighboring input arrays <span class="math">X^0, X^1</span> and parameter <span class="math">B</span> such that $B \\geq (4</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">X^0</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">/ Z) + 1<span class="math">. For </span>b \\in \\{0,1\\}<span class="math">, let </span>T^b \\gets \\mathcal{F}_{\\mathrm{threshbins}}(\\lambda, X^b, B, \\epsilon_0)<span class="math">, </span>L^b = \\gets \\mathcal{L}(\\lambda, T^b, \\epsilon_0)<span class="math">, and </span>R^b<span class="math"> be the bin load vector used in </span>\\mathcal{F}_{\\mathrm{threshbins}}<span class="math">. Let </span>S$ be an arbitrary subset in the support of the leakage. We need to show that</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="my-4 text-center"><span class="math-block">\\Pr [ L ^ {0} \\in S ] \\leq e ^ {2 \\epsilon_ {0} + 4 \\epsilon} \\Pr [ L ^ {1} \\in S ] + \\delta_ {\\mathbf {b a d}} + 4 \\delta</span></div>

    <p class="text-gray-300">This is proved by the following calculation, where the function <span class="math">f</span> is from Lemma A.6.</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{aligned} \\operatorname{Pr}[L^0 \\in S] &amp;amp;\\leq \\left(\\sum_{R^0 \\in \\text{good}} \\operatorname{Pr}[R^0] \\operatorname{Pr}[L^0 \\in S \\mid X^0, R^0]\\right) + \\delta_{\\text{bad}} \\\\ &amp;amp;\\leq \\left(\\sum_{R^0 \\in \\text{good}} \\operatorname{Pr}[R^0] \\left(e^{4\\epsilon} \\cdot \\operatorname{Pr}[L^1 \\in S \\mid X^1, f(R^0)] + 4\\delta\\right)\\right) + \\delta_{\\text{bad}} \\\\ &amp;amp;\\leq \\left(\\sum_{R^0 \\in \\text{good}} \\operatorname{Pr}[R^0] \\left(e^{4\\epsilon} \\cdot \\operatorname{Pr}[L^1 \\in S \\mid X^1, f(R^0)]\\right)\\right) + 4\\delta + \\delta_{\\text{bad}} \\\\ &amp;amp;\\leq \\left(\\sum_{R^0 \\in \\text{good}} \\left(e^{2\\epsilon_0} \\cdot \\operatorname{Pr}[f(R^0)]\\right) \\cdot \\left(e^{4\\epsilon} \\cdot \\operatorname{Pr}[L^1 \\in S \\mid X^1, f(R^0)]\\right)\\right) + 4\\delta + \\delta_{\\text{bad}} \\\\ &amp;amp;= \\left(e^{2\\epsilon_0 + 4\\epsilon} \\cdot \\sum_{R^0 \\in \\text{good}} \\operatorname{Pr}[f(R^0)] \\cdot \\operatorname{Pr}[L^1 \\in S \\mid X^1, f(R^0)]\\right) + 4\\delta + \\delta_{\\text{bad}} \\\\ &amp;amp;\\leq e^{2\\epsilon_0 + 4\\epsilon} \\cdot \\operatorname{Pr}[L^1 \\in S] + 4\\delta + \\delta_{\\text{bad}} \\end{aligned}</span></div>

    <p class="text-gray-300">In the above calculation, we make <span class="math">X^b</span> explicit in the conditioning even though it is not random. The key step is the second inequality, where we use the property that <span class="math">T^0 = T(X^0, R^0)</span> and <span class="math">T^1 = T(X^1, f(R^0))</span> are 4-neighboring, and <span class="math">\\mathcal{L}(\\lambda, T^b, \\epsilon_0)</span> is <span class="math">(\\epsilon, \\delta)</span>-differentially private with respect to <span class="math">T</span>. Also the fourth inequality uses the property that <span class="math">\\operatorname{Pr}[R^0] \\leq e^{2\\epsilon_0} \\cdot \\operatorname{Pr}[f(R^1)]</span> for <span class="math">R^0 \\in \\text{good}</span>. Both properties are from Lemma A.6.</p>

    <h2 id="sec-113" class="text-2xl font-bold">A.3 ThreshBins Obliviously Realize <span class="math">\\mathcal{F}_{\\mathrm{threshbins}}</span></h2>

    <p class="text-gray-300">Here we analyze the ThreshBins subroutine and show that it obliviously realize <span class="math">\\mathcal{F}_{\\mathrm{threshbins}}</span> with differentially private leakages. Specifically, the leakage is the interior points <span class="math">P</span> and the estimated prefix sums <span class="math">D</span> associated with the output thresh-bin <span class="math">T</span>. Namely, the leakage function <span class="math">\\mathcal{L}_{\\mathrm{threshbins}}(\\lambda, X, B, \\epsilon_0)</span> simply output <span class="math">L = (P, D)</span>.</p>

    <p class="text-gray-300"><strong>Lemma A.7.</strong> The algorithm ThreshBins 0-obliviously realize <span class="math">\\mathcal{F}_{\\mathrm{threshbins}}</span> with leakage function <span class="math">\\mathcal{L}_{\\mathrm{threshbins}}</span>. Moreover, its running time is <span class="math">O(BZ(\\log \\frac{1}{\\epsilon_0} + \\log \\log \\lambda))</span>.</p>

    <p class="text-gray-300"><strong>Proof.</strong> We first observe that by construction, the access pattern of ThreshBins is determined by the leakage <span class="math">(P,D)</span>. Thus, given the leakage, the access pattern can be readily simulated. Now, note that the output of <span class="math">\\mathcal{F}_{\\mathrm{threshbins}}</span> is determined by the input <span class="math">X</span>, the bin load vector <span class="math">R</span>, the estimated prefix sums <span class="math">D</span> and the interior points <span class="math">P</span>, and that these values are computed in an identical way in ThreshBins. Thus, it remains to show that ThreshBins computes exactly the same function as <span class="math">\\mathcal{F}_{\\mathrm{threshbins}}</span> correctly for every <span class="math">(X,R,D,P)</span>.</p>

    <p class="text-gray-300">By definition, <span class="math">\\mathcal{F}_{\\mathrm{threshbins}}</span> simply puts the first <span class="math">\\sum_{i=1}^{B} R_i</span> elements of <span class="math">X</span> in <span class="math">B</span> bins in order with bin load specified by the vector <span class="math">R</span>. On the other hand, at each iteration <span class="math">i</span>, ThreshBins places the first <span class="math">R_i</span> elements in Buf in the <span class="math">i</span>-th bin. We show that ThreshBins places the correct elements with the help of the following invariant: at the beginning of iteration each <span class="math">i</span>, the non-dummy elements in Buf consists of the <span class="math">((\\sum_{j=1}^{i-1} R_j) + 1)</span>-th to <span class="math">(D[i-1] + s)</span>-th elements in <span class="math">X</span>.</p>

    <p class="text-gray-300">Clearly, the invariant holds for <span class="math">i = 1</span> (with the convention that <span class="math">(\\sum_{j=1}^{i-1} R_j) = 0</span>). Assume that the invariant holds for <span class="math">i</span>, we observe that after the first step in the iteration, Buf consists</p>

    <p class="text-gray-300">40</p>

    <p class="text-gray-300">of the <span class="math">((\\sum_{j=1}^{i-1}R_{j})+1)</span>-th to <span class="math">(D[i]+s)</span>-th elements in <span class="math">X</span> in sorted order. Since the output of PrefixSum has at most <span class="math">s</span> additive error, we have <span class="math">(D[i]+s)\\geq\\sum_{j=1}^{i}R_{j}</span>. Also, there are at most <span class="math">(D[i]+s)-(\\sum_{j=1}^{i-1}R_{j})\\leq Z+s</span> non-dummy elements in <span class="math">\\mathsf{Buf}</span>, so no elements are lost after truncation. Hence, ObliviousBinPlace(<span class="math">\\mathsf{Buf},(R_{i}),Z</span>) can place the <span class="math">((\\sum_{j=1}^{i-1}R_{j})+1)</span>-th to <span class="math">(\\sum_{j=1}^{i}R_{j})</span>-th elements in <span class="math">X</span> in the <span class="math">i</span>-th bin as <span class="math">\\mathcal{F}_{\\text{threshbins}}</span>. Then after the first <span class="math">R_{i}</span> elements in <span class="math">\\mathsf{Buf}</span> are marked as dummy, the non-dummy elements in <span class="math">\\mathsf{Buf}</span> consists of the <span class="math">((\\sum_{j=1}^{i}R_{j})+1)</span>-th to <span class="math">(D[i]+s)</span>-th elements in <span class="math">X</span>, so the invariant holds for <span class="math">i+1</span>.</p>

    <p class="text-gray-300">Observe that the running time of the algorithm is also dominated by the <span class="math">B</span> iterations, each of which takes time <span class="math">O(Z\\log Z)=O(Z(\\log\\frac{1}{\\epsilon_{0}}+\\log\\log\\lambda))</span> due to oblivious sorting, which implies the desired running time. ∎</p>

    <p class="text-gray-300">Noting that the leakage <span class="math">\\mathcal{L}_{\\text{threshbins}}</span> is the outputs of differentially private mechanisms with input determined by the thresh-bins <span class="math">T</span> (since <span class="math">T</span> implicitly determines the bin load <span class="math">R</span>), <span class="math">\\mathcal{L}_{\\text{threshbins}}</span> is differentially private with respect to <span class="math">T</span>. By Lemma A.5, <span class="math">\\mathcal{L}_{\\text{threshbins}}</span> is differentially private with respect to <span class="math">X</span>. We state this in the following lemma.</p>

    <h6 id="sec-114" class="text-base font-medium mt-4">Lemma A.8.</h6>

    <p class="text-gray-300">The leakage <span class="math">\\mathcal{L}_{\\text{threshbins}}</span> is <span class="math">(O(\\epsilon_{0}),\\delta)</span>-differentially private with respect to the input <span class="math">X</span> for <span class="math">\\delta=O(\\exp(-\\Theta(\\log^{2}\\lambda)))</span></p>

    <h3 id="sec-115" class="text-xl font-semibold mt-8">A.4 Proof of Theorem 5.3</h3>

    <p class="text-gray-300">We are ready to prove Theorem 5.3. We will show that Merge obliviously realize an ideal merge functionality <span class="math">\\mathcal{F}_{\\text{merge}}</span> defined below with differentially private leakage <span class="math">\\mathcal{L}_{\\text{merge}}</span>, which implies that Merge is differentially oblivious by Fact A.4.</p>

    <p class="text-gray-300"><span class="math">\\mathcal{F}_{\\text{merge}}(\\lambda,I_{0},I_{1},\\epsilon)</span>:</p>

    <p class="text-gray-300">Assume: The same setting as Merge.</p>

    <p class="text-gray-300">Functionality:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Output a sorted array <span class="math">T</span> that merges elements from <span class="math">I_{0}</span> and <span class="math">I_{1}</span>, where the dummy elements appear at the end of the array.</li>

    </ul>

    <p class="text-gray-300">The leakage function <span class="math">\\mathcal{L}_{\\text{merge}}</span> is defined to be the concatenation of the leakage <span class="math">\\mathcal{L}_{\\text{threshbins}}</span> on <span class="math">I_{0}</span> and <span class="math">I_{1}</span>. Namely, <span class="math">\\mathcal{L}_{\\text{merge}}(\\lambda,I_{0},I_{1},\\epsilon)=(\\mathcal{L}_{\\text{threshbins}}(\\lambda,I_{0},B_{b},0.1\\epsilon)</span>, <span class="math">\\mathcal{L}_{\\text{threshbins}}(\\lambda,I_{1},B_{1},0.1\\epsilon)</span>) Clearly, the leakage is differentially private with respect to the input <span class="math">(I_{0},I_{1})</span>.</p>

    <h6 id="sec-116" class="text-base font-medium mt-4">Lemma A.9.</h6>

    <p class="text-gray-300">The algorithm <span class="math">\\mathsf{Merge}</span> <span class="math">\\delta</span>-obliviously realize <span class="math">\\mathcal{F}_{\\text{merge}}</span> with leakage function <span class="math">\\mathcal{L}_{\\text{merge}}</span> and <span class="math">\\delta=O(\\exp(-\\Theta(\\log^{2}\\lambda)))</span>. Moreover, <span class="math">\\mathsf{Merge}</span> has perfect correctness and its running time is <span class="math">O(BZ(\\log\\frac{1}{\\epsilon_{0}}+\\log\\log\\lambda))</span>.</p>

    <h6 id="sec-117" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">We observe that by construction, the access pattern of <span class="math">\\mathsf{Merge}</span> is determined by the leakage <span class="math">(P_{0},D_{0},P_{1},D_{1})</span>, unless the check in Step 3 fails. Thus, given the leakage, the access pattern can be readily simulated if the check in Step 3 does not fail. We will show that that check fails with exponentially small probability later and focus on the case when the check does not fail.</p>

    <p class="text-gray-300">Let us consider a hybrid functionality <span class="math">\\mathcal{F}^{\\prime}_{\\text{merge}}</span> that on input <span class="math">(\\lambda,I_{0},I_{1},\\epsilon)</span>, instead of merging <span class="math">I_{0}</span> and <span class="math">I_{1}</span> directly, <span class="math">\\mathcal{F}^{\\prime}_{\\text{merge}}</span> first calls <span class="math">\\mathcal{F}_{\\text{threshbins}}(\\lambda,I_{b},B_{b},0.1\\epsilon)</span> to obtain <span class="math">T_{b}</span> for <span class="math">b\\in\\{0,1\\}</span>, and then outputs <span class="math">T</span> that merges elements from <span class="math">T_{0}</span> and <span class="math">T_{1}</span>. Note that the output of <span class="math">\\mathcal{F}_{\\text{merge}}</span> and <span class="math">\\mathcal{F}^{\\prime}_{\\text{merge}}</span> are the same, except for the case that the bin load <span class="math">R_{b}</span> is not enough to accommodate <span class="math">I_{b}</span> for some</p>

    <p class="text-gray-300"><span class="math">b\\in\\{0,1\\}</span>, which happens with probability at most <span class="math">O(\\exp(-\\Theta(\\log^{2}\\lambda)))</span> by Lemma C.4. Thus, up to an <span class="math">O(\\exp(-\\Theta(\\log^{2}\\lambda)))</span> statistical error, we can switch to consider the hybrid functionality <span class="math">\\mathcal{F}^{\\prime}_{\\text{merge}}</span>.</p>

    <p class="text-gray-300">Now, note that <span class="math">\\mathcal{F}^{\\prime}_{\\text{merge}}</span> and <span class="math">\\mathsf{Merge}</span> call <span class="math">\\mathcal{F}_{\\text{threshbins}}(\\lambda,I_{b},B_{b},0.1\\epsilon)</span> and <span class="math">\\mathsf{ThreshBins}(\\lambda,I_{b},B_{b},0.1\\epsilon)</span> for <span class="math">b\\in\\{0,1\\}</span>, respectively. Since <span class="math">\\mathsf{ThreshBins}</span> obliviously realized <span class="math">\\mathcal{F}_{\\text{threshbins}}</span> (with no error), we know that the output thresh-bins <span class="math">T_{b}</span> (which are tagged with <span class="math">P_{b},D_{b}</span>) of <span class="math">\\mathsf{ThreshBins}</span> and <span class="math">\\mathcal{F}_{\\text{threshbins}}</span> are identical. From here, the difference between <span class="math">\\mathcal{F}^{\\prime}_{\\text{merge}}</span> and <span class="math">\\mathsf{Merge}</span> is that <span class="math">\\mathcal{F}^{\\prime}_{\\text{merge}}</span> directly merges <span class="math">T_{0}</span> and <span class="math">T_{1}</span>, whereas <span class="math">\\mathsf{Merge}</span> uses <span class="math">\\mathsf{MergeThreshBins}</span>. We now argue that in fact, for any thresh-bins <span class="math">T_{0},T_{1}</span> (with tagged <span class="math">P_{b},D_{b}</span>), <span class="math">\\mathcal{F}^{\\prime}_{\\text{merge}}</span> and <span class="math">\\mathsf{MergeThreshBins}</span> produce exactly the same answers.</p>

    <p class="text-gray-300">Observe that <span class="math">\\mathsf{MergeThreshBins}</span> merges <span class="math">T_{0}</span> and <span class="math">T_{1}</span> by inserting the bins into a buffer <span class="math">\\mathsf{Buf}</span> in a certain order, and along the way outputting certain numbers of smallest elements in <span class="math">\\mathsf{Buf}</span>. To argue that <span class="math">\\mathsf{MergeThreshBins}</span> computes the same merged result as <span class="math">\\mathcal{F}^{\\prime}_{\\text{merge}}</span>, it suffices to show that (i) the buffer <span class="math">\\mathsf{Buf}</span> never overflows (i.e., we never truncate non-dummy elements), and (ii) the elements outputted from <span class="math">\\mathsf{Buf}</span> are indeed the smallest elements among the remaining elements, since the two conditions imply that <span class="math">\\mathsf{MergeThreshBins}</span> correctly output smallest elements in <span class="math">T_{0}</span> and <span class="math">T_{1}</span> step by step.</p>

    <p class="text-gray-300">For (i), we claim that at any iteration <span class="math">i</span>, both <span class="math">T_{0}[j_{0}-1]</span> and <span class="math">T_{1}[j_{1}-1]</span> are safe bins (i.e., <span class="math">k_{b}\\geq j_{b}-1</span>), and hence the number of bins that are inserted into <span class="math">\\mathsf{Buf}</span> but not safe is at most <span class="math">4</span>. Note that by construction, both bins <span class="math">T_{0}[j_{0}+1]</span> and <span class="math">T_{1}[j_{1}+1]</span> are inserted in <span class="math">\\mathsf{Buf}</span> and that <span class="math">P_{b}[j_{b}]\\leq P_{1-b}[j_{1-b}+1]</span> for <span class="math">b\\in\\{0,1\\}</span>. This implies that <span class="math">P_{1-b}[j_{1-b}+1]\\geq P_{b}[(j_{b}-1)+1]</span>; namely, <span class="math">k_{b}\\geq j_{b}-1</span> for <span class="math">b\\in\\{0,1\\}</span>. Now, note that at each iteration, the first <span class="math">T_{b}[j_{b}+1]</span> bins are inserted in <span class="math">\\mathsf{Buf}</span> and the first <span class="math">D_{0}[k_{0}]+D_{1}[k_{1}]-2s</span> elements are outputted. Since the output of <span class="math">\\mathsf{PrefixSum}</span> has at most <span class="math">s</span> additive error and the bins contains at most <span class="math">Z</span> elements, the number of non-dummy elements in <span class="math">\\mathsf{Buf}</span> is at most <span class="math">5Z+2s</span> at any point of each iteration. Thus, the buffer<span class="math">\\mathsf{Buf}</span> never overflows.</p>

    <p class="text-gray-300">For (ii), we argue that as remarked in the construction, when a bin is marked safe, any bins with elements smaller that the elements in the safe bin are already inserted, or equivalently, all bins that are not inserted contains only elements larger than the elements in the safe bins. Thus, the elements outputted from <span class="math">\\mathsf{Buf}</span> are indeed the smallest elements among the remaining elements. To see this, consider a safe bin <span class="math">T_{b}[k]</span>. Clearly, any un-inserted bin <span class="math">T_{b}[v]</span> from <span class="math">T_{b}</span> has elements greater than elements in <span class="math">T_{b}[k]</span>. For bins in <span class="math">T_{1-b}</span>, by definition, there exists some inserted bin <span class="math">T_{1-b}[u]</span> with <span class="math">P_{1-b}[u]\\geq P_{b}[k+1]</span>. Hence, for any un-inserted bin <span class="math">T_{1-b}[v]</span> in <span class="math">T_{1-b}</span>, the elements in <span class="math">T_{1-b}[v]</span> has key value <span class="math">\\geq P_{1-b}[u]\\geq P_{b}[k+1]</span>, and hence greater than elements in <span class="math">T_{b}[k]</span>.</p>

    <p class="text-gray-300">Now, we argued that the first two steps of <span class="math">\\mathsf{Merge}</span> produces identical answers to <span class="math">\\mathcal{F}^{\\prime}_{\\text{merge}}</span> with probability <span class="math">1</span>, and that <span class="math">\\mathcal{F}^{\\prime}_{\\text{merge}}</span> and <span class="math">\\mathcal{F}_{\\text{merge}}</span> output identical answers, except when the bin load <span class="math">R_{b}</span> is not enough to accommodate <span class="math">I_{b}</span> for some <span class="math">b\\in\\{0,1\\}</span>, which happens with probability at most <span class="math">O(\\exp(-\\Theta(\\log^{2}\\lambda)))</span> by Lemma C.4. Furthermore, note that when this happens, Step 3 of <span class="math">\\mathsf{Merge}</span> will detect the error and output a correct merged output by a non-private merge algorithm. Therefore, we can conclude that <span class="math">\\mathsf{Merge}</span> <span class="math">\\delta</span>-obliviously realize <span class="math">\\mathcal{F}_{\\text{merge}}</span> with <span class="math">\\delta=O(\\exp(-\\Theta(\\log^{2}\\lambda)))</span>. Furthermore, <span class="math">\\mathsf{Merge}</span> outputs correct merged output with probability <span class="math">1</span>.</p>

    <p class="text-gray-300">Finally, the running time is dominated by the operations on <span class="math">\\mathsf{Buf}</span>. Since each iteration takes <span class="math">O(K\\log K)=O(Z(\\log\\frac{1}{\\epsilon_{0}}+\\log\\log\\lambda))</span> time due to oblivous sorting, it follows that the total time is <span class="math">O(BZ(\\log\\frac{1}{\\epsilon_{0}}+\\log\\log\\lambda))</span>, as required. ∎</p>

    <h3 id="sec-118" class="text-xl font-semibold mt-8">A.5 Obliviously Realizing the Interior Point Mechanism</h3>

    <p class="text-gray-300">We start by recalling the recursive differentially private algorithm <span class="math">\\mathsf{InteriorPoint}</span> of <em>Bun et al. [8]</em> for the interior point problem below and then discuss how to make the algorithm oblivious and implement it with a RAM machine with finite word size, as well as analyze its complexity. For</p>

    <p class="text-gray-300">the algorithm to be a useful building block for differentially oblivious algorithms, we assume that the input database may contain certain dummy elements but with sufficiently many non-dummy elements. The following algorithm is taken almost verbatim from Bun et al. [8], where the algorihtm simply ignores the dummy elements. We refer the readers to [8] for the intuition behind the algorithm.</p>

    <p class="text-gray-300"><strong>InteriorPoint <span class="math">(S, \\beta, \\epsilon, \\delta)</span></strong></p>

    <p class="text-gray-300"><strong>Assume:</strong> Database <span class="math">S = (x_{j})_{j=1}^{n&#x27;} \\in (X \\cup \\{\\text{dummy}\\})^{n&#x27;}</span> with <span class="math">n</span> non-dummy elements.</p>

    <p class="text-gray-300"><strong>Algorithm:</strong></p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">1. If $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">X</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\leq 32<span class="math">, then use the exponential mechanism with privacy parameter </span>\\epsilon<span class="math"> and quality function </span>q(S, x) = \\min \\{ \\# \\{ j : x_j \\geq x \\}, \\# \\{ j : x_j \\leq x \\} \\}<span class="math"> to choose and return a point </span>x \\in X<span class="math">. Specifically, each </span>x \\in X<span class="math"> is chosen with probability proportion to </span>e^{\\epsilon \\cdot q(S, x)/2}$ (since the sensitivity of the quality function is 1).</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Let <span class="math">k = \\left\\lfloor \\frac{386}{\\epsilon} \\ln \\left( \\frac{4}{\\beta \\epsilon \\delta} \\right) \\right\\rfloor</span>, and let <span class="math">Y = (y_{1}, y_{2}, \\ldots, y_{n-2k})</span> be a random permutation of the smallest <span class="math">(n-2k)</span> elements in <span class="math">S</span>.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>For <span class="math">j = 1</span> to <span class="math">\\frac{n - 2k}{2}</span>, define <span class="math">z_{j}</span> as the length of the longest prefix for which <span class="math">y_{2j-1}</span> agrees with <span class="math">y_{2j}</span> (in base 2 notation).</li>

    </ol>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">4. Execute <strong>InteriorPoint</strong> recursively on <span class="math">S&#x27; = (z_j)_{j=1}^{(n-2k)/2} \\in (X&#x27;)^{(n-2k)/2}</span> with parameters <span class="math">\\beta, \\epsilon, \\delta</span>. Recall that $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">X'</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">= \\log</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">X</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math">. Denote the returned value by </span>z$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Use the choosing mechanism to choose a prefix <span class="math">L</span> of length <span class="math">(z + 1)</span> with a large number of agreements among elements in <span class="math">Y</span>. Use parameters <span class="math">\\beta, \\epsilon, \\delta</span>, and the quality function <span class="math">q: X^* \\times \\{0,1\\}^{z+1} \\to \\mathbb{N}</span>, where <span class="math">q(Y,L)</span> is the number of agreements on prefix <span class="math">L</span> among <span class="math">y_1, \\ldots, y_{n-2k}</span>. Specifically, the choosing mechanism simply chooses one of prefixes with non-zero quality using exponential mechanism. Namely, each prefix <span class="math">L</span> with <span class="math">q(Y,L) \\geq 1</span> is chosen with probability proportion to <span class="math">e^{\\epsilon \\cdot q(Y,L)/2}</span>.</li>

    </ol>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">6. For <span class="math">\\sigma \\in \\{0,1\\}</span>, define <span class="math">L_{\\sigma} \\in X</span> to be the prefix <span class="math">L</span> followed by $(\\log</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">X</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">- z - 1)<span class="math"> appearances of </span>\\sigma$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Compute <span class="math">b\\hat{i}g = \\mathrm{Lap}\\left(\\frac{1}{\\epsilon}\\right) + \\# \\{j : x_j \\geq L_1 \\}</span>.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>If <span class="math">b\\hat{i}g \\geq \\frac{3k}{2}</span> then let <span class="math">\\text{ret} = L_1</span>. Otherwise let <span class="math">\\text{ret} = L_0</span>.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>If <span class="math">\\text{ret}</span> is not an interior point, then return any <span class="math">x \\in X</span>. Otherwise, return <span class="math">\\text{ret}</span>.</li>

    </ol>

    <p class="text-gray-300">Our goal is to implement the algorithm obliviously in a RAM machine with a finite word size efficiently. Note that for obliviousness, we cannot reveal the number <span class="math">n</span> of non-dummy elements. This is simple for step 3, 6, 7, and 8 by adding dummy access. For the recursion step 4, we can invoke the recursion with <span class="math">(n&#x27; - 2k)/2</span> size database with <span class="math">(n - 2k)/2</span> non-dummy elements. For step 2, we need to use random permutation to pair up <span class="math">n - 2k</span> smallest non-dummy elements in an oblivious way. This can be done with the help of oblivious sorting as follows.</p>

    <p class="text-gray-300">8See [8] for definition and properties of the choosing mechanism. Here we only describe the behavior of the choosing mechanism in this specific case.</p>

    <p class="text-gray-300">9We remark that the algorithm in [8] directly returns ref and may fail to output an interior point with probabilitiy <span class="math">O(\\beta)</span>. We modify it so that it always output an interior point at the price of increasing the privacy parameter by <span class="math">O(\\beta)</span> and set <span class="math">\\beta = O(\\delta)</span>.</p>

    <p class="text-gray-300">43</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>We first use oblivious sorting to identify the <span class="math">n-2k</span> smallest non-dummy elements in <span class="math">Y</span>. Namely, we sort the elements according to its value and mark the <span class="math">n-2k</span> smallest non-dummy elements.</li>

      <li>We apply a random permutation to the <span class="math">n^{\\prime}</span> elements. Note that this permutes the <span class="math">n-2k</span> marked elements uniformly. Also note that we do not need to hide the permutation since it is data independent.</li>

      <li>We use a stable oblivious sorting again to move the <span class="math">n-2k</span> marked elements together. Specifically, we view the marked and unmarked elements as having values <span class="math">0</span> and <span class="math">1</span>, respecitvely and we sort according to this value. The output <span class="math">Y=\\{y_{1},y_{2}\\ldots,y_{n-2k}\\}</span> are the first <span class="math">n-2k</span> elements. Since we use stable sorting, the order among the <span class="math">n-2k</span> elements remains randomly permuted.</li>

    </ul>

    <p class="text-gray-300">The time complexity of this step is <span class="math">O(n^{\\prime}\\cdot\\log n^{\\prime})</span></p>

    <p class="text-gray-300">We now discuss how to implmement the exponential and choosing mechanisms in step 1 and 5, which involves sampling from a distribution defined by numbers of the form <span class="math">e^{(\\epsilon/2)\\cdot j}</span> for integer <span class="math">j\\in\\mathbb{Z}</span>. To implement the sampling in a RAM machine with a finite word size, we need to compute the values with enough precision to maintain privacy, which may cause efficiency overhead. We discuss how to do it efficiently. We focus on step 5 since it is the more involved step. Step 1 can be done in an analogous way.</p>

    <p class="text-gray-300">To implement the choosing mechanisms in step 5, the first step is to compute the quality function for all prefixes <span class="math">L\\in\\{0,1\\}^{z+1}</span> with <span class="math">q(Y,L)\\geq 1</span> obliviously. Let <span class="math">P=\\{L:q(Y,L)\\geq 1\\}</span> denote the set of such prefixes. Recall that <span class="math">q(Y,L)=\\#\\{y_{i}\\in Y:\\mathsf{pref}(y_{i})=L\\}</span>, where <span class="math">\\mathsf{pref}(y_{i})</span> denote the <span class="math">z+1</span> bits prefix of <span class="math">y_{i}</span>. This can be done by invoking oblivious aggregation (see Section C.3) with key <span class="math">k_{i}=\\mathsf{pref}(y_{i})</span> and value <span class="math">v_{i}=1</span> (with padded dummy entries to size <span class="math">n^{\\prime}-2k</span>), and the aggregation function <span class="math">\\mathsf{Aggr}</span> being the summation function. The ouput is an array <span class="math">\\{(L_{j},q_{j})\\}</span> of the same size <span class="math">n^{\\prime}-2k</span> where the first half contains all prefixes <span class="math">L_{j}\\in P</span> with <span class="math">q_{j}=q(Y,L_{j})\\geq 1</span> and the remaining are dummy entries.</p>

    <p class="text-gray-300">Now we need to sample a prefix <span class="math">L_{j}</span> with probability proportion to <span class="math">e^{(\\epsilon/2)\\cdot q_{j}}</span>. To optimize the efficiency, we do it as follows. Let <span class="math">q_{\\max}=\\max_{j}q_{j}</span>. We compute <span class="math">w_{j}=e^{(\\epsilon/2)\\cdot(q_{j}-q_{\\max})}\\in(0,1]</span> with <span class="math">p=2\\cdot\\log(n/\\delta)</span> bits of precision. We set <span class="math">w_{j}=0</span> for the dummy entries. Then we compute the accumulated sum <span class="math">v_{j}=\\sum_{\\ell\\leq j}\\lfloor 2^{p}\\cdot w_{j}\\rfloor</span>. Finally, we sample a uniform <span class="math">u\\leftarrow_{R}[v_{n^{\\prime}-2k}]</span> and output the <span class="math">L_{j}</span> such that <span class="math">v_{j-1}&lt;u\\leq v_{j}</span> (we set <span class="math">v_{0}=0</span>). It is not hard to see that this samples <span class="math">L_{j}</span> with the correct distribution up to an <span class="math">o(\\delta)</span> statistical distance error due to the finite precision. To compute <span class="math">w_{j}=\\{e^{(\\epsilon/2)\\cdot(q_{j}-q_{\\max})}\\}</span> with <span class="math">p</span> bits of precision, note that these are values of the form <span class="math">e^{-(\\epsilon/2)\\cdot t}</span> for <span class="math">t\\in\\mathbb{N}</span>. Since we only need <span class="math">p</span> bits of precision, the value rounds to <span class="math">0</span> when <span class="math">t</span> is too large. Let <span class="math">t_{\\max}=O((1/\\epsilon)\\cdot\\log p)</span> be the largest <span class="math">t</span> we need to consider. We precompute the values <span class="math">\\alpha_{k}=e^{-(\\epsilon/2)\\cdot 2^{k}}</span> for <span class="math">k\\in\\{0,1,\\ldots,\\lfloor\\log t_{\\max}\\rfloor\\}</span>, and compute <span class="math">w_{j}</span> by multiplying a subset of <span class="math">\\alpha_{k}</span>, i.e., by the standard repeated squaring algorithm. (Note that for obliviousness, the access pattern needs to go over all <span class="math">\\alpha_{k}</span> to hide the subset). Finally, to compute <span class="math">\\alpha_{k}</span>, we first compute <span class="math">\\alpha_{0}=e^{-\\epsilon/2}</span> using Taylor expansion, and then compute <span class="math">\\alpha_{k}=\\alpha_{k-1}^{2}</span> by multiplication.</p>

    <p class="text-gray-300">We summarize below the implementation of the choosing mechanism in step 5 discussed above. Note that it has a deterministic access pattern.</p>

    <p class="text-gray-300">Choosing<span class="math">(Y,z,\\epsilon)</span></p>

    <p class="text-gray-300">Assume: <span class="math">Y=(y_{j})_{j=1}^{n^{\\prime}-2k}\\in(X\\cup\\{\\mathsf{dummy}\\})^{n^{\\prime}-2k}</span> with <span class="math">n-2k</span> non-dummy elements.</p>

    <p class="text-gray-300">Algorithm:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Compute the quality function: Invoke oblivious aggregation (see Section C.3) with key <span class="math">k_{i}=\\mathsf{pref}(y_{i})</span> and value <span class="math">v_{i}=1</span> (with padded dummy entries to size <span class="math">n^{\\prime}-2k</span>), and the aggregation function <span class="math">\\mathsf{Aggr}</span> being the summation function. The ouput is an array <span class="math">\\{(L_{j},q_{j})\\}</span> of the same size <span class="math">n^{\\prime}-2k</span> where the first half contains all prefixes <span class="math">L_{j}\\in P</span> with <span class="math">q_{j}=q(Y,L_{j})\\geq 1</span> and the remaining are dummy entries.</li>

      <li>Compute <span class="math">\\alpha_{0}=e^{-\\epsilon/2}</span> by Taylor expansion, and <span class="math">\\alpha_{k}=\\alpha_{k-1}^{2}</span> for <span class="math">k\\in\\{1,\\ldots,\\lfloor\\log t_{\\max}\\rfloor\\}</span>.</li>

      <li>Compute the weights <span class="math">w_{j}=e^{(\\epsilon/2)\\cdot(q_{j}-q_{\\max})}</span> by multiplying a proper subset of <span class="math">\\alpha_{k}</span>. Set <span class="math">w_{j}=0</span> for the dummy entries. (Note that for oblivious security, we need to do dummy computation to go over all <span class="math">\\alpha_{k}</span> for all <span class="math">j</span>.)</li>

      <li>Compute the accumulated sum <span class="math">v_{j}=\\sum_{\\ell\\leq j}\\lfloor 2^{p}\\cdot w_{j}\\rfloor</span>. Set <span class="math">v_{0}=0</span>.</li>

      <li>Sample a uniform <span class="math">u\\leftarrow_{R}[v_{n^{\\prime}-2k}]</span>. Use a linear scan to find <span class="math">j</span> such that <span class="math">v_{j-1}&lt;u\\leq v_{j}</span> and output <span class="math">L_{j}</span>. (Note that we cannot do binary search here for oblivious security.)</li>

    </ol>

    <p class="text-gray-300">We now analyze the complexity of the above choosing mechanism. The first step takes <span class="math">O(n^{\\prime}\\log n^{\\prime})</span> time for oblivious aggregation. For the sampling steps, for clarity, let us use <span class="math">\\mathsf{time}_{\\mathsf{add}}(p)</span>, <span class="math">\\mathsf{time}_{\\mathsf{mult}}(p)</span>, <span class="math">\\mathsf{time}_{\\mathsf{exp}}(p)</span>, etc., to denote the time to perform addition and multiplication, and compute <span class="math">e^{-\\epsilon/2}</span>, etc., with <span class="math">p</span> bits of precision. We note that the dominating cost is the computation of the weights <span class="math">w_{j}</span>, which takes <span class="math">O(n^{\\prime}\\cdot(\\log t_{\\max})\\cdot\\mathsf{time}_{\\mathsf{mult}}(p))</span> time. Other costs that are linear in <span class="math">n^{\\prime}</span> are the computation of accumulated sum and the search of index <span class="math">j</span> such that <span class="math">v_{j-1}&lt;u\\leq v_{j}</span>, which takes linear number of addition and comparison, respectively. The remaining costs are the computation of <span class="math">\\alpha_{k}</span>’s, which takes time <span class="math">\\mathsf{time}_{\\mathsf{exp}}+(\\log t_{\\max})\\cdot\\mathsf{time}_{\\mathsf{mult}}(p)</span>, and the sampling of <span class="math">u\\leftarrow_{R}[v_{n^{\\prime}-2k}]</span>. Note that <span class="math">\\mathsf{time}_{\\mathsf{exp}}(p)</span> is at most <span class="math">O(p\\cdot\\mathsf{time}_{\\mathsf{mult}}(\\mathsf{p}))\\leq O(n^{\\prime}\\cdot\\mathsf{time}_{\\mathsf{mult}}(\\mathsf{p}))</span>. All these terms are dominated by the cost of computing <span class="math">w_{j}</span>’s.</p>

    <p class="text-gray-300">Therefore, the total cost of InteriorPoint without considering the recursion is</p>

    <p class="text-gray-300"><span class="math">O(n^{\\prime}\\log n^{\\prime})+O(n^{\\prime}\\cdot(\\log t_{\\max})\\cdot\\mathsf{time}_{\\mathsf{mult}}(p)),</span></p>

    <p class="text-gray-300">where <span class="math">\\log t_{\\max}=O(\\log((1/\\epsilon)\\cdot\\log(n/\\delta)))</span> and <span class="math">p=O(\\log(n/\\delta))</span>. Finally, note that InteriorPoint only invokes the recursion once with size shrinking by a factor of 2, so the overall complexity remains the same.</p>

    <p class="text-gray-300">We remark that one may consider to precompute not only <span class="math">\\alpha_{k}=e^{-(\\epsilon/2)\\cdot 2^{k}}</span> for <span class="math">k\\in\\{0,1,\\ldots,\\lfloor\\log t_{\\max}\\rfloor\\}</span>, but all the values <span class="math">e^{-(\\epsilon/2)\\cdot t}</span> for <span class="math">t\\in\\{0,1,\\ldots,t_{\\max}\\}</span> so that we do not need to compute the weight <span class="math">w_{j}</span>’s by repeated squaring. However, note that we cannot directly fetch the precomputed value for <span class="math">w_{j}</span> since it breaks the oblivious security, and it seems that to maintain oblivious security, an <span class="math">O(\\log t_{\\max})</span> overhead is needed. This can yield a small asymptotic improvement over the above solution, but we choose to present the above solution for simplicity.</p>

    <p class="text-gray-300">Let <span class="math">w</span> denote the word size of a RAM machine and suppose word operations have unit cost. We know that <span class="math">\\mathsf{time}_{\\mathsf{mult}}(p)\\leq O((p/w)^{2})</span>. When <span class="math">\\delta=\\mathsf{negl}(n^{\\prime})</span> and <span class="math">w=O(\\log n^{\\prime})</span>, we have <span class="math">p/w=\\omega(1)</span> and the overall complexity is <span class="math">O(n^{\\prime}\\log n^{\\prime})</span>. We summarize the above discussion in the following theorem.</p>

    <h6 id="sec-119" class="text-base font-medium mt-4">Theorem A.10 (Differentially private interior point, finite word size version).</h6>

    <p class="text-gray-300">Let <span class="math">w</span> be the word size of a RAM machine. Let <span class="math">\\beta,\\epsilon,\\delta&gt;0</span> be parameters. There exists an algorithm such that given any array <span class="math">S</span> containing <span class="math">n^{\\prime}</span> elements from a finite universe <span class="math">X=[0..U-1]</span> with some dummy elements but at least <span class="math">n</span> non-dummy, where <span class="math">n\\geq\\frac{18500}{\\epsilon}\\cdot 2^{\\log^{<em>}U}\\cdot\\log^{</em>}U\\cdot\\ln\\frac{4\\log^{*}U}{\\epsilon\\delta}</span>, the algorithm</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>completes consuming only <span class="math">O((n^{\\prime}\\log n^{\\prime})+(n^{\\prime}\\cdot\\log((1/\\epsilon)\\cdot\\log p)\\cdot(p/w)^{2}))</span> time and number of memory accesses, where <span class="math">p=2\\log(n/\\delta)</span></li>

    </ul>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>the algorithm produces an <span class="math">(\\epsilon,\\delta)</span>-differential private outcome;</li>

      <li>the algorithm is perfect correct, i.e., the outcome is a correct interior point of the input array <span class="math">S</span> with probability <span class="math">1</span>; and</li>

      <li>the algorithm’s memory access patterns are independent of the input array.</li>

    </ul>

    <h2 id="sec-120" class="text-2xl font-bold">Appendix B Limits of Differentially Oblivious Algorithms with Imperfect Correctness</h2>

    <p class="text-gray-300">In this section, we discuss how to extend our lower bounds for differentially oblivious algorithms to handle imperfect correctness (with small correctness error). The main observation is that it sufficient to generalize Lemma 4.3 to handle imperfect correctness. Then, the lower bound follows by replacing Lemma 4.3 to the generalized version in the original proof. We first refine the definition of plausible access pattern.</p>

    <h6 id="sec-121" class="text-base font-medium mt-4">Definition B.1 (Good access pattern).</h6>

    <p class="text-gray-300">An access pattern <span class="math">A</span> produced by a mechanism <span class="math">M</span> is good for an input <span class="math">I</span>, if</p>

    <p class="text-gray-300"><span class="math">\\Pr[(\\mathbf{Accesses}^{M}(\\lambda,I)=A)\\wedge(\\mathit{M}\\text{outputs correct answers})]&gt;0;</span></p>

    <p class="text-gray-300">otherwise, we say that <span class="math">A</span> is bad for <span class="math">I</span>.</p>

    <h6 id="sec-122" class="text-base font-medium mt-4">Lemma B.2.</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Suppose <span class="math">I_{0}</span> is some input for a mechanism <span class="math">M</span> that is <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious with correctness error <span class="math">\\gamma</span>, and <span class="math">\\mathcal{C}</span> is a collection of inputs that are <span class="math">r</span>-neighbors of <span class="math">I_{0}</span>. Then, the probability that <span class="math">\\mathbf{Accesses}^{M}(\\lambda,I_{0})</span> is good for all inputs in <span class="math">\\mathcal{C}</span> is at least <span class="math">1-\\eta</span>, where $\\eta:=</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathcal{C}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\cdot(e^{r\\epsilon}\\cdot\\gamma+\\frac{e^{\\epsilon r}-1}{e^{\\epsilon}-1}\\cdot\\delta)\\leq</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathcal{C}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\cdot e^{r\\epsilon}\\cdot(\\gamma+\\delta)$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h6 id="sec-123" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">Let <span class="math">S</span> be the set of access patterns that are good for input <span class="math">I_{0}</span>, which happens with probability at least <span class="math">1-\\gamma</span>. For each <span class="math">I_{i}\\in\\mathcal{C}</span>, define <span class="math">S_{i}\\subset S</span> to be the subset of access patterns in <span class="math">S</span> that are bad for <span class="math">I_{i}</span>.</p>

    <p class="text-gray-300">By Fact C.1, we have</p>

    <p class="text-gray-300"><span class="math">\\Pr[\\mathbf{Accesses}^{M}(\\lambda,I_{0})\\in S_{i}]\\leq e^{r\\epsilon}\\cdot\\Pr[\\mathbf{Accesses}^{M}(\\lambda,I_{i})\\in S_{i}]+\\frac{e^{\\epsilon r}-1}{e^{\\epsilon}-1}\\cdot\\delta\\leq e^{r\\epsilon}\\cdot\\gamma+\\frac{e^{\\epsilon r}-1}{e^{\\epsilon}-1}\\cdot\\delta,</span></p>

    <p class="text-gray-300">where the last equality follows because the access patterns in <span class="math">S_{i}</span> are bad for <span class="math">I_{i}</span>, which happens with probability at most <span class="math">\\gamma</span>. Therefore, by the union bound, we have</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$\\Pr[\\mathbf{Accesses}^{M}(\\lambda,I_{0})\\in\\cup_{I_{i}\\in\\mathcal{C}}S_{i}]\\leq</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathcal{C}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\cdot(e^{r\\epsilon}\\cdot\\gamma+\\frac{e^{\\epsilon r}-1}{e^{\\epsilon}-1}\\cdot\\delta).$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Finally, observe that <span class="math">\\mathbf{Accesses}^{M}(\\lambda,I_{0})\\in\\cup_{I_{i}\\in\\mathcal{C}}S_{i}</span> is the complement of the event that <span class="math">\\mathbf{Accesses}^{M}(\\lambda,I_{0})</span> is plausible for all inputs in <span class="math">\\mathcal{C}</span>. Hence, the result follows. ∎</p>

    <h4 id="sec-124" class="text-lg font-semibold mt-6">Extending our lower bounds for imperfect correctness.</h4>

    <p class="text-gray-300">One interpretation of Lemma B.2 is that an <span class="math">(\\epsilon,\\delta)</span>-differentially oblivious mechanism <span class="math">M</span> with correctness error <span class="math">\\gamma</span> can be compared to an <span class="math">(\\epsilon,\\delta+\\gamma)</span>-differentially oblivious mechanism <span class="math">\\widetilde{M}</span> that is perfectly correct. To get an informal intuition, suppose for the special case, it can be checked (efficiently in an oblivious manner) whether the output of mechanism <span class="math">M</span> is correct. Then, in the case that the output is incorrect (which happens with probability at most <span class="math">\\gamma</span>), a non-oblivious (but always correct) algorithm can be run. Hence, the</p>

    <p class="text-gray-300">resulting mechanism <span class="math">\\widehat{M}</span> is <span class="math">(\\epsilon, \\delta + \\gamma)</span>-differentially oblivious and perfectly correct. However, since in general there might be no efficient and oblivious manner to check the correctness of a mechanism, Lemma B.2 is needed to extend our lower bounds for imperfect correctness. Given Lemma B.2, our oblivious sorting lower bound can be extended to the following:</p>

    <p class="text-gray-300">Corollary B.3. Suppose <span class="math">0 &amp;lt; s \\leq \\sqrt{N}</span>, <span class="math">\\epsilon &amp;gt; 0</span>, <span class="math">0 &amp;lt; \\beta &amp;lt; 1</span> and <span class="math">0 \\leq \\delta + \\gamma \\leq \\beta \\cdot \\frac{\\epsilon}{s} \\cdot e^{-2\\epsilon s}</span>. Then, any (randomized) stable 1-bit-key sorting algorithm in the balls-and-bins model that is <span class="math">(\\epsilon, \\delta)</span>-differentially oblivious must, on some input, incur at least <span class="math">\\Omega(N \\log s)</span> memory accesses with probability at least <span class="math">1 - \\beta</span>.</p>

    <p class="text-gray-300">Proof. Same as the proof for Theorem 4.7 but replace the usage of "plausible access pattern" there with "good access pattern" instead.</p>

    <p class="text-gray-300">Similarly, our lower bounds for merging and data structures can be extended in the same way using Lemma B.2 to account for imperfect correctness, and we omit the detailed statements.</p>

    <h2 id="sec-125" class="text-2xl font-bold">C Extra Preliminaries</h2>

    <h3 id="sec-126" class="text-xl font-semibold mt-8">C.1 Technical Lemmas Concerning <span class="math">r</span>-Neighbors</h3>

    <p class="text-gray-300">Fact C.1 (<span class="math">r</span>-Neighbors Produce Similar Access Patterns [44]). Suppose a (randomized) algorithm <span class="math">M</span> satisfies <span class="math">(\\epsilon, \\delta)</span>-differential obliviousness, where <span class="math">\\epsilon</span> and <span class="math">\\delta</span> can depend on some security parameter <span class="math">\\lambda</span>. Then, for any two inputs <span class="math">I</span> and <span class="math">I&#x27;</span> that are <span class="math">r</span>-neighboring and any set <span class="math">S</span> of access patterns, we have</p>

    <div class="my-4 text-center"><span class="math-block">\\Pr[\\mathbf{Accesses}^M(\\lambda, I) \\in S] \\leq e^{r\\epsilon} \\cdot \\Pr[\\mathbf{Accesses}^M(\\lambda, I&#x27;) \\in S] + \\frac{e^{\\epsilon r} - 1}{e^{\\epsilon} - 1} \\cdot \\delta.</span></div>

    <p class="text-gray-300">Proof of Lemma 4.3. Let <span class="math">S</span> be the set of access patterns that are plausible for input <span class="math">I_0</span>. For each <span class="math">I_i \\in \\mathcal{C}</span>, define <span class="math">S_i \\subset S</span> to be the subset of access patterns in <span class="math">S</span> that are implausible for <span class="math">I_i</span>.</p>

    <p class="text-gray-300">By Fact C.1, we have</p>

    <div class="my-4 text-center"><span class="math-block">\\Pr[\\mathbf{Accesses}^M(\\lambda, I_0) \\in S_i] \\leq e^{r\\epsilon} \\cdot \\Pr[\\mathbf{Accesses}^M(\\lambda, I_i) \\in S_i] + \\frac{e^{\\epsilon r} - 1}{e^{\\epsilon} - 1} \\cdot \\delta = \\frac{e^{\\epsilon r} - 1}{e^{\\epsilon} - 1} \\cdot \\delta,</span></div>

    <p class="text-gray-300">where the last equality follows because the access patterns in <span class="math">S_i</span> are implausible for <span class="math">I_i</span>. Therefore, by the union bound, we have</p>

    <div class="my-4 text-center"><span class="math-block"></span></div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\Pr[\\mathbf{Accesses}^M(\\lambda, I_0) \\in \\cup_{I_i \\in \\mathcal{C}} S_i] \\leq</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathcal{C}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\cdot \\frac{e^{\\epsilon r} - 1}{e^{\\epsilon} - 1} \\cdot \\delta.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="my-4 text-center"><span class="math-block"></span></div>

    <p class="text-gray-300">Finally, observe that <span class="math">\\mathbf{Accesses}^M(\\lambda, I_0) \\in \\cup_{I_i \\in \\mathcal{C}} S_i</span> is the complement of the event that <span class="math">\\mathbf{Accesses}^M(\\lambda, I_0)</span> is plausible for all inputs in <span class="math">\\mathcal{C}</span>. Hence, the result follows.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Proof of Lemma 4.4. Before contraction, there are exactly $t \\cdot</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">N</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">+ 2t<span class="math"> edges in the access pattern graph. For each layer </span>1 \\leq i \\leq t - 1<span class="math">, there are exactly </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">N</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">- 2<span class="math"> nodes with in-degree and out-degree being 1. Therefore, the number of edges decreases by </span>(t - 1) \\cdot (</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">N</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">- 2)$ to form the compact graph.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Finally, we observe that $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">N</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\leq t<span class="math">, because at most </span>t<span class="math"> memory location can be accessed in </span>t$ accesses.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">C.2 Stochastic Analysis for Geometric Distribution</p>

    <p class="text-gray-300">The following simple fact follows in a straightforward manner from the definition of Geom.</p>

    <h6 id="sec-127" class="text-base font-medium mt-4">Fact C.2.</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">For any even <span class="math">Z</span> and any <span class="math">\\epsilon&gt;0</span>, for any integers <span class="math">a,a^{\\prime}\\in[0,Z]</span> such that $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">a-a^{\\prime}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">=1$, we have that</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><span class="math">\\Pr[\\mathsf{Geom}^{Z}(e^{\\epsilon})=a]\\leq e^{\\epsilon}\\cdot\\Pr[\\mathsf{Geom}^{Z}(e^{\\epsilon})=a^{\\prime}].</span></p>

    <h6 id="sec-128" class="text-base font-medium mt-4">Lemma C.3 (Moment Generating Function).</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Suppose <span class="math">G</span> is a random variable having truncated geometric distribution <span class="math">\\mathsf{Geom}^{Z}(\\alpha)</span> with support <span class="math">[0..Z]</span> and mean <span class="math">\\frac{Z}{2}</span>, for some <span class="math">\\alpha&gt;1</span>. Then, for $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">t</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\leq\\min\\{\\frac{1}{2},\\sqrt{2\\ln\\frac{(\\alpha+1)^{2}}{4\\alpha}}\\}$, we have</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><span class="math">E[e^{tG}]\\leq\\exp(\\frac{Z}{2}\\cdot t+\\frac{4\\alpha}{(\\alpha-1)^{2}}\\cdot t^{2}).</span></p>

    <h6 id="sec-129" class="text-base font-medium mt-4">Proof.</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Let <span class="math">V</span> be a random variable whose distribution the untruncated variant of <span class="math">\\mathsf{Geom}(\\alpha)</span> that is symmetric around <span class="math">0</span>, i.e., for all integers <span class="math">x</span>, its probability mass function is proportional to $\\alpha^{-</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Let <span class="math">W</span> be the truncated variant of <span class="math">\\mathsf{Geom}(\\alpha)</span> that is symmetric around <span class="math">0</span> and has support in <span class="math">[-\\frac{Z}{2},\\frac{Z}{2}]</span>. Hence, <span class="math">G</span> has the same distribution as <span class="math">\\frac{Z}{2}+W</span>.</p>

    <p class="text-gray-300">It can be shown that for any real <span class="math">t</span>, <span class="math">E[e^{tW}]\\leq E[e^{tV}]</span>. This follows from the fact that the function <span class="math">i\\mapsto e^{ti}+e^{-ti}</span> is increasing for positive integers <span class="math">i</span>.</p>

    <p class="text-gray-300">Therefore, <span class="math">E[e^{tG}]\\leq e^{\\frac{Z}{2}\\cdot t}\\cdot E[e^{tV}]</span>. Finally, the result follows from applying a technical result from <em>[13, Lemma 1, Appendix B.1]</em> to get an upper bound on <span class="math">E[e^{tV}]</span> the specified range of <span class="math">t</span>. ∎</p>

    <h6 id="sec-130" class="text-base font-medium mt-4">Lemma C.4 (Measure concentration for truncated geometric random variables).</h6>

    <p class="text-gray-300">Let <span class="math">G_{B}</span> denote the sum of <span class="math">B</span> independent <span class="math">\\mathsf{Geom}^{Z}(e^{\\epsilon_{0}})</span> random variables (each of which having mean <span class="math">\\frac{Z}{2}</span> and support <span class="math">[0..Z]</span>). For any <span class="math">B</span>, for sufficiently large <span class="math">\\lambda</span> and <span class="math">Z\\geq\\frac{\\log^{5}\\lambda}{\\epsilon_{0}}</span>, it holds that</p>

    <p class="text-gray-300"><span class="math">\\Pr\\left[G_{B}\\geq\\frac{BZ}{2}\\cdot\\left(1+\\frac{1}{\\log^{2}\\lambda}\\right)\\right]\\leq\\exp(-\\log^{2}\\lambda)</span></p>

    <p class="text-gray-300">and</p>

    <p class="text-gray-300"><span class="math">\\Pr\\left[G_{B}\\leq\\frac{BZ}{2}\\cdot\\left(1-\\frac{1}{\\log^{2}\\lambda}\\right)\\right]\\leq\\exp(-\\log^{2}\\lambda).</span></p>

    <h6 id="sec-131" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">We prove the first inequality. The second inequality can be proved using the same approach. Denote <span class="math">R:=\\frac{Z}{2\\log^{2}\\lambda}</span> and <span class="math">\\alpha=e^{\\epsilon_{0}}</span>. Using the standard argument as in the proof of the Chernoff Bound, for positive <span class="math">t\\leq\\min\\{\\frac{1}{2},\\sqrt{2\\ln\\frac{(\\alpha+1)^{2}}{4\\alpha}}\\}</span> in the range specified in Lemma C.3, we have</p>

    <p class="text-gray-300"><span class="math">\\Pr\\left[G_{B}\\geq\\frac{BZ}{2}\\cdot\\left(1+\\frac{1}{\\log^{2}\\lambda}\\right)\\right]\\leq\\exp\\{B(\\frac{4\\alpha}{(\\alpha-1)^{2}}\\cdot t^{2}-Rt)\\}\\leq\\exp\\{\\frac{4\\alpha}{(\\alpha-1)^{2}}\\cdot t^{2}-Rt\\},</span></p>

    <p class="text-gray-300">where the last inequality holds if we pick <span class="math">t&gt;0</span> such that the exponent in the last term is negative.</p>

    <p class="text-gray-300">Hence, it suffices to analyze the exponent for two cases of <span class="math">\\epsilon_{0}</span>.</p>

    <p class="text-gray-300">The first case is when <span class="math">\\epsilon_{0}</span> is some large enough constant. In this case, we set <span class="math">t&gt;0</span> to be some appropriate constant, and the exponent is <span class="math">-\\Theta(R)=-\\Theta(\\frac{Z}{2\\log^{2}\\lambda})\\leq-\\log^{2}\\lambda</span>.</p>

    <p class="text-gray-300">The second case is when <span class="math">\\epsilon_{0}</span> is small. In this case, <span class="math">\\frac{4\\alpha}{(\\alpha-1)^{2}}=\\Theta(\\frac{1}{\\epsilon_{0}})^{2}</span>. We set <span class="math">t</span> to <span class="math">\\sqrt{2\\ln\\frac{(\\alpha+1)^{2}}{4\\alpha}}=\\Theta(\\epsilon_{0})</span>. Hence, the exponent is <span class="math">-\\Theta(R\\epsilon_{0})=-\\Theta(\\frac{Z\\epsilon_{0}}{2\\log^{2}\\lambda})\\leq-\\log^{2}\\lambda</span>. This completes the proof of the first inequality. ∎</p>

    <h2 id="sec-132" class="text-2xl font-bold">Appendix</h2>

    <p class="text-gray-300">C.3 Existing Building Blocks</p>

    <h4 id="sec-133" class="text-lg font-semibold mt-6">C.3.1 Oblivious Aggregation</h4>

    <p class="text-gray-300">Oblivious aggregation is the following primitive where given an array of (key, value) pairs, each representative element for a key will learn some aggregation function computed over all pairs with the same key.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Input: An array <span class="math">\\mathsf{Inp}:=\\{k_{i},v_{i}\\}_{i\\in[n]}</span> of possibly dummy (key, value) pairs Henceforth we refer to all elements with the same key as the same group. We say that index <span class="math">i\\in[n]</span> is a representative for its group if <span class="math">i</span> is the leftmost element of the group.</li>

      <li>Output: Let Aggr be a publicly known, commutative and associative aggregation function and we assume that its output range can be described by <span class="math">O(1)</span> number of blocks. The goal of oblivious aggregation is to output the following array:</li>

    </ul>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\[ \\mathsf{Outp}_{i}:=\\begin{cases}\\mathsf{Aggr}\\left(\\{(k,v)</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">(k,v)\\in\\mathsf{Inp}\\text{ and }k=k_{i}\\}\\right)&\\text{if <span class="math">i</span> is a representative}\\\\</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">\\bot&\\text{o.w.}\\end{cases} \\]</p>

    <p class="text-gray-300">Oblivious aggregation can be implemented in time <span class="math">O(n\\log n)</span> with a deterministic access pattern.</p>

    <h4 id="sec-134" class="text-lg font-semibold mt-6">C.3.2 Oblivious Propagation for a Sorted Array</h4>

    <p class="text-gray-300">Oblivious propagation <em>[38]</em> is the opposite of aggregation. Given an array of possibly dummy (key, value) pairs where all elements with the same key appear consecutively, we say that an element is the representative if it is the leftmost element with its key. Oblivious propagation aims to achieve the following: for each key, propagate the representative’s value to all other elements with the same key. Nayak et al. <em>[38]</em> show that such oblivious propagation can be achieved in <span class="math">O(\\log n)</span> steps consuming <span class="math">n</span> CPUs where <span class="math">n</span> is the size of the input array.</p>

    <h4 id="sec-135" class="text-lg font-semibold mt-6">C.3.3 Oblivious Bin Placement</h4>

    <p class="text-gray-300">Oblivious bin placement is the following task: given an input array <span class="math">X</span>, and a vector <span class="math">V</span> where <span class="math">V[i]</span> denotes the intended load of bin <span class="math">i</span>, the goal is to place the first <span class="math">V[1]</span> elements of <span class="math">X</span> into bin 1, place the next <span class="math">V[2]</span> elements of <span class="math">X</span> into bin 2, and so on. All output bins are padded with dummies to a maximum capacity <span class="math">Z</span>. Once the input <span class="math">X</span> is fully consumed, all remaining bins will contain solely dummies.</p>

    <p class="text-gray-300">ObliviousBinPlace<span class="math">(X,V,Z)</span>:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Let <span class="math">W</span> be the accumulated sum of <span class="math">V</span>, i.e., <span class="math">W[i]=\\sum_{j\\leq i}V[j]</span>. Obliviously sort all elements of <span class="math">X</span> and <span class="math">W</span> together, where each element carries the information whether it comes from <span class="math">X</span> or <span class="math">Z</span>. The sorting is governed by the following key assignments: an element in <span class="math">X</span> is assigned the key that equals to its position in <span class="math">X</span>, and the <span class="math">i</span>-th element in <span class="math">W</span> is assigned the key that is equal to <span class="math">W[i]</span>. If two elements have the same key, then the one from <span class="math">W</span> appears later.</li>

      <li>In this sorted array, every element from <span class="math">X</span> wants to hear from the first element from <span class="math">W</span> that appears after itself. We accomplish this by calling an oblivious propagation algorithm (see Section C.3.2) such that at the end, each element from <span class="math">X</span> learns which bin it is destined for</li>

    </ul>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">- In one scan of the resulting array, mark every element from <span class="math">W</span> as dummy. For every <span class="math">i\\in[B]</span> where $B=</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">W</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math">, append </span>Z<span class="math"> filler elements destined for bin </span>i$ to the array (note that fillers are not dummies).</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Obliviously sort the outcome array by destined bin number, leaving all dummies at the end. If two elements have the same bin number, filler appear after real elements.</li>

      <li>In one linear scan, mark all but the first <span class="math">Z</span> elements of every bin as dummy.</li>

      <li>Obliviously sort by bin number, leaving all dummies at the end. If two elements have the same bin number, break ties by arranging smaller elements first, and having fillers appear after real elements.</li>

      <li>Truncate the result and preserve only the first <span class="math">BZ</span> elements. In one scan, rewrite every filler as dummy.</li>

      <li>Return the outcome.</li>

    </ul>`;
---

<BaseLayout title="Foundations of Differentially Oblivious Algorithms (2017/1033)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2017 &middot; eprint 2017/1033
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <p class="mt-4 text-xs text-gray-500">
        All content below belongs to the original authors. This page
        reproduces the paper for educational purposes. Always
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >cite the original</a>.
      </p>
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <Fragment set:html={CONTENT} />

  </article>
</BaseLayout>
