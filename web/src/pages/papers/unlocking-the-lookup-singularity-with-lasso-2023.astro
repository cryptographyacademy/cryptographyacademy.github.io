---
import BaseLayout from '../../layouts/BaseLayout.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2023/1216';
const CRAWLER = 'mistral';
const CONVERTED_DATE = '2026-02-16';
const TITLE_HTML = 'Unlocking the lookup singularity with Lasso';
const AUTHORS_HTML = 'Srinath Setty, Justin Thaler, Riad Wahby';

const CONTENT = `    <p class="text-gray-300">Srinath Setty* Justin Thaler† Riad Wahby‡</p>

    <h2 id="sec-2" class="text-2xl font-bold">Abstract</h2>

    <p class="text-gray-300">This paper introduces Lasso, a new family of lookup arguments, which allow an untrusted prover to commit to a vector <span class="math">a \\in \\mathbb{F}^m</span> and prove that all entries of <span class="math">a</span> reside in some predetermined table <span class="math">t \\in \\mathbb{F}^n</span>. Lasso's performance characteristics unlock the so-called "lookup singularity". Lasso works with any multilinear polynomial commitment scheme, and provides the following efficiency properties.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>For <span class="math">m</span> lookups into a table of size <span class="math">n</span>, Lasso's prover commits to just <span class="math">m + n</span> field elements. Moreover, the committed field elements are small, meaning that, no matter how big the field <span class="math">\\mathbb{F}</span> is, they are all in the set <span class="math">\\{0,\\ldots,m\\}</span>. When using a multiexponentiation-based commitment scheme, this results in the prover's costs dominated by only <span class="math">O(m + n)</span> group operations (e.g., elliptic curve point additions), plus the cost to prove an evaluation of a multilinear polynomial whose evaluations over the Boolean hypercube are the table entries. This represents a significant improvement in prover costs over prior lookup arguments (e.g., plookup, Halo2's lookups, lookup arguments based on logarithmic derivatives).</li>

    </ul>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Unlike all prior lookup arguments, if the table <span class="math">t</span> is structured (in a precise sense that we define), then no party needs to commit to <span class="math">t</span>, enabling the use of much larger tables than prior works (e.g., of size <span class="math">2^{128}</span> or larger). Moreover, Lasso's prover only "pays" in runtime for table entries that are accessed by the lookup operations. This applies to tables commonly used to implement range checks, bitwise operations, big-number arithmetic, and even transitions of a full-fledged CPU such as RISC-V. Specifically, for any integer parameter <span class="math">c &amp;gt; 1</span>, Lasso's prover's dominant cost is committing to <span class="math">3 \\cdot c \\cdot m + c \\cdot n^{1/c}</span> field elements. Furthermore, all these field elements are "small", meaning they are in the set <span class="math">\\{0, \\ldots, \\max\\{m, n^{1/c}, q\\} - 1\\}</span>, where <span class="math">q</span> is the maximum value in <span class="math">a</span>.</li>

    </ul>

    <p class="text-gray-300">Lasso's starting point is Spark, a time-optimal polynomial commitment scheme for sparse polynomials in Spartan (CRYPTO 2020). We first provide a stronger security analysis for Spark. Spartan's security analysis assumed that certain metadata associated with a sparse polynomial is committed by an honest party (this is acceptable for its purpose in Spartan, but not for Lasso). We prove that Spark remains secure even when that metadata is committed by a malicious party. This provides the first "standard" commitment scheme for sparse multilinear polynomials with optimal prover costs. We then generalize Spark to directly support a lookup argument for both structured and unstructured tables, with the efficiency characteristics noted above.</p>

    <p class="text-gray-300">*Microsoft Research</p>

    <p class="text-gray-300">\\dagger a16 crypto research and Georgetown University</p>

    <p class="text-gray-300">\\ddagger Carnegie Mellon University</p>

    <p class="text-gray-300">1 Introduction</p>

    <p class="text-gray-300">Suppose that an untrusted prover <span class="math">\\mathcal{P}</span> claims to know a witness <span class="math">w</span> satisfying some property. For example, <span class="math">w</span> might be a pre-image of a designated value <span class="math">y</span> of a cryptographic hash function <span class="math">h</span>, i.e., a <span class="math">w</span> such that <span class="math">h(w)=y</span>. A trivial proof is for <span class="math">\\mathcal{P}</span> to send <span class="math">w</span> to the verifier <span class="math">\\mathcal{V}</span>, who checks that <span class="math">w</span> satisfies the claimed property.</p>

    <p class="text-gray-300">A zero-knowledge succinct non-interactive argument of knowledge (zkSNARK) achieves the same, but with better verification costs (and proof sizes) and privacy properties. Succinct means that verifying a proof is much faster than checking the witness directly (this also implies that proofs are much smaller than the size of the statement proven). Zero-knowledge means that the verifier does not learn anything about the witness beyond the validity of the statement proven.</p>

    <h4 id="sec-3" class="text-lg font-semibold mt-6">Fast algorithms via lookup tables.</h4>

    <p class="text-gray-300">A common technique in the design of fast algorithms is to use <em>lookup tables</em>. These are pre-computed tables of values that, once computed, enable certain operations to be computed quickly. For example, in <em>tabulation-based universal hashing</em> <em>[x21, x22]</em>, the hashing algorithm is specified via some small number <span class="math">c</span> of tables <span class="math">T_{1},\\ldots,T_{c}</span>, each of size <span class="math">n^{1/c}</span>. Each cell of each table is filled with a random <span class="math">q</span>-bit number in a preprocessing step. To hash a key <span class="math">x</span> of length <span class="math">n</span>, the key is split into <span class="math">c</span> “chunks” <span class="math">x_{1},\\ldots,x_{c}\\in\\{0,1\\}^{n/c}</span>, and the hash value is defined to be the bitwise XOR of <span class="math">c</span> <em>table lookups</em> i.e., <span class="math">\\oplus_{i=1}^{c}T_{i}[x_{i}]</span>.</p>

    <p class="text-gray-300">Lookup tables are also useful in the context of SNARKs. Recall that to apply SNARKs to prove the correct execution of computer programs, one must express the execution of the program in a specific form that is amenable to probabilistic checking (e.g., as arithmetic circuits or generalizations thereof). Lookup tables can facilitate the use of substantially smaller circuits.</p>

    <p class="text-gray-300">For example, imagine that a prover wishes to establish that at no point in a program’s execution did any integer ever exceed <span class="math">2^{128}</span>, say, because were that to happen then an uncorrected “overflow error” would occur. A naive approach to accomplish this inside a circuit-satisfiability instance is to have the circuit take as part of its “non-deterministic advice inputs” 128 field elements for each number <span class="math">x</span> arising during the execution. If the prover is honest, these 128 advice elements will be set to the binary representation of <span class="math">x</span>. The circuit must check that all of the 128 advice elements are in <span class="math">\\{0,1\\}</span> and that they indeed equal the binary representation of <span class="math">x</span>, i.e., <span class="math">x=\\sum_{i=0}^{127}2^{i}\\cdot b_{i}</span>, where <span class="math">b_{0},\\ldots,b_{127}</span> denotes the advice elements. This is very expensive: a simple overflow check turns into at least 129 constraints and an additional 128 field elements in the prover’s witness that must be cryptographically committed by the prover.</p>

    <p class="text-gray-300">Lookup tables offer a better approach. Imagine for a moment that the prover and the verifier initialize a lookup table containing all integers between <span class="math">0</span> and <span class="math">2^{128}-1</span>. Then the overflow check above amounts to simply confirming that <span class="math">x</span> is in the table, i.e., the overflow check <em>is</em> a single table lookup. Of course, a table of size <span class="math">2^{128}</span> is far too large to be explicitly represented—even by the prover. This paper describes techniques to enable such a table lookup without requiring a table such as this to ever be explicitly materialized, by either the prover or the verifier.</p>

    <p class="text-gray-300">Table lookups are now used pervasively in deployed applications that employ SNARKs. They are very useful for representing “non-arithmetic” operations efficiently inside circuits <em>[BCG^{+}18, x11, x12]</em>. The above example is often called a <em>range check</em> for the range <span class="math">\\{0,1,\\ldots,2^{128}-1\\}</span>. Other example operations for which lookups are useful include bitwise operations such as XOR and AND <em>[BCG^{+}18]</em>, and any operations that require big-number arithmetic.</p>

    <h4 id="sec-4" class="text-lg font-semibold mt-6">Lookup arguments.</h4>

    <p class="text-gray-300">To formalize the above discussion regarding the utility of lookup tables in SNARKs, a (non-interactive) <em>lookup argument</em> is a SNARK for the following claim made by the prover.</p>

    <h6 id="sec-5" class="text-base font-medium mt-4">Definition 1.1 (Statement proven in a lookup argument).</h6>

    <p class="text-gray-300">Given a commitment <span class="math">\\mathsf{cm}_{a}</span> and a public set <span class="math">T</span> of <span class="math">N</span> field elements, represented as vector <span class="math">t=(t_{0},\\ldots,t_{N-1})\\in\\mathbb{F}^{N}</span> to which the verifier has (possibly) been</p>

    <p class="text-gray-300">provided a commitment <span class="math">\\mathsf{cm}_t</span>, the prover knows an opening <span class="math">a = (a_0, \\ldots, a_{m-1}) \\in \\mathbb{F}^m</span> of <span class="math">\\mathsf{cm}_a</span> such that all elements of <span class="math">a</span> are in <span class="math">T</span>. That is, for each <span class="math">i = 0, \\ldots, m-1</span>, there is a <span class="math">j \\in \\{0, \\ldots, N-1\\}</span> such that <span class="math">a_i = t_j</span>.</p>

    <p class="text-gray-300">The set <span class="math">T</span> in Definition 1.1 is the contents of a lookup table and the vector <span class="math">a</span> is the sequence of "lookups" into the table. The prover in the lookup argument proves to the verifier that every element of <span class="math">a</span> is in <span class="math">T</span>.</p>

    <p class="text-gray-300">A recent flurry of works (Caulk [ZBK+22], Caulk+ [PK22], flookup [GK22], Baloo [ZGK+22], and cq [EFG22]) have sought to give lookup arguments in which the prover's runtime is sublinear in the table size <span class="math">N</span>. This is important in applications where the lookup table itself is much larger than the number of lookups into that table. As a simple and concrete example, if the verifier wishes to confirm that <span class="math">a_0, \\ldots, a_{m-1}</span> are all in a large range (say, in <span class="math">\\{0, 1, \\ldots, 2^32 - 1\\}</span>), then performing a number of cryptographic operations linear in <span class="math">N</span> will be slow or possibly untenable. For performance reasons, these papers also express a desire for the commitment scheme used to commit to <span class="math">a</span> and <span class="math">t</span> to be additively homomorphic. However, these prior works all require generating a structured reference string of size <span class="math">N</span> as well as an additional pre-processing work of <span class="math">O(N \\log N)</span> group exponentiations. This limits the size of the tables to which they can be applied. For example, the largest structured reference strings generated today are many gigabytes in size and still only support <span class="math">N &amp;lt; 2^{30}</span>.</p>

    <p class="text-gray-300"><strong>Indexed lookup arguments.</strong> Definition 1.1 is a standard formulation of lookup arguments in SNARKs (e.g., see [ZGK+22]). It treats the table as an unordered list of values—<span class="math">T</span> is a set and, accordingly, reordering the vector <span class="math">t</span> does not alter the validity of the prover's claim. However, for reasons that will become apparent shortly (see Section 1.3), we consider a variant notion to be equally natural. We refer to this variant as an indexed lookup argument (and refer to the standard variant in Definition 1.1 as an unindexed lookup argument.) In an indexed lookup argument, in addition to a commitment to <span class="math">a \\in \\mathbb{F}^m</span>, the verifier is also handed a commitment to a second vector <span class="math">b \\in \\mathbb{F}^m</span>. The prover claims that for all <span class="math">i = 1, \\ldots, m</span>, <span class="math">a_i = t_{b_i}</span>. We refer to <span class="math">a</span> as the vector of looked-up values, and <span class="math">b</span> as the vector of indices.</p>

    <p class="text-gray-300"><strong>Definition 1.2 (Statement proven in an indexed lookup argument).</strong> Given commitment <span class="math">\\mathsf{cm}_a</span> and <span class="math">\\mathsf{cm}_b</span>, and a public array <span class="math">T</span> of <span class="math">N</span> field elements, represented as vector <span class="math">t = (t_0, \\ldots, t_{N-1}) \\in \\mathbb{F}^N</span> to which the verifier has (possibly) been provided a commitment <span class="math">\\mathsf{cm}_t</span>, the prover knows an opening <span class="math">a = (a_0, \\ldots, a_{m-1}) \\in \\mathbb{F}^m</span> of <span class="math">\\mathsf{cm}_a</span> and <span class="math">b = (b_0, \\ldots, b_{m-1}) \\in \\mathbb{F}^m</span> of <span class="math">\\mathsf{cm}_b</span> such that for each <span class="math">i = 0, \\ldots, m-1</span>, <span class="math">a_i = T[b_j]</span>, where <span class="math">T[b_j]</span> is short hand for the <span class="math">b_j</span>'th entry of <span class="math">t</span>.</p>

    <p class="text-gray-300">Any indexed lookup argument can easily be turned into an unindexed lookup argument: the unindexed lookup argument prover simply commits to a vector <span class="math">b</span> such that <span class="math">a_i = T[b_j]</span> for all <span class="math">i</span>, and then applies the indexed lookup argument to prove that indeed this holds. There is also a generic transformation that turns any unindexed lookup argument into an indexed one, at least in fields of large enough characteristic (see Appendix A). However, the protocols we describe in this work directly yield indexed lookup arguments, without having to invoke this transformation. Accordingly, our primary focus in this work is on indexed lookup arguments.</p>

    <h2 id="sec-6" class="text-2xl font-bold">1.1 Lasso: A new lookup argument</h2>

    <p class="text-gray-300">We describe a new lookup argument, Lasso.³ Lasso's starting point is a polynomial commitment scheme for sparse multilinear polynomials. In particular, Lasso builds on Spark, an optimal polynomial commitment scheme for sparse multilinear polynomials from Spartan [Set20]. Spark itself is based on the linear-time sum-check protocol [LFKN90] and offline memory checking [BEG+91].</p>

    <p class="text-gray-300">Lasso can be instantiated with any multilinear polynomial commitment scheme. Furthermore, Lasso can be used with any SNARK, including those that prove R1CS or Plonkish satisfiability. This is particularly seamless for SNARKs that have the prover commit to the witness using a multilinear polynomial commitment scheme. This includes many known prover-efficient SNARKs [Set20, GLS+21, XZS22, CBBZ23, STW23]. If a SNARK does not natively use multilinear polynomial commitments (e.g., Marlin [CHM+20] and Plonk</p>

    <p class="text-gray-300">³Lasso is short for LASSO-of-Truth: Lookup Arguments via Sparse-polynomial-commitments and the Sum-check protocol, including for Oversized Tables.</p>

    <p class="text-gray-300">[GWC19], which use univariate polynomial commitments), then one would need an auxiliary argument that the commitment <span class="math">\\mathsf{cm}_a</span> used in Lasso is a commitment to the multilinear extension of the vector of all lookups performed in the SNARK.</p>

    <p class="text-gray-300">Below, we provide an overview of Lasso's technical components.</p>

    <p class="text-gray-300">(1) A stronger analysis of Spark, an optimal commitment scheme for sparse polynomials. A sparse polynomial commitment allows an untrusted prover to cryptographically commit to a sparse multilinear polynomial <span class="math">g</span> and later provide a requested evaluation <span class="math">g(r)</span> along with a proof that the provided value is indeed equal to the committed polynomial's evaluation at <span class="math">r</span>. Crucially, we require that the prover's runtime depends only on the sparsity of the polynomial. <span class="math">^4</span> Spartan [Set20] provides such a commitment scheme, which it calls Spark. Spartan assumed that certain metadata associated with the sparse polynomial is committed honestly, which was sufficient for its purposes. But, as we see later, Lasso requires an untrusted prover to commit to sparse polynomials (and the associated metadata).</p>

    <p class="text-gray-300">A naive extension Spark to handle a maliciously committed metadata incurs concrete and asymptotic overheads, which is undesirable. Nevertheless, we prove that Spark in fact satisfies a stronger security property without any modifications (i.e., it is secure even if the metadata is committed by a potentially malicious party). This provides the first "standard" sparse polynomial commitment scheme with optimal prover costs, a result of independent interest. Furthermore, we specialize Spark for Lasso's use to obtain concrete efficiency benefits.</p>

    <p class="text-gray-300">(2) Surge: A generalization of Spark. We reinterpret Spark sparse polynomial commitment scheme as a technique for computing the inner product of an <span class="math">m</span>-sparse committed vector of length <span class="math">N</span> with a dense—but highly structured—lookup table of size <span class="math">N</span> (the table is represented as a vector of size <span class="math">N</span>). Specifically, in the sparse polynomial commitment scheme, the table consists of all <span class="math">(\\log N)</span>-variate Lagrange basis polynomials evaluated at a specific point <span class="math">r \\in \\mathbb{F}^{\\log N}</span>. Furthermore, this table is a tensor product of <span class="math">c \\geq 2</span> smaller tables, each of size <span class="math">N^{1/c}</span> (here, <span class="math">c</span> can be set to any desired integer in <span class="math">\\{1, \\ldots, \\log N\\}</span>). We further observe that many other lookup tables can similarly be decomposed as product-like expressions of <span class="math">O(c)</span> tables of size <span class="math">N^{1/c}</span>, and that Spark extends to support all such tables.</p>

    <p class="text-gray-300">Exploiting this perspective, we describe Surge, a generalization of Spark that allows an untrusted prover to commit to any sparse vector and establish the sparse vector's inner product with any dense, structured vector. We refer to the structure required for this to work as Spark-only structure (SOS for short). We also refer to this property as decomposability. In more detail, an SOS table <span class="math">T</span> is one that can be decomposed into <span class="math">\\alpha = O(c)</span> "sub-tables" <span class="math">\\{T_1, \\ldots, T_\\alpha\\}</span> of size <span class="math">N^{1/c}</span> satisfying the following two properties. First, any entry <span class="math">T[j]</span> of <span class="math">T</span> can be expressed as a simple expression of a corresponding entry into each of <span class="math">T_1, \\ldots, T_\\alpha</span>. Second, the so-called multilinear extension polynomial of each <span class="math">T_i</span> can be evaluated quickly (for any such table, we call <span class="math">T_i</span> MLE-structured, where MLE stands for multilinear extension). For example, as noted above, the table <span class="math">T</span> arising in Spark itself is simply the tensor product of MLE-structured sub-tables <span class="math">\\{T_1, \\ldots, T_\\alpha\\}</span>, where <span class="math">\\alpha = c</span>.</p>

    <p class="text-gray-300">(3) Lasso: A lookup argument for SOS tables and small/unstructured tables. We observe that Surge directly provides a lookup argument for tables with SOS structure. We call the resulting lookup argument Lasso. Lasso has the important property that all field elements committed by the prover are "small", meaning they are in the set <span class="math">\\{0,1,\\ldots,\\max\\{m,N^{1/c},q\\}-1\\}</span>, where <span class="math">q</span> is such that <span class="math">\\{T_1,\\dots,T_\\alpha\\}</span> all have entries in the set <span class="math">\\{0,1,\\ldots,q-1\\}</span>. As elaborated upon shortly (Section 1.2), this property of Lasso has substantial implications for prover efficiency.</p>

    <p class="text-gray-300">Lasso has new and attractive costs when applied to small and unstructured tables in addition to large SOS ones. Specifically, by setting <span class="math">c = 1</span>, the Lasso prover commits to only about <span class="math">m + N</span> field elements, and all of</p>

    <p class="text-gray-300"><span class="math">^4</span>For multilinear polynomials, <span class="math">m</span>-sparse refers to polynomials <span class="math">g\\colon \\mathbb{F}^{\\ell}\\to \\mathbb{F}</span> in <span class="math">\\ell</span> variables such that <span class="math">g(x)\\neq 0</span> for at most <span class="math">m</span> values of <span class="math">x\\in \\{0,1\\}^{\\ell}</span>. In other words, <span class="math">g</span> has at most <span class="math">m</span> non-zero coefficients in the so-called multilinear Lagrange polynomial basis. There are <span class="math">n\\coloneqq 2^{\\ell}</span> Lagrange basis polynomials, so if <span class="math">m\\ll 2^{\\ell}</span>, then only a tiny fraction of the possible coefficients are non-zero. In contrast, if <span class="math">m = \\Theta (2^{\\ell})</span>, then we refer to <span class="math">g</span> as a dense polynomial.</p>

    <p class="text-gray-300">4</p>

    <p class="text-gray-300">the committed elements are <span class="math">\\{0,1,\\ldots,\\max\\{m,N,q\\}\\}</span> where <span class="math">q</span> is the size of the largest value in the table.56 Lasso is the first lookup argument with this property, which substantially speeds up commitment computation when <span class="math">m</span>, <span class="math">N</span>, and <span class="math">q</span> are all much smaller than the size of the field over which the commitment scheme is defined. For <span class="math">c&amp;gt;1</span>, the number of field elements that the Lasso prover commits to is <span class="math">3cm+\\alpha\\cdot N^{1/c}</span>.</p>

    <p class="text-gray-300">(4) GeneralizedLasso: Beyond SOS and small/unstructured tables. Finally, we describe a lookup argument that we call GeneralizedLasso, which applies to any MLE-structured table, not only decomposable ones. The main disadvantage of GeneralizedLasso relative to Lasso is that <span class="math">cm</span> out of the <span class="math">3cm+cN^{1/c}</span> field elements committed by the GeneralizedLasso prover are random rather than small. The proofs are also somewhat larger, as GeneralizedLasso involves one extra invocation of the sum-check protocol compared to Lasso.</p>

    <p class="text-gray-300">GeneralizedLasso is reminiscent of a sum-check based SNARK (e.g., Spartan [Set20]) and is similarly built from a combination of the sum-check protocol and the Spark sparse polynomial commitment scheme. There are two key differences: (1) in GeneralizedLasso, the (potentially adversarial) prover commits to a sparse polynomial, rather than an honest "setup algorithm" committing to a sparse polynomial in a preprocessing step in the context of Spartan (where the sparse polynomial encodes the circuit or constraint system of interest); and (2) invoking the standard linear-time sum-check protocol [LFKN90, CTY11, Tha13] makes the prover incur costs linear in the table size rather than the number of lookups. To address (1), we invoke our stronger security analysis of Spark. To address (2), we introduce a new variant of the sum-check protocol tailored for our setting, which we refer to as the sparse-dense sum-check protocol. Conceptually, GeneralizedLasso can be viewed as using the sparse-dense sum-check protocol to reduce lookups into any MLE-structured table into lookups into a decomposable table (namely, a certain lookup table arising within the Spark polynomial commitment scheme).</p>

    <p class="text-gray-300">Additional discussion of the benefits and costs of GeneralizedLasso relative to Lasso can be found in Section 1.4.</p>

    <h2 id="sec-7" class="text-2xl font-bold">1.2 Additional discussion of Lasso's costs</h2>

    <p class="text-gray-300">Polynomial commitments and MSMs. As indicated above, a central component of most SNARKs is a cryptographic protocol called a polynomial commitment scheme. Such a scheme allows an untrusted prover to succinctly commit to a polynomial <span class="math">p</span> and later reveal an evaluation <span class="math">p(r)</span> for a point <span class="math">r</span> chosen by the verifier (the prover will also return a proof that the claimed evaluation is indeed equal to the committed polynomial's evaluation at <span class="math">r</span>). In Lasso, the bottleneck for the prover is the polynomial commitment scheme.</p>

    <p class="text-gray-300">Many popular polynomial commitments are based on multiexponentiations (also known as multi-scalar multiplications, or MSMs). This means that the commitment to a polynomial <span class="math">p</span> (with <span class="math">n</span> coefficients <span class="math">c_{0},\\ldots ,c_{n - 1}</span> over an appropriate basis) is</p>

    <div class="my-4 text-center"><span class="math-block">\\prod_{i=0}^{n-1} g_{i}^{c_{i}},</span></div>

    <p class="text-gray-300">for some public generators <span class="math">g_{1},\\ldots ,g_{n}</span> of a multiplicative group <span class="math">\\mathbb{G}</span>. Examples include KZG [KZG10], Bulletproofs/IPA [BCC+16, BBB+18], Hyrax [WTS+18], and Dory [Lee21].</p>

    <p class="text-gray-300">The naive MSM algorithm performs <span class="math">n</span> group exponentiations and <span class="math">n</span> group multiplications (note that each group exponentiation is about <span class="math">400 \\times</span> slower than a group multiplication). But Pippenger's MSM algorithm saves a factor of about <span class="math">\\log(n)</span> relative to the naive algorithm. This factor can be well over <span class="math">10 \\times</span> in practice.</p>

    <p class="text-gray-300">5 Lasso makes blackbox use of any so-called grand product argument. If using the grand product argument from [SL20, Section 6], a low-order number, say at most <span class="math">O(m / \\log^3 m)</span>, of large field elements need to be committed (see Section E for discussion).</p>

    <p class="text-gray-300">6 If Lasso is used as an indexed lookup argument, the prover commits to <span class="math">m + N</span> field elements. If used as an unindexed lookup argument, the number can increase to <span class="math">2m + N</span> because in the unindexed setting one must "charge" for the prover to commit to the index vector <span class="math">b \\in \\mathbb{F}^m</span>.</p>

    <p class="text-gray-300">7 In fact, GeneralizedLasso applies to any table with some low-degree extension, not necessarily its multilinear one, that is evaluable in logarithmic time.</p>

    <p class="text-gray-300">8 In Hyrax and Dory, the prover does <span class="math">\\sqrt{n}</span> MSMs each of size <span class="math">\\sqrt{n}</span>.</p>

    <p class="text-gray-300">Working over large fields, but committing to small elements. If all exponents appearing in the multiexponentiation are "small", one can save another factor of  <span class="math">10 \\times</span>  relative to applying Pippenger's algorithm to an MSM involving random exponents. This is analogous to how computing  <span class="math">g_{i}^{\\mathbb{F}^{16}}</span>  is  <span class="math">10 \\times</span>  faster than computing  <span class="math">g_{i}^{\\mathbb{F}^{160}}</span> : the first requires 16 squaring operations, while the second requires 160 such operations.</p>

    <p class="text-gray-300">In other words, if one is promised that all field elements (i.e., exponents) to be committed via an MSM are in the set  <span class="math">\\{0,1,\\ldots ,K\\} \\subset \\mathbb{F}</span> , the number of group operations required to compute the MSM depend only on  <span class="math">K</span>  and not on the size of  <span class="math">\\mathbb{F}</span> .</p>

    <p class="text-gray-300">Quantitatively, if all exponents are upper bounded by some value  <span class="math">K</span> , with  <span class="math">K \\ll n</span> , then Pippenger's algorithm only needs (about) one group operation per term in the multiexponentiation.10 More generally, with any MSM-based commitment scheme, Pippenger's algorithm allows the prover to commit to roughly  <span class="math">k \\cdot \\log(n)</span> -bit field elements (meaning field elements in  <span class="math">\\{0, 1, \\ldots, n\\}</span> ) with only  <span class="math">k</span>  group operations per committed field element.</p>

    <p class="text-gray-300">Polynomial evaluation proofs. In any SNARK or lookup argument, the prover not only has to commit to one or more polynomials, but also reveal to the verifier an evaluation of the committed polynomials at a point of the verifier's choosing. This requires the prover to compute a so-called evaluation proof, which establishes that the returned evaluation is indeed consistent with the committed polynomial. For some polynomial commitment schemes, such as Bulletproofs/IPA [BCC+16, BBB+18], evaluation proofs are quite slow and this cost can bottleneck the prover. However, for others, evaluation proof computation is a low-order cost [WTS+18, BBHR18]. In this work, we add another commitment scheme to this list, introducing Sona (Section 1.5), which combines the excellent commitment time of Hyrax, and evaluation proof computation involving sublinear cryptographic work, with the excellent verification costs of Nova.</p>

    <p class="text-gray-300">Moreover, evaluation proofs exhibit excellent batching properties (whereby the prover can commit to many polynomials and only produce a single evaluation proof across all of them) [BGH19, KST22, BDFG20]. So in many contexts, computing opening proofs is not a bottleneck even when a scheme such as Bulletproofs/IPA.</p>

    <p class="text-gray-300">For all of the above reasons, our accounting of prover cost in this work generally ignores the cost of polynomial evaluation proofs.</p>

    <p class="text-gray-300">Summarizing Lasso's prover costs. Based on the above accounting, Lasso's prover costs when applied to a lookup table  <span class="math">T</span>  can be summarized as follows.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Setting the parameter  <span class="math">c = 1</span> , the Lasso prover commits to just  <span class="math">m + N</span>  field elements (using any multilinear polynomial commitment scheme), all of which are in  <span class="math">\\{0, \\dots, m\\}</span> . Using an MSM-based commitment scheme, this translates to very close to  <span class="math">m + N</span>  group operations.</li>

      <li>For  <span class="math">c &amp;gt; 1</span> , the Lasso prover applied to any decomposable table commits to  <span class="math">3cm + \\alpha N^{1/c}</span>  field elements, all of which are in the set  <span class="math">\\{0, \\dots, \\max\\{m, N^{1/c}, q\\} - 1\\}</span> , where  <span class="math">q</span>  is the largest value in any of the  <span class="math">\\alpha</span>  sub-tables  <span class="math">T_1, \\dots, T_\\alpha</span> .</li>

      <li>The GeneralizedLasso prover applies to any MLE-structured table, and commits to the same number of field elements as the Lasso prover, but  <span class="math">cm</span>  of them are random field elements, instead of small ones.</li>

    </ul>

    <p class="text-gray-300">In all cases above, no party needs to cryptographically commit to the table  <span class="math">T</span>  or subtables  <span class="math">T_{1},\\ldots ,T_{\\alpha}</span> , so long as they are MLE-structured.</p>

    <p class="text-gray-300">In Appendix B, we compare these costs with those of existing lookup arguments.</p>

    <p class="text-gray-300">1.3 A companion work: Jolt, and the lookup singularity</p>

    <p class="text-gray-300">In the context of SNARKs, a front-end is a transformation or compiler that turns any computer program into an intermediate representation—typically a variant of circuit-satisfiability—so that a back-end (i.e., a SNARK for circuit-satisfiability) can be applied to establish that the prover correctly ran the computer program on a witness. A companion paper called Jolt (for “Just One Lookup Table”) shows that Lasso’s ability to handle gigantic tables without either prover or verifier ever materializing the whole table (so long as the table is modestly “structured”) enables substantial improvements in the front-end design.</p>

    <p class="text-gray-300">Jolt’s idea is cleanest to describe in the context of a front-end for a simple virtual machine (VM), which in SNARK design has become synonymous with the notion of a CPU. A VM is defined by a set of primitive instructions (called an instruction set), one of which is executed at each step of the program. Typically, a front-end for a SNARK outputs a circuit, that for each step of the computation, (a) determines which instruction should be executed at that step and (b) executes the instruction. Jolt uses Lasso to replace part (b) at each step with a single lookup, into a gigantic lookup table. Specifically, consider the popular RISC-V instruction set <em>[x21]</em>, targeted by the RISC-Zero project. For each of the primitive RISC-V instructions <span class="math">f_{i}</span>, the idea of Jolt to create a lookup table that contains the entire evaluation table of <span class="math">f_{i}</span>. For example, if <span class="math">f_{i}</span> takes two 64-bit inputs, the table will have <span class="math">2^{128}</span> entries, whose <span class="math">(x,y)</span>’th entry is <span class="math">f_{i}(x,y)</span>. One can “glue together” the tables for each instruction, into a single table of size <span class="math">2^{128}</span> times the number of instructions.</p>

    <p class="text-gray-300">Jolt shows that for each of the RISC-V instructions (including multiplication instructions and division and remainder instructions), the resulting table has the structure that we require to apply Lasso. This leads to a front-end for VMs such as RISC-V that outputs much smaller circuits than prior front-ends, and has additional benefits such as easier auditability. Preliminary estimates from Jolt show that, when applied to the RISC-V instruction set over 64-bit data types, the prover commits to <span class="math">\\leq 65</span> field elements per step of the RISC-V CPU. Of these field elements, about a third lie in <span class="math">\\{0,1\\}</span>, only five are larger than about <span class="math">2^{22}</span>, and none are larger than <span class="math">2^{64}</span>. This means that Jolt’s prover costs when applied to a <span class="math">T</span>-step execution of the RISC-V CPU on 64-bit data types is equivalent to computing roughly 6 multiexponentiations of size <span class="math">T</span> if using a 256-bit field. Put another way, the Jolt prover’s runtime is equivalent to committing to about 6 arbitrary field elements per step of the RISC-V CPU.</p>

    <p class="text-gray-300">We believe that Lasso and Jolt together essentially achieve a vision outlined by Barry Whitehat called the lookup singularity <em>[x23]</em>. The lookup singularity seeks to transform arbitrary computer program into “circuits” that only perform lookups. Whitehat’s post outlines many benefits to achieving this vision, from improved performance to auditability and formal verification of the correctness of the front-end.</p>

    <h3 id="sec-8" class="text-xl font-semibold mt-8">1.4 Lasso vs. GeneralizedLasso</h3>

    <h5 id="sec-9" class="text-base font-semibold mt-4">The relationship between MLE-structured and decomposable tables.</h5>

    <p class="text-gray-300">For any decomposable table <span class="math">T\\in\\mathbb{F}^{N}</span>, there is always some low-degree extension polynomial <span class="math">\\hat{T}</span> of <span class="math">T</span> (namely, an extension of degree at most <span class="math">k</span> in each variable) that can be evaluated in <span class="math">O(\\log N)</span> time. In general, <span class="math">\\hat{T}</span> is not necessarily multilinear, so a table being decomposable does not necessarily imply that it is MLE-structured. But GeneralizedLasso actually applies to any table with a low-degree extension that is evaluable in logarithmic time. In this sense, decomposability (the condition required to apply Lasso) is a stronger condition than what is necessary to apply GeneralizedLasso.</p>

    <h5 id="sec-10" class="text-base font-semibold mt-4">Pros and cons of GeneralizedLasso.</h5>

    <p class="text-gray-300">We currently do not know specific tables of interest for which GeneralizedLasso applies but Lasso does not. In particular, all lookup tables arising in our companion paper Jolt are decomposable. However, there are benefits to GeneralizedLasso that may justify its increased costs.</p>

    <p class="text-gray-300">For example, Jolt works conceptually by taking one lookup table for each primitive RISC-V instruction and concatenating them together into a single gigantic table. Jolt shows that each of the constituent tables (one per instruction) is both MLE-structured and decomposable. It is trivial to show that the concatenation of MLE-structured tables is MLE-structured, and the GeneralizedLasso verifier when applied to the concatenated table is essentially no more complicated than the GeneralizedLasso verifier when applied to each table individually.</p>

    <p class="text-gray-300">In contrast, while it is true that the concatenation of decomposable tables is decomposable, implementing the concatenated table's decomposition can be quite involved (at least, when the decompositions of the constituent tables are all different, as is the case with Jolt). This is particularly relevant because any implementation of the Lasso verifier applied to a given table depends on the decomposition of the table.</p>

    <p class="text-gray-300">In summary, although Lasso is more performative than GeneralizedLasso in the context of decomposable tables, for some lookup tables the Lasso verifier implementation may be more complicated. Hence, even if future work does not identify MLE-structured tables of interest that are not decomposable, there are nonetheless simplicity and auditability benefits to GeneralizedLasso that may compensate for its diminished relative performance.[13]</p>

    <h2 id="sec-11" class="text-2xl font-bold">1.5 Sona: A new transparent polynomial commitment scheme</h2>

    <p class="text-gray-300">Hyrax [WTS+18] provides a multilinear polynomial commitment scheme (for random evaluation queries) with attractive prover costs. To commit to an <span class="math">\\ell</span>-variate multilinear polynomial (which means the polynomial has <span class="math">m = 2^{\\ell}</span> coefficients), the prover performs <span class="math">\\sqrt{m}</span> multiexponentiations each of length <span class="math">\\sqrt{m}</span>. To compute an evaluation proof, the prover performs <span class="math">O(m)</span> field operations and a <span class="math">O(\\sqrt{m})</span> exponentiations (this requires applying Bulletproofs to prove an inner product instance consisting of vectors of length <span class="math">\\sqrt{m}</span>); an evaluation proof consists of <span class="math">O(\\log m)</span> group elements.</p>

    <p class="text-gray-300">The downside of Hyrax's commitment scheme is that the verification costs are large: commitments consist of <span class="math">\\sqrt{m}</span> group elements, and to verify an evaluation proof, the verifier has to perform two multiexponentiations of size <span class="math">\\sqrt{m}</span>. Dory [Lee21] can be thought of as reducing the Hyrax verifier's costs from <span class="math">O(\\sqrt{m})</span> to <span class="math">O(\\log m)</span>, at the cost of requiring pairings, and requiring the verifier to perform a logarithmic number of operations in the target group of a pairing-friendly group.</p>

    <p class="text-gray-300">We propose a new polynomial commitment scheme (for random evaluation queries) called Sona, which reduces Hyrax's verification costs in a different way. It uses two tools: Nova [KST22] and BabyHyrax (a simplified version of Hyrax in a manner that we describe next). In particular, BabyHyrax's evaluation proofs consist of <span class="math">O(\\sqrt{m})</span> field elements, but it requires no cryptographic operations (BabyHyrax does not invoke Bulletproofs and instead proves the inner product instance by sending the underlying vectors).</p>

    <p class="text-gray-300">With these tools in hand, in Sona, rather than sending a commitment cm consisting of <span class="math">\\sqrt{n}</span> group elements as in BabyHyrax, the Sona prover sends the hash <span class="math">a = h(\\mathsf{cm})</span> of the group elements. And rather than sending an evaluation proof <span class="math">\\pi</span> that consists of <span class="math">\\sqrt{n}</span> group elements and convinces the BabyHyrax verifier that <span class="math">p(r) = v</span>, the Sona prover uses Nova to prove that it knows:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>A vector cm in <span class="math">\\mathbb{G}^{\\sqrt{m}}</span> such that <span class="math">a = h(\\mathsf{cm})</span>.</li>

      <li>A proof <span class="math">\\pi</span> that would have convinced the BabyHyrax verifier that cm is a commitment to a polynomial <span class="math">p</span> such that <span class="math">p(r) = v</span>.</li>

    </ul>

    <p class="text-gray-300">The primary operations that Nova is applied to in this context are thus hashing a length-<span class="math">\\sqrt{m}</span> vector cm, and applying the BabyHyrax verifier's checks on <span class="math">\\pi</span>, which mainly consists of two multiexponentiations of size <span class="math">\\sqrt{m}</span> in <span class="math">\\mathbb{G}</span>. Applying the Nova prover to these computations results in <span class="math">O(\\sqrt{m} \\log(\\lambda) / \\log(m))</span> group operations for the prover. Hence, the prover's total work to compute an evaluation proof for Sona is <span class="math">O(m)</span> field operations and <span class="math">O(\\sqrt{m} \\log(\\lambda) / \\log(m))</span> group operations. Sona's evaluation proofs are a constant number of field elements and takes a constant-sized multiexponentiation to verify.</p>

    <p class="text-gray-300">[13] Minimizing the number of field operations done by the prover in GeneralizedLasso is highly involved, and is the focus of Appendix G.5. However, this does not affect auditability, as the verifier is very simple, and only the verifier needs to be implemented correctly for the SNARK to be secure. Furthermore, there is a relatively simple GeneralizedLasso prover implementation that performs <span class="math">O(m \\log N)</span> field operations for many lookup tables (Appendix G.4). We believe that this in many applications, including Jolt, this will be few enough field operations enough to avoid bottlenecking the prover, relative to commitment costs.</p>

    <p class="text-gray-300">2 Preliminaries</p>

    <p class="text-gray-300">We use <span class="math">\\lambda</span> to denote the security parameter and <span class="math">\\mathbb{F}</span> to denote a finite field (e.g., the prime field <span class="math">\\mathbb{F}_{p}</span> for a large prime <span class="math">p</span>). We use “PPT algorithms” to refer to probabilistic polynomial time algorithms. Throughout this manuscript, we consider any field addition or multiplication to require constant time.</p>

    <h3 id="sec-12" class="text-xl font-semibold mt-8">2.1 Multilinear extensions</h3>

    <p class="text-gray-300">An <span class="math">\\ell</span>-variate polynomial <span class="math">p\\colon\\mathbb{F}^{\\ell}\\to\\mathbb{F}</span> is said to be <em>multilinear</em> if <span class="math">p</span> has degree at most one in each variable. Let <span class="math">f\\colon\\{0,1\\}^{\\ell}\\to\\mathbb{F}</span> be any function mapping the <span class="math">\\ell</span>-dimensional Boolean hypercube to a field <span class="math">\\mathbb{F}</span>. A polynomial <span class="math">g\\colon\\mathbb{F}^{\\ell}\\to\\mathbb{F}</span> is said to <em>extend</em> <span class="math">f</span> if <span class="math">g(x)=f(x)</span> for all <span class="math">x\\in\\{0,1\\}^{\\ell}</span>. It is well-known that for any <span class="math">f\\colon\\{0,1\\}^{\\ell}\\to\\mathbb{F}</span>, there is a unique <em>multilinear</em> polynomial <span class="math">\\widetilde{f}\\colon\\mathbb{F}\\to\\mathbb{F}</span> that extends <span class="math">f</span>. The polynomial <span class="math">\\widetilde{f}</span> is referred to as the <em>multilinear extension</em> (MLE) of <span class="math">f</span>.</p>

    <p class="text-gray-300">The <em>total degree</em> of an <span class="math">\\ell</span>-variate polynomial <span class="math">p</span> refers to the maximum sum of the exponents in any monomial of <span class="math">p</span>. Observe that if <span class="math">p</span> is multilinear, then its total degree is at most <span class="math">\\ell</span>. However, note that <em>not</em> all polynomials of total degree <span class="math">\\ell</span> are multilinear.</p>

    <p class="text-gray-300">A particular multilinear extension that arises frequently in the design of proof systems is <span class="math">\\widetilde{\\mathsf{eq}}</span>, which is the MLE of the function <span class="math">\\mathsf{eq}:\\{0,1\\}^{s}\\times\\{0,1\\}^{s}\\to\\mathbb{F}</span> defined as follows:</p>

    <p class="text-gray-300">\\[ \\mathsf{eq}(x,e)=\\begin{cases}1&\\text{if }x=e\\\\ 0&\\text{otherwise}.\\end{cases} \\]</p>

    <p class="text-gray-300">An explicit expression for <span class="math">\\widetilde{\\mathsf{eq}}</span> is:</p>

    <p class="text-gray-300"><span class="math">\\widetilde{\\mathsf{eq}}(x,e)=\\prod_{i=1}^{s}\\left(x_{i}e_{i}+(1-x_{i})(1-e_{i})\\right).</span> (1)</p>

    <p class="text-gray-300">Indeed, one can easily check that the right hand side of Equation (1) is a multilinear polynomial, and that if evaluated at any input <span class="math">(x,e)\\in\\{0,1\\}^{s}\\times\\{0,1\\}^{s}</span>, it outputs <span class="math">1</span> if <span class="math">x=e</span> and <span class="math">0</span> otherwise. Hence, the right hand side of Equation (1) is the unique multilinear polynomial extending <span class="math">\\mathsf{eq}</span>. Equation (1) implies that <span class="math">\\widetilde{\\mathsf{eq}}(r_{1},r_{2})</span> can be evaluated at any point <span class="math">(r_{1},r_{2})\\in\\mathbb{F}^{s}\\times\\mathbb{F}^{s}</span> in <span class="math">O(s)</span> time.</p>

    <h4 id="sec-13" class="text-lg font-semibold mt-6">Multilinear extensions of vectors.</h4>

    <p class="text-gray-300">Given a vector <span class="math">u\\in\\mathbb{F}^{m}</span>, we will often refer to the <em>multilinear extension of <span class="math">u</span></em> and denote this multilinear polynomial by <span class="math">\\widetilde{u}</span>. <span class="math">\\widetilde{u}</span> is obtained by viewing <span class="math">u</span> as a function mapping <span class="math">\\{0,1\\}^{\\log m}\\to\\mathbb{F}</span> in the following natural way: the function interprets its <span class="math">(\\log m)</span>-bit input <span class="math">(i_{1},\\ldots,i_{\\log m})</span> as the binary representation of an integer <span class="math">i</span> between <span class="math">0</span> and <span class="math">n-1</span>, and outputs <span class="math">u_{i}</span>. <span class="math">\\widetilde{u}</span> is defined to be the multilinear extension of this function.</p>

    <h4 id="sec-14" class="text-lg font-semibold mt-6">Lagrange interpolation.</h4>

    <p class="text-gray-300">An explicit expression for the MLE of any function is given by the following standard lemma (see <em>[x23, Lemma 3.6]</em>).</p>

    <h6 id="sec-15" class="text-base font-medium mt-4">Lemma 1.</h6>

    <p class="text-gray-300">Let <span class="math">f\\colon\\{0,1\\}^{\\ell}\\to\\mathbb{F}</span> be any function. Then the following multilinear polynomial <span class="math">\\widetilde{f}</span> extends <span class="math">f</span>:</p>

    <p class="text-gray-300"><span class="math">\\widetilde{f}(x_{1},\\ldots,x_{\\ell})=\\sum_{w\\in\\{0,1\\}^{\\ell}}f(w)\\cdot\\chi_{w}(x_{1},\\ldots,x_{\\ell}),</span> (2)</p>

    <p class="text-gray-300">where, for any <span class="math">w=(w_{1},\\ldots,w_{\\ell})</span>,</p>

    <p class="text-gray-300"><span class="math">\\chi_{w}(x_{1},\\ldots,x_{\\ell})\\coloneqq\\prod_{i=1}^{\\ell}\\left(x_{i}w_{i}+(1-x_{i})(1-w_{i})\\right).</span> (3)</p>

    <p class="text-gray-300">Equivalently, <span class="math">\\chi_{w}(x_{1},\\ldots,x_{\\ell})=\\widetilde{\\mathsf{eq}}(x_{1},\\ldots,x_{\\ell},w_{1},\\ldots,w_{\\ell})</span>.</p>

    <p class="text-gray-300">######</p>

    <p class="text-gray-300">The polynomials <span class="math">\\{\\chi_{w}\\colon w\\in\\{0,1\\}^{\\ell}\\}</span> are called the Lagrange basis polynomials for <span class="math">\\ell</span>-variate multilinear polynomials. The evaluations <span class="math">\\{\\widetilde{f}(w)\\colon w\\in\\{0,1\\}^{\\ell}\\}</span> are sometimes called the coefficients of <span class="math">\\widetilde{f}</span> in the Lagrange basis, terminology that is justified by Equation (2).</p>

    <p class="text-gray-300">Dense representation for multilinear polynomials. Since the MLE of a function is unique, it offers the following method to represent any multilinear polynomial. Given a multilinear polynomial <span class="math">g:\\mathbb{F}^{\\ell}\\to\\mathbb{F}</span>, it can be represented uniquely by the list of tuples <span class="math">L</span> such that for all <span class="math">i\\in\\{0,1\\}^{\\ell}</span>, <span class="math">(\\mathsf{to}\\text{-field}(i),g(i))\\in L</span> if and only if <span class="math">g(i)\\neq 0</span>, where <span class="math">\\mathsf{to}\\text{-field}</span> is the canonical injection from <span class="math">\\{0,1\\}^{\\ell}</span> to <span class="math">\\mathbb{F}</span>. We denote such a representation of <span class="math">g</span> as <span class="math">\\mathsf{DenseRepr}(g)</span>.</p>

    <h6 id="sec-16" class="text-base font-medium mt-4">Definition 2.1.</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A multilinear polynomial <span class="math">g</span> in <span class="math">\\ell</span> variables is a sparse multilinear polynomial if $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathsf{DenseRepr}(g)</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> is sub-linear in </span>O(2^{\\ell})$. Otherwise, it is a dense multilinear polynomial.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">As an example, suppose <span class="math">g:\\mathbb{F}^{2s}\\to\\mathbb{F}</span>. Suppose $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathsf{DenseRepr}(g)</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">=O(2^{s})<span class="math">, then </span>g<span class="math"> is a sparse multilinear polynomial because </span>O(2^{s})<span class="math"> is sublinear in </span>O(2^{2s})$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">The sum-check protocol. Let <span class="math">g</span> be some <span class="math">\\ell</span>-variate polynomial defined over a finite field <span class="math">\\mathbb{F}</span>. The purpose of the sum-check protocol is for prover to provide the verifier with the following sum:</p>

    <p class="text-gray-300"><span class="math">H\\coloneqq\\sum_{b\\in\\{0,1\\}^{\\ell}}g(b).</span> (4)</p>

    <p class="text-gray-300">To compute <span class="math">H</span> unaided, the verifier would have to evaluate <span class="math">g</span> at all <span class="math">2^{\\ell}</span> points in <span class="math">\\{0,1\\}^{\\ell}</span> and sum the results. The sum-check protocol allows the verifier to offload this “hard work” to the prover. It consists of <span class="math">\\ell</span> rounds, one per variable of <span class="math">g</span>. In round <span class="math">i</span>, the prover sends a message consisting of <span class="math">d_{i}</span> field elements, where <span class="math">d_{i}</span> is the degree of <span class="math">g</span> in its <span class="math">i</span>’th variable, and the verifier responds with a single (randomly chosen) field element. The verifier’s runtime is <span class="math">O\\left(\\sum_{i=1}^{\\ell}d_{i}\\right)</span>, plus the time required to evaluate <span class="math">g</span> at a single point <span class="math">r\\in\\mathbb{F}^{\\ell}</span>. In the typical case that <span class="math">d_{i}=O(1)</span> for each round <span class="math">i</span>, this means the total verifier time is <span class="math">O(\\ell)</span>, plus the time required to evaluate <span class="math">g</span> at a single point <span class="math">r\\in\\mathbb{F}^{\\ell}</span>. This is exponentially faster than the <span class="math">2^{\\ell}</span> time that would generally be required for the verifier to compute <span class="math">H</span>. See <em>[x1, Chapter 8]</em> or <em>[x27, §4.1]</em> for details.</p>

    <p class="text-gray-300">SNARKs We adapt the definition provided in <em>[x21]</em>.</p>

    <h6 id="sec-17" class="text-base font-medium mt-4">Definition 2.2.</h6>

    <p class="text-gray-300">Consider a relation <span class="math">\\mathcal{R}</span> over public parameters, structure, instance, and witness tuples. A non-interactive argument of knowledge for <span class="math">\\mathcal{R}</span> consists of PPT algorithms <span class="math">(\\mathcal{G},\\mathcal{P},\\mathcal{V})</span> and deterministic <span class="math">\\mathcal{K}</span>, denoting the generator, the prover, the verifier and the encoder respectively with the following interface.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathcal{G}(1^{\\lambda})\\to\\mathsf{pp}</span>: On input security parameter <span class="math">\\lambda</span>, samples public parameters <span class="math">\\mathsf{pp}</span>.</li>

      <li><span class="math">\\mathcal{K}(\\mathsf{pp},\\mathsf{s})\\to(pk,\\mathsf{vk})</span>: On input structure <span class="math">\\mathsf{s}</span>, representing common structure among instances, outputs the prover key pk and verifier key <span class="math">\\mathsf{vk}</span>.</li>

      <li><span class="math">\\mathcal{P}(pk,u,w)\\to\\pi</span>: On input instance <span class="math">u</span> and witness <span class="math">w</span>, outputs a proof <span class="math">\\pi</span> proving that <span class="math">(\\mathsf{pp},\\mathsf{s},u,w)\\in\\mathcal{R}</span>.</li>

      <li><span class="math">\\mathcal{V}(\\mathsf{vk},u,\\pi)\\to\\{0,1\\}</span>: On input the verifier key <span class="math">\\mathsf{vk}</span>, instance <span class="math">u</span>, and a proof <span class="math">\\pi</span>, outputs 1 if the instance is accepting and 0 otherwise.</li>

    </ul>

    <p class="text-gray-300">A non-interactive argument of knowledge satisfies completeness if for any PPT adversary <span class="math">\\mathcal{A}</span></p>

    <p class="text-gray-300">\\[ \\Pr\\left[\\begin{array}[]{c c}\\mathcal{V}(\\mathsf{vk},u,\\pi)=1&\\mathsf{pp}\\leftarrow\\mathcal{G}(1^{\\lambda}),\\\\ (\\mathsf{s},(u,w))\\leftarrow\\mathcal{A}(\\mathsf{pp}),\\\\ (\\mathsf{pp},\\mathsf{s},u,w)\\in\\mathcal{R},\\\\ (pk,\\mathsf{vk})\\leftarrow\\mathcal{K}(\\mathsf{pp},\\mathsf{s}),\\\\ \\pi\\leftarrow\\mathcal{P}(pk,u,w)\\end{array}\\right]=1. \\]</p>

    <p class="text-gray-300">A non-interactive argument of knowledge satisfies knowledge soundness if for all PPT adversaries <span class="math">\\mathcal{A}</span></p>

    <p class="text-gray-300">exists a PPT extractor <span class="math">\\mathcal{E}</span> such that for all randomness <span class="math">\\rho</span></p>

    <div class="my-4 text-center"><span class="math-block">\\operatorname{Pr} \\left[ \\begin{array}{l l} \\mathcal{V}(\\mathsf{vk}, u, \\pi) = 1, &amp;amp; \\mathsf{pp} \\leftarrow \\mathcal{G}(1^{\\lambda}), \\\\ (\\mathsf{pp}, \\mathsf{s}, u, w) \\notin \\mathcal{R} &amp;amp; (\\mathsf{s}, u, \\pi) \\leftarrow \\mathcal{A}(\\mathsf{pp}; \\rho), \\\\ &amp;amp; (pk, \\mathsf{vk}) \\leftarrow \\mathcal{K}(\\mathsf{pp}, \\mathsf{s}), \\\\ &amp;amp; w \\leftarrow \\mathcal{E}(\\mathsf{pp}, \\rho) \\end{array} \\right] = \\text{negl}\\lambda.</span></div>

    <p class="text-gray-300">A non-interactive argument of knowledge is succinct if the size of the proof <span class="math">\\pi</span> is polylogarithmic in the size of the statement proven.</p>

    <h2 id="sec-18" class="text-2xl font-bold">Polynomial commitment scheme</h2>

    <p class="text-gray-300">We adapt the definition from [BFS20]. A polynomial commitment scheme for multilinear polynomials is a tuple of four protocols <span class="math">\\mathsf{PC} = (\\mathsf{Gen}, \\mathsf{Commit}, \\mathsf{Open}, \\mathsf{Eval})</span>:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">pp \\gets \\mathsf{Gen}(1^{\\lambda}, \\mu)</span>: takes as input <span class="math">\\mu</span> (the number of variables in a multilinear polynomial); produces public parameters <span class="math">pp</span>.</li>

      <li><span class="math">\\mathcal{C} \\gets \\text{Commit}(pp, g)</span>: takes as input a <span class="math">\\mu</span>-variate multilinear polynomial over a finite field <span class="math">g \\in \\mathbb{F}[\\mu]</span>; produces a commitment <span class="math">\\mathcal{C}</span>.</li>

      <li><span class="math">b \\gets \\text{Open}(pp, \\mathcal{C}, g)</span>: verifies the opening of commitment <span class="math">\\mathcal{C}</span> to the <span class="math">\\mu</span>-variate multilinear polynomial <span class="math">g \\in \\mathbb{F}[\\mu]</span>; outputs <span class="math">b \\in \\{0,1\\}</span>.</li>

      <li><span class="math">b \\gets \\text{Eval}(pp, \\mathcal{C}, r, v, \\mu, g)</span> is a protocol between a PPT prover <span class="math">\\mathcal{P}</span> and verifier <span class="math">\\mathcal{V}</span>. Both <span class="math">\\mathcal{V}</span> and <span class="math">\\mathcal{P}</span> hold a commitment <span class="math">\\mathcal{C}</span>, the number of variables <span class="math">\\mu</span>, a scalar <span class="math">v \\in \\mathbb{F}</span>, and <span class="math">r \\in \\mathbb{F}^{\\mu}</span>. <span class="math">\\mathcal{P}</span> additionally knows a <span class="math">\\mu</span>-variate multilinear polynomial <span class="math">g \\in \\mathbb{F}[\\mu]</span>. <span class="math">\\mathcal{P}</span> attempts to convince <span class="math">\\mathcal{V}</span> that <span class="math">g(r) = v</span>. At the end of the protocol, <span class="math">\\mathcal{V}</span> outputs <span class="math">b \\in \\{0,1\\}</span>.</li>

    </ul>

    <p class="text-gray-300"><strong>Definition 2.3.</strong> A tuple of four protocols <span class="math">(\\mathsf{Gen}, \\mathsf{Commit}, \\mathsf{Open}, \\mathsf{Eval})</span> is an extractable polynomial commitment scheme for multilinear polynomials over a finite field <span class="math">\\mathbb{F}</span> if the following conditions hold.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><strong>Completeness.</strong> For any <span class="math">\\mu</span>-variate multilinear polynomial <span class="math">g \\in \\mathbb{F}[\\mu]</span>,</li>

    </ul>

    <div class="my-4 text-center"><span class="math-block">\\operatorname{Pr} \\left\\{ \\begin{array}{c} pp \\leftarrow \\mathsf{Gen}(1^{\\lambda}, \\mu); \\mathcal{C} \\leftarrow \\mathsf{Commit}(pp, g): \\\\ \\mathsf{Eval}(pp, \\mathcal{C}, r, v, \\mu, g) = 1 \\wedge v = g(r) \\end{array} \\right\\} \\geq 1 - \\mathsf{negl}(\\lambda)</span></div>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><strong>Binding.</strong> For any PPT adversary <span class="math">\\mathcal{A}</span>, size parameter <span class="math">\\mu \\geq 1</span>,</li>

    </ul>

    <div class="my-4 text-center"><span class="math-block">\\operatorname{Pr} \\left\\{ \\begin{array}{c} pp \\leftarrow \\mathsf{Gen}(1^{\\lambda}, m); (\\mathcal{C}, g_0, g_1) = \\mathcal{A}(pp); \\\\ b_0 \\leftarrow \\mathsf{Open}(pp, \\mathcal{C}, g_0); b_1 \\leftarrow \\mathsf{Open}(pp, \\mathcal{C}, g_1): \\\\ b_0 = b_1 \\neq 0 \\wedge g_0 \\neq g_1 \\end{array} \\right\\} \\leq \\mathsf{negl}(\\lambda)</span></div>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><strong>Knowledge soundness.</strong> <span class="math">\\mathsf{Eval}</span> is a succinct argument of knowledge for the following NP relation given <span class="math">pp \\gets \\mathsf{Gen}(1^{\\lambda}, \\mu)</span>.</li>

    </ul>

    <div class="my-4 text-center"><span class="math-block">\\mathcal{R}_{\\mathsf{Eval}}(pp) = \\left\\{ \\langle (\\mathcal{C}, r, v), (g) \\rangle : g \\in \\mathbb{F}[\\mu] \\wedge g(r) = v \\wedge \\mathsf{Open}(pp, \\mathcal{C}, g) = 1 \\right\\}</span></div>

    <h2 id="sec-19" class="text-2xl font-bold">2.2 Polynomial IOPs and polynomial commitments</h2>

    <p class="text-gray-300">Most modern SNARKs work by combining a type of interactive protocol called a polynomial IOP [BFS20] with a cryptographic primitive called a polynomial commitment scheme [KZG10]. The combination yields a succinct interactive argument, which can then be rendered non-interactive via the Fiat-Shamir transformation [FS86], yielding a SNARK. Roughly, a polynomial IOP is an interactive protocol where, in one or more rounds, the prover may "send" to the verifier a large polynomial <span class="math">g</span>. Because <span class="math">g</span> is so large, one does not wish for the verifier to read a complete description of <span class="math">g</span>. Instead, in any efficient polynomial IOP, the verifier only "queries" <span class="math">g</span> at one point (or a handful of points). This means that the only information the verifier needs about <span class="math">g</span> to check that the prover is behaving honestly is one (or a few) evaluations of <span class="math">g</span>.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Scheme</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Commit Size</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Proof Size</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">V time</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Commit time</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">P time</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">KZG + Gemini</td>

            <td class="px-3 py-2 border-b border-gray-700">1</td>

            <td class="px-3 py-2 border-b border-gray-700">G1</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(log N)</td>

            <td class="px-3 py-2 border-b border-gray-700">G1</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(log N) G1</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N) G1</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N) G1</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Brakedown-commit</td>

            <td class="px-3 py-2 border-b border-gray-700">1</td>

            <td class="px-3 py-2 border-b border-gray-700">H</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(√N·λ)</td>

            <td class="px-3 py-2 border-b border-gray-700">F</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(√N·λ) F</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N) F, H</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N) F, H</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Orion-commit</td>

            <td class="px-3 py-2 border-b border-gray-700">1</td>

            <td class="px-3 py-2 border-b border-gray-700">H</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(λ log2 N)</td>

            <td class="px-3 py-2 border-b border-gray-700">H</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(λ log2 N) H</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N) F, H</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N) F, H</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Hyrax-commit</td>

            <td class="px-3 py-2 border-b border-gray-700">O(√N)</td>

            <td class="px-3 py-2 border-b border-gray-700">G</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(√N)</td>

            <td class="px-3 py-2 border-b border-gray-700">G</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(√N) G</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N) G</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N) F</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Dory</td>

            <td class="px-3 py-2 border-b border-gray-700">1</td>

            <td class="px-3 py-2 border-b border-gray-700">GT</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(log N)</td>

            <td class="px-3 py-2 border-b border-gray-700">GT</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(log N) GT</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N) G1</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N) F</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Sona (this work)</td>

            <td class="px-3 py-2 border-b border-gray-700">1</td>

            <td class="px-3 py-2 border-b border-gray-700">H</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(1)</td>

            <td class="px-3 py-2 border-b border-gray-700">G</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(√N) G</td>

            <td class="px-3 py-2 border-b border-gray-700">O(1) G</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N) F, O(√N)G</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Figure 1: Costs of polynomial commitment schemes when committing to a multilinear  <span class="math">\\ell</span> -variate polynomial over  <span class="math">\\mathbb{F}</span> , with  <span class="math">N = 2^{\\ell}</span> . All are transparent.  <span class="math">\\mathcal{P}</span>  time refers to the time to compute evaluation proofs. In addition to the reported  <span class="math">O(N)</span>  field operations, Hyrax and Dory require roughly  <span class="math">O(N^{1/2})</span>  cryptographic work to compute evaluation proofs.  <span class="math">\\mathbb{F}</span>  refers to a finite field,  <span class="math">\\mathbb{H}</span>  refers to a collision-resistant hash,  <span class="math">\\mathbb{G}</span>  refers to a cryptographic group where DLOG is hard, and  <span class="math">(\\mathbb{G}_1, \\mathbb{G}_2, \\mathbb{G}_T)</span>  refer to pairing-friendly groups. Columns with a suffix of "size" depict to the number of elements of a particular type, and columns with a suffix of "time" depict the number of operations (e.g., field multiplications or the size of multiexponentiations). Orion also requires  <span class="math">O(\\sqrt{N})</span>  pre-processing time for the verifier.</p>

    <p class="text-gray-300">In turn, a polynomial commitment scheme enables an untrusted prover to succinctly commit to a polynomial  <span class="math">g</span> , and later provide to the verifier any evaluation  <span class="math">g(r)</span>  for a point  <span class="math">r</span>  chosen by the verifier, along with a proof that the returned value is indeed consistent with the committed polynomial. Essentially, a polynomial commitment scheme is exactly the cryptographic primitive that one needs to obtain a succinct argument from a polynomial IOP. Rather than having the prover send a large polynomial  <span class="math">g</span>  to the verifier as in the polynomial IOP, the argument system prover instead cryptographically commits to  <span class="math">g</span>  and later reveals any evaluations of  <span class="math">g</span>  required by the verifier to perform its checks.</p>

    <p class="text-gray-300">Whether or not a SNARK requires a trusted setup, as well as whether or not it is plausibly post-quantum secure, is determined by the polynomial commitment scheme used. If the polynomial commitment scheme does not require a trusted setup, neither does the resulting SNARK, and similarly if the polynomial commitment scheme is plausibly secure against quantum adversaries, then the SNARK is plausibly post-quantum sound.</p>

    <p class="text-gray-300">Lasso can make use of any commitment schemes for multilinear polynomials. Note that any univariate polynomial commitment scheme can be transformed into a multilinear one, though the transformations introduce some overhead (e.g.,[ZXZS20, BCHO22, CBBZ23]). A brief summary of the multilinear polynomial commitment schemes is provided in Figure 2.2. All of the schemes in the figure, except for KZG-based scheme, are transparent; Brakedown-commit and Orion-commit are plausibly post-quantum secure.</p>

    <p class="text-gray-300">Suppose that the verifier has a commitment to a table  <span class="math">t \\in \\mathbb{F}^n</span>  as well as a commitment to another vector  <span class="math">a \\in \\mathbb{F}^m</span> . Suppose that a prover wishes to prove that all entries in  <span class="math">a</span>  are in the table  <span class="math">t</span> . A simple observation in prior works [ZBK+22, ZGK+22] is that the prover can prove that it knows a sparse matrix  <span class="math">M \\in \\mathbb{F}^{m \\times n}</span>  such that for each row of  <span class="math">M</span> , only one cell has a value of 1 and the rest are zeros and that  <span class="math">M \\cdot t = a</span> , where  <span class="math">\\cdot</span>  is the matrix-vector multiplication. This turns out to be equivalent, up to negligible soundness error, to confirming that</p>

    <div class="my-4 text-center"><span class="math-block">\\sum_ {y \\in \\{0, 1 \\} ^ {\\log N}} \\widetilde {M} (r, y) \\cdot \\tilde {t} (y) = \\widetilde {a} (r), \\tag {5}</span></div>

    <p class="text-gray-300">for an  <span class="math">r \\in \\mathbb{F}^{\\log m}</span>  chosen at random by the verifier. Here,  <span class="math">\\widetilde{M}, \\widetilde{a}</span>  and  <span class="math">\\widetilde{t}</span>  are the so-called multilinear extension polynomials (MLEs) of  <span class="math">M, t</span> , and  <span class="math">a</span>  (see Section 2.1 for details).</p>

    <p class="text-gray-300">Lasso proves Equation (5) by having the prover commit to the sparse polynomial  <span class="math">\\widetilde{M}</span>  using Spark and then prove the equation directly with a generalization of Spark called Surge. This provides the most efficient lookup argument when either the table  <span class="math">t</span>  is "decomposable" (we discuss details of this below), or when  <span class="math">t</span>  is unstructured but small. It turns out most tables that occur in practice (e.g., the ones that arise in Jolt are decomposable). When  <span class="math">t</span>  is not decomposable, but still structured, a generalization of Lasso, which we refer to as GeneralizedLasso, proves Equation (5) using a combination of a new form of the sum-check protocol (which</p>

    <p class="text-gray-300">we refer to as the sparse-dense sum-check protocol) and the Spark polynomial commitment scheme. We defer further details of GeneralizedLasso to Appendix F.</p>

    <h3 id="sec-21" class="text-xl font-semibold mt-8">3.1 Lasso’s starting point: The Spark sparse polynomial commitment scheme</h3>

    <p class="text-gray-300">Lasso’s starting point is Spark, an optimal sparse polynomial commitment scheme from Spartan <em>[x21]</em>. It allows an untrusted prover to prove evaluations of a sparse multilinear polynomial with costs proportional to the size of the dense representation of the sparse multilinear polynomial. Spartan established security of Spark under the assumption that certain metadata associated with a sparse polynomial is committed honestly, which sufficed for its application in the context of Spartan. In this paper, perhaps surprisingly, we prove that Spark remains secure even if that metadata is committed by an untrusted party (e.g., the prover), providing a standard commitment scheme for sparse polynomials.</p>

    <p class="text-gray-300">The Spark sparse polynomial commitment scheme works as follows. The prover commits to a unique dense representation of the sparse polynomial <span class="math">g</span>, using any polynomial commitment scheme for “dense” (multilinear) polynomials. The dense representation of <span class="math">g</span> is effectively a list of all of the monomials of <span class="math">g</span> with a non-zero coefficient (and the corresponding coefficient). More precisely, the list specifies all <em>multilinear Lagrange basis polynomials</em> with non-zero coefficient. Details as to what are the multilinear Lagrange basis polynomials are not relevant to this overview (but can be found in Section 2.1).</p>

    <p class="text-gray-300">When the verifier requests an evaluation <span class="math">g(r)</span> of the committed polynomial <span class="math">g</span>, the prover returns the claimed evaluation <span class="math">v</span> and needs to prove that <span class="math">v</span> is indeed equal to the committed polynomial evaluated at <span class="math">r</span>. Let <span class="math">c</span> be such that <span class="math">N=m^{c}</span>. As explained below, there is a simple and natural algorithm that takes as input the dense representation of <span class="math">g</span>, and outputs <span class="math">g(r)</span> in <span class="math">O(c\\cdot m)</span> time. Spark amounts to the bespoke SNARK establishing that the prover correctly ran this sparse-polynomial-evaluation algorithm on the committed description of <span class="math">g</span>. Note that this perspective on Spark is somewhat novel, though it is partially implicit in the scheme itself and in an exposition of <em>[x23, Section 16.2]</em>.</p>

    <h4 id="sec-22" class="text-lg font-semibold mt-6">A time-optimal algorithm for evaluating a multilinear polynomial of sparsity <span class="math">m</span>.</h4>

    <p class="text-gray-300">We first describe a naive solution and then describe an optimal solution used in Spark.</p>

    <p class="text-gray-300"><em>A naive solution.</em> Consider an algorithm that iterates over each Lagrange basis polynomials specified in the committed dense representation, evaluates that basis polynomial at <span class="math">r</span>, multiplies by the corresponding coefficient, and adds the result to the evaluation. Unfortunately, a naive evaluation of a <span class="math">(\\log N)</span>-variate Lagrange basis polynomial at <span class="math">r</span> would take <span class="math">O(\\log N)</span> time, resulting in a total runtime of <span class="math">O(m\\cdot\\log N)</span>.</p>

    <p class="text-gray-300"><em>Eliminating the logarithmic factor.</em> The key to achieving time <span class="math">O(c\\cdot m)</span> is to ensure that each Lagrange basis polynomial can be evaluated in <span class="math">O(c)</span> time. This is done via the following procedure. This procedure is reminiscent of Pippenger’s algorithm for multiexponentiation, with <span class="math">m</span> being the size of the multiexponentiation, and Lagrange basis polynomials with non-zero coefficients corresponding to exponents.</p>

    <p class="text-gray-300">Decompose the <span class="math">\\log N=c\\cdot\\log m</span> variables of <span class="math">r</span> into <span class="math">c</span> blocks, each of size <span class="math">\\log m</span>, writing <span class="math">r=(r_{1},\\ldots,r_{c})\\in\\left(\\mathbb{F}^{\\log m}\\right)^{c}</span>. Then any <span class="math">(\\log N)</span>-variate Lagrange basis polynomial evaluated at <span class="math">r</span> can be expressed as a product of <span class="math">c</span> “smaller” Lagrange basis polynomials, each defined over only <span class="math">\\log m</span> variables, with the <span class="math">i</span>’th such polynomial evaluated at <span class="math">r_{i}</span>. There are only <span class="math">2^{\\log m}=m</span> multilinear Lagrange basis polynomials over <span class="math">\\log m</span> variables. Moreover, there are now-standard algorithms that, for any input <span class="math">r_{i}\\in\\mathbb{F}^{\\log m}</span>, run in time <span class="math">m</span> and evaluate all <span class="math">m</span> of the <span class="math">(\\log m)</span>-variate Lagrange basis polynomials at <span class="math">r_{i}</span>. Hence, in <span class="math">O(c\\cdot m)</span> total time, one can evaluate <em>all</em> <span class="math">m</span> of these basis polynomials at each <span class="math">r_{i}</span>, storing the results in a (write-once) memory <span class="math">M</span>.</p>

    <p class="text-gray-300">Given <span class="math">M</span>, the time-optimal algorithm can evaluate <em>any</em> given <span class="math">\\log(N)</span>-variate Lagrange basis polynomial at <span class="math">r</span> by performing <span class="math">c</span> lookups into memory, one for each block <span class="math">r_{i}</span>, and multiplying together the results.</p>

    <p class="text-gray-300">Note that we chose to decompose the <span class="math">\\log N</span> variables into <span class="math">c</span> blocks of length <span class="math">\\log m</span> (rather than more, smaller blocks, or fewer, bigger blocks) to balance the runtime of the two phases of the algorithm, namely:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>The time required to “write to memory” the evaluations of all <span class="math">(\\log m)</span>-variate Lagrange basis polynomials at <span class="math">r_{1},\\ldots,r_{c}</span>.</li>

      <li>The time required to evaluate <span class="math">p(r)</span> given the contents of memory.</li>

    </ul>

    <p class="text-gray-300">In general, if we break the variables into <span class="math">c</span> blocks of size <span class="math">\\ell=\\log(N)/c=\\log(m)</span>, the first phase will require time <span class="math">c\\cdot 2^{\\ell}=cm</span>, and the second will require time <span class="math">O(m\\cdot c)</span>.</p>

    <h5 id="sec-23" class="text-base font-semibold mt-4">How the <span class="math">\\mathsf{Spark}</span> prover proves it correctly ran the above time-optimal algorithm.</h5>

    <p class="text-gray-300">To enable an untrusted prover to efficiently prove that it correctly ran the above algorithm to compute an evaluation of a sparse polynomial <span class="math">g</span> at <span class="math">r</span>, <span class="math">\\mathsf{Spark}</span> uses <em>offline memory checking</em> <em>[BEG^{+}91]</em> to prove read-write consistency. Furthermore, the contents of the memory is determined succinctly by <span class="math">r</span>, so the verifier does not need any commitments to the contents of the memory. <span class="math">\\mathsf{Spark}</span> effectively forces the prover to commit to the “execution trace” of the algorithm (which has size roughly <span class="math">c\\cdot m</span>, because the algorithm runs in time <span class="math">O(c)</span> for each of the <span class="math">m</span> Lagrange basis polynomials with non-zero coefficient) plus <span class="math">c\\cdot N^{1/c}=O(c\\cdot m)</span>. The latter term arises because at the end of <span class="math">m</span> operations, the offline memory-checking technique requires the prover to supply certain access counts indicating the number of times a particular memory location was read during the course of the protocol. Moreover, note that this memory has size <span class="math">c\\cdot N^{1/c}</span> if the algorithm breaks the <span class="math">\\log N</span> variables into <span class="math">c</span> blocks of size <span class="math">\\log(N)/c</span>. As we will see later, this is why <span class="math">\\mathsf{Lasso}</span>’s prover winds up cryptographically committing to <span class="math">3\\cdot c\\cdot m+c\\cdot N^{1/c}</span> field elements.</p>

    <h6 id="sec-24" class="text-base font-medium mt-4">Remark 1.</h6>

    <p class="text-gray-300">The cost incurred by <span class="math">\\mathsf{Spark}</span>’s prover to “replay” to provide access counts at the very end of the algorithm’s execution can be amortized over multiple sparse polynomial evaluations. In particular, if the prover proves an evaluation of <span class="math">k</span> sparse polynomials in the same number of variables, the aforementioned cost in the offline memory checking is reused across all <span class="math">k</span> sparse polynomials.</p>

    <h3 id="sec-25" class="text-xl font-semibold mt-8">3.2 Surge: A generalization of <span class="math">\\mathsf{Spark}</span></h3>

    <h5 id="sec-26" class="text-base font-semibold mt-4">Re-imagining <span class="math">\\mathsf{Spark}</span>.</h5>

    <p class="text-gray-300">A sparse polynomial commitment scheme can be viewed as having the prover commit to an <span class="math">m</span>-sparse vector <span class="math">u</span> of length <span class="math">N</span>, where <span class="math">m</span> is the number of non-zero coefficients of the polynomial, and <span class="math">N</span> is the number of elements in a suitable basis. For univariate polynomials in the standard monomial basis, <span class="math">N</span> is the degree, <span class="math">m</span> is the number of non-zero coefficients, and <span class="math">u</span> is the vector of coefficients. For an <span class="math">\\ell</span>-variate multilinear polynomial <span class="math">g</span> over the Lagrange basis, <span class="math">N=2^{\\ell}</span>, <span class="math">m</span> is the number of evaluation points over the Boolean hypercube <span class="math">x\\in\\{0,1\\}^{\\ell}</span> such that <span class="math">g(x)\\neq 0</span>, and <span class="math">u</span> is the vector of evaluations of <span class="math">g</span> at all evaluation points over the hypercube <span class="math">\\{0,1\\}^{\\ell}</span>.</p>

    <p class="text-gray-300">An evaluation query to <span class="math">g</span> at input <span class="math">r</span> returns the inner product of the sparse vector <span class="math">u</span> with the dense vector <span class="math">t</span> consisting of the evaluations of all basis polynomials at <span class="math">r</span>. In the multilinear case, for each <span class="math">S\\in\\{0,1\\}^{\\ell}</span>, the <span class="math">S</span>’th entry of <span class="math">t</span> is <span class="math">\\chi_{S}(r)</span>. In this sense, <em>any</em> sparse polynomial commitment scheme achieves the following: it allows the prover to establish the value of the inner product <span class="math">\\langle u,t\\rangle</span> of a sparse (committed) vector <span class="math">u</span> with a dense, structured vector <span class="math">t</span>.</p>

    <h5 id="sec-27" class="text-base font-semibold mt-4"><span class="math">\\mathsf{Spark}\\to\\mathsf{Surge}</span>.</h5>

    <p class="text-gray-300">To obtain <span class="math">\\mathsf{Surge}</span> from <span class="math">\\mathsf{Spark}</span>, we critically examine the type of structure in <span class="math">t</span> that is exploited by <span class="math">\\mathsf{Spark}</span>, and introduce <span class="math">\\mathsf{Surge}</span> as a natural generalization of <span class="math">\\mathsf{Spark}</span> that supports any table <span class="math">t</span> with this structure. More importantly, we observe that many lookup tables critically important in practice (e.g., those that arise in <span class="math">\\mathsf{Jolt}</span>) exhibit this structure.</p>

    <p class="text-gray-300">In more detail, the <span class="math">\\mathsf{Surge}</span> prover essentially establishes that it correctly ran a natural <span class="math">O(c\\cdot m)</span>-time algorithm for computing <span class="math">\\langle u,t\\rangle</span>. This algorithm is a natural analog of the sparse polynomial evaluation algorithm described in Section 3.1: it iterates over every non-zero entry <span class="math">u_{i}</span> of <span class="math">u</span>, quickly computes <span class="math">t_{i}=T[i]</span> by performing one lookup into each of <span class="math">O(c)</span> “sub-tables” of size <span class="math">N^{1/c}</span>, and quickly “combines” the result of each lookup to obtain <span class="math">t_{i}</span> and hence <span class="math">u_{i}\\cdot t_{i}</span>. In this way, this algorithm takes just <span class="math">O(c\\cdot m)</span> time to compute the desired inner product <span class="math">\\sum_{i:~{}u_{i}\\neq 0}u_{i}\\cdot t_{i}</span>.</p>

    <h5 id="sec-28" class="text-base font-semibold mt-4">Details of the structure needed to apply <span class="math">\\mathsf{Surge}</span>.</h5>

    <p class="text-gray-300">In the case of <span class="math">\\mathsf{Spark}</span> itself, the dense vector <span class="math">t</span> is simply the <em>tensor product</em> of smaller vectors, <span class="math">t_{1},\\ldots,t_{c}</span>, each of size <span class="math">N^{1/c}</span>. Specifically, <span class="math">\\mathsf{Spark}</span> breaks <span class="math">r</span> into <span class="math">c</span></p>

    <p class="text-gray-300">“chunks” <span class="math">r=(r_{1},\\ldots,r_{c})\\in\\left(\\mathbb{F}^{(\\log N)/c}\\right)^{c}</span>, where <span class="math">r</span> is the point at which the <span class="math">\\mathsf{Spark}</span> verifier wants to evaluate the committed polynomial. Then <span class="math">t_{i}</span> contains the evaluations of all <span class="math">((\\log N)/c)</span>-variate Lagrange basis polynomials evaluated at <span class="math">r_{i}</span>. And for each <span class="math">S=(S_{1},\\ldots,S_{c})\\in\\left(\\{0,1\\}^{(\\log N)/c}\\right)^{c}</span>, the <span class="math">S</span>’th entry of <span class="math">t</span> is:</p>

    <p class="text-gray-300"><span class="math">\\prod_{i=1}^{c}t_{i}(r_{i}).</span></p>

    <p class="text-gray-300">In general, <span class="math">\\mathsf{Spark}</span> applies to any table vector <span class="math">t</span> that is “decomposable” in a manner similar to the above. Specifically, suppose that <span class="math">k\\geq 1</span> is an integer and there are <span class="math">\\alpha=k\\cdot c</span> tables <span class="math">T_{1},\\ldots,T_{\\alpha}</span> of size <span class="math">N^{1/c}</span> and an <span class="math">\\alpha</span>-variate multilinear polynomial <span class="math">g</span> such that the following holds. For any <span class="math">r\\in\\{0,1\\}^{\\log N}</span>, write <span class="math">r=(r_{1},\\ldots,r_{c})\\in\\left(\\{0,1\\}^{\\log(N)/c}\\right)^{c}</span>, i.e., break <span class="math">r</span> into <span class="math">c</span> pieces of equal size. Suppose that for every <span class="math">r\\in\\{0,1\\}^{\\log N}</span>,</p>

    <p class="text-gray-300"><span class="math">T[r]=g\\left(T_{1}[r_{1}],\\ldots,T_{k}[r_{1}],T_{k+1}[r_{2}],\\ldots,T_{2k}[r_{2}],\\ldots,T_{\\alpha-k+1}[r_{c}],\\ldots,T_{\\alpha}[r_{c}]\\right).</span> (6)</p>

    <p class="text-gray-300">Simplifying slightly, <span class="math">\\mathsf{Surge}</span> allows the prover to commit to a <span class="math">m</span>-sparse vector <span class="math">u\\in\\mathbb{F}^{N}</span> and prove that the inner product of <span class="math">u</span> and the table <span class="math">T</span> (or more precisely the associated vector <span class="math">t</span>) equals some claimed value. And the cost for the prover is dominated by the following operations.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Committing to <span class="math">3\\cdot\\alpha\\cdot m+\\alpha\\cdot N^{1/c}</span> field elements, where <span class="math">2\\cdot\\alpha\\cdot m+\\alpha\\cdot N^{1/c}</span> of the committed elements are in the set</li>

    </ul>

    <p class="text-gray-300"><span class="math">\\{0,1,\\ldots,\\max\\{m,N^{1/c}\\}-1\\},</span></p>

    <p class="text-gray-300">and the remaining <span class="math">\\alpha\\cdot m</span> of them are elements of the sub-tables <span class="math">T_{1},\\ldots,T_{\\alpha}</span>. For many lookup tables <span class="math">T</span>, these elements are themselves in the set <span class="math">\\{0,1,\\ldots,N^{1/c}-1\\}</span>.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Let <span class="math">b</span> be the number of monomials in <span class="math">g</span>. Then the <span class="math">\\mathsf{Surge}</span> prover performs <span class="math">O(k\\cdot\\alpha N^{1/c})=O(b\\cdot c\\cdot N^{1/c})</span> field operations. In many cases, the factor of <span class="math">b</span> in the number of prover field operations can be removed.</li>

    </ul>

    <p class="text-gray-300">We refer to tables that can be decomposed into sub-tables of size <span class="math">N^{1/c}</span> as per Equation (6) as having <span class="math">\\mathit{Spark}</span>-only structure (SOS), or more simply as being decomposable.</p>

    <h2 id="sec-29" class="text-2xl font-bold">4 <span class="math">\\mathsf{Spark}</span>: Spartan’s sparse polynomial commitment scheme, with a stronger security analysis</h2>

    <p class="text-gray-300">We prove a substantial strengthening of a result from Spartan <em>[x23, Lemma 7.6]</em>. In particular, we prove that in Spartan’s sparse polynomial commitment scheme, which is called <span class="math">\\mathsf{Spark}</span>, one does not need to assume that certain metadata associated with a sparse polynomial is committed honestly (in the case of Spartan, the metadata is committed by the setup algorithm, so it was sufficient for its purposes). We thereby obtain the first “standard” polynomial commitment scheme (i.e., meeting Definition 2.3) with prover costs linear in the number of non-zero coefficients. We prove this result without any substantive changes to <span class="math">\\mathsf{Spark}</span>.</p>

    <p class="text-gray-300">For simplicity of presentation, we make a minor change that does not affect costs nor analysis: we have the prover commit to metadata associated with the sparse polynomial at the time of proving an evaluation rather than when the prover commits to the sparse polynomial (the metadata depends only on the sparse polynomial, and in particular, it is independent of the point at which the sparse polynomial is evaluation, so the metadata can be committed either in the commit phase or when proving an evaluation). Our text below is adapted from an exposition of Spartan’s result by Golovnev et al. <em>[GLS^{+}21]</em>. It is natural for the reader to conceptualize the <span class="math">\\mathsf{Spark}</span> sparse polynomial commitment scheme as a bespoke SNARK for a prover to prove it correctly ran the sparse <span class="math">(\\log N)</span>-variate multilinear polynomial evaluation algorithm described in Section 3.1 using <span class="math">c</span> memories of size <span class="math">N^{1/c}</span>.</p>

    <h3 id="sec-30" class="text-xl font-semibold mt-8">4.1 A (slightly) simpler result: <span class="math">c=2</span></h3>

    <p class="text-gray-300">We begin proving a special case of the final result, the proof of which exhibits all of the ideas and techniques. This special case (Theorem 1) describes a transformation from any commitment scheme for dense polynomials</p>

    <p class="text-gray-300">defined over <span class="math">\\log m</span> variables to one for sparse multilinear polynomials defined over <span class="math">\\log N=2\\log m</span> variables. It is the bespoke SNARK mentioned above when using <span class="math">c=2</span> memories of size <span class="math">N^{1/2}</span>.</p>

    <p class="text-gray-300">The dominant costs for the prover in <span class="math">\\mathsf{Spark}</span> is committing to 7 dense multilinear polynomials over <span class="math">\\log(m)</span>-many variables, and 2 dense multilinear polynomials over <span class="math">\\log(N^{1/c})</span>-many variables. In dense <span class="math">\\ell</span>-variate multilinear polynomial commitment schemes, the prover time is roughly linear in <span class="math">2^{\\ell}</span>. Hence, so long as <span class="math">m\\geq N^{1/c}</span>, the prover time is dominated by the commitments to the 7 dense polynomials over <span class="math">\\log(m)</span>-many variables. This ensures that the prover time is linear in the sparsity of the committed polynomial as desired (rather than linear in <span class="math">2^{2\\log m}=m^{2}</span>, which would be the runtime of applying a dense polynomial commitment scheme directly to the sparse polynomial over <span class="math">2\\log m</span> variables).</p>

    <h4 id="sec-31" class="text-lg font-semibold mt-6">The full result.</h4>

    <p class="text-gray-300">If we wish to commit to a sparse multilinear polynomial over <span class="math">\\ell</span> variables, let <span class="math">N\\coloneqq 2^{\\ell}</span> denote the dimensionality of the space of <span class="math">\\ell</span>-variate multilinear polynomials. For any desired integer <span class="math">c\\geq 2</span>, our final, general, result replaces these two memories (each of size equal to <span class="math">N^{1/2}</span>) with <span class="math">c</span> memories of size equal to <span class="math">N^{1/c}</span>. Ultimately, the prover must commit to <span class="math">(3c+1)</span> many dense <span class="math">(\\log m)</span>-variate multilinear polynomials, and <span class="math">c</span> many dense <span class="math">(\\log(N^{1/c}))</span>-variate polynomials.</p>

    <p class="text-gray-300">We begin with the simpler result where <span class="math">c</span> equals 2 before stating and proving the full result.</p>

    <h6 id="sec-32" class="text-base font-medium mt-4">Theorem 1 (Special case of Theorem 2 with <span class="math">c=2</span>).</h6>

    <p class="text-gray-300">Let <span class="math">\\mathsf{M}=N^{1/2}</span>. Given a polynomial commitment scheme for <span class="math">(\\log\\mathsf{M})</span>-variate multilinear polynomials with the following parameters (where <span class="math">\\mathsf{M}</span> is a positive integer and WLOG a power of 2):</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>the size of the commitment is <span class="math">\\mathsf{c}(\\mathsf{M})</span>;</li>

      <li>the running time of the commit algorithm is <span class="math">\\mathsf{tc}(\\mathsf{M})</span>;</li>

      <li>the running time of the prover to prove a polynomial evaluation is <span class="math">\\mathsf{tp}(\\mathsf{M})</span>;</li>

      <li>the running time of the verifier to verify a polynomial evaluation is <span class="math">\\mathsf{tv}(\\mathsf{M})</span>;</li>

      <li>the proof size is <span class="math">\\mathsf{p}(\\mathsf{M})</span>,</li>

    </ul>

    <p class="text-gray-300">there exists a polynomial commitment scheme for multilinear polynomials over <span class="math">2\\log\\mathsf{M}=\\log N</span> variables that evaluate to a non-zero value at at most <span class="math">m</span> locations over the Boolean hypercube <span class="math">\\{0,1\\}^{2\\log\\mathsf{M}}</span>, with the following parameters:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>the size of the commitment is <span class="math">\\mathsf{7c}(m)+\\mathsf{2c}(\\mathsf{M})</span>;</li>

      <li>the running time of the commit algorithm is <span class="math">O(\\mathsf{tc}(m)+\\mathsf{tc}(\\mathsf{M}))</span>;</li>

      <li>the running time of the prover to prove a polynomial evaluation is <span class="math">O(\\mathsf{tp}(m)+\\mathsf{tc}(\\mathsf{M}))</span>;</li>

      <li>the running time of the verifier to verify a polynomial evaluation is <span class="math">O(\\mathsf{tv}(m)+\\mathsf{tv}(\\mathsf{M}))</span>; and</li>

      <li>the proof size is <span class="math">O(\\mathsf{p}(m)+\\mathsf{p}(\\mathsf{M}))</span>.</li>

    </ul>

    <h4 id="sec-33" class="text-lg font-semibold mt-6">Representing sparse polynomials with dense polynomials.</h4>

    <p class="text-gray-300">Let <span class="math">D</span> denote a <span class="math">(2\\log\\mathsf{M})</span>-variate multilinear polynomial that evaluates to a non-zero value at at most <span class="math">m</span> locations over <span class="math">\\{0,1\\}^{2\\log\\mathsf{M}}</span>. For any <span class="math">r\\in\\mathbb{F}^{2\\log\\mathsf{M}}</span>, we can express the evaluation of <span class="math">D(r)</span> as follows. Interpret <span class="math">r\\in\\mathbb{F}^{2\\log\\mathsf{M}}</span> as a tuple <span class="math">(r_{x},r_{y})</span> in a natural manner, where <span class="math">r_{x},r_{y}\\in\\mathbb{F}^{\\log\\mathsf{M}}</span>. Then by multilinear Lagrange interpolation (Lemma 1), we can write</p>

    <p class="text-gray-300"><span class="math">D(r_{x},r_{y})=\\sum_{(i,j)\\in\\{0,1\\}^{\\log\\mathsf{M}}\\times\\{0,1\\}^{\\log\\mathsf{M}}\\colon D(i,j)\\neq 0}D(i,j)\\cdot\\widetilde{eq}(i,r_{x})\\cdot\\widetilde{eq}(j,r_{y}).</span> (7)</p>

    <h6 id="sec-34" class="text-base font-medium mt-4">Claim 1.</h6>

    <p class="text-gray-300">Let <span class="math">\\mathsf{to}</span>-field be the canonical injection from <span class="math">\\{0,1\\}^{\\log\\mathsf{M}}</span> to <span class="math">\\mathbb{F}</span> and <span class="math">\\mathsf{to}</span>-bits be its inverse. Given a <span class="math">2\\log\\mathsf{M}</span>-variate multilinear polynomial <span class="math">D</span> that evaluates to a non-zero value at at most <span class="math">m</span> locations over <span class="math">\\{0,1\\}^{2\\log\\mathsf{M}}</span>, there exist three <span class="math">(\\log m)</span>-variate multilinear polynomials <span class="math">\\mathsf{row},\\mathsf{col},\\mathsf{val}</span> such that the following holds</p>

    <p class="text-gray-300">for all <span class="math">r_{x},r_{y}\\in\\mathbb{F}^{\\log\\mathsf{M}}</span>.</p>

    <p class="text-gray-300"><span class="math">D(r_{x},r_{y})=\\sum_{k\\in\\{0,1\\}^{\\log m}}\\mathsf{val}(k)\\cdot\\widetilde{eq}(\\mathsf{to-bits}(\\mathsf{row}(k)),r_{x})\\cdot\\widetilde{eq}(\\mathsf{to-bits}(\\mathsf{col}(k)),r_{y}).</span> (8)</p>

    <p class="text-gray-300">Moreover, the polynomials’ coefficients in the Lagrange basis can be computed in <span class="math">O(m)</span> time.</p>

    <h6 id="sec-35" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">Since <span class="math">D</span> evaluates to a non-zero value at at most <span class="math">m</span> locations over <span class="math">\\{0,1\\}^{2\\log\\mathsf{M}}</span>, <span class="math">D</span> can be represented uniquely with <span class="math">m</span> tuples of the form <span class="math">(i,j,D(i,j))\\in(\\{0,1\\}^{\\log\\mathsf{M}},\\{0,1\\}^{\\log\\mathsf{M}},\\mathbb{F})</span>. By using the natural injection to-field from <span class="math">\\{0,1\\}^{\\log\\mathsf{M}}</span> to <span class="math">\\mathbb{F}</span>, we can view the first two entries in each of these tuples as elements of <span class="math">\\mathbb{F}</span> (let to-bits denote its inverse). Furthermore, these tuples can be represented with three <span class="math">m</span>-sized vectors <span class="math">R,C,V\\in\\mathbb{F}^{m}</span>, where tuple <span class="math">k</span> (for all <span class="math">k\\in[m]</span>) is stored across the three vectors at the <span class="math">k</span>th location in the vector, i.e., the first entry in the tuple is stored in <span class="math">R</span>, the second entry in <span class="math">C</span>, and the third entry in <span class="math">V</span>. Take row as the unique MLE of <span class="math">R</span> viewed as a function <span class="math">\\{0,1\\}^{\\log m}\\to\\mathbb{F}</span>. Similarly, <span class="math">\\mathsf{col}</span> is the unique MLE of <span class="math">C</span>, and <span class="math">\\mathsf{val}</span> is the unique MLE of <span class="math">V</span>. The claim holds by inspection since Equations (7) and (8) are both multilinear polynomials in <span class="math">r_{x}</span> and <span class="math">r_{y}</span> and agree with each other at every pair <span class="math">r_{x},r_{y}\\in\\{0,1\\}^{\\log\\mathsf{M}}</span>. ∎</p>

    <p class="text-gray-300">Conceptually, the sum in Equation (8) is <em>exactly</em> what the sparse polynomial evaluation algorithm described in Section 3.1 computes term-by-term. Specifically, that algorithm (using <span class="math">c=2</span> memories) filled up one memory with the quantities <span class="math">\\widetilde{eq}(i,r_{x})</span> as <span class="math">i</span> ranges over <span class="math">\\{0,1\\}^{\\log\\mathsf{M}}</span> (see Equation (7), and the other memory with the quantities <span class="math">\\widetilde{eq}(j,r_{x})</span>, and then computed each term of Equation (8) via one lookup into each memory, to the respective memory cells with (binary) indices <span class="math">\\mathsf{to-bits}(\\mathsf{row}(k))</span> and <span class="math">\\mathsf{to-bits}(\\mathsf{col}(k))</span>, followed by two field multiplications.</p>

    <h4 id="sec-36" class="text-lg font-semibold mt-6">Commit phase.</h4>

    <p class="text-gray-300">To commit to <span class="math">D</span>, the committer can send commitments to the three <span class="math">(\\log m)</span>-variate multilinear polynomials <span class="math">\\mathsf{row},\\mathsf{col},\\mathsf{val}</span> from Claim 1. Using the provided polynomial commitment scheme, this costs <span class="math">O(m)</span> finite field operations, and the size of the commitment to <span class="math">D</span> is <span class="math">O_{\\lambda}(\\mathsf{c}(m))</span>.</p>

    <p class="text-gray-300">Intuitively, the commit phase commits to a “dense” representation of the sparse polynomial, which simply lists all the Lagrange basis polynomial with non-zero coefficients (each specified as an element in <span class="math">\\{0,\\ldots,\\mathsf{M}-1\\}^{2}</span>), along with the associated coefficient. This is exactly the input to the sparse polynomial evaluation algorithm described in Section 3.1.</p>

    <p class="text-gray-300">In the evaluation phase described below, the prover proves that it correctly ran the sparse polynomial evaluation algorithm sketched in Section 3.1 on the committed polynomial in order to evaluate it at the requested evaluation point <span class="math">(r_{x},r_{y})\\in\\mathbb{F}^{2\\log\\mathsf{M}}</span>.</p>

    <h4 id="sec-37" class="text-lg font-semibold mt-6">A first attempt at the evaluation phase.</h4>

    <p class="text-gray-300">Given <span class="math">r_{x},r_{y}\\in\\mathbb{F}^{\\log\\mathsf{M}}</span>, to prove an evaluation of a committed polynomial, i.e., to prove that <span class="math">D(r_{x},r_{y})=v</span> for a purported evaluation <span class="math">v\\in\\mathbb{F}</span>, consider the polynomial IOP in Figure 2, where the polynomial IOP assumes that the verifier has oracle access to the three <span class="math">(\\log m)</span>-variate multilinear polynomial oracles that encode <span class="math">D</span> (namely <span class="math">\\mathsf{row},\\mathsf{col},\\mathsf{val}</span>).</p>

    <p class="text-gray-300">Here, the oracles <span class="math">E_{\\mathsf{rs}}</span> and <span class="math">E_{\\mathsf{ry}}</span> should be thought of as the (purported) multilinear extensions of the values returned by each memory reads that the algorithm of Section 3.1 performed into each of its two memories, step-by-step over the course of its execution.</p>

    <p class="text-gray-300">If the prover is honest, it is easy to see that it can convince the verifier about the correct of evaluations of <span class="math">D</span>. Unfortunately, the two oracles that the prover sends in the first step of the depicted polynomial IOP can be completely arbitrary. To fix, this, <span class="math">\\mathcal{V}</span> must <em>additionally</em> check that the following two conditions hold.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\forall k\\in\\{0,1\\}^{\\log m}</span>, <span class="math">E_{\\mathsf{rs}}(k)=\\widetilde{eq}(\\mathsf{to-bits}(\\mathsf{row}(k)),r_{x})</span>; and</li>

      <li><span class="math">\\forall k\\in\\{0,1\\}^{\\log m}</span>, <span class="math">E_{\\mathsf{ry}}(k)=\\widetilde{eq}(\\mathsf{to-bits}(\\mathsf{col}(k)),r_{y})</span>.</li>

    </ul>

    <p class="text-gray-300">A core insight of Spartan <em>[x23]</em> is to check these two conditions using memory-checking techniques <em>[BEG^{+}91]</em>. These techniques amount to an efficient randomized procedure to confirm that every memory read over the course of an algorithm’s execution returns the value last written to that location.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathcal{P} \\to \\mathcal{V}</span> : two  <span class="math">(\\log m)</span> -variate multilinear polynomials  <span class="math">E_{\\mathrm{rx}}</span>  and  <span class="math">E_{\\mathrm{ry}}</span>  as oracles. These polynomials are purported to respectively equal the multilinear extensions of the functions mapping  <span class="math">k \\in \\{0, 1\\}^{\\log m}</span>  to  <span class="math">\\widetilde{eq}(\\text{to-bits}(\\text{row}(k)), r_x)</span>  and  <span class="math">\\widetilde{eq}(\\text{to-bits}(\\text{col}(k)), r_y)</span> .</li>

      <li><span class="math">\\mathcal{V} \\leftrightarrow \\mathcal{P}</span> : run the sum-check reduction to reduce the check that</li>

    </ol>

    <div class="my-4 text-center"><span class="math-block">v = \\sum_ {k \\in \\{0, 1 \\} ^ {\\log m}} \\operatorname {v a l} (k) \\cdot E _ {\\mathrm {r x}} (k) \\cdot E _ {\\mathrm {r y}} (k)</span></div>

    <p class="text-gray-300">to checking if the following hold, where  <span class="math">r_z \\in \\mathbb{F}^{\\log m}</span>  is chosen at random by the verifier over the course of the sum-check protocol:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\operatorname{val}(r_z) \\stackrel{?}{=} v_{\\operatorname{val}}</span> ;</li>

      <li><span class="math">E_{\\mathrm{rx}}(r_z) \\stackrel{?}{=} v_{E_{\\mathrm{rx}}}</span>  and  <span class="math">E_{\\mathrm{ry}}(r_z) \\stackrel{?}{=} v_{E_{\\mathrm{ry}}}</span> . Here,  <span class="math">v_{\\mathrm{val}}</span> ,  <span class="math">v_{E_{\\mathrm{rx}}}</span> , and  <span class="math">v_{E_{\\mathrm{ry}}}</span>  are values provided by the prover at the end of the sum-check protocol.</li>

    </ul>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathcal{V}</span> : check if the three equalities hold with an oracle query to each of  <span class="math">\\mathsf{val}, E_{\\mathrm{rx}}, E_{\\mathrm{ry}}</span> .</li>

    </ol>

    <p class="text-gray-300">Figure 2: A first attempt at a polynomial IOP for revealing a requested evaluation of a  <span class="math">(2\\log (\\mathsf{M}))</span> -variate multilinear polynomial  <span class="math">p</span>  over  <span class="math">\\mathbb{F}</span>  such that  <span class="math">p(x)\\neq 0</span>  for at most  <span class="math">m</span>  values of  <span class="math">x\\in \\{0,1\\}^{2\\log (\\mathsf{M})}</span> .</p>

    <p class="text-gray-300">We take a detour to introduce new results that we rely on here.</p>

    <p class="text-gray-300">Detour: Offline memory checking. Recall that in the offline memory checking algorithm of  <span class="math">\\left[\\mathrm{BEG}^{+}91\\right]</span> , a trusted checker issues operations to an untrusted memory. For our purposes, it suffices to consider only operation sequences in which each memory address is initialized to a certain value, and all subsequent operations are read operations. To enable efficient checking using multiset-fingerprinting techniques, the memory is modified so that in addition to storing a value at each address, the memory also stores a timestamp with each address. Moreover, each read operation is followed by a write operation that updates the timestamp associated with that address (but not the value stored there).</p>

    <p class="text-gray-300">In prior descriptions of offline memory checking [BEG+91, CDD+03, SAGL18], the trusted checker maintains a single timestamp counter and uses it to compute write timestamps, whereas in Spark and our description below, the trusted checker does not use any local timestamp counter; rather, each memory cell maintains its own counter, which is incremented by the checker every time the cell is read. For this reason, we depart from the standard terminology in the memory-checking literature and henceforth refer to these quantities as counters rather than timestamps.</p>

    <p class="text-gray-300">The memory-checking procedure is captured in the codebox below.</p>

    <p class="text-gray-300">Local state of the checker: Two sets: RS and WS, which are initialized as follows. <span class="math">^{16}</span> <span class="math">\\mathsf{RS} = \\{\\}</span> , and for an M-sized memory, WS is initialized to the following set of tuples: for all  <span class="math">i \\in [N^{1/c}]</span> , the tuple  <span class="math">(i, v_i, 0)</span>  is included in WS, where  <span class="math">v_i</span>  is the value stored at address  <span class="math">i</span> , and the third entry in the tuple,  <span class="math">0</span> , is an "initial count" associated with the value (intuitively capturing the notion that when  <span class="math">v_i</span>  was written to address  <span class="math">i</span> , it was the first time that address was accessed). Here,  <span class="math">[\\mathsf{M}]</span>  denotes the set  <span class="math">\\{0, 1, \\ldots, \\mathsf{M} - 1\\}</span> .</p>

    <p class="text-gray-300">Read operations and an invariant. For a read operation at address  <span class="math">a</span> , suppose that the untrusted memory responds with a value-count pair  <span class="math">(v, t)</span> . Then the checker updates its local state as follows:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathsf{RS}\\leftarrow\\mathsf{RS}\\cup\\{(a,v,t)\\}</span>;</li>

      <li>store <span class="math">(v,t+1)</span> at address <span class="math">a</span> in the untrusted memory; and</li>

      <li><span class="math">\\mathsf{WS}\\leftarrow\\mathsf{WS}\\cup\\{(a,v,t+1)\\}</span>.</li>

    </ol>

    <p class="text-gray-300">The following claim captures the invariant maintained on the sets of the checker:</p>

    <h6 id="sec-38" class="text-base font-medium mt-4">Claim 2.</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Let <span class="math">\\mathbb{F}</span> be a prime order field. Assuming that the domain of counts is <span class="math">\\mathbb{F}</span> and that <span class="math">m</span> (the number of reads issued) is smaller than the field characteristic $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math">. Let </span>\\mathsf{WS}<span class="math"> and </span>\\mathsf{RS}<span class="math"> denote the multisets maintained by the checker in the above algorithm at the conclusion of </span>m<span class="math"> read operations. If for every read operation, the untrusted memory returns the tuple last written to that location, then there exists a set </span>S<span class="math"> with cardinality </span>\\mathsf{M}<span class="math"> consisting of tuples of the form </span>(k,v_{k},t_{k})<span class="math"> for all </span>k\\in[\\mathsf{M}]<span class="math"> such that </span>\\mathsf{WS}=\\mathsf{RS}\\cup S<span class="math">. Moreover, </span>S<span class="math"> is computable in time linear in </span>\\mathsf{M}$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Conversely, if the untrusted memory ever returns a value <span class="math">v</span> for a memory call <span class="math">k\\in[\\mathsf{M}]</span> such <span class="math">v</span> does not equal the value initially written to cell <span class="math">k</span>, then there does not exist any set <span class="math">S</span> such that <span class="math">\\mathsf{WS}=\\mathsf{RS}\\cup S</span>.</p>

    <h6 id="sec-39" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">If for every read operation, the untrusted memory returns the tuple last written to that location, then it is easy to see the existence of the desired set <span class="math">S</span>. It is simply the current state of the untrusted memory viewed as the set of address-value-count tuples.</p>

    <p class="text-gray-300">We now prove the other direction in the claim. For notational convenience, let <span class="math">\\mathsf{WS}_{i}</span> and <span class="math">\\mathsf{RS}_{i}</span> (<span class="math">0\\leq i\\leq m</span>) denote the multisets maintained by the trusted checker at the conclusion of the <span class="math">i</span>th read operation (i.e., <span class="math">\\mathsf{WS}_{0}</span> and <span class="math">\\mathsf{RS}_{0}</span> denote the multisets before any read operation is issued). Suppose that there is some read operation <span class="math">i</span> that reads from address <span class="math">k</span>, and the untrusted memory responds with a tuple <span class="math">(v,t)</span> such that <span class="math">v</span> differs from the value initially written to address <span class="math">k</span>. This ensures that <span class="math">(k,v,t)\\in\\mathsf{RS}_{j}</span> for all <span class="math">j\\geq i</span>, and in particular that <span class="math">(k,v,t)\\in\\mathsf{RS}</span>, where recall that <span class="math">\\mathsf{RS}</span> is the read set at the conclusion of the <span class="math">m</span> read operations. Hence, to ensure that there exists a set <span class="math">S</span> such that <span class="math">\\mathsf{RS}\\cup S=\\mathsf{WS}</span> at the conclusion of the procedure (i.e., to ensure that <span class="math">\\mathsf{RS}\\subseteq\\mathsf{WS}</span>), there must be some other read operation during which address <span class="math">k</span> is read, and the untrusted memory returns tuple <span class="math">(k,v,t-1)</span>. This is because we have assumed that the value <span class="math">v</span> was not written in the initialization phase, and outside of the initialization phase, the only way that the checker writes <span class="math">(k,v,t)</span> to memory is if a read to address <span class="math">k</span> returns tuple <span class="math">(v,t-1)</span>.</p>

    <p class="text-gray-300">Accordingly, the same reasoning as above applies to tuple <span class="math">(k,v,t-1)</span>. That is, to ensure that <span class="math">\\mathsf{RS}=\\mathsf{WS}</span> at the conclusion of the procedure, there must be some other read operation at which address <span class="math">k</span> is read, and the untrusted memory returns tuple <span class="math">(k,v,t-2)</span>. And so on. We conclude that for <em>every</em> field element in <span class="math">\\mathbb{F}</span> of the form <span class="math">t-i</span> for <span class="math">i=1,2,\\ldots,\\mathrm{char}(\\mathbb{F})</span>, there is some read operation that returns <span class="math">(k,v,t^{\\prime})</span>. Since there are <span class="math">m</span> many read operations and the characteristic of field is greater than <span class="math">m</span>, we obtain a contradiction. ∎</p>

    <h6 id="sec-40" class="text-base font-medium mt-4">Remark 2.</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Claim 2 assumes that the characteristic of the field is at least the number of read operations (if this is not the case, there is no contradiction in the conclusion that tuples of the form <span class="math">(v,t-i)</span> where written for all <span class="math">i\\in{1,\\ldots,\\mathrm{char}(\\mathbb{F})}</span>). We can nonetheless work over fields of smaller characteristic by modifying the procedure by which the checker updates the counts returned by each read operation. Specifically, rather than initializing counts to <span class="math">0</span> and replacing a count <span class="math">t</span> returned by a read operation with <span class="math">t+1</span>, we instead initialize the counts to <span class="math">1</span>, and replace a returned count <span class="math">t</span> with <span class="math">t\\cdot g</span>, where <span class="math">g</span> is a fixed generator of the multiplicative group of the field <span class="math">\\mathbb{F}</span>. With this modification, Claim 2 applies so long as $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">>m$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h6 id="sec-41" class="text-base font-medium mt-4">Remark 3.</h6>

    <p class="text-gray-300">The proof of Claim 2 implies that, if the checker ever performs a read to an “invalid” memory cell <span class="math">k</span>, meaning a cell indexed by <span class="math">k\\not\\in[\\mathsf{M}]</span>, then regardless of the value and timestamp returned by the untrusted prover in response to that read, there does not exist any set <span class="math">S</span> such that <span class="math">\\mathsf{WS}=\\mathsf{RS}\\cup S</span>.</p>

    <p class="text-gray-300">##</p>

    <h5 id="sec-42" class="text-base font-semibold mt-4">Counter polynomials.</h5>

    <p class="text-gray-300">To aid the polynomial evaluation proof of the sparse polynomial the prover commits to additional multilinear polynomials beyond <span class="math">E_{\\text{rx}}</span> and <span class="math">E_{\\text{ry}}</span>. We now describe these additional polynomials and how they are constructed.</p>

    <p class="text-gray-300">Observe that given the size <span class="math">\\mathsf{M}</span> of memory and a list of <span class="math">m</span> addresses involved in read operations, one can compute two vectors <span class="math">C_{r}\\in\\mathbb{F}^{m},C_{f}\\in\\mathbb{F}^{\\mathsf{M}}</span> defined as follows. For <span class="math">k\\in[m]</span>, <span class="math">C_{r}[k]</span> stores the count that would have been returned by the untrusted memory if it were honest during the <span class="math">k</span>th read operation. Similarly, for <span class="math">j\\in[\\mathsf{M}]</span>, let <span class="math">C_{f}[j]</span> store the final count stored at memory location <span class="math">j</span> of the untrusted memory (if the untrusted memory were honest) at the termination of the <span class="math">m</span> read operations. Computing these three vectors requires computation comparable to <span class="math">O(m)</span> operations over <span class="math">\\mathbb{F}</span>.</p>

    <p class="text-gray-300">Let <span class="math">\\mathsf{read\\_ts}=\\widetilde{C_{r}},\\mathsf{write\\_cts}=\\widetilde{C_{r}}+1,\\mathsf{final\\_cts}=\\widetilde{C_{f}}</span>. We refer to these polynomials as <em>counter polynomials</em>, which are unique for a given memory size <span class="math">\\mathsf{M}</span> and a list of <span class="math">m</span> addresses involved in read operations.</p>

    <h5 id="sec-43" class="text-base font-semibold mt-4">The actual evaluation proof.</h5>

    <p class="text-gray-300">To prove the evaluation of a given a <span class="math">(2\\log\\mathsf{M})</span>-variate multilinear polynomial <span class="math">D</span> that evaluates to a non-zero value at at most <span class="math">m</span> locations over <span class="math">\\{0,1\\}^{2\\log\\mathsf{M}}</span>, the prover sends the following polynomials in addition to <span class="math">E_{\\text{rx}}</span> and <span class="math">E_{\\text{ry}}</span>: two <span class="math">(\\log m)</span>-variate multilinear polynomials as oracles <span class="math">(\\mathsf{read\\_ts}_{\\text{row}},\\mathsf{read\\_ts}_{\\text{col}})</span>, and two <span class="math">(\\log\\mathsf{M})</span>-variate multilinear polynomials <span class="math">(\\mathsf{final\\_cts}_{\\text{row}},\\mathsf{final\\_cts}_{\\text{col}})</span>, where <span class="math">(\\mathsf{read\\_ts}_{\\text{row}},\\mathsf{final\\_cts}_{\\text{row}})</span> and <span class="math">(\\mathsf{read\\_ts}_{\\text{col}},\\mathsf{final\\_cts}_{\\text{col}})</span> are respectively the counter polynomials for the <span class="math">m</span> addresses specified by <span class="math">\\mathsf{row}</span> and <span class="math">\\mathsf{col}</span> over a memory of size <span class="math">\\mathsf{M}</span>.</p>

    <p class="text-gray-300">After that, in addition to performing the polynomial IOP depicted earlier in the proof (Figure 2), the core idea is to check if the two oracles sent by the prover satisfy the conditions identified earlier using Claim 2.</p>

    <h6 id="sec-44" class="text-base font-medium mt-4">Claim 3.</h6>

    <p class="text-gray-300">Given a <span class="math">(2\\log\\mathsf{M})</span>-variate multilinear polynomial, suppose that <span class="math">(\\mathsf{row},\\mathsf{col},\\mathsf{val})</span> denote multilinear polynomials committed by the commit algorithm. Furthermore, suppose that</p>

    <p class="text-gray-300"><span class="math">(E_{\\text{rx}},E_{\\text{ry}},\\mathsf{read\\_ts}_{\\text{row}},\\mathsf{final\\_cts}_{\\text{row}},\\mathsf{read\\_ts}_{\\text{col}},\\mathsf{final\\_cts}_{\\text{col}})</span></p>

    <p class="text-gray-300">denote the additional polynomials sent by the prover at the beginning of the evaluation proof.</p>

    <p class="text-gray-300">For any <span class="math">r_{x}\\in\\mathbb{F}^{\\log\\mathsf{M}}</span>, suppose that</p>

    <p class="text-gray-300"><span class="math">\\forall k\\in\\{0,1\\}^{\\log m},\\ E_{\\text{rx}}(k)=\\widetilde{eq}(\\mathsf{to-bits}(\\mathsf{row}(k)),r_{x}).</span> (9)</p>

    <p class="text-gray-300">Then the following holds: <span class="math">\\mathsf{WS}=\\mathsf{RS}\\cup S</span>, where</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathsf{WS}=\\{(\\mathsf{to-field}(i),\\widetilde{eq}(i,r_{x}),0)\\colon i\\in\\{0,1\\}^{\\log(\\mathsf{M})}\\}\\cup\\{(\\mathsf{row}(k),E_{\\text{rx}}(k),\\mathsf{write\\_cts}_{\\text{row}}(k)=\\mathsf{read\\_ts}_{\\text{row}}(k)+1)\\colon k\\in\\{0,1\\}^{\\log m}\\}</span>;</li>

      <li><span class="math">\\mathsf{RS}=\\{(\\mathsf{row}(k),E_{\\text{rx}}(k),\\mathsf{read\\_ts}_{\\text{row}}(k))\\colon k\\in\\{0,1\\}^{\\log m}\\}</span>; and</li>

      <li><span class="math">S=\\{(\\mathsf{to-field}(i),\\widetilde{eq}(i,r_{x}),\\mathsf{final\\_cts}_{\\text{row}}(i))\\colon i\\in\\{0,1\\}^{\\log(\\mathsf{M})}\\}</span>.</li>

    </ul>

    <p class="text-gray-300">Meanwhile, if Equation (9) does not hold, then there is no set <span class="math">S</span> such that <span class="math">\\mathsf{WS}=\\mathsf{RS}\\cup S</span>, where <span class="math">\\mathsf{WS}</span> and <span class="math">\\mathsf{RS}</span> are defined as above.</p>

    <p class="text-gray-300">Similarly, for any <span class="math">r_{y}\\in\\mathbb{F}^{\\log\\mathsf{M}}</span>, checking that <span class="math">\\forall k\\in\\{0,1\\}^{\\log m}</span>, <span class="math">E_{\\text{ry}}(k)=\\widetilde{eq}(\\mathsf{to-bits}(\\mathsf{col}(k)),r_{y})</span> is equivalent (in the sense above) to checking that <span class="math">\\mathsf{WS}^{\\prime}=\\mathsf{RS}^{\\prime}\\cup S^{\\prime}</span>, where</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathsf{WS}^{\\prime}=\\{(\\mathsf{to-field}(j),\\widetilde{eq}(j,r_{y}),0)\\colon j\\in\\{0,1\\}^{\\log(\\mathsf{M})}\\}\\cup\\{(\\mathsf{col}(k),E_{\\text{ry}}(k),\\mathsf{write\\_cts}_{\\text{col}}(k)=\\mathsf{read\\_ts}_{\\text{col}}(k)+1)\\colon k\\in\\{0,1\\}^{\\log m}\\}</span>;</li>

      <li><span class="math">\\mathsf{RS}^{\\prime}=\\{(\\mathsf{col}(k),E_{\\text{ry}}(k),\\mathsf{read\\_ts}_{\\text{col}}(k))\\colon k\\in\\{0,1\\}^{\\log m}\\}</span>; and</li>

      <li><span class="math">S^{\\prime}=\\{(\\mathsf{to-field}(j),\\widetilde{eq}(j,r_{y}),\\mathsf{final\\_cts}_{\\text{col}}(j))\\colon j\\in\\{0,1\\}^{\\log(\\mathsf{M})}\\}</span>.</li>

    </ul>

    <h6 id="sec-45" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">The result follows from an application of the invariant in Claim 2.</p>

    <p class="text-gray-300">Here, we clarify the following subtlety. The expression <span class="math">\\mathsf{to-bits}(\\mathsf{row}(k))</span> appearing in Equation (9) is not defined if <span class="math">\\mathsf{row}(k)</span> is outside of <span class="math">[\\mathsf{M}]</span> for any <span class="math">k\\in\\{0,1\\}^{\\log m}</span>. But in this event, Remark 3 nonetheless implies the conclusion of the theorem, namely that there is no set <span class="math">S</span> such that <span class="math">\\mathsf{WS}=\\mathsf{RS}\\cup S</span>. The analogous conclusion holds by the same reasoning if <span class="math">\\mathsf{col}(k)</span> is outside of <span class="math">[\\mathsf{M}]</span> for any <span class="math">k\\in\\{0,1\\}^{\\log m}</span>. ∎</p>

    <p class="text-gray-300">There is no direct way to prove that the checks on sets in Claim 3 hold. Instead, we rely on public-coin, multiset hash functions to compress RS, WS, and <span class="math">S</span> into a single element of <span class="math">\\mathbb{F}</span> each. Specifically:</p>

    <h6 id="sec-46" class="text-base font-medium mt-4">Claim 4 (<em>[x23]</em>).</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Given two multisets <span class="math">A,B</span> where each element is from <span class="math">\\mathbb{F}^{3}</span>, checking that <span class="math">A=B</span> is equivalent to checking the following, except for a soundness error of $O(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">+</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">B</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)/</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math"> over the choice of </span>\\gamma,\\tau<span class="math">: </span>\\mathcal{H}_{\\tau,\\gamma}(A)=\\mathcal{H}_{\\tau,\\gamma}(B)<span class="math">, where </span>\\mathcal{H}_{\\tau,\\gamma}(A)=\\prod_{(a,v,t)\\in A}(h_{\\gamma}(a,v,t)-\\tau)<span class="math">, and </span>h_{\\gamma}(a,v,t)=a\\cdot\\gamma^{2}+v\\cdot\\gamma+t<span class="math">. That is, if </span>A=B<span class="math">, </span>\\mathcal{H}_{\\tau,\\gamma}(A)=\\mathcal{H}_{\\tau,\\gamma}(B)<span class="math"> with probability </span>1<span class="math"> over randomly chosen values </span>\\tau<span class="math"> and </span>\\gamma<span class="math"> in </span>\\mathbb{F}<span class="math">, while if </span>A\\neq B<span class="math">, then </span>\\mathcal{H}_{\\tau,\\gamma}(A)=\\mathcal{H}_{\\tau,\\gamma}(B)<span class="math"> with probability at most </span>O(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">+</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">B</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)/</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Intuitively, Claim 4 gives an efficient randomized procedure for checking whether two sequences of tuples are permutations of each other. First, the procedure Reed-Solomon fingerprints each tuple (see <em>[x20, Section 2.1]</em> for an exposition). This is captured by the function <span class="math">h_{\\gamma}</span> and intuitively replaces each tuple with a single field element, such that distinct tuples are unlikely to collide. Second, the procedure applies a permutation-independent fingerprinting procedure <span class="math">H_{r,\\gamma}</span> to confirm that the resulting two sequences of fingerprints are permutations of each other.</p>

    <p class="text-gray-300">We are now ready to depict a polynomial IOP for proving evaluations of a committed sparse multilinear polynomial. Given <span class="math">r_{x},r_{y}\\in\\mathbb{F}^{\\log\\mathsf{M}}</span>, to prove that <span class="math">D(r_{x},r_{y})=v</span> for a purported evaluation <span class="math">v\\in\\mathbb{F}</span>, consider the polynomial IOP given in Figure 3, which assumes that the verifier has an oracle access to multilinear polynomial oracles that encode <span class="math">D</span> (namely, row, col, val)</p>

    <h5 id="sec-47" class="text-base font-semibold mt-4">Completeness.</h5>

    <p class="text-gray-300">Perfect completeness follows from perfect completeness of the sum-check protocol and the fact that the multiset equality checks using their fingerprints hold with probability <span class="math">1</span> over the choice of <span class="math">\\tau,\\gamma</span> if the prover is honest.</p>

    <h5 id="sec-48" class="text-base font-semibold mt-4">Soundness.</h5>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Applying a standard union bound to the soundness error introduced by probabilistic multiset equality checks with the soundness error of the sum-check protocol <em>[x16]</em>, we conclude that the soundness error for the depicted polynomial IOP as at most $O(m)/</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h5 id="sec-49" class="text-base font-semibold mt-4">Round and communication complexity.</h5>

    <p class="text-gray-300">There are three invocations of the sum-check protocol. First, the sum-check protocol is applied on a polynomial with <span class="math">\\log m</span> variables where the degree is at most <span class="math">3</span> in each variable, so the round complexity is <span class="math">O(\\log m)</span> and the communication cost is <span class="math">O(\\log m)</span> field elements. Second, four sum-check-based “grand product” protocols are computed in parallel. Two of the grand products are over vectors of size <span class="math">\\mathsf{M}</span> and the remaining two are over vectors of size <span class="math">m</span>. Third, the depicted IOP runs four additional “grand products”, which incurs the same costs as above. In total, with the protocol of <em>[x21, Section 6]</em> for grand products, the round complexity of the depicted IOP is <span class="math">\\tilde{O}(\\log m+\\log(N))</span> and the communication cost is <span class="math">\\tilde{O}(\\log m+\\log N)</span> field elements, where the <span class="math">\\tilde{O}</span> notation hides doubly-logarithmic factors. The prover commits to an extra <span class="math">O(m/\\log^{3}m)</span> field elements.</p>

    <h5 id="sec-50" class="text-base font-semibold mt-4">Verifier time.</h5>

    <p class="text-gray-300">The verifier’s runtime is dominated by its runtime in the grand product sum-check reductions, which is <span class="math">\\tilde{O}(\\log m)</span> field operations.</p>

    <h5 id="sec-51" class="text-base font-semibold mt-4">Prover Time.</h5>

    <p class="text-gray-300">Using linear-time sum-checks <em>[x21]</em> in all three sum-check reductions (and using the linear-time prover in the grand product protocol <em>[x22, x23]</em>), the prover’s time is <span class="math">O(N)</span> finite field operations for unstructured tables.</p>

    <p class="text-gray-300">Finally, to prove Theorem 1, applying the compiler of <em>[x3]</em> to the depicted polynomial IOP with the given dense polynomial commitment primitive, followed by the Fiat-Shamir transformation <em>[x10]</em>, provides the desired non-interactive argument of knowledge for proving evaluations of committed sparse multilinear polynomials, with efficiency claimed in the theorem statement. Appendix E provides additional details of the grand product argument.</p>

    <p class="text-gray-300">//During the commit phase,  <span class="math">\\mathcal{P}</span>  has committed to three  <span class="math">(\\log m)</span> -variate multilinear polynomials row, col, val.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathcal{P} \\to \\mathcal{V}</span> : four  <span class="math">(\\log m)</span> -variate multilinear polynomials  <span class="math">E_{\\mathrm{rx}}, E_{\\mathrm{ry}}</span> , read_tsrow, read_tscol and two  <span class="math">(\\log \\mathsf{M})</span> -variate multilinear polynomials final_ctsrow, final_ctscol.</li>

      <li>Recall that Claim 1 (see Equation (8)) shows that  <span class="math">D(r_{x},r_{y}) = \\sum_{k\\in \\{0,1\\}^{\\log m}}\\mathsf{val}(k)\\cdot E_{\\mathrm{rx}}(k)\\cdot E_{\\mathrm{ry}}(k)</span>  assuming that</li>

    </ol>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\forall k\\in \\{0,1\\}^{\\log m}</span> <span class="math">E_{\\mathrm{rx}}(k) = \\widetilde{eq} (\\mathsf{to - bits}(\\mathsf{row}(k)),r_x)</span>  ; and</li>

      <li><span class="math">\\forall k\\in \\{0,1\\}^{\\log m}</span> <span class="math">E_{\\mathrm{ry}}(k) = \\widetilde{eq} (\\mathsf{to - bits}(\\mathsf{col}(k)),r_y)</span></li>

    </ul>

    <p class="text-gray-300">Hence,  <span class="math">\\mathcal{V}</span>  and  <span class="math">\\mathcal{P}</span>  apply the sum-check protocol to the polynomial  <span class="math">\\mathsf{val}(k)\\cdot E_{\\mathrm{rx}}(k)\\cdot E_{\\mathrm{ry}}(k)</span> , which reduces the check that  <span class="math">v = \\sum_{k\\in \\{0,1\\}^{\\log m}}\\mathsf{val}(k)\\cdot E_{\\mathrm{rx}}(k)\\cdot E_{\\mathrm{ry}}(k)</span>  to checking that the following equations hold, where  <span class="math">r_z\\in \\mathbb{F}^{\\log m}</span>  chosen at random by the verifier over the course of the sum-check protocol:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\operatorname{val}(r_z) \\stackrel{?}{=} v_{\\operatorname{val}}</span> ; and</li>

      <li><span class="math">E_{\\mathrm{rx}}(r_z) \\stackrel{?}{=} v_{E_{\\mathrm{rx}}}</span>  and  <span class="math">E_{\\mathrm{ry}}(r_z) \\stackrel{?}{=} v_{E_{\\mathrm{ry}}}</span> . Here,  <span class="math">v_{\\mathrm{val}}</span> ,  <span class="math">v_{E_{\\mathrm{rx}}}</span>  and  <span class="math">v_{E_{\\mathrm{ry}}}</span>  are values provided by the prover at the end of the sum-check protocol.</li>

    </ul>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathcal{V}</span> : check if the three equalities above hold with one oracle query each to each of  <span class="math">\\mathsf{val}, E_{\\mathsf{rx}}, E_{\\mathsf{ry}}</span> .</li>

      <li>// The following checks if  <span class="math">E_{\\mathrm{rx}}</span>  is well-formed as per the first bullet in Step 2 above.</li>

      <li><span class="math">\\mathcal{V}\\to \\mathcal{P}</span>  ..  <span class="math">\\tau ,\\gamma \\in_{R}\\mathbb{F}</span></li>

      <li><span class="math">\\mathcal{V} \\leftrightarrow \\mathcal{P}</span> : run a sum-check-based protocol for "grand products" ([Tha13, Proposition 2] or [SL20, Section 5 or 6]) to reduce the check that  <span class="math">\\mathcal{H}_{\\tau,\\gamma}(\\mathsf{WS}) = \\mathcal{H}_{\\tau,\\gamma}(\\mathsf{RS}) \\cdot \\mathcal{H}_{\\tau,\\gamma}(S)</span> , where  <span class="math">\\mathsf{RS}, \\mathsf{WS}, S</span>  are as defined in Claim 3 and  <span class="math">\\mathcal{H}</span>  is defined in Claim 4 to checking if the following hold, where  <span class="math">r_{\\mathsf{M}} \\in \\mathbb{F}^{\\log \\mathsf{M}}, r_m \\in \\mathbb{F}^{\\log m}</span>  are chosen at random by the verifier over the course of the sum-check protocol:</li>

    </ol>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\widetilde{eq}(r_{\\mathsf{M}}, r_x) \\stackrel{?}{=} v_{eq}</span></li>

      <li><span class="math">E_{\\mathrm{rx}}(r_m) \\stackrel{?}{=} v_{E_{\\mathrm{rx}}}</span></li>

      <li><span class="math">\\operatorname{row}(r_m) \\stackrel{?}{=} v_{\\operatorname{row}}</span> ;  <span class="math">\\operatorname{read\\_ts}_{\\operatorname{row}}(r_m) \\stackrel{?}{=} v_{\\operatorname{read\\_ts}_{\\operatorname{row}}}</span> ; and  <span class="math">\\operatorname{final\\_cts}_{\\operatorname{row}}(r_M) \\stackrel{?}{=} v_{\\operatorname{final\\_cts}_{\\operatorname{row}}}</span></li>

    </ul>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathcal{V}</span> : directly check if the first equality holds, which can be done with  <span class="math">O(\\log \\mathsf{M})</span>  field operations; check the remaining equations hold with an oracle query to each of  <span class="math">E_{\\mathrm{rx}}</span> , row, read_tsrow, final_ctsrow.</li>

      <li>// The following steps check if  <span class="math">E_{\\mathrm{ry}}</span>  is well-formed as per the second bullet in Step 2 above.</li>

      <li><span class="math">\\mathcal{V}\\to \\mathcal{P}</span>  ..  <span class="math">\\tau^{\\prime},\\gamma^{\\prime}\\in_{R}\\mathbb{F}</span></li>

      <li><span class="math">\\mathcal{V} \\leftrightarrow \\mathcal{P}</span> : run a sum-check-based reduction for "grand products" ([Tha13, Proposition2] or [SL20, Sections 5 and 6]) to reduce the check that  <span class="math">\\mathcal{H}_{\\tau&#x27;, \\gamma&#x27;}(\\mathsf{WS}&#x27;) = \\mathcal{H}_{\\tau&#x27;, \\gamma&#x27;}(\\mathsf{RS}&#x27;) \\cdot \\mathcal{H}_{\\tau&#x27;, \\gamma&#x27;}(S&#x27;)</span> , where  <span class="math">\\mathsf{RS}&#x27;, \\mathsf{WS}&#x27;, S&#x27;</span>  are as defined in Claim 3 and  <span class="math">\\mathcal{H}</span>  is defined in Claim 4 to checking if the following hold, where  <span class="math">r_{\\mathsf{M}}&#x27; \\in \\mathbb{F}^{\\log \\mathsf{M}}</span> ,  <span class="math">r_m&#x27; \\in \\mathbb{F}^{\\log m}</span>  are chosen at random by the verifier in the sum-check protocol:</li>

    </ol>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\widetilde{eq}(r_{\\mathsf{M}}&#x27;, r_y) \\stackrel{?}{=} v_{eq}&#x27;</span></li>

      <li><span class="math">E_{\\mathrm{ry}}(r_m^{\\prime}) \\stackrel{?}{=} v_{E_{\\mathrm{ry}}}</span></li>

      <li><span class="math">\\operatorname{col}(r_m&#x27;) \\stackrel{?}{=} v_{\\operatorname{col}}</span> ;  <span class="math">\\operatorname{read\\_ts}_{\\operatorname{col}}(r_m&#x27;) \\stackrel{?}{=} v_{\\operatorname{read\\_ts}_{\\operatorname{col}}}</span> ; and  <span class="math">\\operatorname{final\\_cts}_{\\operatorname{col}}(r_M&#x27;) \\stackrel{?}{=} v_{\\operatorname{final\\_cts}_{\\operatorname{col}}}</span></li>

    </ul>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathcal{V}</span> : directly check if the first equality holds, which can be done with  <span class="math">O(\\log \\mathsf{M})</span>  field operations; check the remaining equations hold with an oracle query to each of  <span class="math">E_{\\mathrm{ry}}</span> , col, read_tscol, final_ctscol.</li>

    </ol>

    <p class="text-gray-300">Figure 3: Evaluation procedure of the Spark sparse polynomial commitment scheme.</p>

    <h4 id="sec-52" class="text-lg font-semibold mt-6">Additional discussion and intuition.</h4>

    <p class="text-gray-300">As previously discussed, the protocol in Figure 3 allows the prover to prove that it correctly ran the sparse polynomial evaluation algorithm described in Section 3.1 on the committed representation of the sparse polynomial. The core of the protocol lies in the memory-checking procedure, which enables the untrusted prover to establish that it produced the correct value upon every one of the algorithm’s reads into the <span class="math">c=2</span> memories of size <span class="math">\\mathsf{M}=N^{1/2}</span>. Intuitively, the values that the prover cryptographically commits to in the protocol are simply the values and counters returned by the aforementioned read operations (including a final “read pass” over both memories, which is required by the offline memory-checking procedure).</p>

    <p class="text-gray-300">A key and subtle aspect of the above is that the prover does <em>not</em> have to cryptographically commit to the values written to memory in the algorithm’s first phase, when it initializes the two memories (aka lookup tables, albeit dynamically determined by the evaluation point <span class="math">(r_{x},r_{y})</span>), of size <span class="math">\\mathsf{M}=N^{1/2}</span>. This is because these lookup tables are MLE-structured, meaning that the verifier can evaluate the multilinear extension of these tables on its own. The whole point of cryptographically committing to these values is to let the verifier evaluate the multilinear extension thereof at a randomly chosen point in the grand product argument. Since the verifier can perform this evaluation quickly on its own, there is no need for the prover in the protocol of Figure 3 to commit to these values.</p>

    <h3 id="sec-53" class="text-xl font-semibold mt-8">4.2 The general result</h3>

    <p class="text-gray-300">Theorem 1 gives a commitment scheme for <span class="math">m</span>-sparse multilinear polynomials over <span class="math">\\log N=2\\log(\\mathsf{M})</span> many variables, in which the prover commits to <span class="math">7</span> dense multilinear polynomials over <span class="math">\\log m</span> many variables, and <span class="math">2</span> dense polynomials over <span class="math">\\log(\\mathsf{M})</span> many variables.</p>

    <p class="text-gray-300">Suppose we want to support sparse polynomials over <span class="math">c\\log(\\mathsf{M})</span> variables for constant <span class="math">c&gt;2</span>, while ensuring that the prover still only commits to <span class="math">3c+1</span> many dense multilinear polynomials over <span class="math">\\log m</span> many variables, and <span class="math">c</span> many over <span class="math">\\log(N^{1/c})</span> many variables. We can proceed as follows.</p>

    <h4 id="sec-54" class="text-lg font-semibold mt-6">The function eq and its tensor structure.</h4>

    <p class="text-gray-300">Recall that <span class="math">\\mathsf{eq}_{s}\\colon\\{0,1\\}^{s}\\times\\{0,1\\}^{s}\\to\\{0,1\\}</span> takes as input two vectors of length <span class="math">s</span> and outputs <span class="math">1</span> if and only if the vectors are equal. (In this section, we find it convenient to make explicit the number of variables over which <span class="math">\\mathsf{eq}</span> is defined by including a subscript <span class="math">s</span>.) Recall from Equation (1) that <span class="math">\\widetilde{\\mathsf{eq}}_{s}(x,e)=\\prod_{i=1}^{s}\\left(x_{i}e_{i}+(1-x_{i})(1-e_{i})\\right).</span></p>

    <p class="text-gray-300">Equation (7) expressed the evaluation <span class="math">\\widetilde{D}(r_{x},r_{y})</span> of a sparse <span class="math">2\\log(\\mathsf{M})</span>-variate multilinear polynomial <span class="math">\\widetilde{D}</span> as</p>

    <p class="text-gray-300"><span class="math">\\widetilde{D}(r_{x},r_{y})=\\sum_{(i,j)\\in\\{0,1\\}^{\\log(\\mathsf{M})}\\times\\{0,1\\}^{\\log(\\mathsf{M})}}D(i,j)\\cdot\\widetilde{\\mathsf{eq}}_{\\log(\\mathsf{M})}(i,r_{x})\\cdot\\widetilde{\\mathsf{eq}}_{\\log(\\mathsf{M})}(j,r_{y}).</span> (10)</p>

    <p class="text-gray-300">The last two factors on the right hand side above have effectively factored <span class="math">\\widetilde{\\mathsf{eq}}_{2\\log(\\mathsf{M})}\\left((i,j),(r_{x},r_{y})\\right)</span> as the product of two terms that each test equality over <span class="math">\\log(\\mathsf{M})</span> many variables, namely:</p>

    <p class="text-gray-300"><span class="math">\\widetilde{\\mathsf{eq}}_{2\\log(\\mathsf{M})}\\left((i,j),(r_{x},r_{y})\\right)=\\widetilde{\\mathsf{eq}}_{\\log(\\mathsf{M})}(i,r_{x})\\cdot\\widetilde{\\mathsf{eq}}_{\\log(\\mathsf{M})}(j,r_{y}).</span></p>

    <p class="text-gray-300">Within the sparse polynomial commitment scheme, this ultimately led to checking two different memories, each of size <span class="math">\\mathsf{M}</span>, one of which we referred to as the “row” memory, and one as the “column” memory. For each memory checked, the prover had to commit to three <span class="math">(\\log m)</span>-variate polynomials, e.g., <span class="math">E_{\\mathsf{rx}}</span>, row, read_ts_{row}, and one <span class="math">\\log(\\mathsf{M})</span>-variate polynomial, e.g., final_cts_{row}.</p>

    <h4 id="sec-55" class="text-lg font-semibold mt-6">Supporting <span class="math">\\log N=c\\log M</span> variables rather than <span class="math">2\\log M</span>.</h4>

    <p class="text-gray-300">If we want to support polynomials over <span class="math">c\\log(\\mathsf{M})</span> variables for <span class="math">c&gt;2</span>, we simply factor <span class="math">\\widetilde{\\mathsf{eq}}_{c\\log(\\mathsf{M})}</span> into a product of <span class="math">c</span> terms that test equality over <span class="math">\\log(\\mathsf{M})</span> variables each. For example, if <span class="math">c=3</span>, then we can write:</p>

    <p class="text-gray-300"><span class="math">\\widetilde{\\mathsf{eq}}_{3\\log(\\mathsf{M})}\\left((i,j,k),(r_{x},r_{y},r_{z})\\right)=\\widetilde{\\mathsf{eq}}_{\\log(\\mathsf{M})}(i,r_{x})\\cdot\\widetilde{\\mathsf{eq}}_{\\log(\\mathsf{M})}(j,r_{y})\\cdot\\widetilde{\\mathsf{eq}}_{\\log(\\mathsf{M})}(k,r_{z}).</span></p>

    <p class="text-gray-300">Hence, if <span class="math">D</span> is a <span class="math">(3\\log M)</span>-variate polynomial, we obtain the following analog of Equation (10):</p>

    <p class="text-gray-300"><span class="math">\\widetilde{D}(r_{x},r_{y},r_{z})</span> <span class="math">=\\sum_{(i,j,k)\\in\\{0,1\\}^{\\log(\\mathsf{M})}\\times\\{0,1\\}^{\\log(\\mathsf{M})}\\times\\{0,1\\}^{\\log(\\mathsf{M})}}D(i,j,k)\\cdot\\widetilde{\\mathsf{eq}}_{\\log(\\mathsf{M})}(i,r_{x})\\cdot\\widetilde{\\mathsf{eq}}_{\\log(\\mathsf{M})}(j,r_{y})\\cdot\\widetilde{\\mathsf{eq}}_{\\log(\\mathsf{M})}(k,r_{z}).</span> (11)</p>

    <p class="text-gray-300">Based on the above equation, straightforward modifications to the sparse polynomial commitment scheme lead to checking <span class="math">c</span> different untrusted memories, each of size <span class="math">\\mathsf{M}</span>, rather than two. For example, when <span class="math">c=3</span>, the first memory stores all evaluations of <span class="math">\\widetilde{eq}_{\\log(\\mathsf{M})}(i,r_{x})</span> as <span class="math">i</span> ranges over <span class="math">\\{0,1\\}^{\\log m}</span>, the second stores <span class="math">\\widetilde{eq}_{\\log(\\mathsf{M})}(j,r_{y})</span> as <span class="math">j</span> ranges over <span class="math">\\{0,1\\}^{\\log m}</span>, and the third stores <span class="math">\\widetilde{eq}_{\\log(\\mathsf{M})}(k,r_{z})</span> as <span class="math">k</span> ranges over <span class="math">\\{0,1\\}^{\\log m}</span>. These are exactly the contents of the three lookup tables of size <span class="math">N^{1/c}</span> used by the sparse polynomial evaluation algorithm of Section 3.1 when <span class="math">c=3</span>.</p>

    <p class="text-gray-300">For each memory checked, the prover has to commit to three multilinear polynomials defined over <span class="math">\\log(m)</span>-many variables, and one defined over <span class="math">\\log(\\mathsf{M})=\\log(N)/c</span> variables. We obtain the following theorem.</p>

    <h6 id="sec-56" class="text-base font-medium mt-4">Theorem 2.</h6>

    <p class="text-gray-300">Given a polynomial commitment scheme for <span class="math">(\\log\\mathsf{M})</span>-variate multilinear polynomials with the following parameters (where <span class="math">\\mathsf{M}</span> is a positive integer and WLOG a power of 2):</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>the size of the commitment is <span class="math">\\mathsf{c}(\\mathsf{M})</span>;</li>

      <li>the running time of the commit algorithm is <span class="math">\\mathsf{tc}(\\mathsf{M})</span>;</li>

      <li>the running time of the prover to prove a polynomial evaluation is <span class="math">\\mathsf{tp}(\\mathsf{M})</span>;</li>

      <li>the running time of the verifier to verify a polynomial evaluation is <span class="math">\\mathsf{tv}(\\mathsf{M})</span>;</li>

      <li>the proof size is <span class="math">\\mathsf{p}(\\mathsf{M})</span>,</li>

    </ul>

    <p class="text-gray-300">there exists a polynomial commitment scheme for <span class="math">(c\\log\\mathsf{M})</span>-variate multilinear polynomials that evaluate to a non-zero value at at most <span class="math">m</span> locations over the Boolean hypercube <span class="math">\\{0,1\\}^{c\\log\\mathsf{M}}</span>, with the following parameters:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>the size of the commitment is <span class="math">(3c+1)\\mathsf{c}(m)+c\\cdot\\mathsf{c}(\\mathsf{M})</span>;</li>

      <li>the running time of the commit algorithm is <span class="math">O\\left(c\\cdot(\\mathsf{tc}(m)+\\mathsf{tc}(\\mathsf{M}))\\right)</span>;</li>

      <li>the running time of the prover to prove a polynomial evaluation is <span class="math">O\\left(c\\left(\\mathsf{tp}(m)+\\mathsf{tc}(\\mathsf{M})\\right)\\right)</span>;</li>

      <li>the running time of the verifier to verify a polynomial evaluation is <span class="math">O\\left(c\\left(\\mathsf{tv}(m)+\\mathsf{tv}(\\mathsf{M})\\right)\\right)</span>;</li>

      <li>the proof size is <span class="math">O\\left(c\\left(\\mathsf{p}(m)+\\mathsf{p}(\\mathsf{M})\\right)\\right)</span>.</li>

    </ul>

    <p class="text-gray-300">Many polynomial commitment schemes have efficient batching properties for evaluation proofs. For such schemes, the factor <span class="math">c</span> can be omitted in the final three bullet points of Theorem 2 (i.e., prover and verifier costs for verifying polynomial evaluation do not grow with <span class="math">c</span>).</p>

    <h3 id="sec-57" class="text-xl font-semibold mt-8">4.3 Specializing the Spark sparse commitment scheme to Lasso</h3>

    <p class="text-gray-300">In Lasso, if the prover is honest then the sparse polynomial commitment scheme is applied to the multilinear extension of a matrix <span class="math">M</span> with <span class="math">m</span> rows and <span class="math">N</span> columns, where <span class="math">m</span> is the number of lookups and <span class="math">N</span> is the size of the table. If the prover is honest then each row of <span class="math">M</span> is a unit vector.</p>

    <p class="text-gray-300">In fact, we require the commitment scheme to enforce these properties even when the prover is potentially malicious. Achieving this simplifies the commitment scheme and provides concrete efficiency benefits. It also keeps Lasso’s polynomial IOP simple as it does not need additional invocations of the sum-check protocol to prove that <span class="math">M</span> satisfies these properties.</p>

    <p class="text-gray-300">First, the multilinear polynomial <span class="math">\\mathsf{val}(k)</span> is fixed to <span class="math">1</span>, and it is not committed by the prover. Recall from Claim 1 that <span class="math">\\mathsf{val}(k)</span> extends the function that maps a bit-vector <span class="math">k\\in\\{0,1\\}^{\\log m}</span> to the value of the <span class="math">k</span>’th non-zero evaluation of the sparse function. Since <span class="math">M</span> is a <span class="math">\\{0,1\\}</span>-valued matrix, <span class="math">\\mathsf{val}(k)</span> is just the constant polynomial that evaluates to <span class="math">1</span> at all inputs.</p>

    <p class="text-gray-300">//During the commit phase applied to the multilinear extension  <span class="math">\\widetilde{M}</span>  of  <span class="math">m\\times N</span>  matrix  <span class="math">M</span>  with each row a unit vector,  <span class="math">\\mathcal{P}</span>  has committed to  <span class="math">c</span>  different  <span class="math">\\ell</span> -variate multilinear polynomials  <span class="math">\\dim_1,\\ldots ,\\dim_c</span> , where  <span class="math">\\ell = \\log (N^{1 / c})</span> . These are analogs of the polynomials row and col from Figure 3.  <span class="math">\\dim_i</span>  is purported to provide the indices of the cells of the  <span class="math">i</span> 'th memory that are read by the sparse polynomial evaluation algorithm of Section 3.1. Note that these indices depend only on the locations of the non-zero entries of  <span class="math">\\widetilde{M}</span> .</p>

    <p class="text-gray-300">//If  <span class="math">\\mathcal{P}</span>  is honest, then each  <span class="math">\\dim_i</span>  maps  <span class="math">\\{0,1\\}^{\\log m}</span>  to  <span class="math">\\{0,\\dots ,N^{1 / c} - 1\\}</span> . For each  <span class="math">j\\in \\{0,1\\}^{\\log m}</span> ,  <span class="math">(\\dim_1(j),\\ldots ,\\dim_c(j))</span>  is interpreted as specifying the identity of the unique non-zero entry of row  <span class="math">j</span>  of  <span class="math">M</span> .</p>

    <p class="text-gray-300">//  <span class="math">\\mathcal{V}</span>  requests to evaluate  <span class="math">\\widetilde{M}</span>  at input  <span class="math">(r,r^{\\prime})</span>  where  <span class="math">r^\\prime = (r_1^\\prime ,\\ldots ,r_c^\\prime)\\in \\left(\\mathbb{F}^\\ell\\right)^c</span></p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathcal{P} \\to \\mathcal{V}</span> :  <span class="math">2c</span>  different  <span class="math">(\\log m)</span> -variate multilinear polynomials  <span class="math">E_1, \\ldots, E_c</span> ,  <span class="math">\\text{read\\_ts}_1, \\ldots, \\text{read\\_ts}_c</span>  and  <span class="math">c</span>  different  <span class="math">\\ell</span> -variate multilinear polynomials  <span class="math">\\text{final\\_cts}_1, \\ldots, \\text{final\\_cts}_c</span> .</li>

    </ol>

    <p class="text-gray-300">//If  <span class="math">\\mathcal{P}</span>  is honest, then  <span class="math">\\text{read\\_ts}_1, \\ldots, \\text{read\\_ts}_c</span>  and  <span class="math">\\text{final\\_cts}_1, \\ldots, \\text{final\\_cts}_c</span>  map  <span class="math">\\{0, 1\\}^{\\log m}</span>  to  <span class="math">\\{0, \\ldots, m-1\\}</span> , as these are "counter polynomials" for each of the  <span class="math">c</span>  memories.</p>

    <p class="text-gray-300">//If  <span class="math">\\mathcal{P}</span>  is honest, then  <span class="math">E_1,\\ldots ,E_c</span>  contain the values returned by each read operation that the sparse polynomial evaluation algorithm of Section 3.1 makes to each of the  <span class="math">c</span>  memories.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Recall (Equation 11) that  <span class="math">\\widetilde{M}(r, r&#x27;) = \\sum_{k \\in \\{0,1\\}^{\\log m}} \\widetilde{\\operatorname{eq}}(r, k) \\cdot \\prod_{i=1}^{c} E_i(k)</span> , assuming that</li>

    </ol>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\forall k \\in \\{0,1\\}^{\\log m}</span> ,  <span class="math">E_{i}(k) = \\widetilde{eq}(\\text{to-bits}(\\dim_{i}(k)), r_{i}&#x27;)</span> .</li>

    </ul>

    <p class="text-gray-300">Hence,  <span class="math">\\mathcal{V}</span>  and  <span class="math">\\mathcal{P}</span>  apply the sum-check protocol to the polynomial  <span class="math">g(k)\\coloneqq \\widetilde{\\operatorname{eq}} (r,k)\\cdot \\prod_{i = 1}^{c}E_{i}(k)</span> , which reduces the check that  <span class="math">v = \\sum_{k\\in \\{0,1\\}^{\\log m}}\\widetilde{\\operatorname{eq}} (r,k)\\prod_{i = 1}^{c}E_{i}(k)</span>  to checking that the following equations hold, where  <span class="math">r_z\\in \\mathbb{F}^{\\log m}</span>  chosen at random by the verifier over the course of the sum-check protocol:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">E_{i}(r_{z}) \\stackrel{?}{=} v_{E_{i}}</span>  for  <span class="math">i = 1, \\ldots, c</span> . Here,  <span class="math">v_{E_1}, \\ldots, v_{E_c}</span>  are values provided by the prover at the end of the sum-check protocol.</li>

    </ul>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathcal{V}</span> : check if the above equalities hold with one oracle query to each  <span class="math">E_{i}</span> .</li>

    </ol>

    <p class="text-gray-300">// The following checks if  <span class="math">E_{i}</span>  is well-formed as per the first bullet in Step 2 above.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathcal{V}\\to \\mathcal{P}</span>  ..  <span class="math">\\tau ,\\gamma \\in_{R}\\mathbb{F}</span></li>

    </ol>

    <p class="text-gray-300">//In practice, one would apply a single sum-check protocol to a random linear combination of the below polynomials. For brevity, we describe the protocol as invoking  <span class="math">c</span>  independent instances of sum-check.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathcal{V} \\leftrightarrow \\mathcal{P}</span> : For  <span class="math">i = 1, \\ldots, c</span> , run a sum-check-based protocol for "grand products" ([Tha13, Proposition2] or [SL20, Section 5 or 6]) to reduce the check that  <span class="math">\\mathcal{H}_{\\tau, \\gamma}(\\mathsf{WS}) = \\mathcal{H}_{\\tau, \\gamma}(\\mathsf{RS}) \\cdot \\mathcal{H}_{\\tau, \\gamma}(S)</span> , where  <span class="math">\\mathsf{RS}, \\mathsf{WS}, S</span>  are as defined in Claim 3 and  <span class="math">\\mathcal{H}</span>  is defined in Claim 4 to checking if the following hold, where  <span class="math">r_i&#x27;&#x27; \\in \\mathbb{F}^\\ell, r_i&#x27;&#x27;&#x27; \\in \\mathbb{F}^{\\log m}</span>  are chosen at random by the verifier over the course of the sum-check protocol:</li>

    </ol>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">E_{i}(r_{i}^{\\prime \\prime \\prime}) \\stackrel{?}{=} v_{E_{i}}</span></li>

      <li><span class="math">\\dim_i(r_i^{\\prime \\prime \\prime})\\stackrel {?}{=}v_i</span>  ; read_ts  <span class="math">i(r_{i}^{\\prime \\prime \\prime})\\stackrel {?}{=}v_{\\mathrm{read\\_ts}_{i}}</span>  ; and final_cts  <span class="math">i(r_{i}^{\\prime \\prime})\\stackrel {?}{=}v_{\\mathrm{final\\_cts}_{\\mathrm{row}}}</span></li>

    </ul>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathcal{V}</span> : check that the remaining equations hold with an oracle query to each of  <span class="math">E_{i}, \\dim_{i}, \\text{read\\_ts}_{i}, \\text{final\\_cts}_{i}</span> .</li>

    </ol>

    <p class="text-gray-300">Figure 4: Evaluation procedure of the Spark sparse polynomial commitment scheme, optimized for its application to  <span class="math">M</span>  in Lasso.</p>

    <p class="text-gray-300">Second, for any <span class="math">k = (k_{1},\\ldots ,k_{\\log m})\\in \\{0,1\\}^{\\log m}</span>, the <span class="math">k</span>'th non-zero entry of <span class="math">M</span> is in row to-field <span class="math">(k) = \\sum_{j = 1}^{\\log m}2^{j - 1}\\cdot k_j</span>. Hence, in Equation (8) of Claim 1, to-bits(row(k)) is simply <span class="math">k</span>. This means that <span class="math">E_{\\mathrm{rx}}(k) = \\overline{\\mathrm{eq}} (k,r_x)</span>, which the verifier can evaluate on its own in logarithmic time. With this fact in hand, the prover does not commit to <span class="math">E_{\\mathrm{rx}}</span> nor prove that it is well-formed.</p>

    <p class="text-gray-300">In terms of costs in the resulting sparse polynomial commitment scheme applied to <span class="math">\\widetilde{M}</span>, this effectively removes the contribution of the first <span class="math">\\log m</span> variables of <span class="math">\\widetilde{M}</span> to the costs. Hence, the costs are that of applying the commitment scheme to an <span class="math">m</span>-sparse <span class="math">\\log (N)</span>-variate polynomial (with val fixed to 1).</p>

    <p class="text-gray-300">This means that, setting <span class="math">c = 2</span> for illustration, the prover commits to 6 multilinear polynomials with <span class="math">\\log (m)</span> variables each and to two multilinear polynomials with <span class="math">(1 / 2)\\log N</span> variables each.</p>

    <p class="text-gray-300">Figure 4 describes Spark specialized for Lasso to commit to <span class="math">\\widetilde{M}</span>. The prover commits to <span class="math">3c</span> dense <span class="math">(\\log (m))</span>-variate multilinear polynomials, called <span class="math">\\dim_1,\\ldots ,\\dim_c</span> (the analogs of the row and col polynomials of Section 4.1), <span class="math">E_{1},\\ldots ,E_{c}</span>, and <span class="math">\\text{read\\_ts}_1,\\ldots ,\\text{read\\_ts}_c</span>, as well as <span class="math">c</span> dense multilinear polynomials in <span class="math">\\log (N^{1 / c}) = \\log (N) / c</span> variables, called <span class="math">\\text{final\\_cts}_1,\\ldots ,\\text{final\\_cts}_c</span>. Each <span class="math">\\dim_i</span> is purported to be the memory cell from the <span class="math">i</span>'th memory that the sparse polynomial evaluation algorithm (§3.1) reads at each of its <span class="math">m</span> timesteps, <span class="math">E_{1},\\ldots ,E_{c}</span> the values returned by those reads, and <span class="math">\\text{read\\_ts}_1,\\ldots ,\\text{read\\_ts}_c</span> the associated counts. <span class="math">\\text{final\\_cts}_1,\\ldots ,\\text{final\\_cts}_c</span> are purported to be to counts returned by the memory checking procedure's final pass over each of the <span class="math">c</span> memories.</p>

    <p class="text-gray-300">If the prover is honest, then <span class="math">\\dim_1,\\ldots ,\\dim_c</span> each map <span class="math">\\{0,1\\}^{\\log m}</span> to <span class="math">\\{0,\\dots ,N^{1 / c} - 1\\}</span>, and <span class="math">\\text{read\\_ts}_1,\\ldots ,\\text{read\\_ts}_c</span> each map <span class="math">\\{0,1\\}^{\\log m}</span> to <span class="math">\\{0,\\dots ,m - 1\\}</span> and <span class="math">\\text{final\\_cts}_1,\\ldots ,\\text{final\\_cts}_c</span> each map <span class="math">\\{0,1\\}^{\\log m}</span> to <span class="math">\\{0,\\dots ,m - 1\\}</span>. In fact, for any integer <span class="math">j &amp;gt; 0</span>, at most <span class="math">m / j</span> out of the <span class="math">m</span> evaluations of each counter polynomial <span class="math">\\text{read\\_ts}_i</span> and <span class="math">\\text{final\\_cts}_i</span> can be larger than <span class="math">j</span>.</p>

    <p class="text-gray-300">The technical core of the Lasso lookup argument is Surge, a generalization of Spark. In particular, Lasso is simply a straightforward use of Surge.</p>

    <p class="text-gray-300">Recall from Section 4 and Figure 4 that Spark allows the untrusted Lasso prover to commit to <span class="math">\\widetilde{M}</span>, purported to be the multilinear extension of an <span class="math">m \\times N</span> matrix <span class="math">M</span>, with each row equal to a unit vector, such that <span class="math">M \\cdot t = a</span>. The commitment phase of Surge is same as that of Spark. Surge generalizes Spark in that the Surge prover proves a larger class of statements about the committed polynomial <span class="math">\\widetilde{M}</span> (Spark focused only on proving evaluations of the sparse polynomial <span class="math">\\widetilde{M}</span>).</p>

    <p class="text-gray-300"><strong>Overview of Lasso.</strong> In Lasso, after committing to <span class="math">\\widetilde{M}</span>, the Lasso verifier picks a random <span class="math">r \\in \\mathbb{F}^{\\log m}</span> and seeks to confirm that</p>

    <div class="my-4 text-center"><span class="math-block">\\sum_{j \\in \\{0, 1 \\} ^ {\\log N}} \\widetilde {M} (r, j) \\cdot t (j) = \\widetilde {a} (r). \\tag {12}</span></div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Indeed, if <span class="math">M \\cdot t</span> and <span class="math">a</span> are the same vector, then Equation (12) holds for every choice of <span class="math">r</span>, while if <span class="math">Mt \\neq a</span>, then by the Schwartz-Zippel lemma, Equation (12) holds with probability at most $\\frac{\\log m}{</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}<span class="math">. So up to soundness error </span>\\frac{\\log m}{</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}<span class="math">, checking that </span>Mt = a$ is equivalent to checking that Equation (12) holds.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">In Lasso, the verifier obtains <span class="math">\\widetilde{a}(r)</span> via the polynomial commitment to <span class="math">\\widetilde{a}</span>. Then, the prover establishes Equation (12) using Surge. Specifically, Surge generalizes Spark's procedure for generating evaluation proofs, to directly produce a proof as to the value of the left hand side of Equation (12). Essentially, the proof proves that the prover correctly ran a (very efficient) algorithm for evaluating the left hand side of Equation (12).</p>

    <p class="text-gray-300"><strong>A roughly <span class="math">O(\\alpha m)</span>-time algorithm for computing the LHS of Equation (12).</strong> From Equation (7),</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde {M} (r, y) = \\sum_{(i, j) \\in \\{0, 1 \\} ^ {\\log m + \\log N}} M _ {i, j} \\cdot \\widetilde {e q} (i, r) \\cdot \\widetilde {e q} (j, y).</span></div>

    <p class="text-gray-300">18More precisely, this holds if we define <span class="math">r_x</span> to be in <span class="math">\\mathbb{F}^{\\log m}</span> and <span class="math">r_y</span> to be in <span class="math">\\mathbb{F}^{\\log N}</span>, rather than defining them both to be in <span class="math">\\mathbb{F}^{\\log M} = \\mathbb{F}^{(1/2)(\\log m + \\log n)}</span>.</p>

    <p class="text-gray-300">26</p>

    <p class="text-gray-300">Hence, letting <span class="math">\\mathsf{nz}(i)</span> denote the unique column in row <span class="math">i</span> of <span class="math">M</span> that contains a non-zero value (namely, the value 1), the left hand side of Equation (12) equals</p>

    <p class="text-gray-300"><span class="math">\\sum_{i\\in\\{0,1\\}^{\\log m}}\\widetilde{\\text{eq}}(i,r)\\cdot T[\\mathsf{nz}(i)].</span> (13)</p>

    <p class="text-gray-300">Suppose that <span class="math">T</span> is a SOS table. This means that there is an integer <span class="math">k\\geq 1</span> and <span class="math">\\alpha=k\\cdot c</span> tables <span class="math">T_{1},\\ldots,T_{\\alpha}</span> of size <span class="math">N^{1/c}</span>, as well as an <span class="math">\\alpha</span>-variate multilinear polynomial <span class="math">g</span> such that the following holds. Suppose that for every <span class="math">r=(r_{1},\\ldots,r_{c})\\in\\bigl{(}\\{0,1\\}^{\\log(N)/c}\\bigr{)}^{c}</span>,</p>

    <p class="text-gray-300"><span class="math">T[r]=g\\left(T_{1}[r_{1}],\\ldots,T_{k}[r_{1}],T_{k+1}[r_{2}],\\ldots,T_{2k}[r_{2}],\\ldots,T_{\\alpha-k+1}[r_{c}],\\ldots,T_{\\alpha}[r_{c}]\\right).</span> (14)</p>

    <p class="text-gray-300">For each <span class="math">i\\in\\{0,1\\}^{\\log m}</span>, decompose <span class="math">\\mathsf{nz}(i)</span> and <span class="math">(\\mathsf{nz}_{1}(i),\\ldots,\\mathsf{nz}_{c}(i))\\in[N^{1/c}]^{c}</span>. Then Expression (13) equals</p>

    <p class="text-gray-300"><span class="math">\\sum_{i\\in\\{0,1\\}^{\\log m}}\\widetilde{\\text{eq}}(i,r)\\cdot g\\left(T_{1}[\\mathsf{nz}_{1}(i)],\\ldots,T_{k}[\\mathsf{nz}_{1}(i)],T_{k+1}[\\mathsf{nz}_{2}(i)],\\ldots,T_{2k}[\\mathsf{nz}_{2}(i)],\\ldots,T_{\\alpha-k+1}[\\mathsf{nz}_{c}(i)],\\ldots,T_{\\alpha}[\\mathsf{nz}_{c}(i)]\\right).</span> (15)</p>

    <p class="text-gray-300">The algorithm to compute Expression (15) simply initializes all tables <span class="math">T_{1},\\ldots,T_{\\alpha}</span>, then iterates over every <span class="math">i\\in\\{0,1\\}^{m}</span> and computes the <span class="math">i</span>’th term of the sum with a single lookup into each table (of course, the algorithm evaluates <span class="math">g</span> at the results of the lookups into <span class="math">T_{1},\\ldots,T_{\\alpha}</span>, and multiplies the result by <span class="math">\\widetilde{\\text{eq}}(i,r)</span>).</p>

    <h4 id="sec-59" class="text-lg font-semibold mt-6">Description of <span class="math">\\mathsf{Surge}</span>.</h4>

    <p class="text-gray-300">The commitment to <span class="math">\\widetilde{M}</span> in <span class="math">\\mathsf{Surge}</span> consists of commitments to <span class="math">c</span> multilinear polynomials <span class="math">\\dim_{1},\\ldots,\\dim_{c}</span>, each over <span class="math">\\log m</span> variables. <span class="math">\\dim_{i}</span> is purported to be the multilinear extension of <span class="math">\\mathsf{nz}_{i}</span>.</p>

    <p class="text-gray-300">The verifier chooses <span class="math">r\\in\\{0,1\\}^{\\log m}</span> at random and requests that the <span class="math">\\mathsf{Surge}</span> prover prove that the committed polynomial <span class="math">\\widetilde{M}</span> satisfy Equation (13). The prover does so by proving it ran the aforementioned algorithm for evaluating Expression (15). Following the memory-checking procedure in Section 4, with each table <span class="math">T_{i}\\colon i=1,\\ldots,\\alpha</span> viewed as a memory of size <span class="math">N^{1/c}</span>), this entails committing for each <span class="math">i</span> to <span class="math">\\log(m)</span>-variate multilinear polynomials <span class="math">E_{i}</span> and <span class="math">\\mathsf{read\\_ts}_{i}</span> (purported to capture the value and count returned by each of the <span class="math">m</span> lookups into <span class="math">T_{i}</span>) and a <span class="math">\\log(N^{1/c})</span>-variate multilinear polynomial <span class="math">\\mathsf{final\\_cts}_{i}</span> (purported to capture the final count for each memory cell of <span class="math">T_{i}</span>.)</p>

    <p class="text-gray-300">Let <span class="math">\\widetilde{t}_{i}</span> be the mutlilinear extension of the vector <span class="math">t_{i}</span> whose <span class="math">j</span>’th entry is <span class="math">T_{i}[j]</span>. The sum-check protocol is applied to compute</p>

    <p class="text-gray-300"><span class="math">\\sum_{j\\in\\{0,1\\}^{\\log m}}\\widetilde{\\text{eq}}(r,j)\\cdot g\\left(E_{1}(j),\\ldots,E_{\\alpha}(j)\\right).</span> (16)</p>

    <p class="text-gray-300">At the end of the sum-check protocol, the verifier needs to evaluate <span class="math">\\widetilde{\\text{eq}}(r,r^{\\prime})\\cdot g(E_{1}(r^{\\prime}),\\ldots,E_{\\alpha}(r^{\\prime}))</span> at a random point <span class="math">r^{\\prime}\\in\\mathbb{F}^{\\log m}</span>, which it can do with one evaluation query to each <span class="math">E_{i}</span> (the verifier can compute <span class="math">\\widetilde{\\text{eq}}(r,r^{\\prime})</span> on its own in <span class="math">O(\\log m)</span> time).</p>

    <p class="text-gray-300">The verifier must still check that each <span class="math">E_{i}</span> is well-formed, in the sense that <span class="math">E_{i}(j)</span> equals <span class="math">T_{i}[\\dim_{i}(j)]</span> for all <span class="math">j\\in\\{0,1\\}^{\\log m}</span>. This is done exactly as in <span class="math">\\mathsf{Spark}</span> to confirm that for each of the <span class="math">\\alpha</span> memories, <span class="math">\\mathsf{WS}=\\mathsf{RS}\\cup S</span> (see Claims 3 and 4 and Figure 4). At the end of this procedure, for each <span class="math">i=1,\\ldots,\\alpha</span>, the verifier needs to evaluate each of <span class="math">\\dim_{i}</span>, <span class="math">\\mathsf{read\\_ts}_{i}</span>, <span class="math">\\mathsf{final\\_cts}_{i}</span> at a random point, which it can do with one query to each. The verifier also needs to evaluate the multilinear extension <span class="math">\\widetilde{t}_{i}</span> of each sub-table <span class="math">T_{i}</span> for each <span class="math">i=1,\\ldots,\\alpha</span> at a single point. <span class="math">T</span> being SOS guarantees that the verifier can compute each of these evaluations in <span class="math">O(\\log(N)/c)</span> time.</p>

    <h4 id="sec-60" class="text-lg font-semibold mt-6">Prover time.</h4>

    <p class="text-gray-300">Besides committing to the polynomials <span class="math">\\dim_{i},E_{i},\\mathsf{read\\_ts}_{i},\\mathsf{final\\_cts}_{i}</span> for each of the <span class="math">\\alpha</span> memories and producing one evaluation proof for each (in practice, these would be batched), the prover must compute its messages in the sum-check protocol used to compute Expression (16) and the grand product arguments (which can be batched). Using the linear-time sum-check protocol <em>[x10, x26, x27]</em>, the prover can compute its messages in the sum-check protocol used to compute Expression (16) with <span class="math">O(b\\cdot k\\cdot\\alpha\\cdot m)</span> field operations, where recall that <span class="math">\\alpha=k\\cdot c</span> and <span class="math">b</span> is the number of monomials in <span class="math">g</span>. If <span class="math">k=O(1)</span>, then this is</p>

    <p class="text-gray-300"><span class="math">T</span>  is an SOS lookup table of size  <span class="math">N</span> , meaning there are  <span class="math">\\alpha = kc</span>  tables  <span class="math">T_{1},\\ldots ,T_{\\alpha}</span> , each of size  <span class="math">N^{1 / c}</span> , such that for any  <span class="math">r\\in \\{0,1\\}^{\\log N}</span> ,  <span class="math">T[r] = g(T_1[r_1],\\dots,T_k[r_1],T_{k + 1}[r_2],\\dots,T_{2k}[r_2],\\dots,T_{\\alpha -k + 1}[r_c],\\dots,T_\\alpha [r_c])</span> . During the commit phase,  <span class="math">\\mathcal{P}</span>  commits to  <span class="math">c</span>  multilinear polynomials  <span class="math">\\dim_1,\\ldots ,\\dim_c</span> , each over  <span class="math">\\log m</span>  variables.  <span class="math">\\dim_i</span>  is purported to provide the indices of  <span class="math">T_{(i - 1)k + 1},\\ldots ,T_{ik}</span>  the natural algorithm computing  <span class="math">\\sum_{i\\in \\{0,1\\}^{\\log m}}\\widetilde{\\mathbf{eq}} (i,r)\\cdot T[\\mathsf{nz}[i]]</span>  (see Equation (15)).</p>

    <p class="text-gray-300">//  <span class="math">\\mathcal{V}</span>  requests  <span class="math">\\langle u,t\\rangle</span>  , where the  <span class="math">i</span> th entry of  <span class="math">t</span>  is  <span class="math">T[i]</span>  and the  <span class="math">y</span> th entry of  <span class="math">u</span>  is  <span class="math">\\widetilde{M} (r,y)</span> .</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathcal{P} \\to \\mathcal{V}</span> :  <span class="math">2\\alpha</span>  different  <span class="math">(\\log m)</span> -variate multilinear polynomials  <span class="math">E_1, \\ldots, E_\\alpha</span> ,  <span class="math">\\text{read\\_ts}_1, \\ldots, \\text{read\\_ts}_\\alpha</span>  and  <span class="math">\\alpha</span>  different  <span class="math">(\\log(N)/c)</span> -variate multilinear polynomials  <span class="math">\\text{final\\_cts}_1, \\ldots, \\text{final\\_cts}_\\alpha</span> .</li>

    </ol>

    <p class="text-gray-300">//  <span class="math">E_{i}</span>  is purported to specify the values of each of the  <span class="math">m</span>  reads into  <span class="math">T_{i}</span> .</p>

    <p class="text-gray-300">//read_ts1, ... read_tsα and final_cts1, ..., final_ctsα, are "counter polynomials" for each of the α sub-tables  <span class="math">T_{i}</span> .</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathcal{V}</span>  and  <span class="math">\\mathcal{P}</span>  apply the sum-check protocol to the polynomial  <span class="math">h(k)\\coloneqq \\widetilde{\\mathbf{eq}} (r,k)\\cdot g(E_1(k),\\ldots ,E_\\alpha (k))</span>  which reduces the check that  <span class="math">v = \\sum_{k\\in \\{0,1\\}^{\\log m}}g(E_1(k),\\ldots ,E_\\alpha (k))</span>  to checking that the following equations hold, where  <span class="math">r_z\\in \\mathbb{F}^{\\log m}</span>  chosen at random by the verifier over the course of the sum-check protocol:</li>

    </ol>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">E_{i}(r_{z}) \\stackrel{?}{=} v_{E_{i}}</span>  for  <span class="math">i = 1, \\ldots, \\alpha</span> . Here,  <span class="math">v_{E_1}, \\ldots, v_{E_\\alpha}</span>  are values provided by the prover at the end of the sum-check protocol.</li>

    </ul>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathcal{V}</span> : check if the above equalities hold with one oracle query to each  <span class="math">E_{i}</span> .</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>// The following checks if  <span class="math">E_{i}</span>  is well-formed, i.e., that  <span class="math">E_{i}(j)</span>  equals  <span class="math">T_{i}[\\dim_{i}(j)]</span>  for all  <span class="math">j \\in \\{0,1\\}^{\\log m}</span> .</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathcal{V}\\to \\mathcal{P}</span>  ..  <span class="math">\\tau ,\\gamma \\in_{R}\\mathbb{F}</span></li>

    </ol>

    <p class="text-gray-300">//In practice, one would apply a single sum-check protocol to a random linear combination of the below polynomials. For brevity, we describe the protocol as invoking  <span class="math">c</span>  independent instances of sum-check.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathcal{V} \\leftrightarrow \\mathcal{P}</span> : For  <span class="math">i = 1, \\ldots, \\alpha</span> , run a sum-check-based protocol for "grand products" ([Tha13, Proposition2] or [SL20, Section 5 or 6]) to reduce the check that  <span class="math">\\mathcal{H}_{\\tau, \\gamma}(\\mathsf{WS}) = \\mathcal{H}_{\\tau, \\gamma}(\\mathsf{RS}) \\cdot \\mathcal{H}_{\\tau, \\gamma}(S)</span> , where  <span class="math">\\mathsf{RS}, \\mathsf{WS}, S</span>  are as defined in Claim 3 and  <span class="math">\\mathcal{H}</span>  is defined in Claim 4 to checking if the following hold, where  <span class="math">r_i&#x27;&#x27; \\in \\mathbb{F}^\\ell</span> ,  <span class="math">r_i&#x27;&#x27;&#x27; \\in \\mathbb{F}^{\\log m}</span>  are chosen at random by the verifier over the course of the sum-check protocol:</li>

    </ol>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">E_{i}(r_{i}^{\\prime \\prime \\prime}) \\stackrel{?}{=} v_{E_{i}}</span></li>

      <li><span class="math">\\dim_i(r_i^{\\prime \\prime \\prime})\\stackrel {?}{=}v_i</span>  ; read_ts  <span class="math">(r_i^{\\prime \\prime \\prime})\\stackrel {?}{=}v_{\\mathrm{read\\_ts}_i}</span>  ; and final_cts  <span class="math">(r_i^{\\prime \\prime})\\stackrel {?}{=}v_{\\mathrm{final\\_cts}_i}</span></li>

    </ul>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathcal{V}</span> : Check the equations hold with an oracle query to each of  <span class="math">E_{i},\\dim_{i},\\text{read\\_ts}_{i},\\text{final\\_cts}_{i}</span> .</li>

    </ol>

    <p class="text-gray-300">Figure 5: Surge's polynomial IOP for proving that  <span class="math">\\sum_{y\\in \\{0,1\\}^{\\log N}}\\widetilde{M} (r,y)T[y] = v</span></p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Input: A polynomial commitment to the multilinear polynomials <span class="math">\\widetilde{a}\\colon\\mathbb{F}^{\\log m}\\to\\mathbb{F}</span>, and a description of an SOS table <span class="math">T</span> of size <span class="math">N</span>.</li>

      <li>The prover <span class="math">\\mathcal{P}</span> sends a Surge-commitment to the multilinear extension <span class="math">\\widetilde{M}</span> of a matrix <span class="math">M\\in\\{0,1\\}^{m\\times N}</span>. This consists of <span class="math">c</span> different <span class="math">(\\log(m))</span>-variate multilinear polynomials <span class="math">\\dim_{1},\\ldots,\\dim_{c}</span> (see Figure 5 for details).</li>

      <li>The verifier <span class="math">\\mathcal{V}</span> picks a random <span class="math">r\\in\\mathbb{F}^{\\log m}</span> and sends <span class="math">r</span> to <span class="math">\\mathcal{P}</span>. The verifier makes one evaluation query to <span class="math">\\widetilde{a}</span>, to learn <span class="math">\\widetilde{a}(r)</span>.</li>

      <li><span class="math">\\mathcal{P}</span> and <span class="math">\\mathcal{V}</span> apply Surge (Figure 5), allowing <span class="math">\\mathcal{P}</span> to prove that <span class="math">\\sum_{y\\in\\{0,1\\}^{\\log N}}\\widetilde{M}(r,y)T[y]=\\widetilde{a}(r)</span>.</li>

    </ul>

    <p class="text-gray-300">Figure 6: Description of the Lasso lookup argument. Here, <span class="math">a</span> denotes the vector of lookups and <span class="math">t</span> the vector capturing the lookup table (Definition 1.1). A polynomial commitments to the multilinear extension polynomial <span class="math">\\widetilde{a}\\colon\\mathbb{F}^{\\log m}\\to\\mathbb{F}</span> is given to the verifier as input. If <span class="math">t</span> is unstructured, then <span class="math">c</span> will be set to 1.</p>

    <p class="text-gray-300"><span class="math">O(b\\cdot c\\cdot m)</span> time. For many tables of practical interest, the factor <span class="math">b</span> can be eliminated (e.g., if the <em>total degree</em> of <span class="math">g</span> is a constant independent of <span class="math">b</span>, such as 1 or 2). The costs for the prover in the memory checking argument is similar to Spark: <span class="math">O(\\alpha\\cdot m+\\alpha\\cdot N^{1/c})</span> field operations, plus committing to a low-order number of field elements.</p>

    <h5 id="sec-61" class="text-base font-semibold mt-4">Verification costs.</h5>

    <p class="text-gray-300">The sum-check protocol used to compute Expression (16) consists of <span class="math">\\log m</span> rounds in which the prover sends a univariate polynomial of degree at most <span class="math">1+\\alpha</span> in each round. Hence, the prover sends <span class="math">O(c\\cdot k\\cdot\\log m)</span> field elements, and the verifier performs <span class="math">O(k\\cdot\\log m)</span> field operations. The costs of the memory checking argument (which can be batched) for the verifier are identical to Spark.</p>

    <h5 id="sec-62" class="text-base font-semibold mt-4">Completeness and knowledge soundness of the polynomial IOP.</h5>

    <p class="text-gray-300">Completeness holds by design and by the completeness of the sum-check protocol, and of the memory checking argument.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">By the soundness of the sum-check protocol and the memory checking argument, if the prover passes the verifier’s checks in the polynomial IOP with probability more than an appropriately chosen threshold $\\gamma=O(m+N^{1/c}/</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math">, then </span>\\sum_{y\\in\\{0,1\\}^{\\log N}}\\widetilde{M}(r,y)T[y]=v<span class="math">, where </span>\\widetilde{M}<span class="math"> is the multilinear extension of the following matrix </span>M<span class="math">. For </span>i\\in\\{0,1\\}^{\\log m}<span class="math">, row </span>i<span class="math"> of </span>M<span class="math"> consists of all zeros except for entry </span>M_{i,j}=1<span class="math">, where </span>j=(j_{1},\\ldots,j_{c})\\in\\{0,1,\\ldots,N^{1/c}\\}^{c}<span class="math"> is the unique column index such that </span>j_{1}=\\dim_{1}(i)<span class="math">, …, </span>j_{c}=\\dim_{c}(i)$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">We have established the following theorem.</p>

    <h6 id="sec-63" class="text-base font-medium mt-4">Theorem 3.</h6>

    <p class="text-gray-300">Figure 5 is a complete and knowledge-sound polynomial IOP for establishing that the prover knows an <span class="math">m\\times N</span> matrix <span class="math">M\\in\\{0,1\\}^{m\\times N}</span> with exactly one entry equal to <span class="math">1</span> in each row, such that</p>

    <p class="text-gray-300"><span class="math">\\sum_{y\\in\\{0,1\\}^{\\log N}}\\widetilde{M}(r,y)T[y]=v.</span> (17)</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">The discussion surrounding Equation (12) explained that checking that <span class="math">Mt=a</span> is equivalent, up to soundness error $\\log(m)/</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math">, to Equation (17) holding for a random </span>r\\in\\mathbb{F}^{\\log m}$. Combining this with Theorem 3 implies that the protocol in Figure 6, i.e., Lasso, is a lookup argument.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h6 id="sec-64" class="text-base font-medium mt-4">Remark 4.</h6>

    <p class="text-gray-300">Figure 6 describes an <em>unindexed</em> lookup argument, as for each <span class="math">i\\in\\{0,\\ldots,m-1\\}</span>, it must be the case that <span class="math">a_{i}=t_{j}</span> where <span class="math">j=(j_{1},\\ldots,j_{c})</span> is defined as in the proof of Theorem 3 above. To obtain an indexed lookup argument (Definition 1.2), one would need to additionally have to check that for each <span class="math">i\\in\\{0,\\ldots,m-1\\}</span>, <span class="math">b_{i}=\\sum_{i=1}^{c}(N^{1/c})^{i-1}\\cdot j_{i}</span>, i.e., that the <span class="math">i</span>’th (“chunked”) index encoded by <span class="math">M</span> matches the <span class="math">i</span>’th entry of the committed index vector <span class="math">b</span>.</p>

    <p class="text-gray-300">##</p>

    <p class="text-gray-300">6 Acknowledgements and disclosures</p>

    <h4 id="sec-65" class="text-lg font-semibold mt-6">Acknowledgements.</h4>

    <p class="text-gray-300">We are grateful to Luís Fernando Schultz Xavier da Silveira for optimizations to an earlier version of Lasso. We would also like to thank Luís, Arasu Arun, Patrick Towa for insightful comments and conversations. Justin Thaler was supported in part by NSF CAREER award CCF-1845125 and by DARPA under Agreement No. HR00112020022. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the United States Government or DARPA.</p>

    <h4 id="sec-66" class="text-lg font-semibold mt-6">Disclosures.</h4>

    <p class="text-gray-300">Thaler is a Research Partner at a16z crypto and is an investor in various blockchain-based platforms, as well as in the crypto ecosystem more broadly (for general a16z disclosures, see https://www.a16z.com/disclosures/.)</p>

    <p class="text-gray-300">##</p>

    <p class="text-gray-300">References</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[AB09] Sanjeev Arora and Boaz Barak. Computational complexity: a modern approach. Cambridge University Press, 2009.</li>

      <li>[BBB^{+}18] Benedikt Bünz, Jonathan Bootle, Dan Boneh, Andrew Poelstra, Pieter Wuille, and Greg Maxwell. Bulletproofs: Short proofs for confidential transactions and more. In Proceedings of the IEEE Symposium on Security and Privacy (S&P), 2018.</li>

      <li>[BBHR18] Eli Ben-Sasson, Iddo Bentov, Yinon Horesh, and Michael Riabzev. Fast Reed-Solomon interactive oracle proofs of proximity. In Proceedings of the International Colloquium on Automata, Languages and Programming (ICALP), 2018.</li>

      <li>[BCC^{+}16] Jonathan Bootle, Andrea Cerulli, Pyrros Chaidos, Jens Groth, and Christophe Petit. Efficient zero-knowledge arguments for arithmetic circuits in the discrete log setting. In Proceedings of the International Conference on the Theory and Applications of Cryptographic Techniques (EUROCRYPT), 2016.</li>

      <li>[BCG^{+}18] Jonathan Bootle, Andrea Cerulli, Jens Groth, Sune Jakobsen, and Mary Maller. Arya: Nearly linear-time zero-knowledge proofs for correct program execution. In Proceedings of the International Conference on the Theory and Application of Cryptology and Information Security (ASIACRYPT), 2018.</li>

      <li>[BCHO22] Jonathan Bootle, Alessandro Chiesa, Yuncong Hu, and Michele Orru. Gemini: Elastic snarks for diverse environments. In Proceedings of the International Conference on the Theory and Applications of Cryptographic Techniques (EUROCRYPT), 2022.</li>

      <li>[BDFG20] Dan Boneh, Justin Drake, Ben Fisch, and Ariel Gabizon. Halo Infinite: Recursive zk-SNARKs from any Additive Polynomial Commitment Scheme. Cryptology ePrint Archive, Report 2020/1536, 2020.</li>

      <li>[BEG^{+}91] Manuel Blum, Will Evans, Peter Gemmell, Sampath Kannan, and Moni Naor. Checking the correctness of memories. In Proceedings of the IEEE Symposium on Foundations of Computer Science (FOCS), 1991.</li>

      <li>[BFS20] Benedikt Bünz, Ben Fisch, and Alan Szepieniec. Transparent SNARKs from DARK compilers. In Proceedings of the International Conference on the Theory and Applications of Cryptographic Techniques (EUROCRYPT), 2020.</li>

      <li>[BGH19] Sean Bowe, Jack Grigg, and Daira Hopwood. Recursive proof composition without a trusted setup. Cryptology ePrint Archive, Report 2019/1021, 2019.</li>

      <li>[BGH20] Sean Bowe, Jack Grigg, and Daira Hopwood. Halo2, 2020. URL: https://github.com/zcash/halo2.</li>

      <li>[BMM^{+}21] Benedikt Bünz, Mary Maller, Pratyush Mishra, Nirvan Tyagi, and Psi Vesely. Proofs for inner pairing products and applications. In Proceedings of the International Conference on the Theory and Application of Cryptology and Information Security (ASIACRYPT), 2021.</li>

      <li>[CBBZ23] Binyi Chen, Benedikt Bünz, Dan Boneh, and Zhenfei Zhang. HyperPlonk: Plonk with linear-time prover and high-degree custom gates. In Proceedings of the International Conference on the Theory and Applications of Cryptographic Techniques (EUROCRYPT), 2023.</li>

      <li>[CDD^{+}03] Dwaine Clarke, Srinivas Devadas, Marten Van Dijk, Blaise Gassend, G. Edward, and Suh Mit. Incremental multiset hash functions and their application to memory integrity checking. In Proceedings of the International Conference on the Theory and Application of Cryptology and Information Security (ASIACRYPT), 2003.</li>

      <li>[CHM^{+}20] Alessandro Chiesa, Yuncong Hu, Mary Maller, Pratyush Mishra, Noah Vesely, and Nicholas Ward. Marlin: Preprocessing zkSNARKs with universal and updatable SRS. In Proceedings</li>

    </ul>

    <p class="text-gray-300">of the International Conference on the Theory and Applications of Cryptographic Techniques (EUROCRYPT), 2020.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[CMT12] Graham Cormode, Michael Mitzenmacher, and Justin Thaler. Practical verified computation with streaming interactive proofs. In Proceedings of the Innovations in Theoretical Computer Science (ITCS), 2012.</li>

      <li>[CTY11] Graham Cormode, Justin Thaler, and Ke Yi. Verifying computations with streaming interactive proofs. Proc. VLDB Endow., 5(1):25–36, 2011.</li>

      <li>[DGM21] Justin Drake, Ariel Gabizon, and Izaak Meckler. Checking univariate identities in linear time, 2021. https://hackmd.io/@arielg/ryGTQXWri.</li>

      <li>[EFG22] Liam Eagen, Dario Fiore, and Ariel Gabizon. cq: Cached quotients for fast lookups. Cryptology ePrint Archive, 2022.</li>

      <li>[FS86] Amos Fiat and Adi Shamir. How to prove yourself: Practical solutions to identification and signature problems. In Proceedings of the International Cryptology Conference (CRYPTO), pages 186–194, 1986.</li>

      <li>[GK22] Ariel Gabizon and Dmitry Khovratovich. flookup: Fractional decomposition-based lookups in quasi-linear time independent of table size. Cryptology ePrint Archive, 2022.</li>

      <li>[GLS^{+}21] Alexander Golovnev, Jonathan Lee, Srinath Setty, Justin Thaler, and Riad S. Wahby. Brakedown: Linear-time and post-quantum snarks for R1CS. Cryptology ePrint Archive, 2021.</li>

      <li>[GW20a] Ariel Gabizon and Zachary Williamson. Proposal: The TurboPlonk program syntax for specifying SNARK programs, 2020.</li>

      <li>[GW20b] Ariel Gabizon and Zachary J Williamson. plookup: A simplified polynomial protocol for lookup tables. 2020.</li>

      <li>[GWC19] Ariel Gabizon, Zachary J. Williamson, and Oana Ciobotaru. PLONK: Permutations over Lagrange-bases for oecumenical noninteractive arguments of knowledge. ePrint Report 2019/953, 2019.</li>

      <li>[KST22] Abhiram Kothapalli, Srinath Setty, and Ioanna Tzialla. Nova: Recursive Zero-Knowledge Arguments from Folding Schemes. In Proceedings of the International Cryptology Conference (CRYPTO), 2022.</li>

      <li>[KZG10] Aniket Kate, Gregory M. Zaverucha, and Ian Goldberg. Constant-size commitments to polynomials and their applications. In Proceedings of the International Conference on the Theory and Application of Cryptology and Information Security (ASIACRYPT), pages 177–194, 2010.</li>

      <li>[Lee21] Jonathan Lee. Dory: Efficient, transparent arguments for generalised inner products and polynomial commitments. In Theory of Cryptography Conference, pages 1–34. Springer, 2021.</li>

      <li>[LFKN90] Carsten Lund, Lance Fortnow, Howard Karloff, and Noam Nisan. Algebraic methods for interactive proof systems. In Proceedings of the IEEE Symposium on Foundations of Computer Science (FOCS), October 1990.</li>

      <li>[Pip80] Nicholas Pippenger. On the evaluation of powers and monomials. SIAM Journal on Computing, 9(2):230–250, 1980.</li>

      <li>[PK22] Jim Posen and Assimakis A Kattis. Caulk+: Table-independent lookup arguments. Cryptology ePrint Archive, 2022.</li>

      <li>[PT12] Mihai Pǎtraşcu and Mikkel Thorup. The power of simple tabulation hashing. Journal of the ACM (JACM), 59(3):1–50, 2012.</li>

      <li>[PT13] Mihai Pǎtraşcu and Mikkel Thorup. Twisted tabulation hashing. In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 209–228, 2013.</li>

    </ul>

    <p class="text-gray-300">[RIS] RISC-V Foundation. The RISC-V instruction set manual, volume I: User-Level ISA, Document Version 20180801-draft. May 2017.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[SAGL18] Srinath Setty, Sebastian Angel, Trinabh Gupta, and Jonathan Lee. Proving the correct execution of concurrent services in zero-knowledge. In Proceedings of the USENIX Symposium on Operating Systems Design and Implementation (OSDI), October 2018.</li>

      <li>[Set20] Srinath Setty. Spartan: Efficient and general-purpose zkSNARKs without trusted setup. In Proceedings of the International Cryptology Conference (CRYPTO), 2020.</li>

      <li>[SL20] Srinath Setty and Jonathan Lee. Quarks: Quadruple-efficient transparent zkSNARKs. Cryptology ePrint Archive, Report 2020/1275, 2020.</li>

      <li>[STW23] Srinath Setty, Justin Thaler, and Riad Wahby. Customizable constraint systems for succinct arguments. Cryptology ePrint Archive, 2023.</li>

      <li>[Tha13] Justin Thaler. Time-optimal interactive proofs for circuit evaluation. In Proceedings of the International Cryptology Conference (CRYPTO), 2013.</li>

      <li>[Tha22] Justin Thaler. Proofs, arguments, and zero-knowledge. Foundations and Trends in Privacy and Security, 4(2–4):117–660, 2022.</li>

      <li>[Whi] Barry Whitehat. Lookup singularity. https://zkresear.ch/t/lookup-singularity/65/7.</li>

      <li>[WTS^{+}18] Riad S. Wahby, Ioanna Tzialla, Abhi Shelat, Justin Thaler, and Michael Walfish. Doubly-efficient zkSNARKs without trusted setup. In Proceedings of the IEEE Symposium on Security and Privacy (S&P), 2018.</li>

      <li>[XZS22] Tiancheng Xie, Yupeng Zhang, and Dawn Song. Orion: Zero knowledge proof with linear prover time. In Proceedings of the International Cryptology Conference (CRYPTO), 2022.</li>

      <li>[ZBK^{+}22] Arantxa Zapico, Vitalik Buterin, Dmitry Khovratovich, Mary Maller, Anca Nitulescu, and Mark Simkin. Caulk: Lookup arguments in sublinear time. Cryptology ePrint Archive, 2022.</li>

      <li>[ZGK^{+}22] Arantxa Zapico, Ariel Gabizon, Dmitry Khovratovich, Mary Maller, and Carla Ràfols. Baloo: Nearly optimal lookup arguments. Cryptology ePrint Archive, 2022.</li>

      <li>[ZXZS20] Jiaheng Zhang, Tiancheng Xie, Yupeng Zhang, and Dawn Song. Transparent polynomial delegation and its applications to zero knowledge proof. In Proceedings of the IEEE Symposium on Security and Privacy (S&P), 2020.</li>

    </ul>

    <p class="text-gray-300">A Obtaining an indexed lookup argument from an unindexed one</p>

    <p class="text-gray-300">Let <span class="math">T\\in\\mathbb{F}^{N}</span> be a lookup table, and let <span class="math">R</span> be such that all table elements are in <span class="math">\\{0,1,\\ldots,R-1\\}</span>, and assume that <span class="math">m\\cdot N</span> is less than the field characteristic. Replace each table element <span class="math">T[i]</span> with <span class="math">i\\cdot R+T[i]</span> to obtain a modified table <span class="math">T^{\\prime}\\in\\mathbb{F}^{N}</span>, and replace each lookup pair <span class="math">(b_{j},a_{j})</span> with the field element <span class="math">a^{\\prime}_{j}=b_{j}\\cdot R+a_{j}</span>. Apply a range check to confirm that <span class="math">a_{j}\\in\\{0,1,\\ldots,R-1\\}</span> for all lookups. Then <span class="math">T[b_{j}]=a_{j}</span> implies that <span class="math">a^{\\prime}_{j}=T^{\\prime}[b_{j}]</span>. Conversely, under the guarantee that each <span class="math">a_{j}</span> is in <span class="math">\\{0,\\ldots,R-1\\}</span>, if <span class="math">T[b_{j}]\\neq a_{j}</span> then no entry of <span class="math">T^{\\prime}</span> is equal to <span class="math">a^{\\prime}_{j}</span>. Hence, one can apply a lookup argument for unindexed lookups (Definition 1.1) to <span class="math">a^{\\prime}</span> and <span class="math">T^{\\prime}</span> to confirm that <span class="math">T[b_{j}]=a_{j}</span> for all pairs <span class="math">(b_{j},a_{j})</span> in the list of lookups.</p>

    <p class="text-gray-300">If <span class="math">R</span> is chosen to be the smallest power of 2 bounding all the table elements, then the range check can be implemented via a lookup into the (MLE-structured and decomposable) table <span class="math">\\{0,1,\\ldots,R-1\\}</span>, and moreover <span class="math">T^{\\prime}</span> is decomposable or MLE-structured if and only if <span class="math">t</span> is decomposable or MLE-structured. The range check on <span class="math">a_{j}</span> can be omitted if the value <span class="math">a_{j}</span> is provided by a party that is guaranteed to be honest.</p>

    <h2 id="sec-67" class="text-2xl font-bold">Appendix B Comparison of Lasso’s costs to prior lookup arguments</h2>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Figure 2 below compares the costs of Lasso to prior lookup arguments. We clarify that several prior lookup arguments refer to the cost of a general <span class="math">m</span>-sized multiexponentiation as linear in <span class="math">m</span> <em>[ZGK^{+}22, x11]</em>. However, as discussed in Section 1.2, the fastest known multiexponentiation algorithm, due to Pippenger <em>[x23]</em>, requires a number of group operations that is (slightly) superlinear in <span class="math">m</span>, namely <span class="math">O\\left(m\\lambda/\\log(\\lambda m)\\right)</span>, where $\\lambda=\\Theta(\\log</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{G}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math"> is the security parameter and </span>\\mathbb{G}<span class="math"> is the group in which the multiexponentiation is occurring. Here, </span>\\lambda<span class="math"> must be considered superlogarithmic in </span>m<span class="math">, to ensure that adversaries running in time </span>2^{\\lambda}<span class="math"> are superpolynomial time. Similarly, prior works <em>[ZGK^{+}22, x10]</em> refer to group exponentiations as group operations, when in fact they require up to </span>O(\\log</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{G}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)$ many group operations.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h2 id="sec-68" class="text-2xl font-bold">Appendix C Details on polynomial commitment schemes</h2>

    <p class="text-gray-300">In this section, we give more information about the properties and cost profiles of the polynomial commitment schemes in Figure 2. Below, let <span class="math">n=2^{\\ell}</span>, and assume that the prover knows all <span class="math">n</span> evaluations of <span class="math">q</span> over domain <span class="math">\\{0,1\\}^{\\ell}</span>, i.e., knows <span class="math">\\{(x,q(x))\\colon x\\in\\{0,1\\}^{\\ell}\\}</span>.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>KZG + Gemini <em>[x4]</em> transforms the KZG scheme <em>[x21]</em>, which is designed for univariate polynomials, to provide a polynomial commitment scheme for multilinear polynomials. To commit to <span class="math">q</span>, the prover performs a multiexponentiation each of size <span class="math">n</span>. The commitment size is <span class="math">O(1)</span> group elements. Evaluation proofs are also <span class="math">O(\\log n)</span> group elements. To compute the evaluation proof, the prover performs <span class="math">O(n)</span> field operations and a multiexponentiation of size <span class="math">n</span>. Verifying the evaluation proof requires <span class="math">\\log n</span> group operations and a few pairings.</li>

      <li>Hyrax <em>[WTS^{+}18]</em> is based on the hardness of the discrete logarithm problem. To commit to <span class="math">q</span>, the prover performs <span class="math">\\sqrt{n}</span> multiexponentiations each of size <span class="math">\\sqrt{n}</span>. The commitment size is <span class="math">\\sqrt{n}</span> group elements. Evaluation proofs are also <span class="math">\\sqrt{n}</span> group elements. To compute the evaluation proof, the prover performs <span class="math">O(n)</span> field operations and a multiexponentiation of size <span class="math">\\sqrt{n}</span>. Verifying the evaluation proof requires a multiexponentiation of size <span class="math">\\sqrt{n}</span>.</li>

      <li>Dory <em>[x16]</em> requires pairing-friendly groups and is based on the SXDH assumption. Its primary benefit over Bulletproofs is that verifying evaluation proofs can be done with just logarithmically many group operations. In addition, <em>computing</em> an evaluation proof requires <span class="math">O(n)</span> field operations and roughly <span class="math">O(\\sqrt{n})</span> cryptographic work. Dory does use a transparent pre-processing phase for the verifier that requires <span class="math">O(\\sqrt{n})</span> cryptographic work.</li>

      <li>In Brakedown, Orion, and Orion+ <em>[GLS^{+}21, x29, x12]</em>, evaluation proofs are computed with <span class="math">O(n)</span> field operations and cryptographic hash evaluations. Commitments are just a single hash value. However, Brakedown proof sizes include <span class="math">O(\\sqrt{\\lambda n})</span> field elements, where <span class="math">\\lambda</span> is the security parameter. The verifier performs <span class="math">O(\\sqrt{\\lambda n})</span> field operations and also hashes this many field elements. Brakedown</li>

    </ul>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Scheme</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Proof size</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Prover work group, field</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Verifier work</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Plookup [GW20b]</td>

            <td class="px-3 py-2 border-b border-gray-700">5G1,9F</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N), O(N log N)</td>

            <td class="px-3 py-2 border-b border-gray-700">2P</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Halo2 [BGH20]</td>

            <td class="px-3 py-2 border-b border-gray-700">6G1,5F</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N), O(N log N)</td>

            <td class="px-3 py-2 border-b border-gray-700">2P</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Caulk [ZBK+22]</td>

            <td class="px-3 py-2 border-b border-gray-700">14G1,1G2,4F</td>

            <td class="px-3 py-2 border-b border-gray-700">15m, O(m2+m log(N))</td>

            <td class="px-3 py-2 border-b border-gray-700">4P</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Caulk+ [PK22]</td>

            <td class="px-3 py-2 border-b border-gray-700">7G1,1G2,2F</td>

            <td class="px-3 py-2 border-b border-gray-700">8m, O(m2)</td>

            <td class="px-3 py-2 border-b border-gray-700">3P</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Flookup [GK22]</td>

            <td class="px-3 py-2 border-b border-gray-700">7G1,1G2,4F</td>

            <td class="px-3 py-2 border-b border-gray-700">O(m), O(m log2m)</td>

            <td class="px-3 py-2 border-b border-gray-700">3P</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Baloo [ZGK+22]</td>

            <td class="px-3 py-2 border-b border-gray-700">12G1,1G2,4F</td>

            <td class="px-3 py-2 border-b border-gray-700">14m, O(m log2m)</td>

            <td class="px-3 py-2 border-b border-gray-700">5P</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">cq [EFG22]</td>

            <td class="px-3 py-2 border-b border-gray-700">8G1,3F</td>

            <td class="px-3 py-2 border-b border-gray-700">7m + o(m), O(m log m)</td>

            <td class="px-3 py-2 border-b border-gray-700">5P</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Lasso w/ Dory (SOS table)</td>

            <td class="px-3 py-2 border-b border-gray-700">O(log(m)) G_T</td>

            <td class="px-3 py-2 border-b border-gray-700">o(cm + cN1/c), O(cm)</td>

            <td class="px-3 py-2 border-b border-gray-700">O(log(m)) G_T</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(log(m)) F</td>

            <td class="px-3 py-2 border-b border-gray-700">O(√m) P</td>

            <td class="px-3 py-2 border-b border-gray-700">O(log(m)) F</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Lasso w/ Dory (unstructured table)</td>

            <td class="px-3 py-2 border-b border-gray-700">O(log m) G_T</td>

            <td class="px-3 py-2 border-b border-gray-700">min{2m + O(√N), m + o(N)}, O(m + N)</td>

            <td class="px-3 py-2 border-b border-gray-700">O(log m) G_T</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(log(m)) F</td>

            <td class="px-3 py-2 border-b border-gray-700">O(√N) P</td>

            <td class="px-3 py-2 border-b border-gray-700">O(log(m)) F</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Lasso w/ Sona (SOS table)</td>

            <td class="px-3 py-2 border-b border-gray-700">O(log(m)) F</td>

            <td class="px-3 py-2 border-b border-gray-700">o(cm + cN1/c), O(cm)</td>

            <td class="px-3 py-2 border-b border-gray-700">O(log(m)) F</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(1) G</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(1) G</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Lasso w/ Sona (unstructured table)</td>

            <td class="px-3 py-2 border-b border-gray-700">O(log(m)) F</td>

            <td class="px-3 py-2 border-b border-gray-700">min{2m + O(√N), N}, O(m + N)</td>

            <td class="px-3 py-2 border-b border-gray-700">O(log(m)) F</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(1) G</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(1) G</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Lasso w/ KZG+Gemini (SOS table)</td>

            <td class="px-3 py-2 border-b border-gray-700">O(log m) G1</td>

            <td class="px-3 py-2 border-b border-gray-700">(c+1)m + cN1/c, O(m)</td>

            <td class="px-3 py-2 border-b border-gray-700">O(log(m)) F</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(log(m)) F</td>

            <td class="px-3 py-2 border-b border-gray-700">O(log m) G1</td>

            <td class="px-3 py-2 border-b border-gray-700">2P</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Lasso w/ KZG+Gemini (unstructured table)</td>

            <td class="px-3 py-2 border-b border-gray-700">O(log m) G1</td>

            <td class="px-3 py-2 border-b border-gray-700">(c+1)m + cN1/c, O(m + N)</td>

            <td class="px-3 py-2 border-b border-gray-700">O(log(m)) F</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">O(log(m)) F</td>

            <td class="px-3 py-2 border-b border-gray-700">O(log m) G1</td>

            <td class="px-3 py-2 border-b border-gray-700">2P</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Figure 7: Dominant costs of prior lookup arguments vs. our work. Sona is the polynomial commitment scheme proposed in this work (Section 1.5). Other cost profiles for our schemes are possible by using other polynomial commitments. Notation:  <span class="math">m</span>  is the number of lookups,  <span class="math">N</span>  is the size of the lookup table. We assume  <span class="math">N \\geq m</span>  for simplicity. For verification costs only, we assume that  <span class="math">m \\leq \\mathrm{poly}(N)</span> , so that  <span class="math">\\log m = \\Theta(\\log N)</span> . The notation  <span class="math">\\tilde{O}(\\log m)</span>  notation hides a factor of  <span class="math">\\log \\log m</span> . Throughout,  <span class="math">m</span>  "group work" for the prover refers to a multiexponentiation of size  <span class="math">m</span> , while "  <span class="math">m</span>  exps" refers to  <span class="math">m</span>  group exponentiations ( <span class="math">m</span>  multiexponentiations are subject to a Pippinger speedup of a factor of roughly  <span class="math">O(\\log(m\\lambda))</span>  that  <span class="math">m</span>  exponentiations are not). Operations involving  <span class="math">O(m)</span>  group operations (not exponentiations) are denoted via "  <span class="math">o(m)</span>  group work" to clarify that they are cheaper than a general  <span class="math">m</span> -sized multiexponentiation. SOS tables refer to those to which Lasso applies.  <span class="math">\\mathbb{F}</span>  refers to field operations, and  <span class="math">\\mathbb{G}_1, \\mathbb{G}_2, \\mathbb{G}_T</span>  to relevant group elements or operations in a pairing-friendly group.  <span class="math">P</span>  refers to pairing operations. KZG + Gemini refers to a polynomial commitment scheme for multilinear polynomials given in [BCHO22] obtained by transforming the KZG scheme for univariate polynomials. Finally,  <span class="math">c</span>  denotes an arbitrary positive integer. Plookup and Halo2 are agnostic to the choice of polynomial commitment scheme, and that the reported costs and transparency properties in these rows refer to the case of using KZG commitments.</p>

    <p class="text-gray-300">is also field-agnostic—it applies to polynomials defined over any sufficiently large field  <span class="math">\\mathbb{F}</span> . Orion reduces verification costs to polylogarithmic via SNARK composition, but is not field-agnostic. Neither Brakedown nor Orion are homomorphic, but they are plausibly post-quantum secure. Orion+ [CBBZ23] reduces the proof size further to logarithmic (concretely under 10 KBs) but gives up transparency and post-quantum security in addition to field-agnosticism.</p>

    <p class="text-gray-300">This section describes a sparse polynomial commitment scheme that is suboptimal by a logarithmic factor. We include this merely for illustration, because this suboptimal scheme is substantially simpler than Spark (§4).</p>

    <p class="text-gray-300">Notation. For the remainder of this section, let  <span class="math">\\widetilde{f}</span>  denote an  <span class="math">\\ell</span> -variate multilinear polynomial to be committed, with sparsity  <span class="math">m</span>  in the Lagrange basis. Let  <span class="math">f\\colon \\{0,1\\}^{\\ell}\\to \\mathbb{F}</span>  denote the function with domain equal to the Boolean hypercube that  <span class="math">\\widetilde{f}</span>  extends.</p>

    <p class="text-gray-300">In this work, we only apply a sparse polynomial commitment scheme in a setting where (if the prover is honest)</p>

    <div class="my-4 text-center"><span class="math-block">f (x) \\in \\{0, 1 \\} \\text {f o r a l l} x \\in \\{0, 1 \\} ^ {\\ell}. \\tag {18}</span></div>

    <p class="text-gray-300">We describe a commitment scheme that applies to multilinear extensions of functions of this form. This slightly simplifies the description of the scheme, and makes the bound on the prover runtime to compute the commitment slightly cleaner.<span class="math">^{19}</span></p>

    <p class="text-gray-300">Let <span class="math">v_{f}\\in\\mathbb{F}^{m\\ell}</span> be the following “densified” description of <span class="math">f</span>. Break <span class="math">v_{f}</span> into <span class="math">m</span> blocks each of length <span class="math">\\ell</span>. Impose an arbitrary order on the set <span class="math">S\\coloneqq\\{x\\in\\{0,1\\}^{\\ell}\\colon f(x)\\neq 0\\}</span>, and let <span class="math">x^{(i)}</span> denote the <span class="math">i</span>th element in <span class="math">S</span>. Assign the <span class="math">i</span>th block of <span class="math">v</span> to be <span class="math">x^{(i)}\\in\\{0,1\\}^{\\ell}</span>. In other words, <span class="math">v_{f}</span> simply lists each <span class="math">x</span> such that <span class="math">f(x)\\neq 0</span>.</p>

    <h4 id="sec-70" class="text-lg font-semibold mt-6">The commit phase.</h4>

    <p class="text-gray-300">To commit to a sparse polynomial <span class="math">\\widetilde{f}</span>, we apply any desired dense polynomial commitment scheme to commit to the multilinear extension <span class="math">\\widetilde{v}_{f}</span> of the vector <span class="math">v_{f}</span>.</p>

    <h4 id="sec-71" class="text-lg font-semibold mt-6">Evaluation proofs.</h4>

    <p class="text-gray-300">As a warm-up, we begin with a conceptually simple high-level sketch of an evaluation proof procedure. We then specify full details of a more direct protocol with similar costs. The direct evaluation proof procedure involves a single application of the sum-check protocol.</p>

    <h4 id="sec-72" class="text-lg font-semibold mt-6">Warm-up: a conceptually simple procedure (sketch).</h4>

    <p class="text-gray-300">To reveal an evaluation <span class="math">\\widetilde{f}(r)</span> for <span class="math">r\\in\\mathbb{F}^{\\ell}</span>, we apply any sum-check-based SNARK (e.g., Spartan, Brakedown, Orion, Libra, etc.) to the natural arithmetic circuit of size <span class="math">O(m\\cdot\\ell)</span> that takes as input the densified description <span class="math">v_{f}</span> of <span class="math">f</span> and outputs <span class="math">\\widetilde{f}(r)</span>. This circuit has a “uniform” wiring pattern that ensures that the verifier in any of these SNARKs will run in polylogarithmic time when applied to this circuit (without any pre-processing), plus the time to check a single evaluation proof from the dense polynomial commitment scheme applied to <span class="math">\\widetilde{v}_{f}</span>.</p>

    <h4 id="sec-73" class="text-lg font-semibold mt-6">Complete description of an evaluation proof procedure via direct application of sum-check.</h4>

    <p class="text-gray-300">Let us assume that <span class="math">m</span> and <span class="math">\\ell</span> are both powers of <span class="math">2</span>. If the prover is honest, then</p>

    <p class="text-gray-300"><span class="math">\\widetilde{f}(r)=\\sum_{k\\in\\{0,1\\}^{\\log m}}\\prod_{j\\in\\{0,1\\}^{\\log(\\ell)}}\\left(\\widetilde{v}_{f}(k,j)r_{j}+(1-\\widetilde{v}_{f}(k,j))(1-r_{j})\\right).</span> (19)</p>

    <p class="text-gray-300">To compute Equation (19), we can apply the sum-check protocol to the polynomial <span class="math">g</span> defined below:</p>

    <p class="text-gray-300"><span class="math">g(k)=\\prod_{j\\in\\{0,1\\}^{\\log(\\ell)}}\\left(\\widetilde{v}_{f}(k,j)r_{j}+(1-\\widetilde{v}_{f}(k,j))\\,(1-r_{j})\\right).</span></p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Observe that <span class="math">g</span> has <span class="math">\\log m</span> variables and degree <span class="math">\\ell</span> in each of them, so the proof length of the sum-check protocol applied to <span class="math">g</span> is <span class="math">O(\\ell\\cdot\\log m)</span> field elements. At the end of the sum-check protocol, the verifier has to evaluate <span class="math">g</span> at a random point <span class="math">r^{\\prime}\\in\\mathbb{F}^{\\log m}</span>. This can be done in <span class="math">O(\\ell)</span> time given <span class="math">\\ell</span> evaluations of <span class="math">\\widetilde{v}_{f}</span>, namely <span class="math">\\widetilde{v}_{f}(r^{\\prime},j)</span> for each <span class="math">j\\in\\{0,1\\}^{\\log(\\ell)}</span>. Standard techniques can efficiently reduce these <span class="math">\\ell</span> evaluations of <span class="math">\\widetilde{v}_{f}</span> to a <em>single</em> evaluation of <span class="math">\\widetilde{v}_{f}</span>. Specifically, the prover is asked to send the entire <span class="math">(\\log(\\ell))</span>-variate polynomial <span class="math">h(y)=\\widetilde{v}_{f}(r^{\\prime},y)</span>, i.e., the polynomial obtained from <span class="math">\\widetilde{v}_{f}</span> by fixing the first <span class="math">\\log m</span> variables to <span class="math">r^{\\prime}</span>. This costs only <span class="math">\\ell+1</span> field elements in communication. The verifier picks a random point <span class="math">r^{\\prime\\prime}</span> and confirms that <span class="math">h(r^{\\prime\\prime})=\\widetilde{v}_{f}(r^{\\prime},r^{\\prime\\prime})</span> with a single evaluation query to the committed polynomial <span class="math">\\widetilde{v}_{f}</span>. By the Schwartz-Zippel lemma, if <span class="math">h(y)\\neq\\widetilde{v}_{f}(r^{\\prime},y)</span>, then with probability at least $1-\\log(1+\\ell)/</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$, the verifier’s check will fail.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h6 id="sec-74" class="text-base font-medium mt-4">Theorem 4.</h6>

    <p class="text-gray-300">The above protocol is an extractable polynomial commitment scheme for multilinear polynomials.</p>

    <h6 id="sec-75" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">Suppose that <span class="math">\\mathcal{P}</span> is a prover that, with non-negligible probability, produces evaluation proofs that pass verification. By extractability of the dense polynomial commitment scheme used to commit to <span class="math">\\widetilde{v}_{f}</span>, there is a polynomial time algorithm <span class="math">\\mathcal{E}</span> that produces a multilinear polynomial polynomial <span class="math">p</span> that explains all of <span class="math">\\mathcal{P}</span>’s evaluation proofs, in the following sense. If <span class="math">\\mathcal{P}</span> is able to, with non-negligible probability, produce an evaluation</p>

    <p class="text-gray-300">proof for the claim that the committed polynomial polynomial’s evaluation at any input <span class="math">(r^{\\prime},r^{\\prime\\prime})\\in\\mathbb{F}^{\\log m}\\times\\mathbb{F}^{\\ell}</span> equals value <span class="math">v\\in\\mathbb{F}</span>, then <span class="math">p(r^{\\prime},r^{\\prime\\prime})=v</span>.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">By soundness of the sum-check protocol, if the prover passes the verifier’s checks with probability more than $O\\left((\\log m+\\log(1+\\ell))/</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\right)<span class="math"> then </span>v$ equals</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><span class="math">\\sum_{k\\in\\{0,1\\}^{\\log m}}\\prod_{j\\in\\{0,1\\}^{\\log\\ell}}(\\widetilde{v}_{f}(k,j)r_{j}+(1-\\widetilde{v}_{f}(k,j))(1-r_{j}))\\,.</span></p>

    <p class="text-gray-300">This is a multilinear polynomial in <span class="math">(r^{\\prime},r^{\\prime\\prime})</span>.</p>

    <p class="text-gray-300">∎</p>

    <h4 id="sec-76" class="text-lg font-semibold mt-6">Costs of the sparse polynomial commitment scheme.</h4>

    <p class="text-gray-300">There are two sources of costs in the sparse polynomial commitment scheme above.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>One is applying the dense polynomial commitment scheme to commit to the multilinear extension <span class="math">\\widetilde{v}</span> of a vector <span class="math">v\\in\\{0,1\\}^{m\\ell}</span>, and later produce a single evaluation proof for <span class="math">\\widetilde{v}(r^{\\prime},r^{\\prime\\prime})</span> via this commitment scheme.</li>

    </ul>

    <h4 id="sec-77" class="text-lg font-semibold mt-6">Prover costs.</h4>

    <p class="text-gray-300">If the dense polynomial commitment scheme used is Hyrax <em>[WTS^{+}18]</em>, Dory <em>[x14]</em>, or BMMTV <em>[BMM^{+}21]</em>, the commitment can be computer with only <span class="math">O(m\\ell)</span> group operations. Here, we exploit that the entries of <span class="math">v</span> are al in <span class="math">\\{0,1\\}</span>). Dory and BMMTV require pairing-friendly groups and also require the prover to compute a multi-pairing of length-<span class="math">O(\\sqrt{m\\ell})</span>.</p>

    <p class="text-gray-300">Evaluation proofs for all three commitment schemes require <span class="math">O(m\\log n)</span> field operations and roughly <span class="math">O(\\sqrt{m\\ell})</span> cryptographic work.</p>

    <h4 id="sec-78" class="text-lg font-semibold mt-6">Verifier costs.</h4>

    <p class="text-gray-300">Hyrax’s proofs consist of <span class="math">O(\\sqrt{m\\ell})</span> group elements, and the verifier must perform a multiexponentiation of size <span class="math">O(\\sqrt{m\\ell})</span>. Dory and BMMTV proofs consist of <span class="math">O(\\log(m\\ell))</span> elements of the target group <span class="math">\\mathbb{G}_{t}</span>, and the verifier performs <span class="math">O(\\log(m\\ell))</span> exponentiations/scalar-multiplications in <span class="math">\\mathbb{G}_{t}</span>.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>The other is the costs of the sum-check protocol, and the final step in reducing <span class="math">1+\\ell</span> evaluations of <span class="math">\\widetilde{v}_{f}</span> to a single evaluation. The verification costs of these two protocols is <span class="math">O((\\log m)\\cdot\\log\\ell)</span> field operations. Meanwhile, via standard techniques, the prover can be implemented with <span class="math">O(m\\ell)</span> field operations in total across all rounds of the protocol.</li>

    </ul>

    <h2 id="sec-79" class="text-2xl font-bold">Appendix E Additional details on the grand product argument</h2>

    <p class="text-gray-300">For completeness of exposition, we provide additional details on the grand product argument we use (Lines 6 and 10 of Figure 3) and its application in our context. Note that these details are identical to prior works <em>[x23, x24, GLS^{+}21]</em>.</p>

    <p class="text-gray-300">Thaler’s grand product argument <em>[x25, Proposition 2]</em> is simply an optimized application of the GKR interactive proof for circuit evaluation to a circuit computing a binary tree of multiplication gates. The prover in the interactive proof does a number of field operations that is linear in the circuit size, which in the application of the grand product argument in our lookup argument is <span class="math">O(m)</span>. The verifier in the GKR protocol has to evaluate the MLE of the input vector to the circuit at a randomly chosen point <span class="math">r</span>. In our applications of the grand product argument, the input to the circuit is either:</p>

    <p class="text-gray-300"><span class="math">\\{a\\cdot\\gamma^{2}+v\\cdot\\gamma+t-\\tau\\colon(a,v,t)\\in\\mathsf{WS}\\},</span> <span class="math">\\{a\\cdot\\gamma^{2}+v\\cdot\\gamma+t-\\tau\\colon(a,v,t)\\in\\mathsf{RS}\\}\\cup\\{a\\cdot\\gamma^{2}+v\\cdot\\gamma+t-\\tau\\colon(a,v,t)\\in S\\},</span> <span class="math">\\{a\\cdot\\gamma^{2}+v\\cdot\\gamma+t-\\tau\\colon(a,v,t)\\in\\mathsf{WS}^{\\prime}\\},</span></p>

    <p class="text-gray-300">or</p>

    <p class="text-gray-300"><span class="math">\\{a\\cdot\\gamma^{2}+v\\cdot\\gamma+t-\\tau\\colon(a,v,t)\\in\\mathsf{RS}^{\\prime}\\}\\cup\\{a\\cdot\\gamma^{2}+v\\cdot\\gamma+t-\\tau\\colon(a,v,t)\\in S^{\\prime}\\}.</span></p>

    <p class="text-gray-300">For simplicity of notation, let us assume that <span class="math">N^{1/c}=m</span>, and let <span class="math">k=(k_{1},\\ldots,k_{\\log m})</span> be variables, and let us focus for illustration on the second case above. In this second case above, the multilinear extension of the input to the circuit is</p>

    <p class="text-gray-300"><span class="math">g(k_{0},k_{1},\\ldots,k_{\\log m})=k_{0}\\cdot\\left(\\gamma^{2}\\mathsf{row}(k)+\\gamma E_{\\mathsf{rx}}(k)+\\mathsf{read\\_ts}_{\\mathsf{row}}(k)\\right)+</span> <span class="math">(1-k_{0})\\cdot\\left(\\gamma^{2}\\left(\\sum_{i=1}^{\\log N^{1/c}}2^{i-1}\\cdot k_{i}\\right)+\\gamma\\cdot\\widetilde{\\mathsf{eq}}\\left(k,r_{x}\\right)+\\mathsf{final\\_cts}_{\\mathsf{row}}(k)\\right)-\\tau.</span></p>

    <p class="text-gray-300">Indeed, by the definition of <span class="math">\\mathsf{RS}</span> and <span class="math">S</span> in Claim 2, the expression above is multilinear and agrees with the input to the circuit whenever <span class="math">(k_{0},\\ldots,k_{\\log m})\\in\\{0,1\\}^{\\log m}</span>. Here, <span class="math">k_{0}</span> acts a selector bit—when <span class="math">k_{0}=1</span> (respectively, <span class="math">k_{0}=0</span>), it indicates that <span class="math">(k_{1},\\ldots,k_{\\log m}</span> index into the set <span class="math">\\mathsf{RS}^{\\prime}</span> (respectively, <span class="math">S^{\\prime}</span>). Hence, it must equal the unique multilinear extension of the input. The expression above can be evaluated at any point <span class="math">(k_{0},\\ldots,k_{\\log m})\\in\\mathbb{F}^{1+\\log m}</span> in logarithmic time by the verifier, with one evaluation query to each of <span class="math">\\mathsf{row}</span>, <span class="math">E_{\\mathsf{rx}}</span>, <span class="math">\\mathsf{read\\_ts}_{\\mathsf{row}}</span> and <span class="math">\\mathsf{final\\_cts}_{\\mathsf{row}}</span>. A similar expression holds in the other three cases above.</p>

    <p class="text-gray-300">We propose to use Setty and Lee’s grand product argument <em>[x23, Section 6]</em>, which reduces the proof size of Thaler’s to <span class="math">O(\\log(m)\\cdot\\log\\log m)</span> at the cost of committing to an additional, say, <span class="math">m/\\log^{3}(m)</span>, field elements. The rough idea is that the prover cryptographically commits to the values of the gates at all layers of circuit (the binary-tree of multiplication gates), except for the <span class="math">O(\\log\\log m)</span> layers closest to the inputs. While the committed gates account for <em>most of the layers</em> of the circuit, they account for a tiny fraction of the <em>gates</em> in the circuit, as there are only <span class="math">m/\\log^{3}m</span> gates at these layers. This commitment enables the prover to apply a Spartan-like SNARK to the committed layers, resulting in just logarithmic communication cost (whereas Thaler’s interactive proof applied to those layers would have communication cost <span class="math">O(\\log^{2}n)</span>).</p>

    <p class="text-gray-300">Then Thaler’s protocol is used to handle the <span class="math">O(\\log\\log m)</span> layers that were not committed. The total communication cost of applying Thaler’s protocol just to these layers is <span class="math">O(\\log(m)\\cdot\\log\\log m)</span>.</p>

    <h2 id="sec-80" class="text-2xl font-bold">Appendix F GeneralizedLasso: Beyond decomposable tables</h2>

    <p class="text-gray-300">This section describes how GeneralizedLasso checks Equation (5) to provide a lookup argument. We first provide an overview of the main technical component in GeneralizedLasso that is not present in Lasso.</p>

    <h3 id="sec-81" class="text-xl font-semibold mt-8">F.1 Sparse-dense sum-check protocol</h3>

    <p class="text-gray-300">Equation (5) can be computed with the sum-check protocol of Lund, Fortnow, Karloff, and Nisan <em>[x12]</em>, so long as the verifier can evaluate each of the polynomials in the equation at a random point. The key question for this technical overview is: how fast can the prover be implemented in this application of the sum-check protocol?</p>

    <p class="text-gray-300">The challenge is that this protocol is computing the inner product between two vectors in <span class="math">u,t\\in\\mathbb{F}^{N}</span>, and we are unsatisfied with a prover time of <span class="math">O(N)</span> field operations. Here, entries of <span class="math">u</span> are indexed by vectors <span class="math">y\\in\\{0,1\\}^{\\log N}</span> and the <span class="math">y</span>’th entry of <span class="math">u</span> equals <span class="math">\\widetilde{M}(r,y)</span>. Fortunately, we are <em>guaranteed</em> that at most <span class="math">m</span> entries of <span class="math">u</span> are non-zero. We leverage this guarantee to show how to implement the prover in this sum-check protocol with only <span class="math">O(cm)</span> field operations where <span class="math">c</span> is such that <span class="math">N=M^{c}</span>, so long as <span class="math">t</span> is structured.</p>

    <h4 id="sec-82" class="text-lg font-semibold mt-6">Brief overview of existing linear-time sum-check provers.</h4>

    <p class="text-gray-300">In each round <span class="math">j</span> of the sum-check protocol, the <span class="math">j</span>th input to the polynomials <span class="math">\\widetilde{u}(y_{1},\\ldots,y_{\\log N})</span> and <span class="math">\\widetilde{t}(y_{1},\\ldots,y_{\\log N})</span> gets “bound” to a random field element <span class="math">r_{j}</span> of the verifier’s choosing. Existing linear-time sum-check protocols <em>[x7, x25]</em> applied to compute <span class="math">\\langle u,t\\rangle</span> achieve a prover that runs in <span class="math">O(N)</span> time by treating two or more entries of <span class="math">u</span> (and of <span class="math">t</span>) as a single entity once all their “bit-differences” are bound. That is, if <span class="math">y,y^{\\prime}\\in\\{0,1\\}^{\\log N}</span> agree on their last <span class="math">\\ell</span> entries, then existing prover implementations treat <span class="math">u_{y}</span> and <span class="math">u_{y^{\\prime}}</span> as a “single entity” for the final <span class="math">\\ell</span> rounds of the protocol. This ensures that in each round <span class="math">j</span>, the prover only needs to process <span class="math">N/2^{j}</span> entries, yielding total runtime of <span class="math">O\\left(\\sum_{j=1}^{\\log N}N/2^{j}\\right)=O(N)</span>.</p>

    <p class="text-gray-300">###</p>

    <p class="text-gray-300">Earlier techniques <em>[x10]</em> can reduce the prover time instead to <span class="math">O(m\\cdot\\text{polylog}N)</span>. However, achieving <span class="math">O(m)</span> prover time is substantially more challenging.</p>

    <h5 id="sec-83" class="text-base font-semibold mt-4">Brief overview of our sparse-dense sum-check prover.</h5>

    <p class="text-gray-300">We reduce the prover time to <span class="math">O(cm)</span> as follows. Whereas prior works on the linear-time sum-check provers treat two indices <span class="math">y,y^{\\prime}\\in\\{0,1\\}^{\\log N}</span> of <span class="math">u</span> as a single entity <em>after their last</em> “bit-difference” gets bound, our key idea (and fundamentally new technique) is to give a way treat any two indices <span class="math">y,y^{\\prime}</span> as a single entity <em>until their first</em> bit-difference gets bound. For example, in round 1, the indices are split into just two entities: those with high-order bit equal to 0 and those with high-order bit equal to 1. In round 2, they are split into four entities, based on their highest-order two bits. And so forth.</p>

    <p class="text-gray-300">This observation lets the prover handle each round <span class="math">j=1,\\ldots,\\log m</span> in time <span class="math">O(2^{j})</span>. <span class="math">\\mathcal{P}</span> begins to run into a problem once the protocol passes round <span class="math">\\log m</span>, since then <span class="math">O(2^{j})</span> time starts to become larger than the <span class="math">O(m)</span> time bound we wish to satisfy. We think of this phenomenon, of the number of “relevant entities” to be tracked by the prover potentially doubling in each round, as <em>expansion</em>.</p>

    <p class="text-gray-300">The idea then is to apply the techniques from the existing linear-time sum-check protocols, spending <span class="math">O(m)</span> time to “make the first <span class="math">\\log m</span> bits of the index of each non-zero entry <span class="math">u_{y}</span> of <span class="math">u</span> no longer relevant to the protocol”, by updating the entries of <span class="math">u</span> to “incorporate” the binding of the first <span class="math">\\log m</span> variables of <span class="math">\\widetilde{u}</span> to the values <span class="math">r_{1},\\ldots,r_{\\log m}</span>. We call this procedure <em>consolidation</em>, as it reduces the number of “relevant entities” tracked by the prover from <span class="math">m</span> down to 1.</p>

    <p class="text-gray-300">The above procedure can be repeated every <span class="math">\\log m</span> rounds. That is, for each contiguous “chunk” of input variables to <span class="math">\\widetilde{u}</span> of size <span class="math">\\log m</span>, <span class="math">\\mathcal{P}</span> spends <span class="math">O(m)</span> field work processing the rounds that bind variables in that chunk. This is <span class="math">O(c\\cdot m)</span> field work total if <span class="math">N=m^{c}</span>, i.e., if there are <span class="math">\\log N=c\\cdot\\log m</span> variables.</p>

    <p class="text-gray-300">In the above procedure, there’s a tension whereby the more rounds that go by without consolidating, the more time <span class="math">\\mathcal{P}</span> is paying each round. But consolidating costs <span class="math">O(m)</span> work. Hence, <span class="math">\\mathcal{P}</span> does not want to consolidate too frequently. The optimal approach is to let expansion occur unchecked for <span class="math">\\log m</span> rounds at a time—this balances the cost of consolidating vs. deferring consolidation.</p>

    <h3 id="sec-84" class="text-xl font-semibold mt-8">F.2 The GeneralizedLasso protocol</h3>

    <h5 id="sec-85" class="text-base font-semibold mt-4">The polynomial IOP.</h5>

    <p class="text-gray-300">In the polynomial IOP, the prover sends <span class="math">\\widetilde{M}</span> to the verifier as the first message in the protocol. (In the succinct arguments resulting from this polynomial IOP, the prover will commit to <span class="math">\\widetilde{M}</span> using our specialized version of Spark, which additionally ensures that each row of <span class="math">M</span> is a unit vector.) Below, we check that <span class="math">M\\cdot t=a</span> using the sparse-dense sum-check protocol.</p>

    <p class="text-gray-300">Following the same approach as Surge (Section 5), define <span class="math">b=M\\cdot t</span>, and let <span class="math">\\widetilde{b}</span> denote the multilinear extension of <span class="math">b</span>. Then it is easy to see that:</p>

    <p class="text-gray-300"><span class="math">\\widetilde{b}(r)=\\sum_{j\\in\\{0,1\\}^{\\log N}}\\widetilde{M}(r,j)\\cdot\\widetilde{t}(j).</span> (20)</p>

    <p class="text-gray-300">Indeed, the RHS is a multilinear polynomial in the variables of <span class="math">r</span>, and by the definition of matrix-vector multiplication, it agrees with <span class="math">b</span> at all inputs in <span class="math">\\{0,1\\}^{\\log m}</span>. Hence, the RHS is the unique multilinear polynomial extending <span class="math">b</span>.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Accordingly, to confirm that <span class="math">M\\cdot t=a</span>, it suffices to confirm that <span class="math">\\widetilde{b}</span> and <span class="math">\\widetilde{a}</span> are the same polynomial. To do this, it suffices for the verifier to pick a random input <span class="math">r\\in\\mathbb{F}^{\\log m}</span> and confirm that <span class="math">\\widetilde{b}(r)=\\widetilde{a}(r)</span> (up to soundness error $\\log(m)/</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math">, by the Schwartz-Zippel lemma). The verifier can learn </span>\\widetilde{a}(r)<span class="math"> with one evaluation query to </span>\\widetilde{a}$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">To learn <span class="math">\\widetilde{b}(r)</span>, the verifier applies the sumcheck protocol to the <span class="math">(\\log N)</span>-variate polynomial <span class="math">g(j)\\coloneqq\\widetilde{M}(r,j)\\cdot\\widetilde{t}(j)</span>, in order to compute the right hand side of Equation (20). At the end of the sum-check protocol, the verifier needs to evaluate <span class="math">\\widetilde{M}(r,r^{\\prime})</span> and <span class="math">\\widetilde{t}(r^{\\prime})</span> for a randomly chosen point <span class="math">r^{\\prime}\\in\\mathbb{F}^{\\log N}</span>. This can be done with one evaluation query to <span class="math">\\widetilde{M}</span> and one to <span class="math">\\widetilde{t}</span>.</p>

    <p class="text-gray-300">As explained later (Appendix G), <span class="math">\\widetilde{M}(r,j)=0</span> for all but at most <span class="math">m</span> values of <span class="math">j\\in\\{0,1\\}^{\\log N}</span>. Hence, standard techniques <em>[x11]</em> suffice to implement the prover in the application of the sum-check protocol</p>

    <p class="text-gray-300">to  <span class="math">g(j) \\coloneqq \\widetilde{M}(r,j) \\cdot \\widetilde{t}(j)</span>  with a number of field operations that is quasilinear in  <span class="math">m</span> . However, we wish to lower this to  <span class="math">O(m)</span> , especially considering that we would like to apply GeneralizedLasso to (MLE-structured) tables so large that  <span class="math">\\log N</span>  is well over one hundred. We call this  <span class="math">O(m)</span> -time prover algorithm the sparse-dense sum-check protocol. We defer a detailed description of the algorithm to Section G. The consequence of this result is captured in Theorem 5.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Polynomial commitments to the multilinear polynomials  <span class="math">\\widetilde{a} \\colon \\mathbb{F}^{\\log m} \\to \\mathbb{F}</span>  and  <span class="math">\\widetilde{t} \\colon \\mathbb{F}^{\\log N} \\to \\mathbb{F}</span>  are given to the verifier as input. The commitment to  <span class="math">\\widetilde{t}</span>  is omitted if  <span class="math">\\widetilde{t}(r)</span>  can be evaluated at any point  <span class="math">r \\in \\mathbb{F}^{\\log N}</span>  in logarithmic time.</li>

      <li>The prover  <span class="math">\\mathcal{P}</span>  sends a polynomial commitment to the MLE  <span class="math">\\widetilde{M}</span>  of a matrix  <span class="math">M \\in \\{0,1\\}^{m \\times N}</span>  using our specialized version of Spartan's sparse polynomial commitment scheme.</li>

      <li>The verifier  <span class="math">\\mathcal{V}</span>  picks a random  <span class="math">r \\in \\mathbb{F}^{\\log m}</span>  and sends  <span class="math">r</span>  to  <span class="math">\\mathcal{P}</span> . The verifier makes one evaluation query to  <span class="math">\\widetilde{a}</span> , to learn  <span class="math">\\widetilde{a}(r)</span> .</li>

      <li><span class="math">\\mathcal{P}</span>  and  <span class="math">\\mathcal{V}</span>  apply the (sparse-dense) sum-check protocol to the  <span class="math">(\\log N)</span> -variate polynomial  <span class="math">g(j) \\coloneqq \\widetilde{M}(r, j) \\cdot \\widetilde{t}(j)</span> , to confirm that</li>

    </ul>

    <div class="my-4 text-center"><span class="math-block">\\widetilde {a} (r) = \\sum_ {j \\in \\{0, 1 \\} ^ {\\log N}} g (j).</span></div>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>At the end of the sum-check protocol, the verifier needs to evaluate  <span class="math">\\widetilde{M}(r, r&#x27;)</span>  and  <span class="math">\\widetilde{t}(r&#x27;)</span>  for a random point  <span class="math">r&#x27; \\in \\mathbb{F}^{\\log N}</span>  that is chosen entry-by-entry over the course of the sum-check protocol. This costs one evaluation query to  <span class="math">\\widetilde{M}</span>  and one to  <span class="math">\\widetilde{t}</span> .</li>

    </ul>

    <p class="text-gray-300">Figure 8: Description of the GeneralizedLasso lookup argument. Here,  <span class="math">a</span>  denotes the vector of lookups and  <span class="math">t</span>  the vector capturing the lookup table (Definition 1.1). Polynomial commitments to the multilinear extension polynomials  <span class="math">\\widetilde{a} \\colon \\mathbb{F}^{\\log m} \\to \\mathbb{F}</span>  and  <span class="math">\\widetilde{t} \\colon \\mathbb{F}^{\\log N} \\to \\mathbb{F}</span>  are given to the verifier as input (the commitment to  <span class="math">\\widetilde{t}</span>  can be omitted if  <span class="math">\\widetilde{t}</span>  can be evaluated at any point in logarithmic time).</p>

    <p class="text-gray-300">Theorem 5. There is a polynomial IOP that can be combined with an appropriate commitment scheme for sparse polynomials to obtain a lookup argument for  <span class="math">m</span>  lookups into a table of size  <span class="math">N</span> . The polynomial IOP requires that the characteristic of the field  <span class="math">\\mathbb{F}</span>  over which the lookup argument is defined is at least  <span class="math">\\max \\{m, N\\}</span> . The polynomial IOP has soundness error at most</p>

    <div class="my-4 text-center"><span class="math-block"></span></div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">(\\log m + 2 \\log N) /</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb {F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="my-4 text-center"><span class="math-block"></span></div>

    <p class="text-gray-300">The honest prover sends one polynomial  <span class="math">\\widetilde{M}</span> , which is the multilinear extension of a matrix in  <span class="math">\\{0,1\\}^{m\\times N}</span>  in which each row is a unit vector. The verifier queries the polynomials  <span class="math">\\widetilde{a}</span> ,  <span class="math">\\widetilde{t}</span> , and  <span class="math">\\widetilde{M}</span>  once each, to obtain the values  <span class="math">\\widetilde{a}(r)</span> ,  <span class="math">\\widetilde{t}(r&#x27;)</span> , and  <span class="math">\\widetilde{M}(r,r&#x27;)</span> . The proof length and verifier time is  <span class="math">O(\\log m + \\log N)</span>  field elements and operations respectively, plus the time to query the aforementioned polynomials. The prover time is  <span class="math">O(m)</span>  field operations if the table satisfies the properties of Theorem 9, plus the time to answer the above queries to the polynomials  <span class="math">\\widetilde{a}</span> ,  <span class="math">\\widetilde{t}</span> , and  <span class="math">\\widetilde{M}</span> .</p>

    <p class="text-gray-300">Proof. Completeness holds because, if the prover is honest, then  <span class="math">Mt = a</span> , and the verifier's checks pass with probability 1.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Soundness holds by the following reasoning. If the prover's claim is false, then  <span class="math">Mt \\neq a</span> . By the Schwartz-Zippel lemma, with probability at least  $1 - \\log(m) /</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> ,  </span>\\widetilde{b}(r) \\neq \\widetilde{a}(r)<span class="math"> . The sum-check protocol (bulletpoint four of Figure 8) forces the prover to provide  </span>\\widetilde{b}(r)<span class="math"> , and it has soundness error  </span>2\\log(N) /</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$ . By a union bound, the total soundness error is at most</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="my-4 text-center"><span class="math-block"></span></div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\frac {\\log (m) + 2 \\log (N)}{</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb {F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="my-4 text-center"><span class="math-block"></span></div>

    <p class="text-gray-300">The prover runtime assertion is immediate from Theorem 9, which is stated in proved in Section G.5.2.  <span class="math">\\square</span></p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">We remark that while the soundness error of the polynomial IOP in Figure 8 is $O(\\log(N)/</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math">, actual SNARKs derived from the polynomial IOP will use the sparse polynomial commitment scheme of Section 4 (Spark) to commit to </span>\\widetilde{M}<span class="math">. And this sparse polynomial commitment scheme is itself based on a polynomial IOP with soundness error </span>O(N/</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math">. So the resulting lookup argument will need to work over fields of size substantially larger than </span>N$ to ensure adequate soundness error.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h3 id="sec-86" class="text-xl font-semibold mt-8">F.3 Details on what is a “structured table” for sparse-dense sum-check</h3>

    <p class="text-gray-300">Recall that our <em>sparse-dense sum-check protocol</em> exploits structure in the table in two ways: to implement the prover in the sum-check protocol in only <span class="math">O(m)</span> field operations, and to ensure that the verifier in the protocol, on its own, can quickly compute the information it needs about the table. Details follow, starting with how the verifier computes the information about the table that it needs to check the proof.</p>

    <h4 id="sec-87" class="text-lg font-semibold mt-6">F.3.1 Ensuring the verifier can quickly compute the information it needs about the table</h4>

    <p class="text-gray-300">For many natural lookup tables, the multilinear polynomial <span class="math">\\widetilde{t}</span> that “captures” the table in our protocol can be directly evaluated by the verifier at any desired point <span class="math">r\\in\\mathbb{F}^{\\log N}</span> in <span class="math">O(\\log N)</span> time. We call tables satisfying this property <em>MLE-structured</em>. In fact, often the evaluation procedure only involves finite field additions and multiplication by powers of 2, rather than general finite field multiplications.</p>

    <p class="text-gray-300">Hence, the verifier’s work is inexpensive even if <span class="math">\\widetilde{t}</span> is not cryptographically committed by the prover. In contrast, all prior lookup arguments require the prover to cryptographically commit to some polynomial “encoding” the table, which requires cryptographic costs at least linear in the table size.</p>

    <p class="text-gray-300">Our companion work, Jolt, demonstrates that the evaluation tables of essentially all primitive RISC-V instructions are MLE-structured. For illustration, we mention some specific examples below.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>All integers between 0 and <span class="math">N-1</span> (this table enables range checks).</li>

      <li>All even (or all odd) integers between 0 and <span class="math">2N</span>.</li>

      <li>All integers between 0 and <span class="math">N^{2}</span> whose natural binary representations have all even-indexed (or odd-indexed) bits set to 0. This table was used in early work on representing bitwise operations within constraint systems defined over large prime-order fields <em>[BCG^{+}18]</em> including bitwise XOR, OR, and AND.</li>

    </ul>

    <p class="text-gray-300">The key technical property that all of the above tables have in common is that the <span class="math">i</span>’th table entry is a specific linear combination of the individual bits of the binary representation of <span class="math">i</span>. Moreover, lookup tables that are the union of <span class="math">O(\\log N)</span> many tables of the form above also fall into the class. That is, for such tables, the polynomial <span class="math">\\tilde{t}</span> used in GeneralizedLasso can be evaluated by the verifier itself in <span class="math">O(\\log N)</span> time.</p>

    <p class="text-gray-300">For illustration, consider the table consisting of all finite field elements between 0 and <span class="math">N-1</span>. If we index the table entries by <span class="math">i\\in\\{0,1\\}^{\\log N}</span>, then the <span class="math">i</span>’th table element is simply the field element <span class="math">\\sum_{j=0}^{\\log(N)-1}2^{i}\\cdot i_{j}</span>. As we explain later, this means that for arbitrary finite field elements <span class="math">(r_{1},\\ldots,r_{\\log N})\\in\\mathbb{F}</span>,</p>

    <p class="text-gray-300"><span class="math">\\tilde{t}(r_{1},\\ldots,r_{\\log n})=\\sum_{j=0}^{\\log N}2^{i}\\cdot r_{j}.</span> (21)</p>

    <p class="text-gray-300">Clearly, Equation (21) can be evaluated in <span class="math">O(\\log N)</span> time, and in fact only requires multiplications-by-powers-of-two and finite field additions.</p>

    <h5 id="sec-88" class="text-base font-semibold mt-4">Two more examples.</h5>

    <p class="text-gray-300">An illustrative example that is somewhat more complicated than any of the above, is a lookup table introduced in our companion paper, Jolt, to handle bitwise operations more efficiently than prior works. For bitwise AND over <span class="math">b</span>-bit inputs <span class="math">x,y\\in\\{0,1\\}^{b}</span>, the <span class="math">(x,y)</span>’th entry of the appropriate (ordered)</p>

    <p class="text-gray-300">table is <span class="math">\\sum_{i=1}^{b} 2^{i-1} \\cdot x_i \\cdot y_i</span>, implying that</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde{t}(x, y) = \\cdot \\sum_{i=1}^{b} 2^{i-1} \\cdot x_i \\cdot y_i. \\tag{22}</span></div>

    <p class="text-gray-300">To derive the above expression for the table entries, observe that for any two bits <span class="math">a, b \\in \\{0,1\\}</span>, <span class="math">\\mathsf{AND}(a,b) = a \\cdot b</span>, and then take the appropriate weighted sum of the bitwise AND of <span class="math">x</span> and <span class="math">y</span> to transform it into the associated integer. The result is that any <span class="math">(x,y) \\in \\{0,1\\}^b \\times \\{0,1\\}^b</span> gets mapped to the field element with binary representation equal to the bitwise AND of <span class="math">x</span> and <span class="math">y</span>.</p>

    <p class="text-gray-300">Observe that the <span class="math">(x,y)</span>'th entry of the evaluation table for the bitwise AND operation is not a weighted sum of the bits of <span class="math">x</span> and <span class="math">y</span>, because the function has total degree 2. Nonetheless, the multilinear extension <span class="math">\\widetilde{t}</span> of this table can be evaluated by the verifier in logarithmic time, and the prover in the sparse-dense sum-check protocol applied to this table (Theorem 9) runs in <span class="math">O(m)</span> time.</p>

    <p class="text-gray-300">As a final, more complicated example, Jolt also considers the lookup table associated with the integer comparison instruction LT (short for "less than"). This instruction takes two 64-bit inputs <span class="math">x</span> and <span class="math">y</span>, interprets them as (say, unsigned) integers, and outputs 1 if and only if <span class="math">x \\geq y</span>. The appropriate lookup table has <span class="math">(x,y)</span>'th entry equal to the output of the LT instruction when run on <span class="math">x</span> and <span class="math">y</span>. As with the table AND above, we show that the multilinear extension of this table can be evaluated by the verifier in logarithmic time, and the prover in the sparse-dense sum-check protocol applied to this table (Theorem 9) runs in <span class="math">O(cm)</span> time when the table size is at most <span class="math">O(m^c)</span>.</p>

    <h2 id="sec-89" class="text-2xl font-bold">F.3.2 Ensuring the sum-check prover runs in time close to <span class="math">m</span></h2>

    <p class="text-gray-300">Depending on just how "structured" the table <span class="math">y</span> is, we prove two results regarding how fast the prover can be implemented in the sparse-dense sum-check protocol. First, using prior techniques [CMT12], we bound the prover time in the sparse-dense sum-check protocol by</p>

    <div class="my-4 text-center"><span class="math-block">O \\left(m \\cdot \\log N \\cdot \\operatorname{evaltime}(\\widetilde{t})\\right),</span></div>

    <p class="text-gray-300">where <span class="math">\\operatorname{evaltime}(\\widetilde{t})</span> denotes the time required to evaluate <span class="math">\\widetilde{t}(r&#x27;)</span> for any point <span class="math">r&#x27; \\in \\mathbb{F}^{\\log N}</span>. In particular, if <span class="math">\\operatorname{evaltime}(\\widetilde{t}) = O(\\log N)</span>, then this means <span class="math">O(m \\cdot \\log^2 N)</span> field operations for the prover.</p>

    <p class="text-gray-300"><strong>Theorem 6.</strong> The prover in the sparse-dense sum-check protocol applied to compute <span class="math">\\langle u, t \\rangle</span> can be implemented in <span class="math">O\\left(m \\cdot \\log N \\cdot \\operatorname{evaltime}(\\widetilde{t})\\right)</span> field operations.</p>

    <p class="text-gray-300">Using the same techniques, we are in fact able to reduce the prover runtime for all tables considered in Section F.3.1 to <span class="math">O(m \\log N)</span> field operations (see Section G.4).</p>

    <p class="text-gray-300">Reducing the prover's runtime to the optimal <span class="math">O(cm)</span> field operations is a substantially more challenging task. Intuitively, this requires the prover to spend a constant amount of work per non-zero entry of <span class="math">u</span>, in total across all <span class="math">\\log N</span> rounds of the protocol. This is a far more exacting task than achieving a constant amount of work per non-zero entry of <span class="math">u</span> in each of the <span class="math">\\log N</span> rounds, which is what the previous paragraph achieved.²⁰</p>

    <p class="text-gray-300">Hence, fundamentally new algorithmic techniques (outlined in Section F.1) are required to reduce the prover's runtime to the optimal <span class="math">O(m)</span> field operations. We achieve this in Theorem 7 below for a large class of tables that captures all of those considered in Section F.3.1. Informally, the key property that we require to achieve <span class="math">O(m)</span> prover time is that, given any evaluation <span class="math">\\widetilde{t}(r_1, \\ldots, r_{\\log N})</span> of <span class="math">\\widetilde{t}</span>, changing the value of one variable from <span class="math">r_j</span> to <span class="math">r_j&#x27;</span> has a "simple" effect on the evaluation of <span class="math">\\widetilde{t}</span>.</p>

    <p class="text-gray-300"><strong>Theorem 7 (Informal version of Theorem 9 in Section G.5.2).</strong> Suppose there is some constant <span class="math">C &amp;gt; 0</span> such that <span class="math">m \\leq O(N^C)</span>. Suppose that <span class="math">\\widetilde{t} \\colon \\mathbb{F}^{\\log N} \\to \\mathbb{F}</span> satisfies the following property. For any <span class="math">(r_1, \\ldots, r_{\\log N}) \\in \\mathbb{F}^{\\log N}</span> and any <span class="math">r&#x27; \\in \\mathbb{F}^{\\log N}</span>,</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde{t}(r_1, \\ldots, r_{j-1}, r_j&#x27;, r_{j+1}, \\ldots, r_{\\log N}) = \\mathfrak{m} \\cdot \\widetilde{t}(r_1, \\ldots, r_{j-1}, r_j, r_{j+1}, \\ldots, r_{\\log N}) + \\mathsf{a}</span></div>

    <p class="text-gray-300">²⁰ Because cryptographic operations are one or more orders of magnitude more expensive than field operations, we believe that even <span class="math">O(m \\log N)</span> field work would not be a bottleneck for the GeneralizedLasso prover, compared to the work of committing to <span class="math">O(m)</span> field elements, unless <span class="math">\\log N</span> is in the thousands or larger.</p>

    <p class="text-gray-300">where <span class="math">\\mathsf{m}</span> and <span class="math">\\mathsf{a}</span> are field elements that depend only on <span class="math">j</span>, <span class="math">r_{1},\\ldots,r_{j},r_{j+1}</span>, and <span class="math">r_{j}^{\\prime}</span> and can be computed in <span class="math">O(1)</span> time. Then the sparse-dense sum-check protocol prover can be implemented in <span class="math">O(m)</span> field operations.</p>

    <p class="text-gray-300">In <span class="math">\\mathsf{GeneralizedLasso}</span>, the <span class="math">O(Cm)</span> field operations incurred by the prover in the sparse-dense sum-check protocol will not be a bottleneck for the prover relative to the task of applying a sparse polynomial commitment scheme to commit to <span class="math">O(cm)</span> field elements (Section 1.1).</p>

    <p class="text-gray-300">When the sparse-dense sum-check protocol is applied to compute <span class="math">\\langle u,t\\rangle</span> where <span class="math">u</span> is <span class="math">m</span>-sparse, the prover ultimately has to provide the verifier with the value <span class="math">\\widetilde{u}(r)</span> for a randomly chosen <span class="math">r\\in\\mathbb{F}^{\\log N}</span>, where <span class="math">\\widetilde{u}</span> is the multilinear extension polynomial of <span class="math">u</span>. The fastest known algorithm for evaluating <span class="math">\\widetilde{u}(r)</span> requires <span class="math">O(Cm)</span> field work. In fact this algorithm underlies <span class="math">\\mathsf{Spark}</span>, our sparse polynomial commitment scheme (it is described in Section 3.1). Hence, if this algorithm for evaluating sparse multilinear polynomials is optimal, then so is the <span class="math">O(Cm)</span>-time algorithm of implementing the sparse-dense sum-check prover.</p>

    <h3 id="sec-90" class="text-xl font-semibold mt-8">F.4 Beyond multilinear extensions, and a generic speedup over bit-decomposition</h3>

    <p class="text-gray-300">In our lookup argument (Figure 8) there is nothing special about using the multilinear extension <span class="math">\\widetilde{t}</span> of <span class="math">t</span>. We can replace <span class="math">\\widetilde{t}</span> with any extension polynomial <span class="math">\\hat{t}</span> of <span class="math">t</span> (recall from Section 2.1 that <span class="math">\\hat{t}</span> extends <span class="math">t</span> if <span class="math">\\hat{t}(i)=t(i)</span> for all <span class="math">i\\in\\{0,1\\}^{\\log N}</span>).</p>

    <p class="text-gray-300">Indeed, the key equality (Equation (20)) that for <span class="math">b=M\\cdot t</span> that</p>

    <p class="text-gray-300"><span class="math">\\widetilde{b}(r)=\\sum_{j\\in\\{0,1\\}^{\\log N}}\\widetilde{M}(r,j)\\cdot\\widetilde{t}(j).</span></p>

    <p class="text-gray-300">holds with <span class="math">\\widetilde{t}</span> replaced by any extension <span class="math">\\hat{t}</span> of <span class="math">t</span>. This is because the right hand side of the equation is multilinear in <span class="math">r</span> regardless of whether or not <span class="math">\\widetilde{t}(j)</span> is multilinear in <span class="math">j</span>.</p>

    <p class="text-gray-300">This means that if the multilinear extension <span class="math">\\widetilde{t}</span> of <span class="math">t</span> cannot be evaluated by the verifier sufficiently quickly (say, in polylogarithmic time), we can replace <span class="math">\\widetilde{t}</span> with another extension <span class="math">\\hat{t}</span> that can be. This does potentially increase the costs of the sum-check protocol applied to compute</p>

    <p class="text-gray-300"><span class="math">\\sum_{j\\in\\{0,1\\}^{\\log N}}\\widetilde{M}(r,j)\\cdot\\widetilde{t}(j)</span></p>

    <p class="text-gray-300">(see Figure 9). In particular, the length of each message <span class="math">j</span> from prover to verifier across the <span class="math">\\log N</span> rounds of the sum-check protocol grows from 3 field elements (specifying a degree-2 univariate polynomial) to <span class="math">1+d_{j}</span> where <span class="math">d_{j}</span> is the degree of <span class="math">\\hat{t}</span> in its <span class="math">j</span>’th variable. Assuming that <span class="math">d_{j}\\leq\\mathrm{polylog}(N)</span> for each variable <span class="math">j=1,\\ldots,\\log N</span>, and applying standard techniques to implement the sum-check protocol prover <em>[x10]</em>, we obtain a prover performing <span class="math">O(m\\cdot\\mathrm{polylog}(N))</span> field operations.</p>

    <p class="text-gray-300">This result can be viewed as formalizing the following intuitive statement: any operation that can be “efficiently performed via bit-decomposition” (meaning there is an arithmetic or Boolean formula of size polynomial in the number of bits in the bit-decomposition that outputs the result of the operation) can be solved by <span class="math">\\mathsf{GeneralizedLasso}</span> with <span class="math">\\mathcal{P}</span> only needing to cryptographically commit to <span class="math">3c</span> many field elements per lookup (<span class="math">\\mathcal{P}</span> also performs polylogarithmic field operations per lookup). In contrast, as explained in Section 1, naive bit-decomposition of integers in the range <span class="math">\\{0,1,\\ldots,R-1\\}</span> requires the prover to commit to <span class="math">\\log R</span> many field elements.</p>

    <h2 id="sec-91" class="text-2xl font-bold">Appendix G Details of the sparse-dense sum-check protocol</h2>

    <p class="text-gray-300">Let <span class="math">u\\in\\mathbb{F}^{N}</span> be a vector with at most <span class="math">m</span> non-zero entries and <span class="math">t\\in\\mathbb{F}^{N}</span> be another vector (which may have <span class="math">N</span> non-zero entries). Let <span class="math">\\widetilde{u}</span> and <span class="math">\\widetilde{t}</span> denote their multilinear extensions. Throughout, we assume that there is some constant <span class="math">C&gt;0</span> such that <span class="math">N\\leq m^{C}</span>.</p>

    <p class="text-gray-300">######</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Polynomial commitment to the multilinear polynomial <span class="math">\\widetilde{a}\\colon\\mathbb{F}^{\\log m}\\to\\mathbb{F}</span>.</li>

      <li>The prover <span class="math">\\mathcal{P}</span> sends a polynomial commitment to the MLE <span class="math">\\widetilde{M}</span> of a matrix <span class="math">M\\in\\{0,1\\}^{m\\times N}</span> using our specialized version of Spartan’s sparse polynomial commitment scheme.</li>

      <li>The verifier <span class="math">\\mathcal{V}</span> picks a random <span class="math">r\\in\\mathbb{F}^{\\log m}</span> and sends <span class="math">r</span> to <span class="math">\\mathcal{P}</span>. The verifier makes one evaluation query to <span class="math">\\widetilde{a}</span>, to learn <span class="math">\\widetilde{a}(r)</span>.</li>

      <li><span class="math">\\mathcal{P}</span> and <span class="math">\\mathcal{V}</span> apply the sum-check protocol to the <span class="math">(\\log N)</span>-variate polynomial <span class="math">g(j)\\coloneqq\\widetilde{M}(r,j)\\cdot\\hat{t}(j)</span>, to confirm that</li>

    </ul>

    <p class="text-gray-300"><span class="math">\\widetilde{a}(r)=\\sum_{j\\in\\{0,1\\}^{\\log N}}g(j).</span></p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>At the end of the sum-check protocol, the verifier needs to evaluate <span class="math">\\widetilde{M}(r,r^{\\prime})</span> and <span class="math">\\hat{t}(r^{\\prime})</span> for a random point <span class="math">r^{\\prime}\\in\\mathbb{F}^{\\log N}</span> that is chosen entry-by-entry over the course of the sum-check protocol. This costs one evaluation query to <span class="math">\\widetilde{M}</span> and one to <span class="math">\\hat{t}</span>.</li>

    </ul>

    <p class="text-gray-300">Figure 9: Description of GeneralizedLasso when using an extension polynomial <span class="math">\\hat{t}</span> of the table vector <span class="math">t</span>, where <span class="math">\\hat{t}</span> may not be multilinear. A polynomial commitment to the multilinear polynomial <span class="math">\\widetilde{a}\\colon\\mathbb{F}^{\\log m}\\to\\mathbb{F}</span> is given to the verifier as input.</p>

    <h3 id="sec-92" class="text-xl font-semibold mt-8">G.1 Establishing that for any <span class="math">r\\in\\mathbb{F}^{\\log m}</span>, <span class="math">\\widetilde{M}(r,y)</span> is <span class="math">m</span>-sparse</h3>

    <p class="text-gray-300">In GeneralizedLasso, <span class="math">t\\in\\mathbb{F}^{N}</span> will be the table (Definition 1.1), while <span class="math">\\widetilde{u}</span> will be <span class="math">\\widetilde{M}(r,y)</span>. If the prover is honest, each row of <span class="math">M</span> has exactly one non-zero entry, and accordingly <span class="math">\\widetilde{M}(r,y)\\neq 0</span> for at most <span class="math">m</span> values of <span class="math">y\\in\\{0,1\\}^{\\log N}</span> by the following reasoning. Let</p>

    <p class="text-gray-300"><span class="math">\\chi_{i}(r)=\\prod_{k=1}^{\\log m}(r_{k}i_{k}+(1-r_{k})(1-i_{k}))</span></p>

    <p class="text-gray-300">denote the <span class="math">i</span>’th Lagrange basis polynomial, which maps <span class="math">i</span> to <span class="math">1</span> and maps all other inputs in <span class="math">\\{0,1\\}^{\\log m}</span> to zero. Standard Lagrange interpolation for multilinear polynomials (see <em>[x18, Chapter 3]</em>) states that</p>

    <p class="text-gray-300"><span class="math">\\widetilde{M}(r,y)=\\sum_{(i,j)\\in\\{0,1\\}^{\\log m+\\log N}}M_{i,j}\\cdot\\chi_{i}(r)\\cdot\\chi_{j}(y).</span> (23)</p>

    <p class="text-gray-300">Since for any <span class="math">i</span>, <span class="math">M_{i,j}\\neq 0</span> for exactly one <span class="math">j</span>, the right hand side of Equation (23) can be non-zero for at most <span class="math">m</span> values of <span class="math">y\\in\\{0,1\\}^{\\log N}</span>, namely those <span class="math">y</span>’s indexing columns of <span class="math">M</span> with at least one non-zero entry.</p>

    <h3 id="sec-93" class="text-xl font-semibold mt-8">G.2 Background on the sum-check protocol</h3>

    <p class="text-gray-300">For each round <span class="math">j=1,\\ldots,\\log N</span> of the sum-check protocol, the prescribed prover message is the degree-2 univariate polynomial <span class="math">s_{j}</span> where</p>

    <p class="text-gray-300"><span class="math">s_{j}(c)=\\sum_{(b_{j+1},b_{j+2},\\ldots,b_{\\log N})\\in\\{0,1\\}^{\\log(N)-j}}\\widetilde{u}(r_{1},\\ldots,r_{j-1},c,b_{j+1},\\ldots,b_{\\log N})\\cdot\\widetilde{t}(r_{1},\\ldots,r_{j-1},c,b_{j+1},\\ldots,b_{\\log N}).</span> (24)</p>

    <p class="text-gray-300">Here, <span class="math">r_{1},\\ldots,r_{j-1}</span> are random field elements chosen by the verifier in rounds <span class="math">1,2,\\ldots,j-1</span>. The prover will specify <span class="math">s_{j}</span> by sending its evaluations at <span class="math">3</span> inputs, say, <span class="math">s_{j}(0)</span>, <span class="math">s_{j}(1)</span>, and <span class="math">s_{j}(-1)</span>.</p>

    <h3 id="sec-94" class="text-xl font-semibold mt-8">G.3 Proof of Theorem 6</h3>

    <h6 id="sec-95" class="text-base font-medium mt-4">Proof of Theorem 6.</h6>

    <p class="text-gray-300">Observe that</p>

    <p class="text-gray-300"><span class="math">\\widetilde{u}(r_{1},b_{2},\\ldots,b_{\\log N})=(1-r_{1})\\cdot\\widetilde{u}(0,b_{2},\\ldots,b_{\\log N})+r_{1}\\cdot\\widetilde{u}(1,b_{2},\\ldots,b_{\\log N})</span></p>

    <p class="text-gray-300">This holds because the left hand size and right hand sides are both multilinear polynomials that agree on all inputs <span class="math">(r_{1},b_{2},\\ldots,b_{\\log N})\\in\\{0,1\\}^{\\log N}</span>.</p>

    <p class="text-gray-300">Let <span class="math">S_{u}=\\{i=(i_{1},\\ldots,i_{\\log N})\\in\\{0,1\\}^{\\log N}\\colon u_{i}\\neq 0\\}</span> denote the non-zero entries of <span class="math">u</span>. For every <span class="math">i\\in S_{u}</span>, and every round <span class="math">j</span> of the sum-check protocol, <span class="math">\\mathcal{P}</span> will store the value</p>

    <p class="text-gray-300"><span class="math">\\chi_{i}(r_{1},\\ldots,r_{j-1},i_{j},i_{j+1},\\ldots,i_{\\log N})=\\prod_{k=1}^{j-1}\\left(i_{k}\\cdot r_{k}+(1-i_{k})\\cdot(1-r_{k})\\right).</span> (25)</p>

    <p class="text-gray-300">Note that given all such values for round <span class="math">j</span>, the prover can compute all the relevant values for round <span class="math">j+1</span> in time <span class="math">O(m)</span>. This is because there are <span class="math">m</span> elements in <span class="math">i\\in S_{u}</span> and computing <span class="math">\\chi_{i}(r_{1},\\ldots,r_{j-1},r_{j},i_{j+1},\\ldots,i_{\\log N})</span> given <span class="math">\\chi_{i}(r_{1},\\ldots,r_{j-1},i_{j},i_{j+1},\\ldots,i_{\\log N})</span> can be done with just one field multiplication. Hence, maintaining all such values across all rounds <span class="math">j</span> takes time <span class="math">O(m\\log N)</span> in total for the prover.</p>

    <p class="text-gray-300">By standard multilinear Lagrange interpolation (see <em>[x23, Chapter 3]</em>),</p>

    <p class="text-gray-300"><span class="math">\\widetilde{u}(r_{1},\\ldots,r_{\\log N})=\\sum_{i\\in S_{u}}u_{i}\\cdot\\chi_{i}(r_{1},\\ldots,r_{\\log N}),</span> (26)</p>

    <p class="text-gray-300">where <span class="math">u_{i}</span> denotes the <span class="math">i</span>’th entry of <span class="math">u</span> when its entries are indexed by bit-vectors <span class="math">\\{0,1\\}^{\\log N}</span>. Hence, for any <span class="math">c\\in\\mathbb{F}</span>,</p>

    <p class="text-gray-300"><span class="math">s_{j}(c)=\\sum_{(b_{j+1},b_{j+2},\\ldots,b_{\\log N})\\in\\{0,1\\}^{\\log(N)-j}}\\widetilde{u}(r_{1},\\ldots,r_{j-1},c,b_{j+1},\\ldots,b_{\\log N})\\cdot\\widetilde{t}(r_{1},\\ldots,r_{j-1},c,b_{j+1},\\ldots,b_{\\log N})</span> <span class="math">=\\sum_{(b_{j+1},b_{j+2},\\ldots,b_{\\log N})\\in\\{0,1\\}^{\\log(N)-j}}\\left(\\sum_{i\\in S_{u}}u_{i}\\cdot\\chi_{i}(r_{1},\\ldots,r_{j-1},c,b_{j+1},\\ldots,b_{\\log N})\\right)\\cdot\\widetilde{t}(r_{1},\\ldots,r_{j-1},c,b_{j+1},\\ldots,b_{\\log N})</span> <span class="math">\\qquad=\\sum_{i\\in S_{u}}u_{i}\\cdot\\chi_{i}(r_{1},\\ldots,r_{j-1},c,i_{j+1},\\ldots,i_{\\log N})\\cdot\\widetilde{t}(r_{1},\\ldots,r_{j-1},c,i_{j+1},\\ldots,i_{\\log N}).</span> (27)</p>

    <p class="text-gray-300">The final equality above exploits the fact that for any <span class="math">(b_{j+1},\\ldots,b_{\\log N})\\in\\{0,1\\}^{\\log(N)-j}</span>, if</p>

    <p class="text-gray-300"><span class="math">(i_{j+1},\\ldots,i_{\\log N})\\neq(b_{j+1},\\ldots,b_{\\log N}),</span></p>

    <p class="text-gray-300">then</p>

    <p class="text-gray-300"><span class="math">\\chi_{i}(r_{1},\\ldots,r_{j-1},c,b_{j+1},\\ldots,b_{\\log N})=0.</span></p>

    <p class="text-gray-300">To see this, observe that</p>

    <p class="text-gray-300"><span class="math">\\chi_{i}(x_{1},\\ldots,x_{\\log N})=\\prod_{k=1}^{\\log N}\\left(i_{k}x_{k}+(1-i_{k})(1-x_{k})\\right),</span></p>

    <p class="text-gray-300">and if <span class="math">i_{k}=1</span> and <span class="math">x_{k}=0</span> or vice versa then the <span class="math">k</span>’th term of this product is zero.</p>

    <p class="text-gray-300">So to compute <span class="math">s_{j}(c)</span> for <span class="math">c\\in\\{0,1,-1\\}</span>, the prover directly computes Expression (27). Given that the prover maintains in each round <span class="math">j</span> the values in Expression (25), in round <span class="math">j</span> computing <span class="math">s_{j}(c)</span> takes time</p>

    <p class="text-gray-300"><span class="math">O\\left(m\\cdot\\textsf{evaltime}\\left(\\widetilde{t}\\right)\\right).</span></p>

    <p class="text-gray-300">Across all <span class="math">\\log N</span> rounds of the sum-check protocol, this entails total prover time</p>

    <p class="text-gray-300"><span class="math">O\\left(m\\cdot\\textsf{evaltime}\\left(\\widetilde{t}\\right)\\right).</span></p>

    <p class="text-gray-300">∎</p>

    <p class="text-gray-300">##</p>

    <p class="text-gray-300">G.4 Improving the runtime to <span class="math">O(m \\log N)</span> if <span class="math">\\widetilde{t}</span> has additional structure</p>

    <p class="text-gray-300">In the proof of Theorem 6, the prover time is bottlenecked by evaluating <span class="math">\\widetilde{t}</span> at all points of the form</p>

    <div class="my-4 text-center"><span class="math-block">(r_1, \\dots, r_{j-1}, c, i_{j+1}, \\dots, i_{\\log N}),</span></div>

    <p class="text-gray-300">where <span class="math">i = (i_1, \\dots, i_{\\log N})</span> ranges over <span class="math">i \\in S_u</span> and <span class="math">c \\in \\{0, 1, -1\\}</span>. For all the example tables in Section F.3.1, given</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde{t}(r_1, \\dots, r_{j-1}, i_j, i_{j+1}, \\dots, i_{\\log N}),</span></div>

    <p class="text-gray-300">it takes only constant time (in most cases, just one multiplication by a power of two and one field addition) to compute</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde{t}(r_1, \\dots, r_{j-1}, r_j, i_{j+1}, \\dots, i_{\\log N}).</span></div>

    <p class="text-gray-300">This reduces the prover time for all such tables from <span class="math">O(m \\log^2 N)</span> of Theorem 6 down to <span class="math">O(m \\log N)</span>.</p>

    <h2 id="sec-96" class="text-2xl font-bold">G.5 Improving the runtime to <span class="math">O(cm)</span></h2>

    <p class="text-gray-300">Fundamentally different techniques are required to reduce the prover's runtime to <span class="math">O(m)</span>.</p>

    <p class="text-gray-300">We begin by explaining how to implement the prover in <span class="math">O(m)</span> time under the assumption that <span class="math">\\widetilde{t}</span> has total degree one, i.e.,</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde{t}(r_1, \\dots, r_{\\log N}) = \\sum_{j=1}^{\\log N} d_j r_j.</span></div>

    <p class="text-gray-300">This is sufficient to capture most of the example tables considered in Section F.3.1. Later (Section G.5.2) we explain how to implement the prover in <span class="math">O(m)</span> time for a larger class of tables.</p>

    <h2 id="sec-97" class="text-2xl font-bold">G.5.1 Handling tables for which <span class="math">\\widetilde{t}</span> has total degree 1</h2>

    <p class="text-gray-300"><strong>Theorem 8.</strong> Suppose that the multilinear extension <span class="math">\\widetilde{t}</span> of <span class="math">t</span> has total degree 1, i.e., can be written as <span class="math">\\widetilde{t}(r_1, \\ldots, r_{\\log N}) = \\sum_{k=1}^{\\log N} d_k \\cdot r_k</span> for some field elements <span class="math">d_1, \\ldots, d_{\\log N} \\in \\mathbb{F}</span>. Then the prover in the sparse-dense sum-check protocol can be implemented in <span class="math">O(m)</span> field operations.²¹</p>

    <p class="text-gray-300"><strong>Proof.</strong> We begin by showing that <span class="math">\\widetilde{t}</span> having total degree 1 ensures that altering the value of a single variable leads to "simple" changes in the output of <span class="math">\\widetilde{t}</span>.</p>

    <p class="text-gray-300"><strong>Understanding the effect on an evaluation of <span class="math">\\widetilde{t}</span> of altering the <span class="math">j</span>'th variable.</strong> Let</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde{t}(r_1, \\dots, r_{\\log N}) = \\sum_{j=1}^{\\log N} d_j r_j.</span></div>

    <p class="text-gray-300">This ensures that for any <span class="math">c \\in \\mathbb{F}</span>, and any <span class="math">(r_1, \\ldots, r_{j-1}, b_j, b_{j+1}, \\ldots, b_{\\log N}) \\in \\mathbb{F}^j \\times \\{0, 1\\}^{\\log(N) - j}</span>,</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde{t}(r_1, \\dots, r_{j-1}, c, b_{j+1}, \\dots, b_{\\log N}) = \\widetilde{t}(r_1, \\dots, r_{j-1}, b_j, b_{j+1}, \\dots, b_{\\log N}) + (c - b_j) \\cdot d_j. \\tag{28}</span></div>

    <p class="text-gray-300">The important implication of Equation (28) is that "revising" the <span class="math">j</span>'th variable value from <span class="math">b_j</span> to <span class="math">c</span> affects the evaluation of <span class="math">\\widetilde{t}</span> by an additive term <span class="math">(c - b_j) \\cdot d_j</span>. The key here is that this term can be computed in <span class="math">O(1)</span> time, and does not depend on the variables <span class="math">b_{\\log(m)+1}, \\ldots, b_{\\log N}</span>.</p>

    <p class="text-gray-300">In Section G.5.2, we extend our techniques to achieve <span class="math">O(m)</span> prover time whenever <span class="math">\\widetilde{t}</span> can be decomposed into a sum of <span class="math">\\eta = O(1)</span> polynomials <span class="math">\\widetilde{t}_1, \\ldots, \\widetilde{t}_\\eta</span> such that the following holds. There exist values <span class="math">\\mathsf{a}_{\\ell}(c,j,b_j)</span>, <span class="math">\\mathsf{m}_{\\ell}(c,j,b_j)</span> such that "revising" the <span class="math">j</span>'th variable from <span class="math">b_j</span> to <span class="math">c</span> affects the evaluation of <span class="math">\\widetilde{t}_{\\ell}</span> by a multiplicative factor of <span class="math">\\mathsf{m}_{\\ell}(c,j,b_j)</span> and an additive factor of <span class="math">\\mathsf{a}_{\\ell}(c,j,b_j)</span>. Moreover, these factors <span class="math">\\mathsf{a}_{\\ell}(c,j,b_j)</span> and <span class="math">\\mathsf{m}_{\\ell}(c,j,b_j)</span> can be</p>

    <p class="text-gray-300">²¹ To clarify, Theorem 8 also assumes that for each index <span class="math">b</span>, the <span class="math">b</span>'th table entry, <span class="math">t_b</span>, can be computed in constant time (otherwise, it is impossible to even compute the correct answer <span class="math">\\langle u, t \\rangle</span> in the sparse-dense sum-check protocol in <span class="math">O(m)</span> time).</p>

    <p class="text-gray-300">evaluated in <span class="math">O(1)</span> time and do <em>not</em> depend on the variables <span class="math">b_{\\log(m)+1},\\ldots,b_{\\log N}</span>. The special case of <span class="math">\\widetilde{t}</span> having total degree 1, which we consider first, corresponds to <span class="math">\\eta=1</span>, <span class="math">\\mathsf{a}_{\\ell}(c,j,b_{j})=(c-b_{j})\\cdot d_{j}</span> and <span class="math">\\mathsf{m}_{\\ell}(c,j,b_{j})=1</span>.</p>

    <h4 id="sec-98" class="text-lg font-semibold mt-6">Values computed by the prover at the start of the protocol.</h4>

    <p class="text-gray-300">For every <span class="math">k\\in\\{0,1\\}^{\\log m}</span>, let <span class="math">\\mathsf{extend}_{\\ell}(k)</span> denote the set of all vectors in <span class="math">\\{0,1\\}^{\\ell}</span> whose first <span class="math">\\log m</span> entries equal <span class="math">k</span>. At the start of the protocol, the prover computes the following two values <span class="math">q_{k}</span> and <span class="math">z_{k}</span> for every <span class="math">k\\in\\{0,1\\}^{\\log m}</span>:</p>

    <p class="text-gray-300"><span class="math">q_{k}</span> <span class="math">\\coloneqq\\sum_{y\\in\\mathsf{extend}_{\\log N}(k)}\\widetilde{u}(y)\\cdot\\widetilde{t}(y)</span> (29) <span class="math">z_{k}</span> <span class="math">\\coloneqq\\sum_{y\\in\\mathsf{extend}_{\\log N}(k)}\\widetilde{u}(y).</span> (30)</p>

    <p class="text-gray-300">Since <span class="math">u</span> is <span class="math">m</span>-sparse, all <span class="math">m</span> quantities <span class="math">q_{k}</span> and <span class="math">z_{k}</span> can be computed in <span class="math">O(m)</span> total time.</p>

    <p class="text-gray-300">The prover also computes “a binary tree of aggregations” of the above values. Specifically, let us think of the <span class="math">m</span> different <span class="math">q_{k}</span> (respectively, <span class="math">z_{k}</span>) values as the roots of a binary tree <span class="math">Q</span> (respectively, <span class="math">Z</span>), and assign each node in <span class="math">Q</span> and <span class="math">Z</span> the value equal to the sum of its leaves. These values can be computed in <span class="math">O(m)</span> time in total.</p>

    <p class="text-gray-300">For example, the roots of <span class="math">Q</span> and <span class="math">Z</span> respectively store</p>

    <p class="text-gray-300"><span class="math">\\sum_{k\\in\\{0,1\\}^{\\log m}}q_{k}=\\sum_{y\\in\\{0,1\\}^{\\log N}}\\widetilde{u}(y)\\cdot\\widetilde{t}(y),</span></p>

    <p class="text-gray-300">and</p>

    <p class="text-gray-300"><span class="math">\\sum_{k\\in\\{0,1\\}^{\\log m}}z_{k}=\\sum_{y\\in\\{0,1\\}^{\\log N}}\\widetilde{u}(y).</span></p>

    <p class="text-gray-300">Likewise, the two children of the root in <span class="math">Q</span> store:</p>

    <p class="text-gray-300"><span class="math">\\sum_{k=(k_{1},\\ldots,k_{\\log m})\\in\\{0,1\\}^{\\log m}:\\;k_{1}=0}q_{k},</span> (31)</p>

    <p class="text-gray-300">and</p>

    <p class="text-gray-300"><span class="math">\\sum_{k=(k_{1},\\ldots,k_{\\log m})\\in\\{0,1\\}^{\\log m}:\\;k_{1}=1}q_{k},</span> (32)</p>

    <p class="text-gray-300">and similarly for <span class="math">Z</span>. In general, let <span class="math">Q^{(j)}\\in\\mathbb{F}^{2^{j}}\\in\\mathbb{F}^{2^{j}}</span> is the vector of values assigned to nodes at at depth <span class="math">j</span> of <span class="math">Q</span>, and similarly let <span class="math">Z^{(j)}</span> denote the corresponding vector of values for <span class="math">Z</span>. For example, <span class="math">Q^{(1)}</span> is the length-2 vector whose two entries are given in Equations (31) and (32).</p>

    <h4 id="sec-99" class="text-lg font-semibold mt-6">The prover’s workflow in the first <span class="math">\\log m</span> rounds.</h4>

    <p class="text-gray-300">Recall from Equation (24) that</p>

    <p class="text-gray-300"><span class="math">s_{j}(c)=\\sum_{(b_{j+1},b_{j+2},\\ldots,b_{\\log N})\\in\\{0,1\\}^{\\log(N)-j}}\\widetilde{u}(r_{1},\\ldots,r_{j-1},c,b_{j+1},\\ldots,b_{\\log N})\\cdot\\widetilde{t}(r_{1},\\ldots,r_{j-1},c,b_{j+1},\\ldots,b_{\\log N}).</span></p>

    <p class="text-gray-300">And recall from Equation (26) that</p>

    <p class="text-gray-300"><span class="math">\\widetilde{u}(r_{1},\\ldots,r_{\\log N})=\\sum_{i\\in S_{u}}u_{i}\\cdot\\chi_{i}(r_{1},\\ldots,r_{\\log N}).</span></p>

    <p class="text-gray-300">Moreover, for any <span class="math">i=(i_{1},\\ldots,i_{\\log N})\\in\\{0,1\\}^{\\log N}</span>, if <span class="math">i_{j}=1</span>, then</p>

    <p class="text-gray-300"><span class="math">\\chi_{i}(r_{1},\\ldots,r_{j-1},c,i_{j+1},\\ldots,i_{\\log N})=c\\cdot\\chi_{i}(r_{1},\\ldots,r_{j-1},i_{j},i_{j+1},\\ldots,i_{\\log N}),</span></p>

    <p class="text-gray-300">and if <span class="math">i_{j}=0</span> then</p>

    <p class="text-gray-300"><span class="math">\\chi_{i}(r_{1},\\ldots,r_{j-1},c,i_{j+1},\\ldots,i_{\\log N})=(1-c)\\cdot\\chi_{i}(r_{1},\\ldots,r_{j-1},i_{j},i_{j+1},\\ldots,i_{\\log N}).</span></p>

    <p class="text-gray-300">Based on the above equations, it can be derived that</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{aligned} s_1(c) &amp;amp;= \\sum_{y \\in \\{0, 1\\}^{\\log N}} \\widetilde{u}(c, y_2, \\dots, y_{\\log N}) \\cdot \\widetilde{t}(c, y_2, \\dots, y_{\\log N}) \\\\ &amp;amp;= \\sum_{y \\in \\{0, 1\\}^{\\log N}} \\left( \\sum_{i = (i_1, \\dots, i_{\\log N}) \\in S_u} u_i \\cdot \\chi_i(c, y_2, \\dots, y_{\\log N}) \\right) \\cdot \\widetilde{t}(c, y_2, \\dots, y_{\\log N}) \\tag{33} \\\\ &amp;amp;= \\sum_{i = (i_1, \\dots, i_{\\log N}) \\in S_u} u_i \\cdot \\chi_i(c, i_2, \\dots, i_{\\log N}) \\cdot \\widetilde{t}(c, i_2, \\dots, i_{\\log N}) \\tag{34} \\\\ &amp;amp;= \\sum_{i = (i_1, \\dots, i_{\\log N}) \\in S_u} \\chi_{i_1}(c) \\cdot u_i \\cdot \\chi_i(i_1, i_2, \\dots, i_{\\log N}) \\cdot \\left( \\widetilde{t}(i_1, i_2, \\dots, i_{\\log N}) + (c - i_1) d_1 \\right) \\tag{35} \\\\ &amp;amp;= \\sum_{i = (i_1, \\dots, i_{\\log N}) \\in S_u: i_1 = 0} (1 - c) \\cdot u_i \\cdot \\left( \\widetilde{t}(i_1, i_2, \\dots, i_{\\log N}) + c d_1 \\right) \\tag{36} \\\\ &amp;amp;+ \\sum_{i = (i_1, \\dots, i_{\\log N}) \\in S_u: i_1 = 1} c \\cdot u_i \\cdot \\left( \\widetilde{t}(i_1, i_2, \\dots, i_{\\log N}) + (c - 1) d_1 \\right). \\tag{37} \\end{aligned}</span></div>

    <p class="text-gray-300">Here, Equation (33) invokes Equation (26). Equation (34) holds because <span class="math">\\chi_i(j) = 0</span> whenever there is even a single index <span class="math">\\ell \\in \\{2, 3, \\dots, \\log N\\}</span> such that <span class="math">i_\\ell, j_\\ell \\in \\{0, 1\\}</span> and <span class="math">i_\\ell \\neq j_\\ell</span>. Equation (35) holds by Equation (28). Equation (36) holds because <span class="math">\\chi_i(i) = 1</span> for all <span class="math">i \\in \\{0, 1\\}^{\\log N}</span>. Equation (37) holds by definition of <span class="math">\\chi_{i_1}</span> (Equation (3)).</p>

    <p class="text-gray-300"><strong>Round 1 computation.</strong> Expression (37) above equals:</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{aligned} &amp;amp;\\sum_{k = (k_1, \\dots, k_{\\log m}) \\in \\{0, 1\\}^{\\log m}: k_1 = 0} ((1 - c) \\cdot q_k + (1 - c) \\cdot c \\cdot d_1 \\cdot z_k) \\\\ &amp;amp;+ \\sum_{k = (k_1, \\dots, k_{\\log m}) \\in \\{0, 1\\}^{\\log m}: k_1 = 1} (c \\cdot q_k + c \\cdot (c - 1) \\cdot d_1 \\cdot z_k) \\end{aligned}</span></div>

    <p class="text-gray-300">For each <span class="math">c \\in \\{-1, 0, 1\\}</span>, computing this expression requires just a constant amount of work given the values of the two children of the root vertex of the trees <span class="math">Q</span> (Equations (31) and (32)) and <span class="math">Z</span>.</p>

    <p class="text-gray-300">48</p>

    <p class="text-gray-300">Round <span class="math">j &amp;gt; 1</span> computation. A similar calculation to the above reveals that <span class="math">s_j(c)</span> equals:</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{aligned} &amp;amp; \\sum_{(y_{j+1}, \\dots, y_{\\log N}) \\in \\{0, 1\\}^{\\log(N) - j}} \\widetilde{u}(r_1, \\dots, r_{j-1}, c, y_{j+1}, \\dots, y_{\\log N}) \\cdot \\widetilde{t}(r_1, \\dots, r_{j-1}, c, y_{j+1}, \\dots, y_{\\log N}) \\\\ &amp;amp; = \\sum_{(y_{j+1}, \\dots, y_{\\log N}) \\in \\{0, 1\\}^{\\log(N) - j}} \\left( \\sum_{i = (i_1, \\dots, i_{\\log N}) \\in S_u} u_i \\cdot \\chi_i(r_1, \\dots, r_{j-1}, c, y_{j+1}, \\dots, y_{\\log N}) \\right) \\cdot \\widetilde{t}(r_1, \\dots, r_{j-1}, c, y_{j+1}, \\dots, y_{\\log N}) \\\\ &amp;amp; = \\sum_{i = (i_1, \\dots, i_{\\log N}) \\in S_u} u_i \\cdot \\chi_i(r_1, \\dots, r_{j-1}, c, i_{j+1}, \\dots, i_{\\log N}) \\cdot \\widetilde{t}(r_1, \\dots, r_{j-1}, c, i_{j+1}, \\dots, i_{\\log N}) \\\\ &amp;amp; = \\sum_{i = (i_1, \\dots, i_{\\log N}) \\in S_u} \\chi_{(i_1, \\dots, i_j)}(r_1, \\dots, r_{j-1}, c) \\cdot u_i \\cdot \\left( \\widetilde{t}(i_1, i_2, \\dots, i_{\\log N}) + \\sum_{k=1}^{j-1} (r_k - i_k) d_k + (c - i_j) d_j \\right) \\\\ &amp;amp; = \\sum_{(b_1, \\dots, b_j) \\in \\{0, 1\\}^j} \\sum_{i = (i_1, \\dots, i_{\\log N}) \\in S_u: (i_1, \\dots, i_j) = (b_1, \\dots, b_j)} u_i \\cdot \\chi_{(b_1, \\dots, b_j)}(r_1, \\dots, r_{j-1}, c) \\cdot \\left( \\widetilde{t}(i_1, i_2, \\dots, i_{\\log N}) + \\sum_{k=1}^{j-1} (r_k - i_k) d_k + (c - i_j) d_j \\right) \\\\ &amp;amp; = \\sum_{(b_1, \\dots, b_j) \\in \\{0, 1\\}^j} \\sum_{i = (i_1, \\dots, i_{\\log N}) \\in S_u: (i_1, \\dots, i_j) = (b_1, \\dots, b_j)} u_i \\cdot \\chi_{(b_1, \\dots, b_j)}(r_1, \\dots, r_{j-1}, c) \\cdot \\widetilde{t}(i_1, i_2, \\dots, i_{\\log N}) \\tag{38} \\\\ &amp;amp; + \\sum_{(b_1, \\dots, b_j) \\in \\{0, 1\\}^j} \\sum_{i = (i_1, \\dots, i_{\\log N}) \\in S_u: (i_1, \\dots, i_j) = (b_1, \\dots, b_j)} u_i \\cdot \\chi_{(b_1, \\dots, b_j)}(r_1, \\dots, r_{j-1}, c) \\cdot \\left( \\sum_{k=1}^{j-1} (r_k - i_k) d_k + (c - i_j) d_j \\right) \\tag{39} \\end{aligned}</span></div>

    <p class="text-gray-300">Recall that <span class="math">Q^{(j)} \\in \\mathbb{F}^{2^j}</span> (respectively <span class="math">Z^{(j)}</span>) is the vector of values assigned to nodes at at depth <span class="math">j</span> of <span class="math">Q</span>. Let <span class="math">v^{(j)}</span> be the length-<span class="math">2^j</span> vector with entries indexed by <span class="math">(b_1, \\ldots, b_j) \\in \\{0, 1\\}^j</span>, with <span class="math">(b_1, \\ldots, b_j)</span>'th entry given by</p>

    <div class="my-4 text-center"><span class="math-block">\\chi_{(b_1, \\dots, b_j)}(r_1, \\dots, r_j). \\tag{40}</span></div>

    <p class="text-gray-300">Let <span class="math">(v&#x27;)^{(j)}</span> be the vector with <span class="math">(b_1, \\ldots, b_j)</span>'th entry given by</p>

    <div class="my-4 text-center"><span class="math-block">\\chi_{(b_1, \\dots, b_j)}(r_1, \\dots, r_{j-1}, c) = v^{(j-1)}[b_1, \\dots, b_{j-1}] \\cdot (b_j c + (1 - b_j)(1 - c)).</span></div>

    <p class="text-gray-300">Note that <span class="math">(v&#x27;)^{(j)}</span> can be computed in <span class="math">O(2^j)</span> time given <span class="math">v^{(j-1)}</span>. And Expression (38) equals</p>

    <div class="my-4 text-center"><span class="math-block">\\langle (v&#x27;)^{(j)}, Q^{(j)} \\rangle.</span></div>

    <p class="text-gray-300">Similarly, let <span class="math">w^{(j)}</span> be the length-<span class="math">2^j</span> vector with entries indexed by <span class="math">(b_1, \\ldots, b_j) \\in \\{0, 1\\}^j</span>, with <span class="math">(b_1, \\ldots, b_j)</span>'th entry given by</p>

    <div class="my-4 text-center"><span class="math-block">\\sum_{k=1}^{j-1} (r_k - i_k) d_k.</span></div>

    <p class="text-gray-300">Let <span class="math">(w&#x27;)^{(j)}</span> be the vector with <span class="math">(b_1, \\ldots, b_j)</span>'th entry given by <span class="math">w^{(j-1)}[b_1, \\ldots, b_j] + (c - b_j) \\cdot d_j</span>. Then Expression (39) equals</p>

    <div class="my-4 text-center"><span class="math-block">\\langle (v&#x27;)^{(j)} \\circ (w&#x27;)^{(j)}, Z^{(j)} \\rangle,</span></div>

    <p class="text-gray-300">where <span class="math">(v&#x27;)^{(j)} \\circ (w&#x27;)^{(j)}</span> denotes the Hadamard (i.e., entry-wise) product of <span class="math">(v&#x27;)^{(j)}</span> and <span class="math">(w&#x27;)^{(j)}</span>.</p>

    <p class="text-gray-300">In summary, we have shown that for <span class="math">j = 1, \\ldots, m</span>, the sum-check prover's <span class="math">j</span>'th message <span class="math">s_j</span> can be computed in time <span class="math">O(2^j)</span> so long as the prover can compute <span class="math">v^{(j)}</span> and <span class="math">w^{(j)}</span> in this time bound. And indeed this is the case. For <span class="math">v^{(j)}</span>, observe that, given all entries of <span class="math">v^{(j-1)}</span>, one can compute <span class="math">v^{(j)}</span> in time <span class="math">O(2^j)</span>. This is because</p>

    <div class="my-4 text-center"><span class="math-block">v^{(j)}[b_1, \\dots, b_j] = v^{(j-1)}[b_1, \\dots, b_{j-1}] \\cdot (r_j b_j + (1 - r_j)(1 - b_j)).</span></div>

    <p class="text-gray-300">Similarly, for <span class="math">w^{(j)}</span>, observe that, given all entries of <span class="math">w^{(j-1)}</span>, one can compute <span class="math">w^{(j)}</span> in time <span class="math">O(2^j)</span>. This is because</p>

    <div class="my-4 text-center"><span class="math-block">w^{(j)}[b_1, \\dots, b_j] = w^{(j-1)}[b_1, \\dots, b_{j-1}] + (r_j - b_j) \\cdot d_j.</span></div>

    <p class="text-gray-300">s.</p>

    <p class="text-gray-300">Rounds <span class="math">\\log(m)+1,\\ldots,2\\log(m).</span> Because the prover’s runtime in round <span class="math">j</span> takes time <span class="math">O(2^{j}),</span> by the time we reach round <span class="math">\\log m,</span> the prover is requiring <span class="math">O(m)</span> time per round. Hence, before the prover proceeds to round <span class="math">m+1,</span> the prover needs to perform a “condensation” operation so that round <span class="math">\\log(m)+1</span> behaves like round <span class="math">1</span> in terms of prover complexity.</p>

    <p class="text-gray-300">Round <span class="math">j=\\log(m)+1</span> of the sparse-dense sum-check protocol is equivalent to round <span class="math">1</span> of the sparse-dense sum-check protocol with the <span class="math">(\\log(N))</span>-variate polynomials <span class="math">\\widetilde{u}</span> and <span class="math">\\widetilde{t}</span> replaced by the following <span class="math">(\\log(N)-\\log(m))</span>-variate polynomials <span class="math">\\widetilde{u}^{\\prime}</span> and <span class="math">\\widetilde{t}^{\\prime}</span>:</p>

    <p class="text-gray-300"><span class="math">\\widetilde{u}^{\\prime}(b_{\\log(m)+1},\\ldots,b_{\\log N})\\coloneqq\\widetilde{u}(r_{1},\\ldots,r_{\\log m},b_{\\log(m)+1},\\ldots,b_{\\log N}),</span> <span class="math">\\widetilde{t}^{\\prime}(b_{\\log(m)+1},\\ldots,b_{\\log N})\\coloneqq\\widetilde{t}(r_{1},\\ldots,r_{\\log m},b_{\\log(m)+1},\\ldots,b_{\\log N}).</span></p>

    <p class="text-gray-300">So we merely need to show that in round <span class="math">m+1</span> of the sparse-dense sum-check protocol, the prover can in <span class="math">O(m)</span> time compute the necessary data structures about <span class="math">\\widetilde{u}^{\\prime}</span> and <span class="math">\\widetilde{t}^{\\prime},</span> namely (per Equations (29) and (30)) the following quantities:</p>

    <p class="text-gray-300"><span class="math">q_{k}^{\\prime}\\coloneqq\\sum_{y\\in\\mathsf{extend}_{\\log(N)-\\log(m)}(k)}\\widetilde{u}^{\\prime}(y)\\cdot\\widetilde{t}^{\\prime}(y)</span> (41) <span class="math">z_{k}^{\\prime}\\coloneqq\\sum_{y\\in\\mathsf{extend}_{\\log(N)-\\log(m)}(k)}\\widetilde{u}^{\\prime}(y).</span> (42)</p>

    <p class="text-gray-300">All <span class="math">m</span> of the <span class="math">z_{k}^{\\prime}</span> can be computed in <span class="math">O(m)</span> time, as <span class="math">z_{k}^{\\prime}=z_{k}\\cdot\\chi_{k}(r_{1},\\ldots,r_{\\log m})</span> and the values</p>

    <p class="text-gray-300"><span class="math">\\{\\chi_{k}(r_{1},\\ldots,r_{\\log m})\\colon k\\in\\{0,1\\}^{\\log m}\\}</span> (43)</p>

    <p class="text-gray-300">can all be computed in <span class="math">O(m)</span> total time, and in fact are precisely the contents of the vector <span class="math">v^{(\\log m)}</span> (see Equation (40)) computed by the prover anyway during the course of the first <span class="math">\\log m</span> rounds of sum-check.</p>

    <p class="text-gray-300">All <span class="math">m</span> of the <span class="math">q_{k}^{\\prime}</span> values can also be computed in <span class="math">O(m)</span> total time. To see this, recall that there are at most <span class="math">m</span> values <span class="math">y=(y_{1},\\ldots,y_{\\log N})\\in\\{0,1\\}^{\\log N}</span> such that <span class="math">\\widetilde{u}(y)\\neq 0</span>. For each such <span class="math">y,</span> let <span class="math">y=(y^{\\prime},y^{\\prime\\prime})\\in\\{0,1\\}^{\\log m}\\times\\{0,1\\}^{\\log(N)-\\log(m)}.</span> Then <span class="math">\\widetilde{u}^{\\prime}(y^{\\prime})=\\sum_{k\\in\\{0,1\\}^{\\log m}}\\chi_{k}(r_{1},\\ldots,r_{\\log m})\\cdot\\widetilde{u}(k,y^{\\prime}).</span> Since the <span class="math">\\chi_{k}(r_{1},\\ldots,r_{\\log m})</span> values (Equation (43)) can all be computed in <span class="math">O(m)</span> total time, this means that all non-zero <span class="math">\\widetilde{u}^{\\prime}(y^{\\prime})</span> values can in turn all be computed in <span class="math">O(m)</span> time. Let <span class="math">S</span> be the set</p>

    <p class="text-gray-300"><span class="math">\\{y^{\\prime}\\in\\{0,1\\}^{\\log(N)-\\log(m)}\\colon\\widetilde{u}^{\\prime}(y)\\neq 0\\},</span></p>

    <p class="text-gray-300">and recall that <span class="math">S</span> has size at most <span class="math">m</span>. For all <span class="math">y^{\\prime}\\in S,</span> we can also compute <span class="math">\\widetilde{t}^{\\prime}(y^{\\prime})</span> in <span class="math">O(m)</span> total time, by the following reasoning.</p>

    <p class="text-gray-300">First, recall from Equation (28) that for round <span class="math">j=\\log m</span> and <span class="math">(b_{1},\\ldots,b_{\\log N})\\in\\{0,1\\}^{\\log N},</span></p>

    <p class="text-gray-300"><span class="math">\\widetilde{t}_{\\ell}(r_{1},\\ldots,r_{j},b_{j+1},\\ldots,b_{\\log N})=\\widetilde{t}_{\\ell}(b_{1},\\ldots,\\ldots,b_{\\log N})+\\sum_{k=1}^{j}(r_{k}-b_{k})\\cdot d_{k}</span></p>

    <p class="text-gray-300">Using dynamic programming, the following values can be computed in total time <span class="math">O(2^{j})=O(m)</span> for all <span class="math">(b_{1},\\ldots,b_{j})\\in\\{0,1\\}^{j}</span>:</p>

    <p class="text-gray-300"><span class="math">\\sum_{k=1}^{j}(r_{k}-b_{k})\\cdot d_{k}.</span> (44)</p>

    <p class="text-gray-300">Indeed, let <span class="math">H^{(j)}</span> be the length <span class="math">2^{j}</span>-array with entries indexed by <span class="math">(b_{1},\\ldots,b_{j})\\in\\{0,1\\}^{j}</span> and such that <span class="math">H^{(j)}[b_{1},\\ldots,b_{j}]=\\sum_{k=1}^{j}(r_{k}-b_{k})\\cdot d_{k}</span>. Then <span class="math">H^{(j+1)}</span> can be computed from <span class="math">H^{(j)}</span> in <span class="math">O(2^{j+1})</span> time, since <span class="math">H^{(j)}[b_{1},\\ldots,b_{j},b_{j+1}]=H^{(j)}[b_{1},\\ldots,b_{j}]+(r_{j+1}-b_{j+1})\\cdot d_{j+1}</span>. This means <span class="math">H^{(\\log m)}</span> can be computed in <span class="math">O(\\sum_{j=1}^{\\log m}2^{j})=O(m)</span> time in total.</p>

    <p class="text-gray-300">The prover, having computed and stored <span class="math">\\widetilde{t}_{\\ell}(y)</span> for all <span class="math">y \\in S</span> in <span class="math">O(m)</span> total time at the start of the protocol (see Footnote 21), can use these values to compute</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde{t}(r_1, \\dots, r_j, b_{j+1}, \\dots, b_{\\log N})</span></div>

    <p class="text-gray-300">for all <span class="math">y \\in S</span> in <span class="math">O(m)</span> total time.</p>

    <p class="text-gray-300"><strong>Remaining rounds.</strong> The prover's computation in the remaining rounds <span class="math">(2\\log(m) + 1, \\dots, \\log N)</span> proceeds analogously to rounds <span class="math">\\log m, \\dots, 2\\log m</span>. Every <span class="math">\\log m</span> rounds, the prover performs a "condensation" operation so that subsequent round behaves like round 1 in terms of prover complexity. This entails computing quantities <span class="math">q_{k,\\ell}^{\\prime}</span> and <span class="math">z_{k}^{\\prime}</span> for each <span class="math">k \\in \\{0,1\\}^{\\log m}</span>, defined analogously to Equations (41) and (42). As above, these <span class="math">m</span> quantities can all be computed in time <span class="math">O(m)</span> per condensation operation. In total, the prover performs <span class="math">O(C)</span> condensation operations, so <span class="math">O(Cm)</span> time is spent on condensation operations. Outside of the condensation operations, the prover implements each "chunk" of <span class="math">\\log m</span> rounds in <span class="math">O(m)</span> time. Since there are <span class="math">O(C)</span> chunks, this means that the total prover time is <span class="math">O(Cm)</span>.</p>

    <p class="text-gray-300">□</p>

    <h2 id="sec-100" class="text-2xl font-bold">G.5.2 An <span class="math">O(cm)</span>-time prover for tables with <span class="math">\\widetilde{t}</span> having total degree larger than one</h2>

    <p class="text-gray-300"><strong>Theorem 9.</strong> Suppose that <span class="math">\\widetilde{t}</span> can be decomposed into a sum of <span class="math">\\eta = O(1)</span> polynomials <span class="math">\\widetilde{t}_1, \\ldots, \\widetilde{t}_\\eta</span>, i.e.,</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde{t} = \\sum_{\\ell=1}^{\\eta} \\widetilde{t}_{\\ell} \\tag{45}</span></div>

    <p class="text-gray-300">and such that the following holds. For any <span class="math">(r_1, \\ldots, r_{j-1}, c) \\in \\mathbb{F}^j</span> and any <span class="math">(b_j, \\ldots, b_{\\log N}) \\in \\{0, 1\\}^{\\log (N) - j+1}</span>, there exist values <span class="math">\\mathsf{a}_{\\ell}(c, j, b_j)</span>, <span class="math">\\mathsf{m}_{\\ell}(c, j, b_j)</span>, each of which can be evaluated in <span class="math">O(1)</span> time, and which do not depend on the variables <span class="math">b_{j+1}, \\ldots, b_{\\log N}</span>, such that:</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde{t}_{\\ell}(r_1, \\dots, r_{j-1}, c, b_{j+1}, \\dots, b_{\\log N}) = \\mathsf{m}_{\\ell}(c, j, b_j) \\cdot \\widetilde{t}_{\\ell}(r_1, \\dots, r_{j-1}, b_j, b_{j+1}, \\dots, b_{\\log N}) + \\mathsf{a}_{\\ell}(c, j, b_j). \\tag{46}</span></div>

    <p class="text-gray-300">Moreover, assume that for each <span class="math">y \\in \\{0,1\\}^{\\log N}</span> such that <span class="math">\\widetilde{t}(y) \\neq 0</span>, and each <span class="math">\\ell = 1, \\ldots, \\kappa</span>, it holds that <span class="math">\\widetilde{t}_{\\ell}(y)</span> can be computed by the prover in <span class="math">O(1)</span> time.²² Then the prover in the sparse-dense sum-check protocol can be implemented in <span class="math">O(m)</span> field operations.</p>

    <p class="text-gray-300">The theorem continues to hold if the values <span class="math">\\mathsf{a}_{\\ell}</span> and <span class="math">\\mathsf{m}_{\\ell}</span> depend on <span class="math">(r_1, \\ldots, r_{j-1})</span> in addition to <span class="math">c</span>, <span class="math">j</span>, and <span class="math">b_j</span>. It also holds if <span class="math">\\mathsf{a}_{\\ell}</span> and <span class="math">\\mathsf{m}_{\\ell}</span> depend on <span class="math">b_{j+1}, \\ldots, b_{j+\\gamma}</span> for some <span class="math">\\gamma = O(1)</span> (in addition to <span class="math">c</span>, <span class="math">j</span>, <span class="math">b_j</span> and <span class="math">(r_1, \\ldots, r_{j-1})</span>).</p>

    <p class="text-gray-300">The last sentence of Theorem 9 is needed to capture the two final example tables in Section F.3.1, namely those capturing bitwise AND and Less-Than (LT) operations (for both of these examples, it suffices to take <span class="math">\\gamma = 1</span>). For example, recall from Equation (22) that for the table <span class="math">t</span> capturing bitwise AND evaluations on <span class="math">b</span>-bit inputs, the following holds:</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde{t}(x, y) = \\sum_{i=1}^{b} 2^{i-1} \\cdot x_i \\cdot y_i.</span></div>

    <p class="text-gray-300">Unfortunately, Theorem 6 does not apply to <span class="math">\\widetilde{t}</span>, which has total degree 2.</p>

    <p class="text-gray-300">Let us order the variables of <span class="math">(x,y)</span> so that <span class="math">x_1</span> comes first and <span class="math">y_1</span> comes second, followed <span class="math">x_2</span> in third and <span class="math">y_2</span> in fourth, and so on. Then for even values of <span class="math">j = 2k</span>, changing the value of the <span class="math">j</span>'th variable from <span class="math">y_k</span> to <span class="math">r_j</span> leads to an additive update to <span class="math">\\widetilde{t}_1(x,y)</span> of</p>

    <div class="my-4 text-center"><span class="math-block">2^{2b+k-1} \\cdot r_{j-1} \\cdot (r_j - y_k),</span></div>

    <p class="text-gray-300">²² As with Footnote 21, if <span class="math">\\widetilde{t}(y)</span> itself cannot be computed in <span class="math">O(1)</span> time for all <span class="math">y \\in \\{0,1\\}^{\\log N}</span> then the correct answer <span class="math">\\langle u,t\\rangle</span> cannot necessarily be computed by the prover in <span class="math">O(m)</span> time.</p>

    <p class="text-gray-300">which depends only on <span class="math">j = 2k</span>, <span class="math">r_{j-1}, r_j</span> and <span class="math">y_k</span>. However, if <span class="math">j = 2k - 1</span> is odd, then the additive "effect" on <span class="math">\\widetilde{t}_1</span> when changing the value of the <span class="math">j</span>th variable from <span class="math">x_j</span> to <span class="math">r_j</span> is</p>

    <div class="my-4 text-center"><span class="math-block">2^{2b + k - 1} \\cdot (r_j - x_k) \\cdot y_k.</span></div>

    <p class="text-gray-300">This depends on variable <span class="math">j + 1</span> (i.e., on <span class="math">y_k</span>).</p>

    <p class="text-gray-300">Conceptually, this means that the value of <span class="math">y_k</span> cannot be "ignored" by the sum-check prover algorithm during round <span class="math">j = 2k - 1</span>, which is the round in which variable <span class="math">x_k</span> is "processed". However, the proof of Theorem 9 shows that this does not substantially affect prover time, essentially because there are only two possible values of <span class="math">y_{k+1} \\in \\{0,1\\}</span> that the algorithm needs to contemplate.</p>

    <p class="text-gray-300">Proof of Theorem 9. We will prove the theorem assuming Equation (46) holds, and then explain what modifications are necessary if <span class="math">\\mathsf{a}_{\\ell}(c,j,b_j)</span> and <span class="math">\\mathsf{m}_{\\ell}(c,j,b_j)</span> have additional dependencies as per the last two sentences of the theorem statement.</p>

    <p class="text-gray-300">A consequence of Equation (46) is that for any <span class="math">j \\in \\{1, \\dots, \\log N\\}</span>, and any <span class="math">(b_1, \\dots, b_{\\log N}) \\in \\{0, 1\\}^{\\log N}</span>,</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde{t}_{\\ell}(r_1, \\dots, r_j, b_{j+1}, \\dots, b_{\\log N}) = \\left(\\prod_{k=1}^{j} \\mathsf{m}_{\\ell}(r_k, k, b_k)\\right) \\cdot \\widetilde{t}_{\\ell}(b_1, \\dots, \\dots, b_{\\log N}) + \\sum_{k=1}^{j} \\mathsf{a}_{\\ell}(r_k, k, b_k). \\tag{47}</span></div>

    <p class="text-gray-300">Values computed by the prover at the start of the protocol. At the start of the protocol, for each polynomial <span class="math">\\widetilde{t}_{\\ell}</span> in the decomposition of <span class="math">\\widetilde{t}</span> of Equation (45), the prover computes the exact same <span class="math">q_{k,\\ell}</span> and <span class="math">z_k</span> values as in Section G.5.1 and builds binary trees <span class="math">Q_{\\ell}</span> and <span class="math">Z</span> over them exactly as in Section G.5.1. That is, for each <span class="math">\\ell = 1, \\dots, \\kappa</span>,</p>

    <div class="my-4 text-center"><span class="math-block">q_{k,\\ell} := \\sum_{y \\in \\text{extend}_{\\log N}(k)} \\widetilde{u}(y) \\cdot \\widetilde{t}_{\\ell}(y) \\tag{48}</span></div>

    <p class="text-gray-300">and <span class="math">z_k</span> is defined exactly as in Definition (30). <span class="math">Q_{\\ell}</span> is a binary tree over the <span class="math">q_{k,\\ell}</span> values, where each internal node stores the sum of its two children, and <span class="math">Z</span> is a binary tree over the <span class="math">z_k</span> values.</p>

    <p class="text-gray-300">The prover's workflow in the first <span class="math">\\log m</span> rounds. Following the derivation in Section G.5.1, we calculate the following convenient expression for the prover's first message polynomial <span class="math">s_1</span>:</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{aligned} s_1(c) &amp;amp;= \\sum_{y \\in \\{0,1\\}^{\\log N}} \\widetilde{u}(c, y_2, \\dots, y_{\\log N}) \\cdot \\widetilde{t}(c, y_2, \\dots, y_{\\log N}) \\\\ &amp;amp;= \\sum_{y \\in \\{0,1\\}^{\\log N}} \\left(\\sum_{i = (i_1, \\dots, i_{\\log N}) \\in S_u} u_i \\cdot \\chi_i(c, y_2, \\dots, y_{\\log N})\\right) \\cdot \\widetilde{t}(c, y_2, \\dots, y_{\\log N}) \\tag{49} \\\\ &amp;amp;= \\sum_{i = (i_1, \\dots, i_{\\log N}) \\in S_u} u_i \\cdot \\chi_i(c, i_2, \\dots, i_{\\log N}) \\cdot \\widetilde{t}(c, i_2, \\dots, i_{\\log N}) \\\\ &amp;amp;= \\sum_{i = (i_1, \\dots, i_{\\log N}) \\in S_u} \\chi_{i_1}(c) \\cdot u_i \\cdot \\chi_i(i_1, i_2, \\dots, i_{\\log N}) \\cdot \\left(\\sum_{\\ell=1}^{\\eta} \\left(\\mathsf{m}_{\\ell}(c, 1, i_1) \\cdot \\widetilde{t}_{\\ell}(i_1, i_2, \\dots, i_{\\log N}) + \\mathsf{a}_{\\ell}(c, 1, i_1)\\right)\\right) \\tag{50} \\end{aligned}</span></div>

    <p class="text-gray-300">Here, Equation (49) invokes Equation (26) and Equation (50) holds by Equations (45) and (46).</p>

    <p class="text-gray-300">Round 1 computation. Expression (50) above equals:</p>

    <p class="text-gray-300">$$ \\begin{array}{l} \\sum_{k = (k_{1},\\dots ,k_{\\log m})\\in \\{0,1\\}^{\\log m}: k_{1} = 0} \\sum_{\\ell = 1}^{\\kappa} \\left(\\chi_{0}(c) \\cdot \\mathfrak{m}_{\\ell}(c,1,0) \\cdot q_{k,\\ell} + \\chi_{0}(c) \\cdot \\mathfrak{a}_{\\ell}(c,1,0) \\cdot z_{k}\\right) \\\\</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>\\sum_{k = (k_{1},\\dots ,k_{\\log m})\\in \\{0,1\\}^{\\log m}: k_{1} = 1} \\sum_{\\ell = 1}^{\\kappa} \\left(\\chi_{1}(c) \\cdot \\mathfrak{m}_{\\ell}(c,1,1) \\cdot q_{k,\\ell} + \\chi_{1}(c) \\cdot \\mathfrak{a}_{\\ell}(c,1,1) \\cdot z_{k}\\right) \\tag{51}</li>

    </ul>

    <p class="text-gray-300">\\end{array} $$</p>

    <p class="text-gray-300">For each <span class="math">c \\in \\{-1,0,1\\}</span>, computing this expression requires just a constant amount of work given the values of the two children of the root vertex of the trees <span class="math">Q_{1}, \\ldots, Q_{\\kappa}</span> (Equations (31) and (32)) and <span class="math">Z_{1}, \\ldots, Z_{\\kappa}</span>.</p>

    <p class="text-gray-300"><strong>Round <span class="math">j &amp;gt; 1</span> computation.</strong> A similar calculation to the above reveals that <span class="math">s_j(c)</span> equals:</p>

    <p class="text-gray-300">$$ \\begin{array}{l} \\sum_{(y_{j+1},\\dots ,y_{\\log N})\\in \\{0,1\\}^{\\log (N)-j}} \\widetilde{u} (r_{1},\\dots ,r_{j-1},c,y_{j+1},\\dots ,y_{\\log N}) \\cdot \\widetilde{t} (r_{1},\\dots ,r_{j-1},c,y_{j+1},\\dots ,y_{\\log N}) \\\\ = \\sum_{(y_{j+1},\\dots ,y_{\\log N})\\in \\{0,1\\}^{\\log (N)-j}} \\left(\\sum_{i = (i_{1},\\dots ,i_{\\log N})\\in S_{u}} u_{i} \\cdot \\chi_{i} \\left(r_{1},\\dots ,r_{j-1},c,y_{j+1},\\dots ,y_{\\log N}\\right)\\right) \\cdot \\widetilde{t} \\left(r_{1},\\dots ,r_{j-1},c,y_{j+1},\\dots ,y_{\\log N}\\right) \\\\ = \\sum_{i = (i_{1},\\dots ,i_{\\log N})\\in S_{u}} u_{i} \\cdot \\chi_{i} \\left(r_{1},\\dots ,r_{j-1},c,i_{j+1},\\dots ,i_{\\log N}\\right) \\cdot \\widetilde{t} \\left(r_{1},\\dots ,r_{j-1},c,i_{j+1},\\dots ,i_{\\log N}\\right) \\\\ = \\sum_{(b_{1},\\dots ,b_{j})\\in \\{0,1\\}^{j}} \\sum_{i = (b_{1},\\dots ,b_{j},i_{j+1},\\dots ,i_{\\log N})\\in S_{u}} u_{i} \\cdot \\chi_{(b_{1},\\dots ,b_{j})} \\left(r_{1},\\dots ,r_{j-1},c\\right) \\cdot \\sum_{\\ell = 1}^{\\kappa} \\left(\\prod_{k = 1}^{\\ell - 1} \\mathfrak{m}_{\\ell} \\left(r_{k},k,b_{k}\\right)\\right) \\widetilde{t}_{\\ell} \\left(i_{1},i_{2},\\dots ,i_{\\log N}\\right) + \\sum_{k = 1}^{j - 1} \\mathfrak{a}_{\\ell} \\left(r_{k},k,b_{k}\\right) \\\\ = \\sum_{(b_{1},\\dots ,b_{j})\\in \\{0,1\\}^{j}} \\sum_{i = (b_{1},\\dots ,b_{j},i_{j+1},\\dots ,i_{\\log N})\\in S_{u}} u_{i} \\cdot \\chi_{(b_{1},\\dots ,b_{j})} \\left(r_{1},\\dots ,r_{j-1},c\\right) \\cdot \\sum_{\\ell = 1}^{\\kappa} \\left(\\prod_{k = 1}^{\\ell - 1} \\mathfrak{m}_{\\ell} \\left(r_{k},k,b_{k}\\right)\\right) \\widetilde{t}_{\\ell} \\left(i_{1},i_{2},\\dots ,i_{\\log N}\\right) \\\\</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>\\sum_{(b_{1},\\dots ,b_{j})\\in \\{0,1\\}^{j}} \\sum_{i = (b_{1},\\dots ,b_{j},i_{j+1},\\dots ,i_{\\log N})\\in S_{u}} u_{i} \\cdot \\chi_{(b_{1},\\dots ,b_{j})} \\left(r_{1},\\dots ,r_{j-1},c\\right) \\left(\\sum_{\\ell = 1}^{\\kappa} \\sum_{k = 1}^{j-1} \\mathfrak{a}_{\\ell} \\left(r_{k},k,b_{k}\\right)\\right). \\tag{53}</li>

    </ul>

    <p class="text-gray-300">\\end{array} $$</p>

    <p class="text-gray-300">Recall that for each <span class="math">\\ell = 1,\\dots ,\\kappa</span>, <span class="math">Q_{\\ell}^{(j)}\\in \\mathbb{F}^{2^j}</span> (respectively <span class="math">Z^{(j)}</span>) is the vector of values assigned to nodes at at depth <span class="math">j</span> of <span class="math">Q_{\\ell}</span>. As in Section G.5.1, let <span class="math">v^{(j)}</span> be the length-<span class="math">2^j</span> vector with entries indexed by <span class="math">(b_{1},\\ldots ,b_{j})\\in \\{0,1\\}^{j}</span>, with <span class="math">(b_{1},\\ldots ,b_{j})</span>'th entry given by</p>

    <div class="my-4 text-center"><span class="math-block">\\chi_{(b_{1},\\dots ,b_{j})} (r_{1},\\dots ,r_{j}).</span></div>

    <p class="text-gray-300">Let <span class="math">(v^{\\prime})^{(j)}</span> be the vector with <span class="math">(b_{1},\\ldots ,b_{j})</span>'th entry given by</p>

    <div class="my-4 text-center"><span class="math-block">\\chi_{(b_{1},\\dots ,b_{j})} (r_{1},\\dots ,r_{j-1},c) = v^{(j-1,\\ell)} [b_{1},\\dots ,b_{j-1}] \\cdot (b_{j}c + (1-b_{j})(1-c)).</span></div>

    <p class="text-gray-300">Note that <span class="math">(v^{\\prime})^{(j)}</span> can be computed in <span class="math">O(2^{j})</span> time given <span class="math">v^{(j-1)}</span>. For each <span class="math">\\ell = 1, \\ldots, k</span>, let <span class="math">x^{(j,\\ell)}</span> denote the vector with entries indexed by <span class="math">b = (b_{1}, \\ldots, b_{j}) \\in \\{0,1\\}^{j}</span> whose <span class="math">b</span>'th entry is <span class="math">\\prod_{k=1}^{j} \\mathfrak{m}_{\\ell}(r_{k}, k, b_{k})</span>, and let <span class="math">(x^{\\prime})^{(j,\\ell)}</span> equal</p>

    <div class="my-4 text-center"><span class="math-block">x^{(j-1,\\ell)} [b_{1},\\dots ,b_{j-1}] \\cdot \\mathfrak{m}_{\\ell}(c,j,b_{j}).</span></div>

    <p class="text-gray-300">Then Expression (52) equals</p>

    <div class="my-4 text-center"><span class="math-block">\\sum_{\\ell = 1}^{\\kappa} \\langle (v^{\\prime})^{(j)} \\circ (x^{\\prime})^{(j,\\ell)}, Q_{\\ell}^{(j)} \\rangle,</span></div>

    <p class="text-gray-300">where <span class="math">(v^{\\prime})^{(j)}\\circ (x^{\\prime})^{(j,\\ell)}</span> denotes the Hadamard (i.e., entry-wise) product of <span class="math">(v^{\\prime})^{(j)}</span> and <span class="math">(x^{\\prime})^{(j,\\ell)}</span>.</p>

    <p class="text-gray-300">53</p>

    <p class="text-gray-300"><span class="math">w^{(j)}</span> be the length-<span class="math">2^{j}</span> vector with entries indexed by <span class="math">(b_{1},\\ldots,b_{j})\\in\\{0,1\\}^{j}</span>, with <span class="math">(b_{1},\\ldots,b_{j})</span>’th entry given by</p>

    <p class="text-gray-300"><span class="math">\\sum_{\\ell=1}^{\\kappa}\\sum_{k=1}^{j-1}\\mathsf{a}_{\\ell}(r_{k},k,b_{k})</span></p>

    <p class="text-gray-300">Let <span class="math">(w^{\\prime})^{(j)}</span> be the vector with <span class="math">(b_{1},\\ldots,b_{j})</span>’th entry given by <span class="math">w^{(j-1)}[b_{1},\\ldots,b_{j}]+\\sum_{\\ell=1}^{\\kappa}\\mathsf{a}_{\\ell}(c,j,b_{j})</span>. Then Expression (53) equals</p>

    <p class="text-gray-300"><span class="math">\\langle(v^{\\prime})^{(j)}\\circ(w^{\\prime})^{(j)},Z^{(j)}\\rangle.</span></p>

    <p class="text-gray-300">In summary, we have shown that for <span class="math">j=1,\\ldots,m</span>, the sum-check prover’s <span class="math">j</span>’th message <span class="math">s_{j}</span> can be computed in time <span class="math">O(\\kappa\\cdot 2^{j})</span> so long as the prover can compute <span class="math">v^{(j)}</span>, <span class="math">w^{(j)}</span>, and <span class="math">x^{(j,\\ell)}</span> for <span class="math">\\ell=1,\\ldots,\\kappa</span> in this time bound. And indeed this is the case. This was explained for <span class="math">v^{(j)}</span> in Section G.5.1. For <span class="math">x^{(j,\\ell)}</span>, observe that, given all entries of <span class="math">x^{(j-1,\\ell)}</span>, one can compute <span class="math">x^{(j,\\ell)}</span> in time <span class="math">O(2^{j})</span>. This is because</p>

    <p class="text-gray-300"><span class="math">x^{(j,\\ell)}[b_{1},\\ldots,b_{j}]=v^{(j-1)}[b_{1},\\ldots,b_{j-1}]\\cdot\\mathsf{m}_{\\ell}(r_{j},j,b_{j}),</span></p>

    <p class="text-gray-300">and we have assumed that <span class="math">\\mathsf{m}_{\\ell}(r_{j},j,b_{j})</span> can be computed in constant time. The case of <span class="math">w^{(j)}</span> is similar.</p>

    <p class="text-gray-300">Rounds <span class="math">\\log(m)+1,\\ldots,2\\log(m).</span> As in Section G.5.1, round <span class="math">j=\\log(m)+1</span> of the sparse-dense sum-check protocol is equivalent to round <span class="math">1</span> of the sparse-dense sum-check protocol with the <span class="math">(\\log(N))</span>-variate polynomials <span class="math">\\widetilde{u}</span> and <span class="math">\\widetilde{t}</span> replaced by the following <span class="math">(\\log(N)-\\log(m))</span>-variate polynomials <span class="math">\\widetilde{u}^{\\prime}</span> and <span class="math">\\widetilde{t}^{\\prime}</span>:</p>

    <p class="text-gray-300"><span class="math">\\widetilde{u}^{\\prime}(b_{\\log(m)+1},\\ldots,b_{\\log N})</span> <span class="math">\\coloneqq\\widetilde{u}(r_{1},\\ldots,r_{\\log m},b_{\\log(m)+1},\\ldots,b_{\\log N}),</span> <span class="math">\\widetilde{t}^{\\prime}(b_{\\log(m)+1},\\ldots,b_{\\log N})</span> <span class="math">\\coloneqq\\widetilde{t}(r_{1},\\ldots,r_{\\log m},b_{\\log(m)+1},\\ldots,b_{\\log N}).</span></p>

    <p class="text-gray-300">For <span class="math">\\ell=1,\\ldots,\\kappa</span>, let</p>

    <p class="text-gray-300"><span class="math">\\widetilde{t}_{\\ell}(b_{\\log(m)+1},\\ldots,b_{\\log N})=\\widetilde{t}_{\\ell}(r_{1},\\ldots,r_{\\log m},b_{\\log(m)+1},\\ldots,b_{\\log N})</span></p>

    <p class="text-gray-300">So we merely need to show that in round <span class="math">m+1</span> of the sparse-dense sum-check protocol, the prover can in <span class="math">O(m)</span> time compute the necessary data structures about <span class="math">\\widetilde{u}^{\\prime}</span> and <span class="math">\\widetilde{t}^{\\prime}_{1},\\ldots,\\widetilde{t}^{\\prime}_{\\ell}</span>, namely (per Equations (48) and (30)) the following quantities:</p>

    <p class="text-gray-300"><span class="math">(q^{\\prime})_{k,\\ell}</span> <span class="math">\\coloneqq\\sum_{y\\in\\mathsf{extend}_{\\log(N)-\\log(m)}(k)}\\widetilde{u}^{\\prime}(y)\\cdot\\widetilde{t}^{\\prime}_{\\ell}(y)</span> (54) <span class="math">z^{\\prime}_{k}</span> <span class="math">\\coloneqq\\sum_{y\\in\\mathsf{extend}_{\\log(N)-\\log(m)}(k)}\\widetilde{u}^{\\prime}(y).</span> (55)</p>

    <p class="text-gray-300">All <span class="math">m</span> of the <span class="math">z^{\\prime}_{k}</span> can be computed in <span class="math">O(m)</span> time, as <span class="math">z^{\\prime}_{k}=z_{k}\\cdot\\chi_{k}(r_{1},\\ldots,r_{\\log m})</span> exactly as per Section G.5.1. All <span class="math">m\\cdot\\kappa</span> of the <span class="math">q^{\\prime}_{k,\\ell}</span> values can also be computed in <span class="math">O(m)</span> total time. To see this, recall that there are at most <span class="math">m</span> values <span class="math">y=(y_{1},\\ldots,y_{\\log N})\\in\\{0,1\\}^{\\log N}</span> such that <span class="math">\\widetilde{u}(y)\\neq 0</span>. For each such <span class="math">y</span>, let <span class="math">y=(y^{\\prime},y^{\\prime\\prime})\\in\\{0,1\\}^{\\log m}\\times\\{0,1\\}^{\\log(N)-\\log(m)}</span>. Then <span class="math">\\widetilde{u}^{\\prime}(y^{\\prime})=\\sum_{k\\in\\{0,1\\}^{\\log m}}\\chi_{k}(r_{1},\\ldots,r_{\\log m})\\cdot\\widetilde{u}(k,y^{\\prime})</span>. Since the <span class="math">\\chi_{k}(r_{1},\\ldots,r_{\\log m})</span> values (Equation (43)) can all be computed in <span class="math">O(m)</span> total time, this means that all non-zero <span class="math">\\widetilde{u}^{\\prime}(y^{\\prime})</span> values can in turn all be computed in <span class="math">O(m)</span> time. Let <span class="math">S</span> be the set</p>

    <p class="text-gray-300"><span class="math">\\{y^{\\prime}\\in\\{0,1\\}^{\\log(N)-\\log(m)}\\colon\\widetilde{u}^{\\prime}(y)\\neq 0\\},</span></p>

    <p class="text-gray-300">and recall that <span class="math">S</span> has size at most <span class="math">m</span>. For all <span class="math">y^{\\prime}\\in S</span>, we can also compute <span class="math">\\widetilde{t}^{\\prime}_{\\ell}(y^{\\prime})</span> and <span class="math">\\ell=1,\\ldots,\\kappa</span> in <span class="math">O(\\kappa\\cdot m)</span> total time, by the following reasoning. First, recall from Equation (47) that for round <span class="math">j=\\log m</span> and <span class="math">(b_{1},\\ldots,b_{\\log N})\\in\\{0,1\\}^{\\log N}</span>,</p>

    <p class="text-gray-300">######</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde {t} _ {\\ell} (r _ {1}, \\dots , r _ {j}, b _ {j + 1}, \\dots , b _ {\\log N}) = \\left(\\prod_ {k = 1} ^ {j} \\mathsf {m} _ {\\ell} (r _ {k}, k, b _ {k})\\right) \\cdot \\widetilde {t} _ {\\ell} (b _ {1}, \\dots , \\dots , b _ {\\log N}) + \\sum_ {k = 1} ^ {j} \\mathsf {a} _ {\\ell} (r _ {k}, k, b _ {k}).</span></div>

    <p class="text-gray-300">Using dynamic programming, the following values can be computed in total time <span class="math">O(2^{j}) = O(m)</span> for all <span class="math">(b_{1},\\ldots ,b_{j})\\in \\{0,1\\}^{j}</span>:</p>

    <div class="my-4 text-center"><span class="math-block">\\prod_ {k = 1} ^ {j} \\mathrm {m} _ {\\ell} \\left(r _ {k}, k, b _ {k}\\right) \\tag {56}</span></div>

    <p class="text-gray-300">and</p>

    <div class="my-4 text-center"><span class="math-block">\\sum_ {k = 1} ^ {j} \\mathrm {a} _ {\\ell} \\left(r _ {k}, k, b _ {k}\\right). \\tag {57}</span></div>

    <p class="text-gray-300">The prover, having computed and stored <span class="math">\\widetilde{t}_{\\ell}(y)</span> for all <span class="math">y \\in S</span> in <span class="math">O(m)</span> total time at the start of the protocol (see Footnote 22), can use these values to compute</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde {t} _ {\\ell} \\left(r _ {1}, \\dots , r _ {j}, b _ {j + 1}, \\dots , b _ {\\log N}\\right)</span></div>

    <p class="text-gray-300">for all <span class="math">y \\in S</span> in <span class="math">O(m)</span> total time.</p>

    <p class="text-gray-300"><strong>Remaining rounds.</strong> The prover's computation in the remaining rounds <span class="math">(2\\log (m) + 1,\\dots ,\\log N)</span> proceeds analogously to rounds <span class="math">\\log m,\\ldots ,2\\log m</span>. Every <span class="math">\\log m</span> rounds, the prover performs a "condensation" operation so that subsequent round behaves like round 1 in terms of prover complexity. This entails computing quantities <span class="math">q_{k,\\ell}^{\\prime}</span> and <span class="math">z_{k}^{\\prime}</span> for each <span class="math">k\\in \\{0,1\\}^{\\log m}</span>. The total prover runtime is <span class="math">O(Cm)</span> as in Section G.5.1.</p>

    <p class="text-gray-300"><strong>Modifications if <span class="math">\\mathsf{a}_{\\ell}(c,j,b_{j})</span> and <span class="math">\\mathsf{m}_{\\ell}(c,j,b_{j})</span> have additional dependencies.</strong> If <span class="math">\\mathsf{a}_{\\ell}(c,j,b_{j})</span> and <span class="math">\\mathsf{m}_{\\ell}(c,j,b_{j})</span> depend on <span class="math">(r_1,\\ldots ,r_{j - 1})</span>, in addition to <span class="math">c, j</span>, and <span class="math">b_{j}</span>, nothing about the prover's computation needs to change because in each round <span class="math">j</span>, there is only one vector <span class="math">(r_1,\\dots ,r_{j - 1})</span> for the algorithm to consider.</p>

    <p class="text-gray-300">If if <span class="math">\\mathsf{a}_{\\ell}</span> and <span class="math">\\mathsf{m}_{\\ell}</span> also depend on <span class="math">b_{j + 1},\\ldots ,b_{j + \\gamma}</span> for some <span class="math">\\gamma = O(1)</span> (in addition to <span class="math">c, j, b_{j}</span> and <span class="math">(r_1,\\dots,r_{j - 1})</span>), then some modifications are required. Conceptually, in each round <span class="math">j</span>, the prover computation groups those <span class="math">y \\in \\{0,1\\}^{\\log N}</span> such that <span class="math">\\widetilde{u}(y) \\neq 0</span> so that elements of the same group all have <span class="math">\\mathsf{a}_{\\ell}</span> and <span class="math">\\mathsf{m}_{\\ell}</span> equal to the same quantities. These groups correspond to the internal nodes at level <span class="math">j</span> of the binary trees <span class="math">Q_{\\ell}</span> and <span class="math">Z</span>. If <span class="math">\\mathsf{a}_{\\ell}</span> and <span class="math">\\mathsf{m}_{\\ell}</span> depend on <span class="math">b_{j + 1},\\ldots ,b_{j + \\gamma}</span>, then the grouping needs to incorporate <span class="math">b_{j + 1},\\ldots ,b_{j + \\gamma}</span> as well. Fortunately, the number of groups under consideration in each round <span class="math">j</span> grows by at most a factor of <span class="math">2^{\\gamma}</span> because <span class="math">(b_{j + 1},\\ldots ,b_{j + \\gamma}) \\in \\{0,1\\}^{\\gamma}</span> can only take <span class="math">2^{\\gamma}</span> values.</p>

    <p class="text-gray-300">Specifically, Equations (51) (capturing the prover's round-one message) is updated to have <span class="math">2^{1 + \\gamma}</span> sums rather than 2 sums. Associating each sum with a bit-vector in <span class="math">\\{0,1\\}^{\\gamma}</span>, the <span class="math">i</span>'th sum is over terms <span class="math">k \\in \\{0,1\\}^{\\log m}</span> that have with <span class="math">(k_1, \\ldots, k_\\gamma) = i</span> (Equation (51) itself corresponds to the case <span class="math">\\gamma = 0</span>). Explicitly, Equation (51) becomes:</p>

    <div class="my-4 text-center"><span class="math-block">\\sum_ {i = (i _ {1}, \\dots , i _ {\\gamma}) \\in \\{0, 1 \\} ^ {\\gamma}} \\sum_ {k = (k _ {1}, \\dots , k _ {\\log m}) \\in \\{0, 1 \\} ^ {\\log m}: (k _ {1}, \\dots , k _ {\\gamma}) = i} \\sum_ {\\ell = 1} ^ {\\kappa} \\left(\\chi_ {i} (c, k _ {2}, \\dots , k _ {\\gamma}) \\cdot \\mathsf {m} _ {\\ell} \\cdot q _ {k, \\ell} + \\chi_ {i} (c, k _ {2}, \\dots , k _ {\\gamma}) \\cdot \\mathsf {a} _ {\\ell} \\cdot z _ {k}\\right),</span></div>

    <p class="text-gray-300">where the quantities <span class="math">\\mathsf{a}_{\\ell}</span> and <span class="math">\\mathsf{m}_{\\ell}</span> may depend on <span class="math">c, k_1, \\ldots, k_\\gamma</span>.</p>

    <p class="text-gray-300">Similarly (52), and (53) are updated to become:</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{array}{l} \\sum_ {(b _ {1}, \\dots , b _ {j}) \\in \\{0, 1 \\} ^ {j}} \\sum_ {i = (b _ {1}, \\dots , b _ {j}, i _ {j + 1}, \\dots , i _ {\\log N}) \\in S _ {u}} u _ {i} \\cdot \\chi_ {(b _ {1}, \\dots , b _ {j + \\gamma})} (r _ {1}, \\dots , r _ {j - 1}, c, b _ {j + 1}, \\dots , b _ {j + \\gamma}) \\left(\\sum_ {\\ell = 1} ^ {\\kappa} \\left(\\prod_ {k = 1} ^ {j - 1} \\mathsf {m} _ {\\ell} (r _ {k}, k, b _ {k})\\right) \\widetilde {t} _ {\\ell} (i _ {1}, i _ {2}, \\dots , i _ {\\log N})\\right) \\\\ + \\sum_ {(b _ {1}, \\dots , b _ {j + \\gamma}) \\in \\{0, 1 \\} ^ {j + \\gamma}} \\sum_ {i = (b _ {1}, \\dots , b _ {j + \\gamma}, i _ {j + \\gamma}, \\dots , i _ {\\log N}) \\in S _ {u}} u _ {i} \\cdot \\chi_ {(b _ {1}, \\dots , b _ {j + \\gamma})} (r _ {1}, \\dots , r _ {j - 1}, c, b _ {j + 1}, \\dots , b _ {j + \\gamma}) \\left(\\sum_ {\\ell = 1} ^ {\\kappa} \\sum_ {k = 1} ^ {j - 1} \\mathsf {a} _ {\\ell} (r _ {k}, k, b _ {k})\\right). \\\\ \\end{array}</span></div>

    <p class="text-gray-300">Here, we write <span class="math">\\mathsf{a}_{\\ell}(r_k,k,b_k)</span> and <span class="math">\\mathsf{m}_{\\ell}(r_k,k,b_k)</span> for simplicity and consistency with Equations (52) and (53), but these quantities may in general depend on <span class="math">k,r_1,\\ldots,r_k</span> and <span class="math">b_{k+1},\\ldots,b_{k+\\gamma}</span>. <span class="math">\\square</span></p>

    <h2 id="sec-101" class="text-2xl font-bold">Applications to the AND and LT tables.</h2>

    <p class="text-gray-300">The discussion following the statement of Theorem 9 explained that the theorem applied to the lookup table for the AND instruction, whose <span class="math">(x,y)</span>'th entry is <span class="math">\\sum_{i=1}^{b} 2^{i-1} x_i \\cdot y_i</span>.</p>

    <p class="text-gray-300">Recall from Section F.3.1 that LT denotes the function that takes two <span class="math">b</span>-bit inputs <span class="math">x</span> and <span class="math">y</span> as input and outputs 1 if <span class="math">x &amp;lt; y</span> and 0 otherwise. The appropriate lookup table to capture this function has <span class="math">(x,y)</span>'th entry equal to <span class="math">\\mathsf{LT}(x,y)</span>. Let <span class="math">\\widetilde{\\mathsf{LT}}</span> denote the multilinear extension of the function <span class="math">\\mathsf{LT} \\colon \\{0,1\\}^b \\times \\{0,1\\}^b \\to \\mathbb{F}</span>.</p>

    <h2 id="sec-102" class="text-2xl font-bold">Deriving an expression for <span class="math">\\widetilde{\\mathsf{LT}}</span>.</h2>

    <p class="text-gray-300">Let <span class="math">x = (x_{1},\\ldots ,x_{b})</span> and <span class="math">y = (y_{1},\\dots ,y_{b})</span>.</p>

    <p class="text-gray-300">For <span class="math">i = 2,\\ldots ,b</span>, define <span class="math">x_{&amp;gt;i} = (x_{i + 1},\\ldots ,x_b)</span>, and define</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde {\\mathsf {L T}} _ {i} (x, y) = (1 - x _ {i}) \\cdot y _ {i} \\cdot \\widehat {\\mathsf {e q}} (x _ {&amp;gt; i}, y _ {&amp;gt; i}),</span></div>

    <p class="text-gray-300">where recall that <span class="math">\\widehat{\\mathsf{eq}}</span> was defined in Equation (1).</p>

    <p class="text-gray-300">We claim that</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde {\\mathsf {L T}} (x, y) = \\sum_ {i = 1} ^ {b} \\widetilde {\\mathsf {L T}} _ {i} (x, y). \\tag {58}</span></div>

    <p class="text-gray-300">Indeed, the right hand side of this equation is multilinear and agrees with <span class="math">\\mathsf{LT}(x,y)</span> at all inputs <span class="math">x,y\\in \\{0,1\\} ^b</span>. This is because, if <span class="math">j</span> is the highest-order bit that differs between <span class="math">x</span> and <span class="math">y</span>, then <span class="math">\\widetilde{\\mathsf{LT}}_{j&#x27;}(x,y) = 0</span> for all <span class="math">j^{\\prime}\\neq j</span> and <span class="math">\\widetilde{\\mathsf{LT}}_j(x,y) = 1</span> if and only if <span class="math">x_{j} = 0</span> and <span class="math">y_{j} = 1</span>.</p>

    <h2 id="sec-103" class="text-2xl font-bold">Showing that <span class="math">\\widetilde{\\mathsf{LT}}(x, y)</span> satisfies the requirement of Theorem 9 so long as <span class="math">m \\geq \\log N</span>.</h2>

    <p class="text-gray-300">Let us order the <span class="math">2b</span> variables of <span class="math">(x, y)</span> (i.e., the variables of the polynomial <span class="math">\\widetilde{\\mathsf{LT}}</span>) so that the low-order bits <span class="math">x_1</span> and <span class="math">y_1</span> get bound in the first two rounds of sparse-dense sum-check, <span class="math">x_2</span> and <span class="math">y_2</span> get bound in the next two rounds, and so on. For any <span class="math">(r_1, \\ldots, r_{j-1}, r_j) \\in \\mathbb{F}^j</span> and any <span class="math">(z_j, \\ldots, z_{2b}) \\in \\{0, 1\\}^{2b-j+1}</span>, we must show that there exist values <span class="math">\\mathsf{a}_\\ell</span>, <span class="math">\\mathsf{m}_\\ell</span>, each of which can be evaluated in <span class="math">O(1)</span> time, and which do not depend on the variables <span class="math">z_{j+2}, \\ldots, z_{2b}</span>, such that:</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde {\\mathsf {L T}} \\left(r _ {1}, \\dots , r _ {j - 1}, r _ {j}, z _ {j + 1}, \\dots , z _ {2 b}\\right) = \\mathsf {m} \\cdot \\widetilde {\\mathsf {L T}} \\left(r _ {1}, \\dots , r _ {j - 1}, z _ {j}, z _ {j + 1}, \\dots , z _ {2 b}\\right) + \\mathsf {a}. \\tag {59}</span></div>

    <p class="text-gray-300">Say that <span class="math">j = 2k</span> is even and let us write</p>

    <div class="my-4 text-center"><span class="math-block">x = \\left(r _ {1}, r _ {3}, r _ {5}, \\dots , r _ {j - 1}, z _ {j + 1}, \\dots , z _ {2 b - 1}\\right)</span></div>

    <p class="text-gray-300">and</p>

    <div class="my-4 text-center"><span class="math-block">y ^ {\\prime} = \\left(r _ {2}, r _ {4}, r _ {6}, \\dots , r _ {j}, z _ {j + 2}, \\dots , z _ {2 b}\\right).</span></div>

    <p class="text-gray-300">Let</p>

    <div class="my-4 text-center"><span class="math-block">y = \\left(r _ {2}, r _ {4}, r _ {6}, \\dots , r _ {j - 1}, z _ {j}, z _ {j + 2}, \\dots , z _ {2 b}\\right).</span></div>

    <p class="text-gray-300">Let <span class="math">k^{<em>}</span> be the highest-order bit such that <span class="math">x_{k^{</em>}} \\neq y_{k^{*}}</span>.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>If <span class="math">k &amp;lt; k^{<em>}</span>, then <span class="math">\\widetilde{t}(x, y) - \\widetilde{t}(x, y&#x27;) = 0</span>. This holds by the following reasoning. For all <span class="math">i \\leq k</span>, <span class="math">\\widetilde{\\mathsf{LT}}_i(x, y) = \\widetilde{\\mathsf{LT}}_i(x, y) = 0</span> due to the factor <span class="math">\\widehat{\\mathsf{eq}}(x_{&amp;gt;i}, y_{&amp;gt;i})</span> appearing in <span class="math">\\widetilde{\\mathsf{LT}}_i</span> and the fact that <span class="math">x_{k^</em>} \\neq y_{k^*}</span>. And for <span class="math">i &amp;gt; k</span>, <span class="math">\\widetilde{\\mathsf{LT}}_i(x, y) = \\widetilde{\\mathsf{LT}}_i(x, y&#x27;)</span> because <span class="math">y</span> and <span class="math">y&#x27;</span> differ only in the <span class="math">k</span>'th coordinate, and for each <span class="math">i &amp;gt; k</span>, <span class="math">\\widetilde{\\mathsf{LT}}_i</span> does not depend on inputs <span class="math">1, \\ldots, i-1</span>.</li>

    </ul>

    <p class="text-gray-300">56</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Suppose <span class="math">k \\geq k^*</span>. Then:</li>

    </ul>

    <div class="my-4 text-center"><span class="math-block">\\begin{aligned} \\widetilde{\\mathrm{LT}}(x, y&#x27;) &amp;amp;= \\sum_{i=1}^{b} \\widetilde{\\mathrm{LT}}_i(x, y&#x27;) = \\sum_{i=1}^{k} (1 - r_{2i-1}) \\cdot r_{2i} \\cdot \\widetilde{\\mathrm{eq}}(x_{&amp;gt;i}, y_{&amp;gt;i}&#x27;) + \\sum_{i=k+1}^{b} \\mathrm{LT}_i(x, y&#x27;) \\\\ &amp;amp;= \\sum_{i=1}^{k} \\left( (1 - r_{2i-1}) \\cdot r_{2i} \\cdot \\prod_{j=i+1}^{b} (x_i y_i&#x27; + (1 - x_i)(1 - y_i&#x27;)) \\right) + \\sum_{j=k+1}^{b} \\mathrm{LT}_j(x, y&#x27;) \\\\ &amp;amp;= \\sum_{i=1}^{k} \\left( (1 - r_{2i-1}) \\cdot r_{2i} \\cdot \\prod_{j=i+1}^{b} (x_i y_i&#x27; + (1 - x_i)(1 - y_i&#x27;)) \\right) \\end{aligned} \\tag{60}</span></div>

    <p class="text-gray-300">Here, Equation (60) holds because <span class="math">x_i = y_i = y_i&#x27;</span> for all <span class="math">i &amp;gt; k</span> by assumption that <span class="math">k^*</span> is the most significant index at which <span class="math">x</span> and <span class="math">y</span> differ.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Suppose <span class="math">k &amp;gt; k^*</span>. Then (assuming each <span class="math">r_j \\notin \\{0,1\\}</span>) Equation (60) equals</li>

    </ul>

    <div class="my-4 text-center"><span class="math-block">\\mathrm{LT}(x, y) \\cdot \\frac{r_{j-1} z_j + (1 - r_{j-1})(1 - z_j)}{r_{j-1} r_j + (1 - r_{j-1})(1 - r_j)}.</span></div>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Suppose <span class="math">k = k^*</span>. Then Equation (60) equals</li>

    </ul>

    <div class="my-4 text-center"><span class="math-block">\\begin{aligned} \\sum_{i=1}^{k-1} \\left( (1 - r_{2i-1}) \\cdot r_{2i} \\cdot \\prod_{j=i+1}^{k} (x_i y_i&#x27; + (1 - x_i)(1 - y_i&#x27;)) \\right) + (1 - r_{2k-1}) \\cdot r_{2k} \\\\ = (1 - r_{j-1}) \\cdot r_j + \\sum_{i=1}^{k-1} \\left( (1 - r_{2i-1}) \\cdot r_{2i} \\cdot \\left( \\prod_{j=i+1}^{k-1} (x_i y_i&#x27; + (1 - x_i)(1 - y_i&#x27;)) \\right) \\right) \\cdot (r_{j-1} r_j + (1 - r_{j-1})(1 - r_j)). \\end{aligned}</span></div>

    <p class="text-gray-300">Here, we have used that <span class="math">x_i = y_i</span> and <span class="math">x_i, y_i \\in \\{0,1\\}</span> for all <span class="math">i &amp;gt; k^*</span>. Meanwhile, <span class="math">\\mathrm{LT}(x,y)</span> equals</p>

    <div class="my-4 text-center"><span class="math-block">(1 - r_{j-1}) \\cdot z_j + \\sum_{i=1}^{k-1} \\left( (1 - r_{2i-1}) \\cdot r_{2i} \\cdot \\left( \\prod_{j=i+1}^{k-1} (x_i y_i&#x27; + (1 - x_i)(1 - y_i&#x27;)) \\right) \\right) \\cdot (r_{j-1} z_j + (1 - r_{j-1})(1 - z_j)).</span></div>

    <p class="text-gray-300">Let</p>

    <div class="my-4 text-center"><span class="math-block">\\beta = \\frac{r_{j-1} r_j + (1 - r_{j-1})(1 - r_j)}{r_{j-1} z_j + (1 - r_{j-1})(1 - z_j)}. \\tag{61}</span></div>

    <p class="text-gray-300">Hence,</p>

    <div class="my-4 text-center"><span class="math-block">\\mathrm{LT}(x, y&#x27;) = \\mathrm{LT}(x, y) \\cdot \\beta + (1 - r_{2k-1}) \\cdot r_{2k} - \\beta (1 - r_{j-1}) \\cdot z_j.</span></div>

    <p class="text-gray-300">Summarizing, if we are in round <span class="math">j = 2k</span> with <span class="math">k &amp;lt; k^<em></span> where <span class="math">k^</em></span> is the most significant where <span class="math">x</span> and <span class="math">y</span> differ, then <span class="math">\\mathsf{a} = 0</span> and <span class="math">\\mathsf{m} = 1</span>. If <span class="math">k &amp;gt; k^*</span> then <span class="math">\\mathsf{a} = 0</span> and</p>

    <div class="my-4 text-center"><span class="math-block">\\mathsf{m} = \\frac{r_{j-1} r_j + (1 - r_{j-1})(1 - r_j)}{r_{j-1} z_j + (1 - r_{j-1})(1 - z_j)}.</span></div>

    <p class="text-gray-300">If <span class="math">k = k^*</span> then <span class="math">\\mathsf{m} = \\beta</span> (defined in Equation (61)) and</p>

    <div class="my-4 text-center"><span class="math-block">\\mathsf{a} = (1 - r_{2k-1}) \\cdot r_{2k} - \\beta (1 - r_{j-1}) \\cdot z_j.</span></div>

    <p class="text-gray-300">23Note that while multiplicative inverses take super-constant time to compute, the denominator of the fraction only involves <span class="math">r_{j-1}</span>, and <span class="math">z_j</span>, and hence only takes two different values, as <span class="math">r_{j-1}</span> is fixed by the verifier before starting round <span class="math">j</span> and <span class="math">z_j</span> is only ever 0 or 1. That is, the prover does not have to compute a different inverse for each tuple <span class="math">(x,y)</span> with <span class="math">\\widetilde{u}(x,y) \\neq 0</span>.</p>

    <p class="text-gray-300">57</p>

    <p class="text-gray-300">The case of <span class="math">j = 2k - 1</span> is similar and we highlight the main differences. In this case, let us write</p>

    <div class="my-4 text-center"><span class="math-block">x&#x27; = (r_1, r_3, r_5, \\dots, r_j, z_{j+2}, \\dots, z_{2b-1})</span></div>

    <p class="text-gray-300">and</p>

    <div class="my-4 text-center"><span class="math-block">x = (r_1, r_3, r_5, \\dots, z_j, z_{j+2}, \\dots, z_{2b-1}),</span></div>

    <p class="text-gray-300">and</p>

    <div class="my-4 text-center"><span class="math-block">y = (r_2, r_4, r_6, \\dots, r_{j-1}, z_j, z_{j+2}, \\dots, z_{2b}).</span></div>

    <p class="text-gray-300">Then we have the following analog of Equation (60):</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{aligned} \\widetilde{\\mathrm{LT}}(x&#x27;, y) &amp;amp;= \\sum_{i=1}^{b} \\widetilde{\\mathrm{LT}}_i(x&#x27;, y) = \\sum_{i=1}^{k} (1 - r_{2i-1}) \\cdot r_{2i} \\cdot \\widetilde{\\mathrm{eq}}(x_{&amp;gt;i}, y_{&amp;gt;i}&#x27;) + \\sum_{i=k+1}^{b} \\mathrm{LT}_i(x&#x27;, y) \\\\ &amp;amp;= \\sum_{i=1}^{k} \\left( (1 - r_{2i-1}) \\cdot r_{2i} \\cdot \\prod_{j=i+1}^{b} (x_i&#x27; y_i + (1 - x_i&#x27;) (1 - y_i)) \\right) + \\sum_{j=k+1}^{b} \\mathrm{LT}_j(x&#x27;, y) \\\\ &amp;amp;= \\sum_{i=1}^{k} \\left( (1 - r_{2i-1}) \\cdot r_{2i} \\cdot \\prod_{j=i+1}^{b} (x_i&#x27; y_i + (1 - x_i&#x27;) (1 - y_i)) \\right) \\end{aligned} \\tag{62}</span></div>

    <p class="text-gray-300">The main difference when <span class="math">j = 2k - 1</span> compared to the analysis for <span class="math">j = 2k</span> lies in the case that <span class="math">k = k^*</span>. In this case, Equation (62) equals</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{aligned} \\sum_{i=1}^{k-1} \\left( (1 - r_{2i-1}) \\cdot r_{2i} \\cdot \\prod_{j=i+1}^{k} (x_i&#x27; y_i + (1 - x_i&#x27;) (1 - y_i)) \\right) + (1 - r_{2k-1}) \\cdot y_k \\\\ = (1 - r_j) \\cdot y_k + \\sum_{i=1}^{k-1} \\left( (1 - r_{2i-1}) \\cdot r_{2i} \\cdot \\left( \\prod_{j=i+1}^{k-1} (x_i&#x27; y_i + (1 - x_i&#x27;) (1 - y_i)) \\right) \\right) \\cdot (r_j y_k + (1 - r_j) (1 - y_k)) \\end{aligned}</span></div>

    <p class="text-gray-300">Meanwhile, <span class="math">\\mathrm{LT}(x,y)</span> equals</p>

    <div class="my-4 text-center"><span class="math-block">(1 - x_k) \\cdot y_k + \\sum_{i=1}^{k-1} \\left( (1 - r_{2i-1}) \\cdot r_{2i} \\cdot \\left( \\prod_{j=i+1}^{k-1} (x_i&#x27; y_i + (1 - x_i&#x27;) (1 - y_i)) \\right) \\right) \\cdot (x_k y_k + (1 - x_k) (1 - y_k)) = -0,</span></div>

    <p class="text-gray-300">where we have used the fact that <span class="math">k = k^<em></span> and <span class="math">x_{k^</em>} \\neq y_{k^*}</span> by assumption.</p>

    <p class="text-gray-300">Hence,</p>

    <div class="my-4 text-center"><span class="math-block">\\mathrm{LT}(x&#x27;, y) = \\mathrm{LT}(x, y) + (1 - r_j) \\cdot y_j - (1 - x_j) \\cdot y_j + \\eta</span></div>

    <p class="text-gray-300">where <span class="math">\\eta</span> equals</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{aligned} &amp;amp;= \\sum_{i=1}^{k-1} \\left( (1 - r_{2i-1}) \\cdot r_{2i} \\cdot \\left( \\prod_{j=i+1}^{k-1} (x_i&#x27; y_i + (1 - x_i&#x27;) (1 - y_i)) \\right) \\right) \\cdot (r_j y_k + (1 - r_j) (1 - y_k)) \\\\ &amp;amp;= \\sum_{i=1}^{k-1} \\left( (1 - r_{2i-1}) \\cdot r_{2i} \\cdot \\left( \\prod_{j=i+1}^{k-1} (r_{2i-1} r_{2i} + (1 - r_{2i-1}) (1 - r_{2i})) \\right) \\right) \\cdot (r_j y_k + (1 - r_j) (1 - y_k)). \\end{aligned} \\tag{63}</span></div>

    <p class="text-gray-300">In this case, we can set <span class="math">\\mathsf{a} = (1 - r_j) \\cdot y_j - (1 - x_j) \\cdot y_j + \\eta</span> and <span class="math">\\mathsf{m} = 1</span>. The prover can devote <span class="math">O(\\log N)</span> total time over the entire course of the sum-check protocol to ensure that <span class="math">\\eta</span> can always be computed in <span class="math">O(1)</span> time.</p>

    <p class="text-gray-300">58</p>

    <p class="text-gray-300">That is, at all odd rounds <span class="math">j = 2k - 1</span> of the sparse-dense sum-check protocol it will update the quantity</p>

    <div class="my-4 text-center"><span class="math-block">\\sum_{i=1}^{k-1} \\left( (1 - r_{2i-1}) \\cdot r_{2i} \\cdot \\left( \\prod_{j=i+1}^{k-1} (r_{2i-1} r_{2i} + (1 - r_{2i-1})(1 - r_{2i})) \\right) \\right).</span></div>

    <p class="text-gray-300">Note that when moving from round <span class="math">j</span> to round <span class="math">j + 2</span> this quantity can be updated in <span class="math">O(1)</span> time. Moreover, given this quantity, for either of the two possible values of <span class="math">y_k \\in \\{0,1\\}</span>, <span class="math">\\eta</span> can be computed in <span class="math">O(1)</span> time.</p>

    <p class="text-gray-300">59</p>`;
---

<BaseLayout title="Unlocking the lookup singularity with Lasso (2023/1216)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2023 &middot; eprint 2023/1216
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <p class="mt-4 text-xs text-gray-500">
        All content below belongs to the original authors. This page
        reproduces the paper for educational purposes. Always
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >cite the original</a>.
      </p>
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <Fragment set:html={CONTENT} />

  </article>
</BaseLayout>
