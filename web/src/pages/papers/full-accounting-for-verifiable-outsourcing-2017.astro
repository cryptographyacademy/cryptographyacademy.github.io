---
import BaseLayout from '../../layouts/BaseLayout.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2017/242';
const CRAWLER = 'mistral';
const CONVERTED_DATE = '2026-02-16';
const TITLE_HTML = 'Full accounting for verifiable outsourcing';
const AUTHORS_HTML = 'Riad S.  Wahby, Ye Ji, Andrew J.  Blumberg, abhi shelat, Justin Thaler, Michael Walfish, Thomas Wies';

const CONTENT = `    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Riad S. Wahby*</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Ye Ji°</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Andrew J. Blumberg†</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">rsw@cs.stanford.edu</td>

            <td class="px-3 py-2 border-b border-gray-700">yj682@nyu.edu</td>

            <td class="px-3 py-2 border-b border-gray-700">blumberg@math.utexas.edu</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">abhi shelat‡</td>

            <td class="px-3 py-2 border-b border-gray-700">Justin Thaler^</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">Michael Walfish°</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">abhi@neu.edu</td>

            <td class="px-3 py-2 border-b border-gray-700">justin.thaler@georgetown.edu</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">mwalfish@cs.nyu.edu</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">*Stanford °NYU †UT Austin ‡Northeastern ^Georgetown</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">Thomas Wies°</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">September 5, 2017</p>

    <p class="text-gray-300">Abstract. Systems for verifiable outsourcing incur costs for a prover, a verifier, and precomputation; outsourcing makes sense when the combination of these costs is cheaper than not outsourcing. Yet, when prior works impose quantitative thresholds to analyze whether outsourcing is justified, they generally ignore prover costs. Verifiable ASICs (VA)—in which the prover is a custom chip—is the other way around: its cost calculations ignore precomputation.</p>

    <p class="text-gray-300">This paper describes a new VA system, called Giraffe; charges Giraffe for all three costs; and identifies regimes where outsourcing is worthwhile. Giraffe's base is an interactive proof geared to data-parallel computation. Giraffe makes this protocol asymptotically optimal for the prover and improves the verifier's main bottleneck by almost  <span class="math">3 \\times</span> , both of which are of independent interest. Giraffe also develops a design template that produces hardware designs automatically for a wide range of parameters, introduces hardware primitives molded to the protocol's data flows, and incorporates program analyses that expand applicability. Giraffe wins even when outsourcing several tens of sub-computations, scales to  <span class="math">500 \\times</span>  larger computations than prior work, and can profitably outsource parts of programs that are not worthwhile to outsource in full.</p>

    <p class="text-gray-300">In probabilistic proofs—Interactive Proofs (IPs) [12, 49, 50, 58, 76], arguments [30, 52, 54, 62], SNARs [48], SNARKs [26, 47], and PCPs [9, 10]—a prover efficiently convinces a verifier of a claim, in such a way that the verifier is highly likely to reject a false claim. These protocols are foundational in complexity theory and cryptography. There has also been substantial progress in implementations over the last six years [14, 15, 17–19, 21, 22, 32, 34, 36, 37, 39, 40, 42, 45, 47, 56, 64, 66, 72–75, 77, 79, 81–83, 87] (for a survey, see [84]), based on theoretical refinements and systems work.</p>

    <p class="text-gray-300">A central application example is verifiable outsourcing. The verifier specifies a computation and input; the prover returns the (purported) output and proves the claim that "the returned output equals the computation applied to the input." The essential property here is that the verifier's probabilistic checks are asymptotically less expensive than executing the computation; as a result, outsourcing can be worthwhile for the verifier. This picture partially motivated the original theory [13, 46, 49, 62] and has reappeared in tales of outsourcing to the cloud. But to validate these stories, one must consider three kinds of costs:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Prover overhead. Even in the best general-purpose probabilistic proof protocols, the prover has enormous overhead in running the protocol versus simply executing the underlying computation: the ratio between these is typically at least  <span class="math">10^{7}</span>  [84, Fig. 5].</li>

      <li>Precomputation. Many of the implemented protocols require a setup phase, performed by the verifier or a party that the verifier trusts. This phase is required for each computation and can be reused over</li>

    </ul>

    <p class="text-gray-300">different input-output instances. Its costs are usually proportional to the time to run the computation. (Precomputation can be asymptotically suppressed or even eliminated, but at vastly higher concrete cost <em>[17, 21, 22, 34]</em>; see §10.)</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><em>Verifier overhead.</em> Separate from precomputation, there are inherent costs that the verifier incurs for each input-output instance. These costs are at least linear in the input and output lengths.</li>

    </ul>

    <p class="text-gray-300">More or less tacitly, “practical” work in this area has bundled in assumptions about the regimes in which these costs are reasonable for the operator of the verifier. For example, one way to tame the costs is not to charge the operator for precomputation. This is the approach taken in Pinocchio, which focuses on per-instance verifier overhead <em>[66, 67]</em>. This choice can be justified if there is a trusted third party with extremely inexpensive cycles.</p>

    <p class="text-gray-300">Another possibility is to target data-parallel computations, meaning identical sub-computations on different inputs. Here, one can charge the operator of the verifier for the precomputation, which amortizes, and then identify <em>cross-over points</em> where the verifier saves work from outsourcing <em>[32, 37, 66, 73, 74, 75, 81, 83, 87]</em>.</p>

    <p class="text-gray-300">In both of these cases, prover overhead is measured but in some sense ignored (when considering whether outsourcing is worthwhile). This would make sense if the prover’s cycles were vastly cheaper than the verifier’s—the required ratio is approximately the prover’s overhead: <span class="math">10^{7}\\times</span>—or if the outsourced computation could not be executed in any other way.</p>

    <p class="text-gray-300">Recently, Zebra <em>[82]</em> used a different justification by observing that one can gain high-assurance execution of custom chips (ASICs) by using trusted slow chips to verify the outputs of untrusted fast chips. In this <em>Verifiable ASICs</em> (VA) domain (§2.3), one can charge the operator for both verifier and prover and still identify regimes where their combination outperforms a baseline of executing the functionality in a trusted slow chip. However, Zebra does not charge for precomputation—and worse, introduces a preposterous assumption about daily delivery of hard drives to handle the problem.</p>

    <p class="text-gray-300">The work of this paper is to create a system, Giraffe; to <em>charge the operator for all three costs</em>; and to seek out regimes where this combined cost is superior to the baseline. Giraffe builds on Zebra and likewise targets the VA setting. However, some of Giraffe’s results and techniques apply to verifiable outsourcing more generally.</p>

    <p class="text-gray-300">Giraffe has two high-level aspects. The first is a new probabilistic proof built on a protocol that we call <em>T13</em> <em>[77, §7]</em>. As with all work in this area, T13 requires computations to be expressed as <em>arithmetic circuits</em>, or ACs (§2.1). T13 has three key advantages: (a) T13 is a variant of CMT <em>[36, 49]</em>, which is Zebra’s base, and thus promises amenability to hardware implementation; (b) in the VA context, T13 can in principle pay for precomputation and break even, because it is geared to the aforementioned data-parallel model: precomputation is proportional to one sub-computation, and amortizes over <span class="math">N</span> sub-computations; and (c) T13 ought to permit breaking even for small <span class="math">N</span>: CMT has low overhead compared to alternatives <em>[84]</em>. From this starting point, Giraffe does the following (§3):</p>

    <p class="text-gray-300">1A variant of this story, exploiting the zero knowledge property of some probabilistic proofs, such as SNARKs [26, 47], includes applications where the proof can incorporate input hidden from the verifier [18, 39, 64, 66]. Here, one does not obsess over the verifier’s overhead or total cost comparisons because the verifier cannot execute locally. Nevertheless, identifying regimes where overhead is reasonable similarly requires some effort. We do not discuss in detail, but see §9 and §10.</p>

    <p class="text-gray-300">2Pinocchio certainly considers precomputation [66, §5.3], but its emphasized comparison is between native execution and verifier overhead.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Giraffe improves T13. Most significantly, Giraffe makes the prover asymptotically time-optimal: for sufficiently large <span class="math">N</span>, the prover’s work is now only a multiple (<span class="math">\\approx 10\\times</span>) of executing the AC (§3.1). This can save an order of magnitude or more for any implementation of T13 in any context (for example, vSQL <em>[87]</em>), and is of independent interest.</li>

      <li>Giraffe develops a <em>design template</em> that automatically instantiates physically realizable, efficient, high-throughput ASIC designs for the prover and verifier. The basic challenge is that, consistent with our search for applicable regimes, there are variables with wide ranges: small and large <span class="math">N</span>, different hardware substrates, etc. As a result, the optimal architectures are diverse. For example, large ACs (large sub-computations and/or large <span class="math">N</span>) must iteratively reuse the underlying hardware whereas small ACs call for high parallelism. Giraffe responds with the <em>RWSR</em>: a new hardware structure that, when applied to the data flows in T13, both runs efficiently in serial execution and parallelizes easily.</li>

      <li>Giraffe demonstrates algorithmic improvements that apply to all CMT-based systems <em>[36, 77, 79, 81, 82, 87]</em>. This includes reducing the verifier’s main bottleneck by <span class="math">\\approx</span>3<span class="math">\\times</span> (§3.3), eliminating a log factor from one of the verifier’s other computations by shifting additional work to the prover, and other optimizations that result in constant-factor improvements (Appx. B.2).</li>

    </ul>

    <p class="text-gray-300">The second aspect of Giraffe is motivated by our search for applicable regimes. In existing systems, protocol overhead limits the maximum size of a computation that can be outsourced. Worse, outsourcing really makes sense only if the computation is naturally expressed as an AC; otherwise, the asymptotic savings do not apply until program sizes are well beyond the aforementioned maximum. While these systems differ in the particulars, their restrictions are qualitatively similar—and there has been no fundamental progress on the expressivity issue over the last six years. As a consequence, it seems imperative to adapt to this situation. Two possible approaches are to handle these constraints by outsourcing amenable pieces of a given computation and to apply program transformations to increase the range of suitable computations. These ideas have of course appeared in the literature on compiling cryptographic protocols <em>[43, 45, 59, 86]</em>, but previous efforts in the context of verifiable outsourcing have been very limited <em>[37, 83]</em>.</p>

    <p class="text-gray-300">We study techniques for each of these approaches, adapted to this setting (§4). Giraffe employs <em>slicing</em>, which takes as input an abstract cost model and a program, automatically identifies amenable subregions of the program, and generates glue code to sew the outsourced pieces into the rest of the program. Slicing is a very general technique that can work with all probabilistic proof implementations. Giraffe also uses <em>squashing</em>, which transforms sequential ACs into parallel ACs and adjusts the verifier to link these computations; this is relevant to CMT and T13, which require parallel ACs.</p>

    <p class="text-gray-300">Our implementation of Giraffe (§5) applies the above transformations to programs written in a subset of C, producing one or more ACs. Giraffe’s design template uses these ACs, along with several physical parameters (hardware substrates, chip area, etc.), and automatically generates concrete hardware designs for the prover and verifier, built in SystemVerilog, that can be used for cycle-accurate simulation or synthesized (i.e., compiled to a chip).</p>

    <p class="text-gray-300">We evaluate using detailed simulation and modeling of these generated hardware designs. Accounting for all costs (prover, precomputation, verifier), Giraffe saves compared to native execution across a wide range of computation sizes and hardware substrates (§6.2). Giraffe breaks even on operating costs for <span class="math">N$$\\approx</span>30 parallel sub-computations; this value is essentially independent of the size of each sub-computation (§6.1). Compared to prior work in the VA setting, Giraffe scales to 500<span class="math">\\times</span> larger computation sizes, holding all else constant (§8.1). A disadvantage of Giraffe is that its verifier is costlier than Zebra’s, and thus Giraffe’s break-even point is higher than Zebra’s. This is not because</p>

    <p class="text-gray-300">Zebra is fundamentally cheaper, but rather because it assumes away precomputation and thus does not have to pay for it. Furthermore, Giraffe’s program analysis techniques expand applicability beyond Zebra; our experiments demonstrate that slicing enables an image-matching application that neither Zebra nor Giraffe could otherwise handle (§8.2).</p>

    <p class="text-gray-300">Nevertheless, Giraffe has limitations (some of which reflect the research area itself (§9)). Breaking even requires data-parallel computations (to amortize precomputation), requires that the computation be naturally expressed as a layered AC, and requires a large gap between the hardware technologies used for the verifier and prover (which holds in some concrete settings; see <em>[82, §1]</em>). Moreover, the absolute cost of verifiability is still very high. Finally, the program transformation techniques have taken only a small first step.</p>

    <p class="text-gray-300">Despite these limitations, we think that Giraffe has a substantial claim to significance: it adopts the most stringent cost regime in the verifiable outsourcing literature and (to our knowledge) is the only system that <em>can</em> profitably outsource under this accounting.</p>

    <h4 id="sec-3" class="text-lg font-semibold mt-6">Debts and contributions.</h4>

    <p class="text-gray-300">Giraffe builds on the T13 protocol <em>[77, §7]</em> and an optimization <em>[78]</em> (§2.2). It also generalizes a prior technique <em>[1, 2, 3, 77, 82]</em> (§3.2, “Algorithm”). Finally, Giraffe borrows from Zebra <em>[82]</em>, specifically: the setting (§2.3), how to evaluate in that setting (§2.3, §6.2), a high-level design strategy (implicit in this paper), a design for a module within the prover (footnote 5), and the application to Curve25519 (§8.1). Giraffe’s contributions are:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[leftmargin=*]</li>

      <li>Algorithmic refinements of the T13 interactive proof, yielding an asymptotically optimal prover (§3.1) and a <span class="math">\\approx</span>3<span class="math">\\times</span> reduction in the verifier’s main bottleneck (§3.3).</li>

      <li>Hardware design templates for prover and verifier chips (§3.2, “Computing in hardware”; §3.3). We note that <em>automatically generating a wide variety of optimized hardware designs</em> is a significant technical challenge; it is achieved here via the introduction of the RWSR (and other hardware primitives), and the observation that RWSRs service a wide range of possible designs.</li>

      <li>Techniques for compiling from a subset of C to ACs while automatically optimizing for outsourcing based on cost models (§4).</li>

      <li>An implemented pipeline that takes as input a program in a subset of C and physical parameters, and produces hardware designs automatically (§5).</li>

      <li>Evaluation of the whole system (§6–§8) and a new application of verifiable outsourcing: image matching using a pyramid (§8.2).</li>

      <li>The first explicit consideration of the stringent, tripartite cost regime, and—for all of Giraffe’s limitations—being the first that can at least sometimes outsource profitably in that regime.</li>

    </ul>

    <h2 id="sec-4" class="text-2xl font-bold">2 Background</h2>

    <h3 id="sec-5" class="text-xl font-semibold mt-8">2.1 Probabilistic proofs for verifiability</h3>

    <p class="text-gray-300">The description below is intended to give necessary terminology; it does not cover all variations in the literature.</p>

    <p class="text-gray-300">Systems for verifiable outsourcing enable the following. A <em>verifier</em> <span class="math">\\mathcal{V}</span> specifies a computation <span class="math">\\Psi</span> (often expressed in a high-level language) to a <em>prover</em> <span class="math">\\mathcal{P}</span>. <span class="math">\\mathcal{V}</span> determines input <span class="math">x</span>; <span class="math">\\mathcal{P}</span> returns <span class="math">y</span>, which is purportedly <span class="math">\\Psi(x)</span>. A protocol between <span class="math">\\mathcal{V}</span> and <span class="math">\\mathcal{P}</span> allows <span class="math">\\mathcal{V}</span> to check whether <span class="math">y=\\Psi(x)</span> but without executing <span class="math">\\Psi</span>. There are few (and sometimes no) assumptions about the scope of <span class="math">\\mathcal{P}</span>’s misbehavior.</p>

    <p class="text-gray-300">These systems typically have a <em>front-end</em> and a <em>back-end</em>. The interface between them is an <em>arithmetic circuit</em> (AC). In an AC, the domain is a finite field <span class="math">\\mathbb{F}</span>, usually <span class="math">\\mathbb{F}_{p}</span> (the integers mod</p>

    <p class="text-gray-300">a prime <span class="math">p</span>); “gates” are field operations (add or multiply), and “wires” are field elements. The front-end transforms <span class="math">\\Psi</span> from its original expression to an AC, denoted <span class="math">\\mathcal{C}</span>; this step often uses a compiler <em>[19, 22, 31, 32, 37, 45, 66, 73, 75, 81, 83]</em>, though is sometimes done manually <em>[18, 36, 77]</em>. The back-end is a probabilistic proof protocol, targeting the assertion “<span class="math">y=\\mathcal{C}(x)</span>”; this step incorporates tools from complexity theory and sometimes cryptography.</p>

    <h3 id="sec-6" class="text-xl font-semibold mt-8">2.2 Starting point for Giraffe’s back-end: T13</h3>

    <p class="text-gray-300">Giraffe’s back-end builds on a line of interactive proofs <em>[12, 49, 50, 58, 76]</em>: GKR <em>[49]</em>, as refined and implemented by CMT <em>[36]</em>, Allspice <em>[81]</em>, Thaler <em>[77]</em>, and Zebra <em>[82]</em>. Our description below sometimes borrows from <em>[81, 82]</em>.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">In these works, the AC <span class="math">\\mathcal{C}</span> must be <em>layered</em>: the gates are partitioned, and there are wires only between adjacent partitions (layers). Giraffe’s specific base is <em>T13</em> <em>[77, §7]</em>, with an optimization <em>[78]</em>. T13 requires data parallelism: <span class="math">\\mathcal{C}</span> must have <span class="math">N</span> identical sub-circuit copies, each with its own inputs and outputs (<span class="math">x</span> and <span class="math">y</span> now denote the aggregate inputs and outputs). We call each copy a <em>sub-AC</em>. Each sub-AC has <span class="math">d</span> layers. For simplicity, we assume that every sub-AC layer has the same width, <span class="math">G</span> (this implies that $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">=</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">y</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">=N\\cdot G<span class="math">). The properties of T13 are given below; probabilities are over </span>\\mathcal{V}$’s random choices (Appx. A justifies these properties, by proof and reference to the literature):</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[leftmargin=*]</li>

      <li>Completeness. If <span class="math">y=\\mathcal{C}(x)</span>, and if <span class="math">\\mathcal{P}</span> follows the protocol, then <span class="math">\\Pr\\{\\mathcal{V}\\text{ accepts}\\}=1</span>.</li>

    </ul>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">- Soundness. If <span class="math">y\\neq\\mathcal{C}(x)</span>, then <span class="math">\\Pr\\{\\mathcal{V}\\text{ accepts}\\}&lt;\\epsilon</span>, where $\\epsilon=(\\lceil\\log</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">y</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\rceil+6d\\log(G\\cdot N))/</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math">. This holds unconditionally (no assumptions about </span>\\mathcal{P}<span class="math">). Typically, </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$ is astronomical, making this error probability tiny.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Prover’s running time. <span class="math">\\mathcal{P}</span>’s running time is <span class="math">O(d\\cdot G\\cdot N\\cdot\\log G)</span>; we improve this later (§3.1).</li>

    </ul>

    <p class="text-gray-300">Details. Within a layer of <span class="math">\\mathcal{C}</span>, each gate is labeled with a pair <span class="math">(n,g)\\in\\{0,1\\}^{b_{N}}\\times\\{0,1\\}^{b_{G}}</span>, where <span class="math">b_{N}\\triangleq\\log N</span> and <span class="math">b_{G}\\triangleq\\log G</span>. (We assume for simplicity that <span class="math">N</span> and <span class="math">G</span> are powers of 2.) We also view labels numerically, as elements in <span class="math">\\{0,\\dots,N-1\\}\\times\\{0,\\dots,G-1\\}</span>. In either case, <span class="math">n</span> (a gate label’s upper bits) selects a sub-AC, and <span class="math">g</span> (a gate label’s lower bits) indexes a gate within the sub-AC.</p>

    <p class="text-gray-300">Each layer <span class="math">i</span> has an <em>evaluator function</em> <span class="math">V_{i}\\colon\\{0,1\\}^{b_{N}}\\times\\{0,1\\}^{b_{G}}\\to\\mathbb{F}</span> that maps a gate’s label to the output of that gate; implicitly, <span class="math">V_{i}</span> depends on the input <span class="math">x</span>. By convention, the layers are numbered in reverse execution order. Thus, <span class="math">V_{0}</span> refers to the output layer, and <span class="math">V_{d}</span> refers to the inputs. For example, <span class="math">V_{0}(n,j_{1})</span> is the correct <span class="math">j_{1}</span>th output in sub-AC <span class="math">n</span>; likewise, <span class="math">V_{d}(n,j_{2})</span> is the <span class="math">j_{2}</span>th input in sub-AC <span class="math">n</span>.</p>

    <p class="text-gray-300">Notice that <span class="math">\\mathcal{V}</span> wants to be convinced that <span class="math">y</span>, the purported outputs, matches the correct outputs, as given by <span class="math">V_{0}</span>. However, <span class="math">\\mathcal{V}</span> cannot check this directly: evaluating <span class="math">V_{0}</span> would require re-executing <span class="math">\\mathcal{C}</span>. Instead, <span class="math">\\mathcal{P}</span> combines all <span class="math">V_{0}(\\cdot)</span> values into a digest. Then, the protocol reduces this digest to another digest, this one (purportedly) corresponding to all of the values <span class="math">V_{1}(\\cdot)</span>. The protocol proceeds in this fashion, layer by layer, until <span class="math">\\mathcal{V}</span> is left with a purported digest of the input <span class="math">x</span>, which <span class="math">\\mathcal{V}</span> can then check itself.</p>

    <p class="text-gray-300">Instantiating the preceding sketch requires some machinery. A key element is the <em>sum-check protocol</em> <em>[58]</em>, which we will return to later (§3.1). For now, let <span class="math">P\\colon\\mathbb{F}^{m}\\to\\mathbb{F}</span> be an <span class="math">m</span>-variate polynomial.</p>

    <p class="text-gray-300">In a sum-check <em>invocation</em>, <span class="math">\\mathcal{P}</span> interactively establishes for <span class="math">\\mathcal{V}</span> a claim about the sum of the evaluations of <span class="math">P</span> over the Boolean hypercube <span class="math">\\{0,1\\}^{m}</span>; the number of protocol rounds is <span class="math">m</span>.</p>

    <p class="text-gray-300">Another key element is <em>extensions</em>. Technically, an extension <span class="math">\\tilde{f}</span> of a function <span class="math">f</span> is a polynomial that is defined over a domain that encloses the domain of <span class="math">f</span> and equals <span class="math">f</span> at all points where <span class="math">f</span> is defined. Informally, one can think of <span class="math">\\tilde{f}</span> as encoding the function table of <span class="math">f</span>. In this paper, extensions will always be <em>multilinear extensions</em>: the polynomial has degree at most one in each of its variables. We notate multilinear extensions with tildes.</p>

    <p class="text-gray-300">Based on the earlier sketch, we are motivated to express <span class="math">\\tilde{V}_{i-1}</span> in terms of <span class="math">\\tilde{V}_{i}</span>. To that end, we define several predicates. The functions <span class="math">\\mathrm{add}(\\cdot)</span> and <span class="math">\\mathrm{mult}(\\cdot)</span> are <em>wiring predicates</em>; they have signatures <span class="math">\\{0,1\\}^{3b_{G}}\\to\\{0,1\\}</span>, and implicitly describe the structure of a sub-AC. <span class="math">\\mathrm{add}_{i}(g,h_{0},h_{1})</span> returns <span class="math">1</span> iff (a) within a sub-circuit, gate <span class="math">g</span> at layer <span class="math">i-1</span> is an add gate and (b) the left and right inputs of <span class="math">g</span> are, respectively, <span class="math">h_{0}</span> and <span class="math">h_{1}</span> at layer <span class="math">i</span>. <span class="math">\\mathrm{mult}_{i}</span> is defined analogously. Note that these predicates ignore the “top bits” (the <span class="math">n</span> component) because all sub-ACs are identical. We also define the <em>equality predicate</em> <span class="math">\\mathrm{eq}:\\{0,1\\}^{2b_{N}}\\to\\{0,1\\}</span> with <span class="math">\\mathrm{eq}(a,b)=1</span> iff <span class="math">a</span> equals <span class="math">b</span>. Notice that these predicates admit extensions: <span class="math">\\mathrm{add},\\mathrm{mult}:\\mathbb{F}^{3b_{G}}\\to\\mathbb{F}</span> and <span class="math">\\mathrm{eq}:\\mathbb{F}^{2b_{N}}\\to\\mathbb{F}</span>. (We give explicit expressions in Appx. A.)</p>

    <p class="text-gray-300">We can now express <span class="math">\\tilde{V}_{i-1}</span> in terms of a polynomial <span class="math">P_{q,i}</span>:</p>

    <p class="text-gray-300"><span class="math">P_{q,i}(r_{0},r_{1},r^{\\prime})\\triangleq\\widetilde{\\mathrm{eq}}(q^{\\prime},r^{\\prime})</span> (1) <span class="math">\\quad\\cdot\\big{[}\\mathrm{add}_{i}(q,r_{0},r_{1})\\cdot\\big{(}\\tilde{V}_{i}(r^{\\prime},r_{0})+\\tilde{V}_{i}(r^{\\prime},r_{1})\\big{)}</span> <span class="math">\\qquad\\qquad\\qquad+\\mathrm{mult}_{i}(q,r_{0},r_{1})\\cdot\\tilde{V}_{i}(r^{\\prime},r_{0})\\cdot\\tilde{V}_{i}(r^{\\prime},r_{1})\\big{]}.</span> <span class="math">\\tilde{V}_{i-1}(q^{\\prime},q)=\\sum_{h_{0},h_{1}\\in\\{0,1\\}^{b_{G}}}\\sum_{n\\in\\{0,1\\}^{b_{N}}}P_{q,i}(h_{0},h_{1},n).</span> (2)</p>

    <p class="text-gray-300">The signatures are <span class="math">P_{q,i}\\colon\\mathbb{F}^{2b_{G}+b_{N}}\\to\\mathbb{F}</span> and <span class="math">\\tilde{V}_{i-1},\\tilde{V}_{i}\\colon\\mathbb{F}^{b_{N}}\\times\\mathbb{F}^{b_{G}}\\to\\mathbb{F}</span>. Equation (2) follows from an observation of <em>[78]</em> applied to a claim in <em>[77, §7]</em>. For intuition, notice that (i) <span class="math">P_{q,i}</span> is being summed only at points where its variables are <span class="math">0</span>-<span class="math">1</span>, and (ii) at these points, if <span class="math">(q^{\\prime},q)</span> is a gate label (rather than an arbitrary value in <span class="math">\\mathbb{F}^{b_{N}}\\times\\mathbb{F}^{b_{G}}</span>), then the extensions of the predicates take on <span class="math">0</span>-<span class="math">1</span> values and in particular eliminate all summands except the one that contains the inputs to the gate <span class="math">(q^{\\prime},q)</span>.</p>

    <p class="text-gray-300">An excerpt of the protocol appears in Figure 1; the remainder appears in Appendix A. It begins with <span class="math">\\mathcal{V}</span> wanting to be convinced that <span class="math">\\tilde{V}_{0}</span> (which is the extension of the correct <span class="math">\\mathcal{C}(x)</span>) is the same polynomial as <span class="math">\\tilde{V}_{y}</span> (which denotes the extension of the purported output <span class="math">y</span>). <span class="math">\\mathcal{V}</span> thus chooses a random point in both polynomials’ domain, <span class="math">(q_{0}^{\\prime},q_{0})</span>, and wants to be convinced that <span class="math">\\tilde{V}_{0}(q_{0}^{\\prime},q_{0})=\\tilde{V}_{y}(q_{0}^{\\prime},q_{0})\\triangleq a_{0}</span>. Notice that (i) <span class="math">\\tilde{V}_{0}(q_{0}^{\\prime},q_{0})</span> can be expressed as the sum over a Boolean hypercube of the polynomial <span class="math">P_{q_{0},1}</span> (Equation (2)), and (ii) <span class="math">P_{q_{0},1}</span> itself is expressed in terms of <span class="math">\\tilde{V}_{1}</span> (Equation (1)). Using a sum-check invocation, the protocol exploits these facts to reduce <span class="math">\\tilde{V}_{0}(q_{0}^{\\prime},q_{0})=a_{0}</span> to a claim: <span class="math">\\tilde{V}_{1}(q_{1}^{\\prime},q_{1})=a_{1}</span>. This continues layer by layer until <span class="math">\\mathcal{V}</span> obtains the claim: <span class="math">\\tilde{V}_{d}(q_{d}^{\\prime},q_{d})=a_{d}</span>. <span class="math">\\mathcal{V}</span> checks that assertion directly.</p>

    <p class="text-gray-300">T13 incorporates one sum-check invocation—each of which is <span class="math">2b_{G}+b_{N}</span> rounds—for each polynomial <span class="math">P_{q_{0},1},\\ldots,P_{q_{d-1},d}</span>.</p>

    <h3 id="sec-7" class="text-xl font-semibold mt-8">2.3 Verifiable ASICs</h3>

    <p class="text-gray-300">Giraffe’s back-end works in the <em>Verifiable ASICs</em> (VA) setting <em>[82]</em>. Giraffe also borrows evaluation metrics and some design elements from <em>[82]</em>; we summarize below.</p>

    <p class="text-gray-300">Consider some <em>principal</em> (a government, fabless semiconductor company, etc.) that wants high-assurance execution of a custom chip (known as an ASIC) <em>[82, §1,§2.1]</em>. The ASIC must be manufactured at a trustworthy foundry, for example one that is onshore. However, for many principals, high-assurance</p>

    <p class="text-gray-300">1: function VERIFY(ArithCircuit c, input  <span class="math">x</span> , output  <span class="math">y</span> ) 2:  <span class="math">(q_0&#x27;, q_0) \\xleftarrow{R} \\mathbb{P}^{\\log N} \\times \\mathbb{P}^{\\log G}</span> 3:  <span class="math">a_0 \\gets \\tilde{V}_y(q_0&#x27;, q_0) // \\tilde{V}_y</span>  is the multilin. ext. of the output  <span class="math">y</span> 4: SendToProver  <span class="math">(q_0&#x27;, q_0)</span> 5:  <span class="math">d \\gets \\mathrm{c.depth}</span> 6: 7: for  <span class="math">i = 1, \\dots, d</span>  do 8: // Reduce  <span class="math">\\tilde{V}_{i-1}(q_{i-1}&#x27;, q_{i-1}) \\stackrel{?}{=} a_{i-1}</span>  to  <span class="math">P_{q,i}(r_0, r_1, r&#x27;) \\stackrel{?}{=} e</span> 9:  <span class="math">(e, r&#x27;, r_0, r_1) \\gets \\text{SUMCHECKV}(i, a_{i-1})</span> 10: 11: // Below,  <span class="math">\\mathcal{P}</span>  describes a univariate polynomial  <span class="math">H(t)</span> , 12: // of degree  <span class="math">\\log G</span> , claimed to be  <span class="math">\\tilde{V}_i(r&#x27;, (r_1 - r_0)t + r_0)</span> 13:  <span class="math">H \\gets</span>  Receive from  <span class="math">\\mathcal{P}</span>  // see Figure 14, line 47 14:  <span class="math">v_0 \\gets H(0)</span> 15:  <span class="math">v_1 \\gets H(1)</span> 16: 17: // Reduce  <span class="math">P_{q,i}(r_0, r_1, r&#x27;) \\stackrel{?}{=} e</span>  to two questions: 18: //  <span class="math">\\tilde{V}_i(r&#x27;, r_0) \\stackrel{?}{=} v_0</span>  and  <span class="math">\\tilde{V}_i(r&#x27;, r_1) \\stackrel{?}{=} v_1</span> 19: 20: if  <span class="math">e \\neq \\widehat{\\mathrm{eq}}(q_{i-1}&#x27;, r&#x27;) \\cdot \\left[\\widehat{\\mathrm{add}}_i(q_{i-1}, r_0, r_1) \\cdot (v_0 + v_1)\\right.</span> 21:  <span class="math">+\\widehat{\\mathrm{mult}}_i(q_{i-1}, r_0, r_1) \\cdot v_0 \\cdot v_1]</span>  then 22: return reject 23: 24: // Reduce the two  <span class="math">v_0, v_1</span>  questions to  <span class="math">\\tilde{V}_i(q_i&#x27;, q_i) \\stackrel{?}{=} a_i</span> 25:  <span class="math">\\tau_i \\xleftarrow{R} \\mathbb{P}</span> 26:  <span class="math">a_i \\gets H(\\tau_i)</span> 27:  <span class="math">(q_i&#x27;, q_i) \\gets (r&#x27;, (r_1 - r_0) \\cdot \\tau_i + r_0)</span> 28: 29: SendToProver  <span class="math">(\\tau_i)</span> 30: 31: //  <span class="math">\\tilde{V}_d(\\cdot)</span>  is the multilinear extension of the input  <span class="math">x</span> 32: if  <span class="math">\\tilde{V}_d(q_d&#x27;, q_d) = a_d</span>  then 33: return accept 34: return reject</p>

    <p class="text-gray-300">FIGURE 1— <span class="math">\\mathcal{V}</span> 's side of T13 [77, §7], with an optimization [78].  <span class="math">\\mathcal{V}</span> 's side of the sum-check protocol and  <span class="math">\\mathcal{P}</span> 's work are described in Appendix A, Figures 11 and 14.</p>

    <p class="text-gray-300">manufacture means an orders-of-magnitude sacrifice in price and performance, relative to an advanced but untrusted foundry. This owes to the economics and scaling behavior of semiconductor technology. In the VA setup, one manufactures a prover in a state-of-the-art but untrusted foundry (we refer to the manufacturing process and hardware substrate as the untrusted technology node) and a verifier in a trusted foundry (the trusted technology node). A trusted integrator combines the two ASICs. This arrangement makes sense if their combined cost is cheaper than the native baseline: an ASIC manufactured in the trusted technology node.</p>

    <p class="text-gray-300">VA is instantiated in a system called Zebra, which implements an optimized variant of CMT [36, 78, 81]. Zebra is evaluated with two metrics [82, §2.3]. The first is energy ( <span class="math">E</span> , in joules/run), which is a proxy for operating cost. Energy tracks asymptotic (serial) running time: it captures the number of operations and the efficiency of their implementation. The second is area/throughput ( <span class="math">A / T</span> , in  <span class="math">\\mathrm{mm}^2/</span>  (ops/sec)). Area is a proxy for manufacturing cost; normalizing by throughput reflects cost at a given performance level.</p>

    <p class="text-gray-300">Furthermore, Zebra is designed to respect two physical constraints. The first is a maximum area, to reflect manufacturability (larger chips have more frequent defects and hence lower yields). The second is a maximum power dissipation, to limit heat. The first constraint limits <span class="math">A</span> (and thus the hardware design space) and the second limits the product of energy and throughput, <span class="math">E\\cdot T</span>.</p>

    <p class="text-gray-300">Zebra’s prover architecture consists of a collection of pipelined <em>sub-provers</em>, each one doing the execution and proving work for one layer of an AC <em>[82, §3.1–3.2]</em>. Within a sub-prover, there is dedicated hardware for each AC gate in a layer. Zebra’s verifier is also organized into layers <em>[82, §3.5]</em>. Giraffe incorporates this overall picture, including some integration details <em>[82, §4]</em>. However, Giraffe requires a different architecture, as we explain next.</p>

    <h2 id="sec-8" class="text-2xl font-bold">3 Protocol and hardware design</h2>

    <p class="text-gray-300">Three goals drive Giraffe’s hardware back-end:</p>

    <p class="text-gray-300">G1: Scale to large <span class="math">N</span> without sacrificing <span class="math">G</span>. <span class="math">\\mathcal{V}</span>’s precomputation scales with the size of one sub-AC (§2.2); it needs to amortize this over multiple sub-AC copies, <span class="math">N</span>. Further, we have an interest in handling large computations (sub-ACs and ACs). This implies that Giraffe’s design must reuse underlying hardware modules: for large <span class="math">N</span> and sub-AC width <span class="math">G</span>, requiring a number of modules proportional to <span class="math">N\\cdot G</span> is too costly. Zebra’s design is not suitable, since it requires logic proportional to the amount of work in an AC layer <em>[82, Fig. 5]</em>.</p>

    <p class="text-gray-300">G2: Be efficient. In this context, good efficiency implies lower cross-over points on the metrics of merit (§2.3). This in turn means custom hardware, which is expected in ASIC designs but, for us, is in tension with the next goal.</p>

    <p class="text-gray-300">G3: Produce designs automatically. Ideally, the goal is to produce a compiler that takes as input a high-level description of the computation along with physical parameters (technology nodes, chip area, etc.; §2.3) and produces synthesizable hardware (§5). This goes beyond convenience: a goal of this work is to understand where, in terms of computations (<span class="math">G</span>, <span class="math">N</span>, etc.) and physical parameters (technology nodes, chip area, etc.), an abstract algorithm (T13) applies. To do this, we need to be able to optimize hardware for both the computations and the physical parameters, which poses a significant challenge: for different computations and physical parameters, different hardware designs make sense. For example, if <span class="math">N</span> and <span class="math">G</span> are small, iteratively reusing hardware might not consume all available chip area; one would prefer to spend this area to gain parallelism and thus increase throughput.</p>

    <p class="text-gray-300">Giraffe answers this challenge by developing a <em>design template</em> that takes as input a description of the desired computation and a set of physical parameters, and produces as output an optimized hardware design. The template’s “primitives” are custom hardware structures that enable efficient reuse (serial execution) when there are few of them, but can be automatically parallelized. To use the design template, the designer simply specifies its inputs; design generation is fully automatic.</p>

    <p class="text-gray-300">In the rest of the section, we modify T13 to obtain an asymptotic improvement in <span class="math">\\mathcal{P}</span>’s work (§3.1); this contributes to Giraffe’s scalability, and is of independent interest. We also describe aspects of the hardware design template for <span class="math">\\mathcal{P}</span> (§3.2). Finally, we do the same for <span class="math">\\mathcal{V}</span>, and also describe optimizations that help offset the cost of precomputation (§3.3). Compared to prior work, these optimizations reduce <span class="math">\\mathcal{V}</span>’s primary cost by nearly <span class="math">3\\times</span> and eliminate a log factor from one of <span class="math">\\mathcal{V}</span>’s secondary costs; since <span class="math">\\mathcal{V}</span>’s costs dominate, these optimizations have a direct effect on end-to-end performance.</p>

    <p class="text-gray-300">Notation. <span class="math">[a,b]</span> denotes <span class="math">\\{a,a+1,\\ldots,b\\}</span>. For a vector <span class="math">u</span>, <span class="math">u[\\ell]</span> denotes the <span class="math">\\ell</span>th entry, indexed from <span class="math">1</span>; <span class="math">u[\\ell_{1}..\\ell_{2}]</span> denotes the sub-vector between indices <span class="math">\\ell_{1}</span> and <span class="math">\\ell_{2}</span>, inclusive. Arrays are accessed sim</p>

    <p class="text-gray-300">larly, but are indexed from <span class="math">0</span>. Vectors are indicated with lower-case letters, arrays with upper-case. Define <span class="math">\\chi_{0},\\chi_{1}\\colon\\mathbb{F}\\to\\mathbb{F}</span> as <span class="math">\\chi_{1}(w)=w</span>, <span class="math">\\chi_{0}(w)=1-w</span>. Similarly, if <span class="math">s\\in\\{0,1\\}^{\\gamma}</span> and <span class="math">u\\in\\mathbb{F}^{\\gamma}</span>, <span class="math">\\chi_{s}(u)\\triangleq\\prod_{\\ell=1}^{\\gamma}\\chi_{s\\{\\ell\\}}(u[\\ell])</span>. Notice that when <span class="math">u</span> comprises <span class="math">0</span>-<span class="math">1</span> values, <span class="math">\\chi_{s}(u)</span> returns <span class="math">1</span> if <span class="math">u=s</span> and <span class="math">0</span> otherwise.</p>

    <h3 id="sec-9" class="text-xl font-semibold mt-8">3.1 Making <span class="math">\\mathcal{P}</span> time-optimal</h3>

    <p class="text-gray-300">This section describes an algorithmic refinement that, by restructuring the application of the sum-check protocol, slashes <span class="math">\\mathcal{P}</span>’s overhead. Specifically, <span class="math">\\mathcal{P}</span>’s running time drops from <span class="math">O(d\\cdot N\\cdot G\\cdot\\log G)</span> to <span class="math">O(d\\cdot(N\\cdot G+G\\cdot\\log G))</span>. If <span class="math">N\\gg\\log G</span>, <span class="math">\\mathcal{P}</span>’s new running time is linear in the number of total gates in the AC—that is, the prover has no asymptotic overhead! Prior work <em>[77, §5]</em> achieved time-optimality in special cases (if the AC’s structure met an ad hoc and restrictive condition); the present refinement applies in general, whenever there are repeated sub-ACs.</p>

    <p class="text-gray-300">The <span class="math">O(\\log G)</span> reduction translates to concrete double digit factors (Appx. D). For example, software provers in this research area <em>[36, 77, 79, 81, 87]</em> typically run with <span class="math">G</span> at least <span class="math">2^{18}</span>; thus, a software T13 prover’s running time improves by at least <span class="math">18\\times</span>. For a hardware prover, the <span class="math">A/T</span> metric improves by approximately <span class="math">\\log G</span>, as computation is the main source of area cost (Appx. C, <em>[82, Fig. 6 and 7]</em>). The gain is less pronounced for the <span class="math">E</span> metric: storage and communication are large energy consumers but are unaffected by the refinement (Appx. C).</p>

    <p class="text-gray-300">Before describing the refinement, we give some background on sum-check protocols; for details, see <em>[8, §8.3; 49, §2.5; 58; 76]</em>. Consider a polynomial <span class="math">P</span> in <span class="math">m</span> variables and a claim that <span class="math">\\sum_{(t_{1},\\ldots,t_{m})\\in\\{0,1\\}^{m}}P(t_{1},\\ldots,t_{m})=L</span>. In round <span class="math">j</span> of the sum-check protocol, <span class="math">\\mathcal{P}</span> must describe to <span class="math">\\mathcal{V}</span> a degree-<span class="math">\\alpha</span> univariate polynomial <span class="math">F_{j}(t^{*})</span>, where <span class="math">\\alpha</span> depends on <span class="math">P</span> and <span class="math">j</span>:</p>

    <p class="text-gray-300"><span class="math">F_{j}(t^{<em>})=\\sum_{(t_{j+1},\\ldots,t_{m})\\in\\{0,1\\}^{m-j}}P(\\rho_{1},\\ldots,\\rho_{j-1},t^{</em>},t_{j+1},\\ldots,t_{m}).</span></p>

    <p class="text-gray-300">To discharge this obligation, <span class="math">\\mathcal{P}</span> computes evaluations <span class="math">F_{j}(k)</span>, for <span class="math">\\alpha+1</span> different values of <span class="math">k</span>. Then, at the end of round <span class="math">j</span>, <span class="math">\\mathcal{V}</span> sends <span class="math">\\rho_{j}</span>, for use in the next round. Notice the abstract pattern: in every round <span class="math">j</span>, <span class="math">\\mathcal{P}</span> computes <span class="math">\\alpha+1</span> sums over a Boolean hypercube of dimension <span class="math">m-j</span>. The number of hypercube vertices shrinks as <span class="math">j</span> increases: variables that were previously summed become set, or <span class="math">bound</span>, to a <span class="math">\\rho_{j}</span>.</p>

    <p class="text-gray-300">Let us map this picture to our context. There is one sum-check run for each layer <span class="math">i\\in[1,\\,d]</span>; <span class="math">P</span> is the per-layer polynomial <span class="math">P_{q,i}</span> defined in Equation (1); <span class="math">m=2b_{G}+b_{N}</span>; the <span class="math">\\rho_{j}</span> are aliases for the components of <span class="math">r_{0},r_{1},r^{\\prime}</span>; likewise, the <span class="math">t_{j}</span> alias the components of <span class="math">h_{0},h_{1},n</span>. Also, <span class="math">\\alpha</span> is <span class="math">2</span> or <span class="math">3</span>; this follows from Equation (1), recalling that each multilinear extension (<span class="math">\\widehat{\\mathrm{eq}}</span>, <span class="math">\\mathrm{ad}\\mathrm{d}</span>, etc.) by definition has degree one in its variables.</p>

    <p class="text-gray-300">There are now two interrelated questions: In what order should the variables be bound? How does <span class="math">\\mathcal{P}</span> compute the <span class="math">\\alpha+1</span> sums per round? In T13, the order is <span class="math">h_{0},h_{1},n</span>, as in Equation (2). This enables <span class="math">\\mathcal{P}</span> to compute the needed sums in time <span class="math">O(N\\cdot G\\cdot\\log G)</span> per-layer <em>[77, §7]</em>. <span class="math">\\mathcal{P}</span>’s total running time is thus <span class="math">O(d\\cdot N\\cdot G\\cdot\\log G)</span>.</p>

    <p class="text-gray-300">Giraffe’s refinement changes the order in which variables are bound, and exploits that order to simplify <span class="math">\\mathcal{P}</span>’s work. Giraffe’s order is <span class="math">n,h_{0},h_{1}</span>. From here on, we write <span class="math">P_{q,i}(h_{0},h_{1},n)</span> as <span class="math">P_{q,i}^{<em>}(n,h_{0},h_{1})</span>; <span class="math">P_{q,i}\\equiv P_{q,i}^{</em>}</span> except for argument order. Below, we describe the structure of <span class="math">\\mathcal{P}</span>’s per-round obligations, fixing a layer <span class="math">i</span>. This serves as background for the hardware design (§3.2) and as a sketch of the argument for the claimed running time. A proof, theorem statement, and pseudocode are in Appendix B.</p>

    <p class="text-gray-300">The rounds decompose into two <em>phases</em>. Phase <span class="math">1</span> is rounds <span class="math">j\\in[1,\\,b_{N}]</span>. Observe that in this phase, <span class="math">\\mathcal{P}</span>’s sums seemingly have the form: <span class="math">F_{j}(k)=\\sum_{n[j+1..b_{N}]}\\sum_{h_{0},h_{1}}P_{q,i}^{*}(r^{\\prime}[1..j-1],\\;k,\\,n[j+1..b_{N}],\\;h_{0},\\,h_{1})</span></p>

    <p class="text-gray-300">where the outer sum is over all  <span class="math">n[j + 1..b_N] \\in \\{0,1\\}^{b_N - j}</span> . However, many  <span class="math">(h_0,h_1)</span>  combinations cause  <span class="math">P_{q,i}^* (\\ldots ,h_0,h_1)</span>  to evaluate to 0. As a result, there is a more convenient form for the inner sum. Define  <span class="math">S_{\\mathrm{all},i} \\subseteq \\{0,1\\}^{3b_G}</span>  as all layer-  <span class="math">(i - 1)</span>  gates with their layer-  <span class="math">i</span>  neighbors, and  <span class="math">\\mathrm{OP}_g</span>  as “+” if  <span class="math">g</span>  is an addition gate and “.” if  <span class="math">g</span>  is a multiplication gate. Then  <span class="math">F_{j}(k)</span>  can be written as:</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{array}{l} F _ {j} (k) = \\sum_ {n [ j + 1.. b _ {N} ]} \\sum_ {(g, g _ {L}, g _ {R}) \\in S _ {\\text {a l l}, i}} \\operatorname {t e r m P 1} _ {j, n, k} \\cdot \\operatorname {t e r m P 2} _ {g} \\\\ \\cdot \\mathrm {O P} _ {g} \\left(\\operatorname {t e r m L} _ {j, n, g _ {L}, k}, \\operatorname {t e r m R} _ {j, n, g _ {R}, k}\\right), \\tag {3} \\\\ \\end{array}</span></div>

    <p class="text-gray-300">where termP1 depends on  <span class="math">j, n, k</span> ; termP2 depends on  <span class="math">g</span> , and so forth; these also depend on values of  <span class="math">\\rho</span>  from prior rounds and prior layers. Section 3.2 makes some of these terms explicit (Appx. B fully specifies).</p>

    <p class="text-gray-300">Phase 2 is the remaining  <span class="math">2b_{G}</span>  rounds. Here, there is only a single sum, over increasingly bound components of  <span class="math">h_0, h_1</span> . As with phase 1, it is convenient to express the sum "gatewise". Specifically, for rounds  <span class="math">j \\in [b_N + 1, b_N + 2b_G]</span> , one can write  <span class="math">F_{j}(k) = \\sum_{(g,g_{L},g_{R}) \\in S_{\\mathrm{all},i}} \\operatorname{termP}_{j,g,k} \\cdot \\mathrm{OP}_{g}(\\operatorname{termL}_{j,g_{L},k}, \\operatorname{termR}_{j,g_{R},k})</span> .</p>

    <p class="text-gray-300">In both phases,  <span class="math">\\mathcal{P}</span>  can compute each sum over  <span class="math">S_{\\mathrm{all},i}</span>  with  <span class="math">O(G)</span>  work. Thus, per-layer, the running time for phase 1 is  <span class="math">O(G\\cdot N / 2) + O(G\\cdot N / 4) + \\dots +O(G) = O(G\\cdot N)</span> , and for phase 2 it is  <span class="math">O(G\\cdot \\log G)</span> , yielding the earlier claim of  <span class="math">O(d\\cdot (N\\cdot G + G\\cdot \\log G))</span> .</p>

    <h2 id="sec-10" class="text-2xl font-bold">3.2 Design of  <span class="math">\\mathcal{P}</span></h2>

    <p class="text-gray-300">Consider  <span class="math">\\mathcal{P}</span> 's obligations in layer  <span class="math">i</span> , summarized at the end of the previous section. Notice that  <span class="math">\\mathcal{P}</span> 's phase-2 obligations are independent of  <span class="math">N</span> . This is a consequence of Section 3.1; there is no such independence in the original variable order [77, §7]. In the current variable order, the bulk of  <span class="math">\\mathcal{P}</span> 's work occurs in phase 1, and so our description below focuses on phase 1. <span class="math">^5</span></p>

    <p class="text-gray-300">Within phase 1, the heaviest work item is computing termL, termR in each round. The rest of this section describes the obligation, the algorithm by which  <span class="math">\\mathcal{P}</span>  discharges it, and the hardware design that computes the algorithm.  <span class="math">\\mathcal{P}</span> 's other obligations (computing termP1 <span class="math">_{j,n,k}</span> , etc.) and algorithms for discharging them are described in Appendix B.</p>

    <p class="text-gray-300"><strong>Algorithm for computing termL,termR</strong> Fixing a layer  <span class="math">i</span> , in round  <span class="math">j</span> , termL and termR are:</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{array}{l} \\operatorname {t e r m L} _ {j, n, g _ {L}, k} \\triangleq \\tilde {V} _ {i} (r ^ {\\prime} [ 1.. j - 1 ], k, n [ j + 1.. b _ {N} ], g _ {L}) \\\\ \\operatorname {t e r m R} _ {j, n, g _ {R}, k} \\triangleq \\tilde {V} _ {i} \\left(r ^ {\\prime} [ 1.. j - 1 ], k, n [ j + 1.. b _ {N} ], g _ {R}\\right) \\tag {4} \\\\ \\end{array}</span></div>

    <p class="text-gray-300">Notice that for each  <span class="math">k</span> , Equation (4) refers to  <span class="math">G \\cdot N / 2^j</span>  values of  <span class="math">\\tilde{V}(\\cdot)</span> .</p>

    <p class="text-gray-300">Figure 2 depicts an algorithm, EvalTermLR, that computes these values in time  <span class="math">O(G \\cdot N / 2^j)</span>  for round  <span class="math">j</span> , by adapting a prior technique [77, §5.4; 82, §3.3] (see also [1-3]). EvalTermLR is oriented around a recurrence. Let  <span class="math">h</span>  be a bottom-bit gate label at layer  <span class="math">i</span> . Then for all  <span class="math">\\sigma \\in \\{0,1\\}^{b_N - j}</span> , the following holds (derived in Appx. B.1):</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{array}{l} \\tilde {V} _ {i} \\left(r ^ {\\prime} [ 1.. j ], \\sigma , h\\right) = \\left(1 - r ^ {\\prime} [ j ]\\right) \\cdot \\tilde {V} _ {i} \\left(r ^ {\\prime} [ 1.. j - 1 ], 0, \\sigma , h\\right) \\\\ + r ^ {\\prime} [ j ] \\cdot \\tilde {V} _ {i} \\left(r ^ {\\prime} [ 1.. j - 1 ], 1, \\sigma , h\\right). \\tag {5} \\\\ \\end{array}</span></div>

    <p class="text-gray-300">10</p>

    <p class="text-gray-300">1: // initialize  <span class="math">W</span> : array of  <span class="math">G</span>  arrays of  <span class="math">N</span>  values 2: for  <span class="math">h = 0, \\dots, G - 1</span>  and  <span class="math">\\sigma = 0, \\dots, N - 1</span>  do 3:  <span class="math">W[h][\\sigma] \\gets V_i(\\sigma, h)</span> 4: 5: function EVALTERMLR(Array-of-arrays  <span class="math">W</span> ) 6: for  <span class="math">j = 1, \\dots, b_N</span>  do 7: look up all termL, termR in  <span class="math">W</span>  (see text) 8: 9:  <span class="math">r&#x27;[j] \\gets</span>  Receive from  <span class="math">\\mathcal{V}</span>  // see Figure 15, line 19 10: 11: for  <span class="math">h = 0, \\dots, G - 1</span>  do 12: Collapse(W[h],  <span class="math">N / 2^{j-1}</span> ,  <span class="math">r&#x27;[j]</span> ) 13: 14: function COLLAPSE(Array  <span class="math">A</span> , size len,  <span class="math">r \\in \\mathbb{F}</span> ) 15: for  <span class="math">\\sigma = 0, \\dots, \\text{len} / 2 - 1</span>  do 16:  <span class="math">A[\\sigma] \\gets (1 - r) \\cdot A[2\\sigma] + r \\cdot A[2\\sigma + 1]</span></p>

    <p class="text-gray-300">FIGURE 2—EvalTermLR: a dynamic programming algorithm for computing termL, termR for all rounds  <span class="math">j</span> . EvalTermLR adapts a prior technique [77, §5.4; 82, §3.3] [1-3].</p>

    <p class="text-gray-300">EvalTermLR relies on a two-dimensional array  <span class="math">W</span> , and maintains the following invariant, justified shortly: at the beginning of every round  <span class="math">j</span> ,  <span class="math">W[h][\\sigma]</span>  stores  <span class="math">\\tilde{V}_i(r&#x27;[1..j-1], \\sigma, h)</span> , for  <span class="math">h \\in [0, G-1]</span>  and  <span class="math">\\sigma \\in [0, N/2^{j-1}-1]</span> .</p>

    <p class="text-gray-300">Given this invariant,  <span class="math">\\mathcal{P}</span>  obtains all of the termL, termR values from  <span class="math">W</span>  (in line 7), as follows. We focus on termL. Write  <span class="math">n[j + 1..b_N]</span>  as  <span class="math">n_{j + 1}</span> . Then, for  <span class="math">k = \\{0,1\\}</span> , termL <span class="math">_{j,n,g_L,k}</span>  is  <span class="math">W[g_L][k + 2\\cdot n_{j + 1}]</span> ; this follows from Equation (4) plus the invariant. Meanwhile, for  <span class="math">k = -1</span> , termL <span class="math">_{j,n,g_L, -1} = 2\\cdot \\mathrm{termL}_{j,n,g_L,0} + (-1)\\cdot \\mathrm{termL}_{j,n,g_L,1}</span> . This follows from Equations (4) and (5);  <span class="math">k = 2</span>  is similar. termR is the same, except  <span class="math">g_R</span>  replaces  <span class="math">g_L</span> . The total time cost is  <span class="math">O(G\\cdot N / 2^j)</span>  in round  <span class="math">j</span> : Collapse performs  <span class="math">(N / 2^{j - 1}) / 2</span>  iterations, and there are  <span class="math">G</span>  calls to Collapse.</p>

    <p class="text-gray-300">The invariant holds for  <span class="math">j = 1</span>  because  <span class="math">\\tilde{V}_i(r&#x27;[1..j - 1], \\sigma, h) = \\tilde{V}_i(\\sigma, h) = V_i(\\sigma, h)</span> , which initializes  <span class="math">W[h][\\sigma]</span>  (line 3); the latter equality holds because functions equal their extensions when evaluated on bit vectors. Now, at the end of  <span class="math">j</span> , line 16 applies Equation (5) to all  <span class="math">\\sigma \\in [0, N/2^j - 1]</span> , thereby setting  <span class="math">W[h][\\sigma]</span>  to  <span class="math">\\tilde{V}_i(r[1..j], \\sigma, h)</span> . This is the required invariant at the start of round  <span class="math">j + 1</span> .</p>

    <p class="text-gray-300">Computing EvalTermLR in hardware. To produce a design template for  <span class="math">\\mathcal{P}</span>  consistent with Giraffe's goals, we must answer three questions. First, what breakdown of  <span class="math">\\mathcal{P}</span> 's work makes sense: which portions are parallelized, and what hardware is iteratively reused in a round (G1)? Second, for iterative parts of the computation, how does  <span class="math">\\mathcal{P}</span>  load operands and store results (G2)? Finally, how can this design be adapted to a range of computations and physical parameters (G3)?</p>

    <p class="text-gray-300">A convenient top-level breakdown is already implied by the prior formulation of  <span class="math">W</span> : since Collapse operates on each  <span class="math">W[h]</span>  array independently, it is natural to parallelize work across these arrays. Giraffe allocates separate storage structures and logic implementing Collapse for each  <span class="math">W[h]</span>  array (and, of course, reuses this hardware from round to round for each array). We therefore focus on the design of one of these modules.</p>

    <p class="text-gray-300">To answer the second question, we first consider two straw men. The first is to imitate a software design: instantiate one module for field arithmetic and a RAM to store the  <span class="math">W[h]</span>  array, then iterate through the  <span class="math">\\sigma</span>  loop sequentially, loading needed values, computing over them, and storing the results. In practice, however, ASIC designers often prefer to avoid building RAM circuits. This is because generality has a price (e.g., address decoding imposes overheads in area and energy), RAM often creates a throughput bottleneck, and RAM is a frequent cause of manufacturability and reliability issues. (Of</p>

    <p class="text-gray-300">RWSR specification</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Power-of-two storage locations, <span class="math">K</span></li>

      <li>Only locations 0 and 1 can be read</li>

      <li>The only write operation is <span class="math">\\stackrel{{\\scriptstyle x}}{{\\leftarrow}}</span>. It is specified below. Informally, it updates one location, and causes all the “even” locations to behave like a distinct shift register (location 6 shifts to 4, etc.), and likewise with all of the “odd” locations.</li>

    </ul>

    <p class="text-gray-300">1: operator RWSR[<span class="math">a</span>] <span class="math">\\stackrel{{\\scriptstyle x}}{{\\leftarrow}}</span> <span class="math">v</span> is 2: // Note that all updates happen simultaneously 3: RWSR[<span class="math">a</span>] <span class="math">\\leftarrow</span> <span class="math">v</span> 4: for <span class="math">\\ell&lt;K,\\ell\\neq a</span> do 5: RWSR[<span class="math">\\ell</span>] <span class="math">\\leftarrow</span> RWSR[<span class="math">\\ell+2</span>] 6: 7: function RWSRCollapse(RWSR <span class="math">R</span>, size len, <span class="math">r\\in\\mathbb{F}</span>) 8: for <span class="math">\\sigma=0,\\ldots,\\text{len}/2-1</span> do 9: <span class="math">R[\\text{len}-2-\\sigma]\\stackrel{{\\scriptstyle x}}{{\\leftarrow}}(1-r)\\cdot R[0]+r\\cdot R[1]</span></p>

    <p class="text-gray-300">Figure 3—Specification of a new hardware primitive, RWSR, used to implement Collapse (Fig. 2) in hardware.</p>

    <p class="text-gray-300">course, RAMs are a dominant cost in many modern ASICs, but that doesn’t mean that designers prefer RAM: often there is simply no alternative. For example, an unpredictable memory access pattern often necessitates RAM.)</p>

    <p class="text-gray-300">The second straw man is essentially the opposite: instantiate a bank of registers to hold values in <span class="math">W[h]</span>, along with two field multipliers and one adder per pair of adjacent registers, then create a wiring pattern such that the adder for registers <span class="math">2\\sigma</span> and <span class="math">2\\sigma+1</span> connects to the input of register <span class="math">\\sigma</span>. This arrangement computes the entire <span class="math">\\sigma</span> loop in parallel. This is similar to prior work <em>[82, §3.3]</em>, but in Giraffe O(<span class="math">NG</span>) multipliers is extremely expensive when <span class="math">N</span> and <span class="math">G</span> are large. It is also inflexible: in this design, the number of multipliers is fixed after selecting <span class="math">N</span> and <span class="math">G</span>.</p>

    <p class="text-gray-300">Giraffe’s solution is a hybrid of these approaches; we first explain a serial version, then describe how to parallelize. Giraffe instantiates two multipliers and one adder that together compute one step of the <span class="math">\\sigma</span> loop. The remaining challenge is to get operands to the multipliers and store the result from the adder. Giraffe does so using a custom hardware structure that is tailored to the access pattern of the <span class="math">W[h]</span> arrays: for each <span class="math">W[h]</span>, read two values, write one value, read two values, and so on. Giraffe uses RWSRs, (“random-write shift registers”), one for each <span class="math">W[h]</span>. Figure 3 specifies the RWSR and shows its use for Collapse.</p>

    <p class="text-gray-300">Compared to the first straw man, Giraffe’s design has several advantages. First, an RWSR only allows two locations to be read; compared to a general-purpose RAM, this eliminates the need for most logic to handle read operations. Second, Giraffe’s RWSR need not be “random-write”: its <span class="math">\\stackrel{{\\scriptstyle x}}{{\\leftarrow}}</span> operator (Fig. 3, line 1) can be specialized to the address sequence of the RWSRCollapse algorithm (Fig. 3, line 9), making its write logic far simpler than a RAM’s, too. This means that an RWSR can be implemented in almost the same way as a standard shift register, and at comparable cost. Alternatively, an RWSR can be implemented like a RAM, using the same data storage circuits but dramatically simplified addressing logic. The latter approach might reduce energy consumption compared to implementing like a standard shift register, and it would still cost less than using a general-purpose RAM; but it would potentially re-introduce the above-mentioned manufacturaility and reliability concerns associated with RAM circuits.</p>

    <p class="text-gray-300">The remaining question is how this design can be efficiently and automatically parallelized. Notice that the loop over <span class="math">\\sigma</span> is serialized (because RWSRs allow only one write at a time); but what if the designer allocates enough chip area to accommodate four multipliers for <span class="math">W[h]</span> instead of two? In other</p>

    <p class="text-gray-300">words, how can Giraffe’s design template automatically improve RWSRCollapse’s throughput by using more chip area?</p>

    <p class="text-gray-300">To demonstrate the approach, we refer to the pseudocode of Figure 2. First, split each <span class="math">W[h]</span> array into two arrays, <span class="math">W1[h]</span> and <span class="math">W2[h]</span>. In place of the Collapse invocation (line 12), run two parallel invocations on <span class="math">W1[h]</span> and <span class="math">W2[h]</span>, each of half the length. Notice that each array has increasing “empty” space as the rounds go on. In round <span class="math">j</span>, the “live values” are the first <span class="math">N/2^{j}</span> elements in each of <span class="math">W1[h]</span> and <span class="math">W2[h]</span>; regard <span class="math">W[h]</span> as their concatenation.</p>

    <p class="text-gray-300">To see why this gives the correct result, notice that each Collapse invocation combines neighboring values of its input array. We can thus regard the values of <span class="math">W[h]</span> as the leaves of a binary tree, and Collapse as reducing the height of the tree by one, combining leaves into their parents. In this view, <span class="math">W1[h]</span> and <span class="math">W2[h]</span> represent the left and right subtrees corresponding to <span class="math">W[h]</span>. As a result, in round <span class="math">j=b_{N}</span>, <span class="math">W1[h]</span> and <span class="math">W2[h]</span> each have one value; to obtain the final value of the Collapse operation, compute <span class="math">(1-r)\\cdot W1[h][0]+r\\cdot W2[h][0]</span>.</p>

    <p class="text-gray-300">To implement this picture in hardware, Giraffe instantiates two RWSRs, each of half the size. For even more parallelism, observe that each RWSR corresponds to a subtree of the full computation, and thus its work can be recursively split into two even smaller RWSRs, each handling a correspondingly smaller subtree. Because of this structure, different choices of parallelism do not require the designer to do any additional design work (§5).</p>

    <h3 id="sec-11" class="text-xl font-semibold mt-8">3.3 Scaling and optimizing <span class="math">\\mathcal{V}</span></h3>

    <p class="text-gray-300">In this section, we explain how <span class="math">\\mathcal{V}</span> meets the starting design goals of scalability, efficiency, and automation. We do so by walking through three main costs for <span class="math">\\mathcal{V}</span>, and how Giraffe handles them. Some of the optimizations apply to any CMT-based back-end <em>[36, 77, 79, 81, 82, 87]</em>.</p>

    <h4 id="sec-12" class="text-lg font-semibold mt-6">Multilinear extensions of I/O</h4>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math">\\mathcal{V}</span>’s principal bottleneck is computing the multilinear extension of its input <span class="math">x</span> and output <span class="math">y</span> (Figure 1, lines 3 and 32). Recall (§2.2) that $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">=</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">y</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">=N\\cdot G<span class="math">; </span>\\mathcal{V}<span class="math">’s computation has at least this cost. When </span>N<span class="math"> and </span>G<span class="math"> are large, this is expensive and must be broken into parallel and serial portions. We show below that this work has a similar form to </span>\\mathcal{P}$’s (termL, termR; §3.2).</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Consider the input <span class="math">x</span> and <span class="math">\\tilde{V}_{d}</span> (<span class="math">y</span> and <span class="math">\\tilde{V}_{y}</span> are similar). <span class="math">\\mathcal{V}</span> must compute <span class="math">\\tilde{V}_{d}(q_{d^{\\prime}}^{\\prime}\\,q_{d})</span>. For <span class="math">\\sigma\\in[0,\\ N\\cdot G-1]</span>, <span class="math">\\tilde{V}_{d}(\\sigma)=V_{d}(\\sigma)</span>, the <span class="math">\\sigma</span>th component of the input (§2.2). For <span class="math">\\sigma\\in\\{0,1\\}^{b_{N}+b_{G}-\\ell}</span>, we have</p>

    <p class="text-gray-300"><span class="math">\\tilde{V}_{d}\\left(r[1..\\ell],\\sigma\\right)</span> <span class="math">=(1-r[\\ell])\\cdot\\tilde{V}_{d}\\left(r[1..\\ell-1],0,\\sigma\\right)</span> <span class="math">\\quad+r[\\ell]\\cdot\\tilde{V}_{d}\\left(r[1..\\ell-1],1,\\sigma\\right).</span> (6)</p>

    <p class="text-gray-300">This form is very close to Equation (5) (its derivation is similar to Appx. B.1). It follows that <span class="math">\\mathcal{V}</span> can use <span class="math">\\mathcal{P}</span>’s EvalTermLR to evaluate <span class="math">\\tilde{V}_{d}(q_{d^{\\prime}}^{\\prime}\\,q_{d})</span>: <span class="math">\\mathcal{V}</span> initializes an array <span class="math">A</span>, setting <span class="math">A[\\sigma]</span> to the <span class="math">\\sigma</span>th input value, for <span class="math">\\sigma\\in[0,\\ N\\cdot G-1]</span> (cf. line 3, Fig. 2). <span class="math">\\mathcal{V}</span> then invokes the algorithm MultiCollapse shown in Figure 4 on <span class="math">A</span>, setting <span class="math">r</span> to <span class="math">(q_{d^{\\prime}}^{\\prime}\\,q_{d})</span>. In total, MultiCollapse costs <span class="math">2\\cdot N\\cdot G-2</span> multiplications. To see how, notice that the initial size of <span class="math">A</span> is <span class="math">N\\cdot G</span>, so the first Collapse invocation costs <span class="math">N\\cdot G</span> multiplications; in each successive invocation, the cost is reduced by half. Summing gives the claimed cost.</p>

    <p class="text-gray-300">MultiCollapse also applies to related systems, improving their constant factors; this is significant because in practice computing the multilinear extensions of <span class="math">x</span> and <span class="math">y</span> dominates <span class="math">\\mathcal{V}</span>’s costs. Allspice’s approach to this computation has leading constant <span class="math">4</span> <em>[81, §5.1]</em>. Zebra <em>[82]</em> reduces the constant to <span class="math">3</span> using a hand-tuned hardware structure; this does not meet Giraffe’s goal of producing designs automatically. MultiCollapse reduces this constant to <span class="math">2</span>. We now show how to reduce the constant to <span class="math">1</span>, leaving <span class="math">\\mathcal{V}</span> with a <span class="math">4\\sqrt{N\\cdot G}</span> <em>additive</em> overhead. For the smallest problem sizes on which Giraffe breaks even (§6–8) this additive overhead is less than <span class="math">25\\%</span>; on large computations it is negligible.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">1: // MultiCollapse costs  $2</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">- 2$  multiplications; see text</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">3: loglen  <span class="math">\\leftarrow</span>  log  $</td>

            <td class="px-3 py-2 border-b border-gray-700">A</td>

            <td class="px-3 py-2 border-b border-gray-700">$</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">4: for  <span class="math">j = 1, \\ldots, \\log</span>  do 5: Collapse(A,  <span class="math">2^{\\log \\mathrm{len} + 1 - j}, r[j]</span> ) // see Figure 2, line 14 6: return A[0] 7:</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">8: function DOTPMULTICOLLAPSE (Array  <span class="math">A</span> ,  $r \\in \\mathbb{F}^{\\log</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}$ )</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">10: 11: //  <span class="math">B</span>  is computed using the algorithm of Fig. 13, Appx. A; 12: // this costs  <span class="math">2 \\cdot 2^{b_{\\mathrm{tot}} / 2} - 2 = 2\\sqrt{N \\cdot G} - 2</span>  multiplications. 13:  <span class="math">B\\gets \\chi_{\\gamma}(r[1..b_{\\mathrm{tot}} / 2]),\\gamma \\in \\{0,1\\}^{b_{\\mathrm{tot}} / 2}</span> 14: 15: // compute  <span class="math">2^{b_{\\mathrm{tot}} / 2}</span>  dot products of length  <span class="math">2^{b_{\\mathrm{tot}} / 2}</span> ; 16: // this costs  <span class="math">N \\cdot G</span>  multiplications 17: for  <span class="math">j = 0, \\dots, 2^{b_{\\mathrm{tot}} / 2} - 1</span>  do 18:  <span class="math">A&#x27;[j] \\gets \\langle B, A[j \\cdot 2^{b_{\\mathrm{tot}} / 2}..(j + 1) \\cdot 2^{b_{\\mathrm{tot}} / 2}] \\rangle</span>  // dot product 19: 20: // MultiCollapse on  <span class="math">A&#x27;</span>  costs  <span class="math">2 \\cdot 2^{b_{\\mathrm{tot}} / 2} - 2</span>  multiplications 21: return MultiCollapse  <span class="math">(A&#x27;, 2^{b_{\\mathrm{tot}} / 2}, r[b_{\\mathrm{tot}} / 2 + 1..b_{\\mathrm{tot}}])</span></p>

    <p class="text-gray-300">FIGURE 4—MultiCollapse and DotPMultiCollapse evaluate the multilinear extensions of  <span class="math">x</span>  and  <span class="math">y</span>  with lower overhead than prior work.</p>

    <p class="text-gray-300">Notice that MultiCollapse describes a binary computation tree of depth  <span class="math">b_{\\mathrm{tot}} = b_N + b_G</span>  whose leaves are the inputs and whose root is the result  <span class="math">\\tilde{V}_d(q_d&#x27;, q_d)</span> . Each node in this tree corresponds to a single step of Collapse, namely two multiplications and one addition (Fig. 2, line 16). The  <span class="math">b_{\\mathrm{tot}} / 2</span>  layers of the computation tree closest to the leaves comprise  <span class="math">2^{b_{\\mathrm{tot}} / 2} = \\sqrt{N \\cdot G}</span>  subtrees, each of which computes the dot product between  <span class="math">2^{b_{\\mathrm{tot}} / 2}</span>  input values and an array  <span class="math">B[\\gamma] = \\chi_\\gamma(r[1..b_{\\mathrm{tot}} / 2])</span> ,  <span class="math">\\gamma \\in \\{0,1\\}^{b_{\\mathrm{tot}} / 2}</span> . This can be seen by expanding the recurrence of Equation (6) for  <span class="math">b_{\\mathrm{tot}} / 2</span>  steps.</p>

    <p class="text-gray-300">The key observation is that MultiCollapse repeatedly recomputes the values of the array  <span class="math">B</span>  when computing these dot products. This means that each subtree costs  <span class="math">2 \\cdot 2^{b_{\\mathrm{tot}} / 2} - 2</span>  multiplications, whereas the dot product costs just  <span class="math">2^{b_{\\mathrm{tot}} / 2}</span>  multiplications once  <span class="math">B</span>  has been computed. DotPMultiCollapse (Fig. 4) improves on MultiCollapse's running time by precomputing  <span class="math">B</span>  once and amortizing that cost over all  <span class="math">2^{b_{\\mathrm{tot}} / 2}</span>  subtrees. To see that two algorithms are equivalent, notice that the  <span class="math">b_{\\mathrm{tot}} / 2</span>  layers of the MultiCollapse computation tree closest to the leaves (which correspond to the first  <span class="math">b_{\\mathrm{tot}} / 2</span>  Collapse invocations) compute the same  <span class="math">2^{b_{\\mathrm{tot}} / 2}</span>  dot products that DotPMultiCollapse stores in  <span class="math">A&#x27;</span>  (for the reasons described above), and that the two algorithms proceed identically thereafter. But DotPMultiCollapse costs just  <span class="math">N \\cdot G + 4 \\cdot 2^{b_{\\mathrm{tot}} / 2} - 4 = N \\cdot G + 4\\sqrt{N \\cdot G} - 4</span>  multiplications in total (see comments in Fig. 4).</p>

    <p class="text-gray-300">DotPMultiCollapse's hardware design uses primitives from other parts of  <span class="math">\\mathcal{V}</span>  and  <span class="math">\\mathcal{P}</span> . MultiCollapse reuses the same design that  <span class="math">\\mathcal{P}</span>  uses for EvalTermLR. The hardware for computing the array  <span class="math">B</span>  shares its design with  <span class="math">\\mathcal{V}</span> 's precomputation hardware ("Precomputation," below; Appx. B.2). The dot product computations are independent of one another and thus easily parallelized using separate multiply-and-accumulate units, which are standard.</p>

    <h4 id="sec-13" class="text-lg font-semibold mt-6">Polynomial evaluation.</h4>

    <p class="text-gray-300">The protocol requires <span class="math">\\mathcal{V}</span> to evaluate polynomials (specified by <span class="math">\\mathcal{P}</span>) at randomly chosen points (specified by <span class="math">\\mathcal{V}</span>). This occurs after the sum-check invocation (Fig. 1, line 26) and in each round of the sum-check protocol (Appx. B; Figure 11, line 21). Our description here focuses on the former: the degree-<span class="math">b_{G}</span> polynomial <span class="math">H</span>, evaluated at <span class="math">\\tau</span>. Giraffe applies the same technique to the latter, namely computing <span class="math">F(r_{j})</span>, but those polynomials are degree-2 or 3, and thus the savings are less pronounced.</p>

    <p class="text-gray-300">In the baseline approach <em>[36, 77, 81, 82]</em> to computing <span class="math">H(\\tau)</span>, <span class="math">\\mathcal{P}</span> sends evaluations (meaning <span class="math">H(0),\\ldots,H(b_{G})</span>), and <span class="math">\\mathcal{V}</span> uses Lagrange interpolation. (Lagrange interpolation expresses <span class="math">H(\\tau)</span> as <span class="math">\\sum_{j=0}^{b_{G}}H(j)\\cdot f_{j}(\\tau)</span>; the <span class="math">\\{f_{j}(\\cdot)\\}</span> are basis polynomials.) But interpolation costs <span class="math">O(b_{G}^{2})</span> <em>[55]</em> for each polynomial (one per layer), making it <span class="math">O(d\\log{}^{2}G)</span> overall. Prior work <em>[81, 82]</em> cut this to O(<span class="math">d\\log G</span>), by precomputing <span class="math">\\{f_{j}(\\tau)\\}</span>, and not charging for that.</p>

    <p class="text-gray-300">Giraffe observes that the protocol works the same if <span class="math">\\mathcal{P}</span> describes <span class="math">H</span> in terms of its coefficients; this is because coefficients and evaluations are informationally equivalent. Thus, in Giraffe, <span class="math">\\mathcal{P}</span> recovers the coefficients by interpolating the evaluations of <span class="math">H</span>, incurring cost <span class="math">O(d\\log{}^{2}G)</span>. <span class="math">\\mathcal{V}</span> uses the coefficients to evaluate <span class="math">H(\\tau)</span> via Horner’s rule <em>[55]</em>. The cost to <span class="math">\\mathcal{V}</span> is now O(<span class="math">b_{G}</span>) per layer, or O(<span class="math">d\\log G</span>) in total, without relying on precomputation.</p>

    <p class="text-gray-300">Summarizing, <span class="math">\\mathcal{V}</span> shifts its burden to <span class="math">\\mathcal{P}</span>, and in return saves a factor <span class="math">\\log G</span>. This refinement is sensible if the same operation at <span class="math">\\mathcal{P}</span> is substantially cheaper (by at least a <span class="math">\\log G</span> factor) than at <span class="math">\\mathcal{V}</span>. This easily holds in the VA context. But it also holds in other contexts in which one would use a CMT-based back-end: if cycles at <span class="math">\\mathcal{P}</span> were not substantially cheaper than at <span class="math">\\mathcal{V}</span>, the latter would not be outsourcing to the former in the first place.</p>

    <h4 id="sec-14" class="text-lg font-semibold mt-6">Precomputation.</h4>

    <p class="text-gray-300"><span class="math">\\mathcal{V}</span> must compute <span class="math">P^{<em>}_{q,i}(r^{\\prime},r_{0},r_{1},)</span>, given claimed <span class="math">\\tilde{V}_{i}(r^{\\prime},r_{0})</span> and <span class="math">\\tilde{V}_{i}(r^{\\prime},r_{1})</span>: Figure 1, lines 20–21. The main costs are computing <span class="math">\\mathrm{a}\\tilde{\\mathrm{d}}\\mathrm{d}_{i}(q,r_{0},r_{1})</span>, <span class="math">\\mathrm{m}\\tilde{\\mathrm{u}}\\mathrm{l}\\mathrm{t}_{i}(q,r_{0},r_{1})</span>, and <span class="math">\\tilde{\\mathrm{e}}\\tilde{\\mathrm{q}}(q^{\\prime},r^{\\prime})</span>. This costs <span class="math">O(G)</span> per layer </em>[81]<em>, and hence <span class="math">O(d\\cdot G)</span> overall. (Appx. A describes the approach; Appx. B.2 briefly discusses the hardware design.) This is the “precomputation” in our context, and what was not charged in prior work in the VA setting </em>[82, §4]*. We note that this is not precomputation per se—it’s done alongside the rest of the protocol—but we retain the vocabulary because of the cost profile: the work is proportional to executing one sub-AC, is input-independent, and is incurred once per sum-check invocation, thereby amortizing over all <span class="math">N</span> sub-ACs.</p>

    <h2 id="sec-15" class="text-2xl font-bold">4 Front-end design</h2>

    <p class="text-gray-300">Giraffe’s front-end compiles a C program into one or more pieces, each of which can be outsourced using the back-end machinery. The front-end incorporates two program transformation techniques that broaden the scope of computations amenable to outsourcing:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Slicing breaks up computations that are too large to be outsourced as a whole or contain parts that cannot be profitably outsourced.</li>

      <li>Squashing rearranges repeated, serial computations like loops to produce data-parallel computations.</li>

    </ul>

    <p class="text-gray-300">While squashing makes some sequential computations amenable to execution in Giraffe’s data-parallel setting (§2.2, §3.1), slicing does not yield data-parallel ACs; thus, outsourcing a sliced computation requires executing multiple copies of the computation in parallel.</p>

    <h4 id="sec-16" class="text-lg font-semibold mt-6">Slicing.</h4>

    <p class="text-gray-300">One approach to handling large outsourced computations is to break the computation into smaller pieces and then to either outsource each piece or to execute it locally at the verifier.</p>

    <p class="text-gray-300">This approach works as follows: a compiler breaks an input program into slices and decides, for each slice, whether to outsource or to execute locally (we describe this process below). The compiler</p>

    <p class="text-gray-300">converts each slice to be outsourced into an AC whose inputs are the program state prior to executing the slice and whose outputs are the program state after execution. To execute a sliced computation, the verifier runs glue code that passes inputs and outputs between slices, executes non-outsourced slices, and orchestrates the back-end machinery. We call this glue code the computation's manifest.</p>

    <p class="text-gray-300">Giraffe's slicing algorithm takes one parameter, a cost model for the target back-end. The algorithm's input is a C program with the following restrictions (commonly imposed by the most efficient front-ends [37, 66, 83, 84]): loop bounds are statically computable, no recursive functions, and no function pointers.</p>

    <p class="text-gray-300">The algorithm first inlines all function calls. It then considers candidate slices comprising consecutive subsequences of top-level program statements. The algorithm transforms each candidate into an AC and uses the back-end cost model to determine the cost to outsource. Then, using a greedy heuristic, the algorithm chooses for outsourcing a set of non-overlapping slices, aiming to maximize savings. Finally, the algorithm handles parts of the program not in any of the outsourced slices: it adds atomic statements (e.g., assignments) to the manifest for local execution, and recursively invokes itself on non-atomic statements (e.g., the branches of if-else statements) to identify more outsourcing opportunities.</p>

    <p class="text-gray-300">Giraffe assumes that the same back-end is used for all sliced subcomputations, but this approach generalizes to considering multiple back-ends simultaneously [45, 81].</p>

    <p class="text-gray-300">Squashing. Giraffe's second technique, squashing, turns a deep but narrow computation (for example, a loop) into a data-parallel one by laying identical chunks of the computation (e.g., iterations of a loop) side by side. The result is a squashed AC. The intermediate values at the output of each chunk in the original computation become additional inputs and outputs of the squashed AC.  <span class="math">\\mathcal{P}</span>  communicates these to  <span class="math">\\mathcal{V}</span> , which uses them to construct the input and output vectors for the squashed AC. This technique also generalizes to the case of code "between" the chunks.</p>

    <p class="text-gray-300">Giraffe's squashing transformation takes C code as input and applies a simple heuristic: the analysis assumes that chunks start and end at loop boundaries and comprise one or more loop iterations. Consider a loop with  <span class="math">I</span>  dependent iterations of a computation  <span class="math">F</span> , where  <span class="math">F</span>  corresponds to an AC of depth  <span class="math">d</span>  and uniform width  <span class="math">G</span> . The squasher chooses  <span class="math">N</span>  such that each chunk contains  <span class="math">I / N</span>  unrolled iterations, and generates a sub-AC of width  <span class="math">G</span>  and depth  <span class="math">d&#x27; = I \\cdot d / N</span> , subject to a supplied cost model.</p>

    <p class="text-gray-300">Putting it together. Giraffe's front-end compiles C programs by combining slicing and squashing. In particular, Giraffe's front-end applies the slicing algorithm as described above except that, when estimating the cost of candidate slices, the front-end also tries to apply the squashing transformation. If a candidate slice can be squashed, the slicer uses the squashed version of the slice instead.</p>

    <p class="text-gray-300">Front-end. The front-end produces an executable manifest in Python plus a high-level AC description for each outsourced slice (these are similar to the one used by Allspice [81] and Zebra). Outsourced slices in the manifest are executed using the simulation framework (below). The front-end comprises about 6100 lines of Scala and 300 lines of miscellaneous glue.</p>

    <p class="text-gray-300">Back-end. Giraffe's back-end has two components. The first is a compiler that takes high-level AC descriptions from the front-end along with technology node specifications, chooses  <span class="math">\\mathcal{P}</span> 's and  <span class="math">\\mathcal{V}</span> 's</p>

    <p class="text-gray-300">7For some back-ends the chunks need not be identical. All CMT-derived back-ends [36, 81, 82] (including T13) have lower costs when working over shallow and wide ACs (vs. narrow but deep ones), so squashing is useful even outside T13's data-parallel regime.</p>

    <p class="text-gray-300">8This heuristic suffices in many cases because loops naturally express repeated subcomputations; more sophisticated analyses exist, e.g., automatic parallelization [28, 29, 33].</p>

    <p class="text-gray-300">hardware parallelism (§3.2; Fig. 16, Appx. C) to optimize throughput and chip area (§6.2), and automatically produces <span class="math">\\mathcal{P}</span> and <span class="math">\\mathcal{V}</span> designs in fully synthesizable SystemVerilog, The second is a cycle-accurate simulation framework built on Icarus Verilog <em>[85]</em>. The back-end comprises 14 600 lines of SystemVerilog, 6800 lines of C/C++, 3300 lines of Python, and 600 lines of miscellaneous glue. The SystemVerilog and C/C++ borrow primitives from Zebra <em>[4]</em>.</p>

    <h2 id="sec-18" class="text-2xl font-bold">6 Back-end evaluation</h2>

    <p class="text-gray-300">We evaluate Giraffe’s back-end by answering:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>When does Giraffe beat “native” (§2.3)?</li>

      <li>What is the largest computation Giraffe supports?</li>

      <li>How does Giraffe’s performance vary with computation and physical parameters?</li>

    </ol>

    <p class="text-gray-300">In Appx. D we also measure the effect of Giraffe’s protocol improvements (§3.1). In sum, they reduce <span class="math">\\mathcal{P}</span>’s cost by a <span class="math">\\log G</span> factor.</p>

    <p class="text-gray-300">Throughout the evaluation we assume that Giraffe is applied to computations most efficiently expressed as a sequence of field operations; we discuss this applicability limitation in Section 9.</p>

    <h3 id="sec-19" class="text-xl font-semibold mt-8">6.1 Cross-over and scaling</h3>

    <h4 id="sec-20" class="text-lg font-semibold mt-6">Method.</h4>

    <p class="text-gray-300">We consider a generic computation in the form of an arithmetic circuit <span class="math">\\mathcal{C}</span> with depth <span class="math">d</span>, sub-AC width <span class="math">G</span>, number of parallel copies <span class="math">N</span>, and fraction of multipliers <span class="math">\\delta</span>. The baseline is direct evaluation of <span class="math">\\mathcal{C}</span> on the same technology node as <span class="math">\\mathcal{V}</span>. To measure the energy cost for the baseline, we sum the total cost of field operations plus the energy associated with receiving inputs and transmitting outputs of the computation.</p>

    <p class="text-gray-300">For Giraffe’s energy costs, we use a combination of simulation and modeling. The simulations are cycle-accurate Verilog simulations of Giraffe’s execution. From these simulations we extract a model for energy costs, parameterized by technology node (a simplified model is given in Fig. 16, Appx. C), and we spot check with additional simulations to check this model. Practical considerations demand this approach: simulating Giraffe over a broad range of parameters would be prohibitively time consuming.</p>

    <p class="text-gray-300">We account for all costs for both <span class="math">\\mathcal{V}</span> and <span class="math">\\mathcal{P}</span>: protocol execution, <span class="math">\\mathcal{V}</span>-<span class="math">\\mathcal{P}</span> communication, storage, random number generation, and the cost to receive inputs and transmit outputs. We simplify the accounting of the protocol execution’s energy cost by counting just the energy consumed by field operations. This approximation neglects the energy consumed by control logic and miscellaneous circuitry associated with protocol execution. As in prior work <em>[82, §7.2]</em>, we expect these costs to be negligible; confirming this is future work. Computations in this section are over <span class="math">\\mathbb{F}_{p},p=2^{61}-1</span>. Costs for trusted and untrusted technology nodes (arithmetic, communication, storage, random number generation, and I/O circuits) are from prior work <em>[82, Figs. 6–7]</em>.</p>

    <h4 id="sec-21" class="text-lg font-semibold mt-6">Results.</h4>

    <p class="text-gray-300">Figure 5 compares Giraffe with the baseline. Giraffe’s total cost is dominated by <span class="math">\\mathcal{V}</span>; <span class="math">\\mathcal{P}</span>’s cost is at most a few percent of the total. For small <span class="math">N</span>, <span class="math">\\mathcal{V}</span>’s precomputation (§3.3) dominates. As <span class="math">N</span> increases, <span class="math">\\mathcal{V}</span>’s multilinear extension evaluation (§3.3) dominates.</p>

    <p class="text-gray-300">The cross-over point for savings versus native in Figure 5 is roughly 30 copies. This value is relatively insensitive to <span class="math">G</span> because both precomputation cost and per-sub-AC savings are proportional to <span class="math">G</span>, and they offset. Varying <span class="math">G</span> and otherwise fixing the parameters as in Figure 5, we find crossover <span class="math">N</span> ranges from an extreme of <span class="math">\\approx</span>40 at <span class="math">G=8</span> to <span class="math">\\approx</span>20 for any <span class="math">G&gt;2^{10}</span>.</p>

    <p class="text-gray-300">###</p>

    <p class="text-gray-300">!<a href="img-0.jpeg">img-0.jpeg</a> FIGURE 5—Evaluation of Giraffe's back-end. We compare Giraffe's costs to the native baseline, varying  <span class="math">N</span> . Giraffe beats native for  <span class="math">N \\approx 30</span> . Fixed AC parameters are: depth  <span class="math">d = 20</span> ; width of sub-AC  <span class="math">G = 2^8</span> ; fraction of multipliers  <span class="math">\\delta = 0.5</span> ; trusted technology  <span class="math">= 350</span>  nm; untrusted technology  <span class="math">= 7</span>  nm; maximum chip area  <span class="math">A_{\\mathrm{max}} = 200</span>  mm²; maximum power dissipation  <span class="math">P_{\\mathrm{max}} = 150</span>  W. Per-gate energy costs for trusted and untrusted nodes are the same as in prior work [82, Figs. 6–7]. In Section 6.2 we consider manufacturing costs; there, Giraffe is less competitive with native.</p>

    <p class="text-gray-300">For the concrete costs we consider here, Giraffe can handle about  <span class="math">2^{16}</span>  parallel executions of a sub-AC with  <span class="math">G = 2^8</span> ,  <span class="math">d = 20</span> , or about 80 million gates total. It can also handle sub-ACs as large as  <span class="math">\\approx 1.5</span>  million gates. For a given hardware substrate, the maximum  <span class="math">N \\cdot G</span>  product is nearly fixed.  <span class="math">\\mathcal{P}</span> 's costs increase with  <span class="math">d</span>  (Fig. 16, Appx. C), so maximum size shrinks as  <span class="math">d</span>  increases. On  <span class="math">A / T</span>  ( <span class="math">\\S 2.3</span> ), Giraffe is not as competitive with the baseline ( <span class="math">\\S 6.2</span> ).</p>

    <p class="text-gray-300">We now explore Giraffe's performance compared to the native baseline on generic arithmetic circuits characterized by  <span class="math">d</span> ,  <span class="math">G</span> ,  <span class="math">N</span> , and  <span class="math">\\delta</span> , and on different technology nodes.</p>

    <p class="text-gray-300">Method. In addition to energy, we now consider manufacturing cost for a given performance level. Our metric is  <span class="math">A_{s} / T</span>  [82].  <span class="math">T</span>  is throughput.  <span class="math">A_{s} = A_{\\mathcal{V}} + A_{\\mathcal{P}} / s</span> , a weighted sum of  <span class="math">\\mathcal{V}</span> 's and  <span class="math">\\mathcal{P}</span> 's chip area;  <span class="math">s</span>  accounts for the difference between untrusted and trusted manufacturing costs. We do not know the exact value of  <span class="math">s</span> , as this depends on the specifics of the technology nodes being used; thus, we evaluate manufacturing cost over a range of values,  <span class="math">s \\in \\{1/3, 1, 3, 10\\}</span> , consistent with prior work [82].</p>

    <p class="text-gray-300">We use the same simulations and detailed cost modeling as in Section 6.1 to compute costs for Giraffe. As a proxy for chip area dedicated to protocol execution, we use the area occupied by field adder and multiplier circuits. This neglects area dedicated to control logic and miscellaneous circuitry associated with protocol execution, but as in prior work [82, §7.2] we expect these costs to be negligible; confirming this is future work.</p>

    <p class="text-gray-300">For throughput, we use cycle-accurate Verilog simulations to measure the delay of each stage of the execution and proving pipeline (Appx. C). End-to-end throughput is given by the inverse of the maximum delay in any stage of the computation. Concrete costs are the same as in Section 6.1. For each experiment we vary one parameter and fix the others; fixed parameters are  <span class="math">d = 20</span> ,  <span class="math">G = 2^8</span> ,  <span class="math">N = 2^{10}</span> ,  <span class="math">\\delta = 0.5</span> , trusted technology node  <span class="math">= 350</span>  nm, and untrusted technology node  <span class="math">= 7</span>  nm.</p>

    <p class="text-gray-300">For the native baseline, we optimize  <span class="math">A / T</span>  given  <span class="math">A_{\\mathrm{max}}</span>  subject to the arithmetic circuit's layering constraints.</p>

    <p class="text-gray-300">Optimizing  <span class="math">A_{s} / T</span>  in Giraffe. We optimize Giraffe's  <span class="math">A_{s} / T</span>  by controlling the amount of hardware parallelism (Appx. C). First, we fix  <span class="math">\\mathcal{V}</span> 's area equal to native baseline, which is no more than  <span class="math">A_{\\mathrm{max}}</span> . We also limit  <span class="math">\\mathcal{P}</span> 's area to no more than  <span class="math">A_{\\mathrm{max}}</span>  and fix  <span class="math">n_{\\mathcal{P},\\mathrm{pl}} = d</span> . Then we optimize  <span class="math">n_{\\mathcal{V},\\mathrm{io}}</span>  and  <span class="math">n_{\\mathcal{V},\\mathrm{sc}}</span>  based on available area and relative delay of sum-check computations and multilinear extensions of inputs and</p>

    <p class="text-gray-300">!<a href="img-1.jpeg">img-1.jpeg</a> (a) Performance vs.  <span class="math">d</span></p>

    <p class="text-gray-300">!<a href="img-2.jpeg">img-2.jpeg</a> (b) Performance vs.  <span class="math">G</span></p>

    <p class="text-gray-300">!<a href="img-3.jpeg">img-3.jpeg</a> (c) Performance vs.  <span class="math">N</span></p>

    <p class="text-gray-300">!<a href="img-4.jpeg">img-4.jpeg</a> (d) Performance vs.  <span class="math">\\delta</span></p>

    <p class="text-gray-300">!<a href="img-5.jpeg">img-5.jpeg</a> (e) Performance vs. trusted technology node.</p>

    <p class="text-gray-300">!<a href="img-6.jpeg">img-6.jpeg</a> (f) Performance vs. untrusted technology node. FIGURE 6—Giraffe's overall performance ( <span class="math">\\mathcal{V}</span>  and  <span class="math">\\mathcal{P}</span>  costs) compared to native baseline on  <span class="math">E</span>  and  <span class="math">A_{s} / T</span>  metrics (§6.2), varying AC parameters and technology nodes. In each case, we vary one parameter and fix the rest. Fixed parameters are: depth of  <span class="math">C</span> ,  <span class="math">d = 20</span> ; width of subcircuit  <span class="math">G = 2^{8}</span> ; number of sub-AC copies  <span class="math">N = 2^{10}</span> ; fraction of multipliers  <span class="math">\\delta = 0.5</span> ; trusted technology node  <span class="math">= 350 \\mathrm{~nm}</span> ; untrusted technology node  <span class="math">= 7 \\mathrm{~nm}</span> ; maximum chip area  <span class="math">A_{\\max} = 200 \\mathrm{~mm}^{2}</span> ; maximum power dissipation  <span class="math">P_{\\max} = 150 \\mathrm{~W}</span> .</p>

    <p class="text-gray-300">outputs. Finally, given  <span class="math">\\mathcal{V}</span> 's optimal delay value, we search for settings of  <span class="math">n_{\\mathcal{P},\\mathrm{ea}}</span> ,  <span class="math">n_{\\mathcal{P},\\tilde{\\nabla}}</span> , and  <span class="math">n_{\\mathcal{P},\\mathrm{sc}}</span>  that optimize  <span class="math">A_s / T</span> .</p>

    <p class="text-gray-300">Results. Figure 6 summarizes results. Giraffe's operating cost (i.e., energy consumption) beats the baseline's over a wide range of AC parameters and hardware substrates.</p>

    <p class="text-gray-300">As in Section 6.1, energy cost is dominated by  <span class="math">\\mathcal{V}</span> . Savings increase with  <span class="math">d</span>  (Fig. 6a) because  <span class="math">\\mathcal{V}</span> 's per-layer work is much less than the native baseline's. Similarly, as  <span class="math">\\delta</span>  increases (Fig. 6d), the native baseline's costs increase but  <span class="math">\\mathcal{V}</span> 's do not.  <span class="math">\\mathcal{V}</span> 's savings are insensitive to  <span class="math">G</span>  (Fig. 6b): the cost of multilinear extensions of I/O scales with  <span class="math">G</span> , balancing the increased savings in per-layer work.</p>

    <p class="text-gray-300">Manufacturing costs are often dominated by  <span class="math">\\mathcal{P}</span> . As  <span class="math">G</span>  increases (Fig. 6b),  <span class="math">\\mathcal{P}</span> 's area also increases (§3.2). As  <span class="math">N</span>  increases (Fig. 6c),  <span class="math">\\mathcal{P}</span> 's storage costs increase (Fig. 16, Appx. C). In these cases, even if Giraffe's operating costs are better than the native baseline's, its manufacturing costs at a given performance level may be worse.</p>

    <p class="text-gray-300">Finally, as the gap between the trusted and untrusted technology nodes shrinks (Figs. 6e and 6f),  <span class="math">\\mathcal{P}</span> 's energy cost increases relative to  <span class="math">\\mathcal{V}</span> 's, reducing overall performance versus the native baseline. As the trusted technology node gets more advanced (i.e., smaller, Fig. 6f),  <span class="math">\\mathcal{V}</span> 's throughput increases and thus  <span class="math">\\mathcal{P}</span> 's size must increase to avoid becoming a bottleneck. As the untrusted technology node gets less advanced (i.e., bigger, Fig. 6e),  <span class="math">\\mathcal{P}</span> 's area grows and throughput decreases, making  <span class="math">A_{s} / T</span>  worse.</p>

    <p class="text-gray-300">!<a href="img-7.jpeg">img-7.jpeg</a> (a) Slicing: a simple computation.</p>

    <p class="text-gray-300">!<a href="img-8.jpeg">img-8.jpeg</a> (c) Slicing: conditionals.</p>

    <p class="text-gray-300">!<a href="img-9.jpeg">img-9.jpeg</a> (e) Squashing: dependent iterations.</p>

    <p class="text-gray-300">!<a href="img-10.jpeg">img-10.jpeg</a> (b) Simple slicing vs.  <span class="math">\\delta_{1}</span> .</p>

    <p class="text-gray-300">!<a href="img-11.jpeg">img-11.jpeg</a> (d) Conditional slicing vs.  <span class="math">\\delta_{1}</span> .</p>

    <p class="text-gray-300">!<a href="img-12.jpeg">img-12.jpeg</a> (f) Squashing vs. number of iterations. FIGURE 7—Evaluation of Giraffe's front-end. Higher is better. F1 and F2 are computations corresponding to arithmetic circuits with  <span class="math">N = 2^{10}</span> ,  <span class="math">G = 2^8</span> ,  <span class="math">d = 20</span> .  <span class="math">\\delta_1</span>  and  <span class="math">\\delta_2</span>  are the fraction of multipliers in F1 and F2, respectively; we fix  <span class="math">\\delta_2 = 0.05</span> . Figures 7a and 7c show inputs to Giraffe's slicing transformation. In Figures 7b and 7d, we vary  <span class="math">\\delta_1</span> , which changes whether F1 is amenable to outsourcing. We compare the efficacy of outsourcing the full computation and of first applying the slicing transform; when outsourcing would not result in savings, Giraffe executes the computation natively. Figure 7e is a deep loop with dependent iterations. Giraffe converts this to a data-parallel computation that can be outsourced, saving compared to native execution.</p>

    <p class="text-gray-300">This section answers the following questions:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>How does slicing result in savings compared to full outsourcing and native execution?</li>

      <li>For deep loops with dependent iterations, how effective is squashing at extracting parallelism?</li>

    </ol>

    <p class="text-gray-300">Setup and method. We create a sequence of programs written in C, each containing two generic blocks, F1 and F2, consisting of purely arithmetic computations. Among the programs, these blocks vary in the fraction  <span class="math">\\delta_{1}</span>  and  <span class="math">\\delta_{2}</span>  of multipliers, width of computation ( <span class="math">G_{1}, G_{2}</span>  respectively), the depth of the computation ( <span class="math">d_{1}, d_{2}</span>  respectively), and number of parallel instances  <span class="math">N</span> . Unless specified, we fix  <span class="math">N = 2^{10}</span> ,  <span class="math">G_{1} = G_{2} = 2^{8}</span> ,  <span class="math">d_{1} = d_{2} = 20</span> ,  <span class="math">\\delta_{2} = 0.05</span> .</p>

    <p class="text-gray-300">We consider two baselines: native execution and full outsourcing. The cost of native execution is defined as in the prior section: the cost of computation in the same technology node as  <span class="math">\\mathcal{V}</span> . We estimate the cost of full outsourcing by applying Giraffe's back-end to the raw program, without Giraffe's front-end transformations (§4).</p>

    <p class="text-gray-300">To compute costs for Giraffe, we apply the selected transformation to produce a manifest (§4), then evaluate the total cost of execution, as dictated by that manifest. We use the model of Section 6 to determine the cost of the outsourced portions of the manifest. For local computations, we sum the cost of all field operations, as in the native baseline.</p>

    <p class="text-gray-300">Slicing. We consider a simple case and then conditionals.</p>

    <p class="text-gray-300">Warmup. Consider the computation of Figure 7a. We vary  <span class="math">\\delta_{1}</span>  from 0 to 1. The front-end decides for F1 and F2 either to outsource or to execute locally. Note that F1's amenability to outsourcing depends on  <span class="math">\\delta_{1}</span> : native execution cost increases with  <span class="math">\\delta_{1}</span>  (multiplies are more expensive than adds) while  <span class="math">\\mathcal{V}</span> 's protocol costs depend only on AC size. Because  <span class="math">\\delta_{2} = 0.05</span> , F2 is not amenable to outsourcing: it native execution</p>

    <p class="text-gray-300">cost is less than the cost to outsource. For full outsourcing we generate a sub-AC that combines F1 and F2, which is conservative because it saves on precomputation.</p>

    <p class="text-gray-300">Figure 7(b) plots the performance of executing the slicer’s manifest and of outsourcing the entire computation, normalized to native execution. Giraffe’s front-end never outsources F2 because native execution is cheaper. F1 is amenable to outsourcing when <span class="math">\\delta_{1}&gt;0.2</span>. In contrast, full outsourcing pays extra costs for F2 compared to native execution. Thus, slicing always beats full outsourcing.</p>

    <p class="text-gray-300"><em>Conditionals.</em> In Figure 7(c) we consider a similar setup, but with a conditional. We assume that pred evaluates to true, so F1 is the desired branch. Naively converting this program to an AC results in a computation that materializes both F1 and F2, and selects the result based on pred. In essence, part of the work is useless.</p>

    <p class="text-gray-300">Figure 7(d) plots the performance of executing the slicer’s manifest and the performance of outsourcing the entire computation, normalized to the performance of native execution. The manifest never invokes F2 because that branch is never taken. When <span class="math">\\delta_{1}&gt;0.2</span>, F1 is amenable to outsourcing and Giraffe’s performance is better than native. Full outsourcing, meanwhile, evaluates an AC that incurs the cost for both branches. For large enough <span class="math">\\delta_{1}</span>, though, the savings from F1 offsets the useless work and full outsourcing beats native.</p>

    <h4 id="sec-24" class="text-lg font-semibold mt-6">Squashing.</h4>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">We also experiment with a loop comprising <span class="math">I</span> iterations of F1 (Fig. 7(e)). Parameters are as above, <span class="math">\\delta_{1}=0.5</span>, and we vary <span class="math">I</span>. This is deep (<span class="math">I\\cdot d_{1}</span>) and narrow (<span class="math">G_{1}</span>), and not data parallel. The squasher (§4) chooses <span class="math">N</span>. Effective depth is <span class="math">d^{\\prime}=I\\cdot d/N</span> for each chunk, balancing <span class="math">\\mathcal{V}</span>’s I/O cost against the per-layer cost. This happens when depth and $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">+</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">y</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> are within a constant factor, i.e., </span>N\\cdot G=d^{\\prime}=\\mathrm{O}\\left(\\sqrt{I}\\right)<span class="math"> (overall cost is the sum). Figure 7(f) shows the results: as </span>I<span class="math"> goes from </span>2^{11}<span class="math"> to </span>2^{14}<span class="math">, performance improves by </span>\\approx 3\\times$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h2 id="sec-25" class="text-2xl font-bold">8 Applications</h2>

    <h3 id="sec-26" class="text-xl font-semibold mt-8">8.1 Curve25519</h3>

    <p class="text-gray-300">Curve25519 is a high-performance elliptic curve used in cryptographic protocols <em>[5, 25]</em>. This section compares three implementations of the <em>point multiplication</em> operation on this curve: a baseline, Zebra, and Giraffe. This operation takes as inputs a 255-bit scalar value <span class="math">v</span> and a curve point <span class="math">Q</span>, and computes the point <span class="math">R=[v]Q</span> via 255 <em>double-and-add</em> steps <em>[11]</em>, one for each bit of <span class="math">v</span>. Our algorithm employs a Montgomery ladder, as is standard <em>[11, 25, 63]</em>. Double-and-add is naturally expressed as an AC over <span class="math">\\mathbb{F}_{p}</span>, <span class="math">p=2^{255}-19</span>, with <span class="math">d=7</span> and <span class="math">G\\approx 8</span>.</p>

    <p class="text-gray-300"><em>Zebra.</em> This implementation <em>[82, §8.2]</em> groups 5 Montgomery ladder steps into a block and requires 51 (= ^{255}/5) iterations of this block per operation. Zebra uses a special <em>mux</em> gate for efficiency, requiring all double-and-add operations in a protocol run to use the same scalar input <span class="math">v</span>. This restriction is acceptable in some applications <em>[82, §8.2]</em>.</p>

    <p class="text-gray-300"><em>Baseline implementation.</em> Consistent with published hardware implementations of point multiplication on Curve25519 <em>[70, 71]</em> and the implementation from Zebra, our baseline directly executes 5 Montgomery ladder steps.</p>

    <p class="text-gray-300"><em>Giraffe.</em> In Giraffe there are two degrees of freedom: <span class="math">L</span>, the number of parallel double-and-add steps in a sub-AC (which determines <span class="math">G</span>); and <span class="math">N</span>. Each copy of the sub-AC uses the same <span class="math">L</span> scalars, <span class="math">\\{v_{1},\\ldots,v_{L}\\}</span>; this is because wiring predicates are reused across the <span class="math">N</span> sub-ACs. In our experiment, we fix <span class="math">L=20</span>, and vary <span class="math">N</span>; larger values of <span class="math">L</span> are also possible.</p>

    <p class="text-gray-300">!<a href="img-13.jpeg">img-13.jpeg</a> FIGURE 8—Energy cost of Giraffe, native baseline (§8.1), and Zebra [82, §8.2] versus number of copies of Curve25519 subcircuit. Each subcircuit computes 20 parallel evaluations of five sequential double-and-add steps. Untrusted technology node = 350 nm; trusted technology node = 7 nm;  <span class="math">A_{\\mathrm{max}} = 200 \\mathrm{~mm}^2</span> ;  <span class="math">P_{\\mathrm{max}} = 150 \\mathrm{~W}</span> . Zebra's scaling is limited to about 1150 parallel evaluations. Giraffe scales to more than  <span class="math">500 \\times</span>  more parallel computations for the same chip area. Because of Giraffe's refinements (§3), its improvement versus native is greater than Zebra's. But Giraffe must amortize precomputation, while Zebra assumes it is free; thus, Giraffe needs larger  <span class="math">N</span>  to break even.</p>

    <p class="text-gray-300">Results. We compute energy for Giraffe and the native baseline as in Section 6.1. For Zebra, we use published results [82, §8.2]. We set the untrusted technology node  <span class="math">= 350 \\mathrm{~nm}</span> , the trusted technology node  <span class="math">= 7 \\mathrm{~nm}</span> , and  <span class="math">A_{\\max} = 200 \\mathrm{~mm}^2</span> , the same as in Zebra.</p>

    <p class="text-gray-300">Figure 8 shows the results. Giraffe breaks even when  <span class="math">N \\approx 30</span> , or at about 600 parallel double-and-add operations, while Zebra breaks even for about 100 such operations. But Giraffe pays the cost of precomputation, whereas Zebra assumes that precomputation is free. Meanwhile, Zebra handles at most 1150 parallel copies for the given chip area, whereas Giraffe can accommodate roughly 32,000 parallel operations corresponding to roughly 100M AC gates, about  <span class="math">500 \\times</span>  more than Zebra, for the same technology nodes.</p>

    <p class="text-gray-300">An image pyramid is a data structure for image processing [6] that holds multiple copies of an image at different resolutions. The "base" of the pyramid is a full resolution image and higher "layers" summarize the image at progressively lower resolutions. One application of an image pyramid is fast pattern matching. In the first step, a coarse pattern is matched against the coarsest layer (top) of the pyramid. Guided by the results, a finer pattern is matched against a small part of the next layer until eventually a portion of the full resolution image is matched against the finest pattern.</p>

    <p class="text-gray-300">We use a convolution-based matching algorithm [35] where the pattern may contain "don't care" symbols that match any input. If the text is  <span class="math">t = t_0t_1 \\ldots t_n</span>  and the pattern is  <span class="math">p = p_0p_1 \\ldots p_m</span> , then the matching algorithm uses convolutions to compute  <span class="math">c_i = \\sum_{j=0}^{m} p_j(p_j - t_{i+j})^2</span> ,  <span class="math">i \\in \\{1, \\ldots, n\\}</span> , and reports a match at  <span class="math">i</span>  if  <span class="math">c_i = 0</span> .</p>

    <p class="text-gray-300">In our implementation, the input consists of a two-layer image pyramid, a coarse pattern, and a fine pattern. The bottom layer of the pyramid has  <span class="math">2^{7} \\times 2^{7}</span>  words, and the top layer has  <span class="math">1 \\times 2^{7}</span>  words. Both patterns comprise  <span class="math">2^{4}</span>  words. Words are represented over  <span class="math">\\mathbb{F}_p</span> ,  <span class="math">p = 2^{61} + 2^{19} + 1</span> , and we implement convolution using the number theoretic transform over  <span class="math">\\mathbb{F}_p</span> . The entire application processes  <span class="math">N</span>  instances in parallel; each instance specifies its own input and pattern. The application is written as a C program.</p>

    <p class="text-gray-300">Baseline implementation. In our baseline implementation, each convolution is implemented using the direct iterative implementation of the number theoretic transform (NTT) and its inverse. Energy costs are accounted as in the baseline of Section 6.</p>

    <p class="text-gray-300">!<a href="img-14.jpeg">img-14.jpeg</a> FIGURE 9—Energy cost of Giraffe and native baseline (§8.2) versus number of parallel image pyramid matching evaluations. Each evaluation matches 16-word needles against 128-word haystacks for a two-level image pyramid (§8.2). Words are represented as elements in  <span class="math">\\mathbb{F}_p</span> ,  <span class="math">p = 2^{61} + 2^{19} + 1</span> . Untrusted technology node = 350 nm; trusted technology node = 7 nm;  <span class="math">A_{\\mathrm{max}} = 200\\mathrm{mm}^2</span> ,  <span class="math">P_{\\mathrm{max}} = 150\\mathrm{W}</span> . Giraffe breaks even for ≈ 30 parallel searches.</p>

    <p class="text-gray-300">Giraffe. We apply Giraffe's front-end to process our C program into a manifest; the local computation selects the needed portion of the next layer. We compute energy consumption of the resulting manifest as in Section 7. Our results reflect fully automated compilation on a realistic application with no hand optimizations.</p>

    <p class="text-gray-300">Results. Figure 9 compares the cost of executing the manifest to the cost of the native baseline. Trusted and untrusted technology nodes and  <span class="math">A_{\\mathrm{max}}</span>  are as in Section 8.1. Giraffe needs roughly 30 parallel evaluations to break even, after which it uses  <span class="math">5 \\times</span>  less energy than the baseline. Giraffe can scale to handle 32,000 parallel instances within the area constraint, or about 100 million AC gates.</p>

    <p class="text-gray-300">To understand Giraffe's results, it is useful to provide context about the limitations of other implemented systems for verifiable outsourcing. All such systems (including Giraffe) are reasonable only when the computation to be outsourced is naturally expressed as an AC. Otherwise, translating the computation to an AC entails such steep overheads compared to native execution that (asymptotic) savings do not kick in for reasonable computation sizes [82, §9]. Moreover, all systems built on the interactive proofs of GKR and CMT (including Giraffe) require the AC to be layered.</p>

    <p class="text-gray-300">Another limitation of all built systems concerns their overheads and break-even points. As an example we consider SNARKs [26, 47]. Before continuing, we note that SNARKs apply more widely, for example to cryptocurrencies [18, 38] and beyond [39, 64]. In those contexts, which exploit SNARKs' zero knowledge property, overheads are less relevant. Thus, the following discussion should be understood to cover only the proposed application of SNARKs to verifiable outsourcing [27, 47, 66].</p>

    <p class="text-gray-300">Careful examination of the SNARK literature indicates that verifier overhead is so high that very large computations are required to break even: millions of AC gates per instance [21; 66, §5.3; 83, §2; 84]. Furthermore, the required number of instances is large: even on the best-case problem of matrix multiplication, Pinocchio [66] requires more than 6,000 instances, and BCTV [20, 22] requires more than 90,000 instances to break even [84, Fig.4]. (Note that we have not even discussed keeping track of prover overhead; even for small ACs, these provers take minutes on stock hardware.)</p>

    <p class="text-gray-300">In contrast, Giraffe's performance (keeping track of prover costs) has only a weak dependence on computation size, even for ACs of only a few hundred gates (Figs. 6a and 6b, §6.2). Moreover, the number of parallel copies required to amortize is small,  <span class="math">\\approx 30</span>  ( <span class="math">\\S 6</span> ,  <span class="math">\\S 8</span> ). The maximum instance size for a Giraffe sub-AC is around 1.5 million gates ( <span class="math">\\S 6.1</span> ); this is largely a function of the constraints imposed by hardware. These numbers are very encouraging (although we note that Allspice [81] achieves similar break-even points).</p>

    <p class="text-gray-300">Of course, SNARKs have distinct advantages: precomputation amortizes indefinitely in the non-interactive setting (eliminating the requirement for data-parallel computations), the communication costs are much lower, and a broader class of computations can be handled (there is no requirement that ACs be layered, and computations can accept non-deterministic input).</p>

    <p class="text-gray-300">Since Giraffe is largely focused on the hardware setting, it is also worthwhile to contrast with Zebra <em>[82]</em>. On the one hand, Zebra does not impose the requirement for data-parallel computations (to amortize precomputation), and its break-even points are lower. On the other hand, Zebra achieves these things by not paying for computation (and making a fanciful assumption about daily delivery of trusted precomputations <em>[82, §4]</em>). Giraffe, by contrast, can break even despite paying for precomputation. Furthermore, Zebra is limited to approximately 500,000 AC gates total, whereas Giraffe supports 1.5 million gates per sub-AC and <span class="math">N</span> scales to about 50 in this case (§6.1); Giraffe is thus two orders of magnitude better than Zebra in total size (this is also demonstrated in §8.1). And, as the image pyramid example (§8.2) shows, Giraffe can be practical in situations where Zebra or Giraffe simply cannot outsource the entire computation.</p>

    <p class="text-gray-300">To be sure, Giraffe has serious limitations. The price of verification remains high; evaluation shows that the overall win is <span class="math">\\approx</span>3–5<span class="math">\\times</span> (Fig. 5, §6; Figs. 8–9, §8). Given the prover overhead, Giraffe still requires a large technology gap between the <span class="math">\\mathcal{P}</span> and <span class="math">\\mathcal{V}</span> technology nodes to be practical (§6.2), though there are scenarios when this holds <em>[82, §1]</em>. And finally, the regime of applicability is fundamentally narrow (as noted in the introduction).</p>

    <h2 id="sec-29" class="text-2xl font-bold">10 Related work</h2>

    <h4 id="sec-30" class="text-lg font-semibold mt-6">Probabilistic proofs.</h4>

    <p class="text-gray-300">Giraffe relates to the extensive body of recent work on verifiable outsourced computation <em>[14, 15, 17, 18, 19, 21, 22, 32, 34, 36, 37, 39, 40, 42, 45, 47, 56, 64, 66, 72, 73, 74, 75, 77, 79, 81, 82, 83, 87]</em>; see <em>[84]</em> for a comparatively recent survey. Specifically, Giraffe descends from the GKR <em>[49]</em> interactive proof line <em>[36, 77, 79, 81, 82]</em>. This line has historically imposed certain limitations: a more restricted class of computations, and deterministic ACs. In work done concurrently with and independently of Giraffe, vSQL <em>[87]</em> has demonstrated how to support <em>non</em>-deterministic ACs, by composing GKR-derived protocols with polynomial commitment schemes <em>[24, 41, 53, 65]</em>. The result is exciting, and for some application lowers prover costs relative to the argument protocols, discussed below. However, the resulting prover is still much more costly than native computation; applying Giraffe’s protocol refinements (§3.1) would reduce this overhead. Indeed, vSQL and Giraffe are complementary: future work is to combine them, and thereby handle non-deterministic ACs in Giraffe’s operating model.</p>

    <p class="text-gray-300">Another line of work uses argument protocols, both interactive <em>[73, 74, 75]</em> and non-interactive <em>[14, 21, 26, 47, 56, 66]</em> (“SNARK” refers to the latter). However, these protocols seem incompatible with hardware implementation (as discussed in prior work <em>[82, §9]</em>) and impose cryptographic assumptions (strong ones in the non-interactive setting). These protocols also have higher precomputation costs (which can be tamed asymptotically, but at very high concrete cost <em>[21, 22, 34]</em>—for example, the prover is two <em>[22]</em> to six <em>[21, 34]</em> orders of magnitude worse). On the other hand, non-interactive arguments can support zero knowledge (zkSNARK) protocols; this enables applications that are not possible otherwise.</p>

    <p class="text-gray-300">Much of the work in the area fits into the cost framework outlined in the introduction: precomputation, verifier overhead, and prover overhead, with native execution as a sensible baseline. There are a few exceptions. In the zkSNARK setting, the cost assessment depends on the premium that one is willing to pay for otherwise unachievable functionality <em>[14, 18, 39]</em>. Also, two works in the verifiable outsourcing</p>

    <p class="text-gray-300">setting do not require precomputation. The first is CMT <em>[36, 79]</em> (and <em>[77]</em>) when applied to highly regular wiring patterns; however, such wiring patterns substantially limit applicability. The second is SCI <em>[17]</em>, which aims to be general purpose. SCI is, roughly speaking, an argument protocol built atop “short PCPs” <em>[23]</em>, and is an exciting development. But inspection of SCI reveals that some of its costs, most notably the verifier’s, are orders of magnitude higher than in other works.</p>

    <h5 id="sec-31" class="text-base font-semibold mt-4">PL techniques in cryptographic protocols.</h5>

    <p class="text-gray-300">Squashing (§4) is related to but distinct from Geppetto’s <em>[37]</em> optimizations for loops. At a high level the goals are similar (use loop transformations to adapt a computation to a protocol), but they differ in particulars because each technique leverages features of its respective back-end. In settings where they are both relevant, we believe the two approaches are complementary. (Giraffe pursues automatic inference for this optimization, which is discussed but not explored in <em>[37]</em>.)</p>

    <p class="text-gray-300">Our work on slicing is in the tradition of a great deal of work adapting PL techniques to implementing cryptographic protocols. In the verifiable outsourcing literature, there are a handful of examples (e.g., Buffet <em>[83]</em> uses sophisticated loop unrolling techniques to optimize loop handling, and Geppetto analyzes conditionals to minimize evaluation of “dead code”).</p>

    <p class="text-gray-300">More generally, the secure multi-party computation literature has seen a great deal of work using program analysis and transformation techniques to produce optimized protocols, starting with Fairplay <em>[59]</em> and notably including the line of work represented by <em>[86]</em>. There has also been relevant work in the Oblivious RAM community, for example <em>[57]</em> uses PL techniques to partition variables to ensure obliviousness. Another area in which these techniques are used is in automatic compilation for secure distributed programming <em>[44]</em>. Perhaps most similar to our slicing protocol are the various compilers for zero knowledge proofs of knowledge <em>[7, 16, 61]</em>, most notably ZQL and ZØ <em>[43, 45]</em>. The latter weaves together explicitly annotated zero knowledge regions with ordinary code, and does automatic inference for assigning functionality to tiers in client-server applications (see also <em>[60]</em> for automatic tier partitioning). Giraffe is distinguished by performing automatic inference for slicing using a cost model, without explicit annotation.</p>

    <h3 id="sec-32" class="text-xl font-semibold mt-8">11 Conclusion</h3>

    <p class="text-gray-300">We have described a number of techniques that are relevant to verifiable outsourcing generally: an improvement to the T13 proof protocol that yields an asymptotically optimal prover and reduces concrete costs by an order of magnitude or more; algorithmic refinements for the verifier that improve its main bottleneck by <span class="math">\\approx</span>3<span class="math">\\times</span> and apply to any CMT-based protocol; and two program transformations that increase the range of suitable applications: squashing, which applies to any CMT-based protocol, and slicing, which applies throughout the research area. We have also developed hardware primitives tailored to the proof protocol and a design template that automatically generates optimized hardware designs across a wide range of computations and physical substrates. And we have used all of these to build Giraffe, a system for Verifiable ASICs. For a range of computations, Giraffe meets or exceeds the performance of a chip built by a trusted manufacturer, while accounting for <em>all</em> costs: prover, verifier, and precomputation. Although the regime in which Giraffe applies is narrow, it is the first system to apply such a strict cost accounting—and win.</p>

    <p class="text-gray-300">##</p>

    <p class="text-gray-300">Acknowledgments</p>

    <p class="text-gray-300">We thank Fraser Brown, Ioanna Tzialla, and Keith Winstein for helpful comments. The authors were supported by NSF grants CNS-1423249, CNS-1514422, and CNS-1646671; AFOSR grant FA9550-15-1-0302; ONR grant N00014-16-1-2154; DARPA grant HR0011-15-2-0047; and a Google Research Award.</p>

    <p class="text-gray-300">Giraffe’s source code is available at: http://www.pepper-project.org/</p>

    <h2 id="sec-33" class="text-2xl font-bold">References</h2>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[1] https://github.com/pepper-project/releases/blob/master/ginger-allspice.tar.gz.</li>

      <li>[2] http://people.cs.georgetown.edu/jthaler/code/code.htm.</li>

      <li>[3] http://people.cs.georgetown.edu/jthaler/TRMPcode.htm.</li>

      <li>[4] https://github.com/pepper-project.</li>

      <li>[5] Things that use Curve25519. https://ianix.com/pub/curve25519-deployment.html.</li>

      <li>[6] E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and J. M. Ogden. Pyramid method in image processing. RCA Engineer, 29(6):33–41, Nov. 1984.</li>

      <li>[7] J. B. Almeida, M. Barbosa, E. Bangerter, G. Barthe, S. Krenn, and S. Z. Béguelin. Full proof cryptography: verifiable compilation of efficient zero-knowledge protocols. In ACM CCS, 2012.</li>

      <li>[8] S. Arora and B. Barak. Computational Complexity: A modern approach. Cambridge University Press, 2009.</li>

      <li>[9] S. Arora, C. Lund, R. Motwani, M. Sudan, and M. Szegedy. Proof verification and the hardness of approximation problems. J. ACM, 45(3):501–555, May 1998.</li>

      <li>[10] S. Arora and S. Safra. Probabilistic checking of proofs: a new characterization of NP. J. ACM, 45(1):70–122, Jan. 1998.</li>

      <li>[11] R. M. Avanzi, H. Cohen, C. Doche, G. Frey, T. Lange, K. Nguyen, and F. Vercauteren. Handbook of Elliptic and Hyperelliptic Curve Cryptography. Chapman & Hall/CRC, 2005.</li>

      <li>[12] L. Babai. Trading group theory for randomness. In STOC, May 1985.</li>

      <li>[13] L. Babai, L. Fortnow, L. A. Levin, and M. Szegedy. Checking computations in polylogarithmic time. In STOC, May 1991.</li>

      <li>[14] M. Backes, M. Barbosa, D. Fiore, and R. M. Reischuk. ADSNARK: Nearly practical and privacy-preserving proofs on authenticated data. In IEEE S&P, May 2015.</li>

      <li>[15] M. Backes, D. Fiore, and R. M. Reischuk. Verifiable delegation of computation on outsourced data. In ACM CCS, Nov. 2013.</li>

      <li>[16] E. Bangerter, J. Camenisch, S. Krenn, A. Sadeghi, and T. Schneider. Automatic generation of sound zero-knowledge protocols. IACR Cryptology ePrint Archive, 2008. http://eprint.iacr.org/2008/471.</li>

      <li>[17] E. Ben-Sasson, I. Ben-Tov, A. Chiesa, A. Gabizon, D. Genkin, M. Hamilis, E. Pergament, M. Riabzev, M. Silberstein, E. Tromer, and M. Virza. Computational integrity with a public random string from quasi-linear PCPs. In EUROCRYPT, 2017.</li>

      <li>[18] E. Ben-Sasson, A. Chiesa, C. Garman, M. Green, I. Miers, E. Tromer, and M. Virza. Decentralized anonymous payments from Bitcoin. In IEEE S&P, May 2014.</li>

      <li>[19] E. Ben-Sasson, A. Chiesa, D. Genkin, E. Tromer, and M. Virza. SNARKs for C: Verifying program executions succinctly and in zero knowledge. In CRYPTO, Aug. 2013.</li>

      <li>[20] E. Ben-Sasson, A. Chiesa, D. Genkin, E. Tromer, and M. Virza. TinyRAM architecture specification, v0.991. http://www.scipr-lab.org/system/files/TinyRAM-spec-0.991.pdf, 2013.</li>

      <li>[21] E. Ben-Sasson, A. Chiesa, E. Tromer, and M. Virza. Scalable zero knowledge via cycles of elliptic curves. In CRYPTO, Aug. 2014.</li>

      <li>[22] E. Ben-Sasson, A. Chiesa, E. Tromer, and M. Virza. Succinct non-interactive zero knowledge for a von Neumann architecture. In USENIX Security, Aug. 2014.</li>

      <li>[23] E. Ben-Sasson and M. Sudan. Short PCPs with polylog query complexity. SIAM J. on Comp., 38(2):551–607, May 2008.</li>

      <li>[24] S. Benabbas, R. Gennaro, and Y. Vahlis. Verifiable delegation of computation over large datasets. In CRYPTO, Aug. 2011.</li>

      <li>[25] D. J. Bernstein. Curve25519: new Diffie-Hellman speed records. In PKC, Apr. 2006.</li>

      <li>[26] N. Bitansky, R. Canetti, A. Chiesa, and E. Tromer. From extractable collision resistance to succinct non-interactive arguments of knowledge, and back again. In ITCS, Jan. 2012.</li>

    </ul>

    <p class="text-gray-300">[27] N. Bitansky, R. Canetti, A. Chiesa, and E. Tromer. Recursive composition and bootstrapping for SNARKs and proof-carrying data. In STOC, 2013.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[28] U. Bondhugula, A. Hartono, J. Ramanujam, and P. Sadayappan. A practical automatic polyhedral parallelizer and locality optimizer. In PLDI, June 2008.</li>

      <li>[29] P. Boulet, A. Darte, G. Silber, and F. Vivien. Loop parallelization algorithms: From parallelism extraction to code generation. Parallel Computing, 24(3-4):421–444, 1998.</li>

      <li>[30] G. Brassard, D. Chaum, and C. Crépeau. Minimum disclosure proofs of knowledge. J. of Comp. and Sys. Sciences, 37(2):156–189, Oct. 1988.</li>

      <li>[31] B. Braun. Compiling computations to constraints for verified computation. UT Austin Honors thesis HR-12-10, Dec. 2012.</li>

      <li>[32] B. Braun, A. J. Feldman, Z. Ren, S. Setty, A. J. Blumberg, and M. Walfish. Verifying computations with state. In SOSP, Nov. 2013.</li>

      <li>[33] S. Campanoni, K. Brownell, S. Kanev, T. M. Jones, G.-Y. Wei, and D. Brooks. HELIX-RC: an architecture-compiler co-design for automatic parallelization of irregular programs. In ISCA, June 2014.</li>

      <li>[34] A. Chiesa, E. Tromer, and M. Virza. Cluster computing in zero knowledge. In EUROCRYPT, Apr. 2015.</li>

      <li>[35] P. Clifford and R. Clifford. Simple deterministic wildcard matching. Information Processing Letters, 101(2):53 – 54, 2007.</li>

      <li>[36] G. Cormode, M. Mitzenmacher, and J. Thaler. Practical verified computation with streaming interactive proofs. In ITCS, Jan. 2012.</li>

      <li>[37] C. Costello, C. Fournet, J. Howell, M. Kohlweiss, B. Kreuter, M. Naehrig, B. Parno, and S. Zahur. Geppetto: Versatile verifiable computation. In IEEE S&P, May 2015.</li>

      <li>[38] G. Danezis, C. Fournet, M. Kohlweiss, and B. Parno. Pinocchio coin: Building zerocoin from a succinct pairing-based proof system. In Workshop on Language Support for Privacy-enhancing Technologies, Nov. 2013.</li>

      <li>[39] A. Delignat-Lavaud, C. Fournet, M. Kohlweiss, and B. Parno. Cinderella: Turning shabby X.509 certificates into elegant anonymous credentials with the magic of verifiable computation. In IEEE S&P, May 2016.</li>

      <li>[40] D. Fiore, C. Fournet, E. Ghosh, M. Kohlweiss, O. Ohrimenko, and B. Parno. Hash first, argue later: Adaptive verifiable computations on outsourced data. In ACM CCS, Oct. 2016.</li>

      <li>[41] D. Fiore and R. Gennaro. Publicly verifiable delegation of large polynomials and matrix computations, with applications. In ACM CCS, Oct. 2012.</li>

      <li>[42] D. Fiore, R. Gennaro, and V. Pastro. Efficiently verifiable computation on encrypted data. In ACM CCS, Nov. 2014.</li>

      <li>[43] C. Fournet, M. Kohlweiss, G. Danezis, and Z. Luo. ZQL: A compiler for privacy-preserving data processing. In USENIX Security, Aug. 2013.</li>

      <li>[44] C. Fournet, G. Le Guernic, and T. Rezk. A security-preserving compiler for distributed programs: From information-flow policies to cryptographic mechanisms. In ACM CCS, 2009.</li>

      <li>[45] M. Fredrikson and B. Livshits. ZØ: An optimizing distributing zero-knowledge compiler. In USENIX Security, Aug. 2014.</li>

      <li>[46] R. Gennaro, C. Gentry, and B. Parno. Non-interactive verifiable computing: Outsourcing computation to untrusted workers. In CRYPTO, Aug. 2010.</li>

      <li>[47] R. Gennaro, C. Gentry, B. Parno, and M. Raykova. Quadratic span programs and succinct NIZKs without PCPs. In EUROCRYPT, 2013.</li>

      <li>[48] C. Gentry and D. Wichs. Separating succinct non-interactive arguments from all falsifiable assumptions. In STOC, June 2011.</li>

      <li>[49] S. Goldwasser, Y. T. Kalai, and G. N. Rothblum. Delegating computation: Interactive proofs for muggles. J. ACM, 62(4):27:1–27:64, Aug. 2015. Prelim version STOC 2008.</li>

      <li>[50] S. Goldwasser, S. Micali, and C. Rackoff. The knowledge complexity of interactive proof systems. SIAM J. on Comp., 18(1):186–208, 1989.</li>

      <li>[51] B. Hoefflinger. ITRS: The international technology roadmap for semiconductors. In Chips 2020. Springer, 2012.</li>

      <li>[52] Y. Ishai, E. Kushilevitz, and R. Ostrovsky. Efficient arguments without short PCPs. In IEEE CCC, June 2007.</li>

      <li>[53] A. Kate, G. M. Zaverucha, and I. Goldberg. Constant-size commitments to polynomials and their applications. In ASIACRYPT, Dec. 2010.</li>

      <li>[54] J. Kilian. A note on efficient zero-knowledge proofs and arguments (extended abstract). In STOC, May 1992.</li>

      <li>[55] D. E. Knuth. Seminumerical Algorithms, volume 2 of The Art of Computer Programming, chapter 4.6.4. Addison-Wesley, third edition, 1997.</li>

      <li>[56] A. E. Kosba, D. Papadopoulos, C. Papamanthou, M. F. Sayed, E. Shi, and N. Triandopoulos. TrueSet: Faster verifiable set computations. In USENIX Security, Aug. 2014.</li>

    </ul>

    <p class="text-gray-300">[57] C. Liu, M. Hicks, and E. Shi. Memory trace oblivious program execution. In Computer Security Foundations Symposium (CSF), June 2013.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[58] C. Lund, L. Fortnow, H. J. Karloff, and N. Nisan. Algebraic methods for interactive proof systems. J. ACM, 39(4):859–868, Oct. 1992.</li>

      <li>[59] D. Malkhi, N. Nisan, B. Pinkas, and Y. Sella. Fairplay—a secure two-party computation system. In USENIX Security, Aug. 2004.</li>

      <li>[60] D. Manolescu, B. Beckman, and B. Livshits. Volta: Developing distributed applications by recompiling. IEEE Software, 2008.</li>

      <li>[61] S. Meiklejohn, C. C. Erway, A. Küpçü, T. Hinkle, and A. Lysyanskaya. ZKPDL: a language-based system for efficient zero-knowledge proofs and electronic cash. In USENIX Security, 2010.</li>

      <li>[62] S. Micali. Computationally sound proofs. SIAM J. on Comp., 30(4):1253–1298, 2000.</li>

      <li>[63] P. L. Montgomery. Speeding the Pollard and elliptic curve methods of factorization. Math. of Computation, 48(177):243–264, Jan. 1987.</li>

      <li>[64] A. Naveh and E. Tromer. PhotoProof: Cryptographic image authentication for any set of permissible transformations. In IEEE S&P, May 2016.</li>

      <li>[65] C. Papamanthou, E. Shi, and R. Tamassia. Signatures of correct computation. In IACR TCC, Mar. 2013.</li>

      <li>[66] B. Parno, C. Gentry, J. Howell, and M. Raykova. Pinocchio: Nearly practical verifiable computation. In IEEE S&P, May 2013.</li>

      <li>[67] B. Parno, C. Gentry, J. Howell, and M. Raykova. Pinocchio: Nearly practical verifiable computation. In Communications of the ACM, volume 59, pages 103–112, Feb. 2016.</li>

      <li>[68] J. M. Rabaey, A. P. Chandrakasan, and B. Nikolic. Digital integrated circuits, volume 2. Prentice Hall Englewood Cliffs, 2002.</li>

      <li>[69] G. N. Rothblum. Delegating Computation Reliably: Paradigms and Constructions. PhD thesis, Massachusetts Institute of Technology, 2009.</li>

      <li>[70] P. Sasdrich and T. Güneysu. Efficient elliptic-curve cryptography using Curve25519 on reconfigurable devices. In ARC, Apr. 2014.</li>

      <li>[71] P. Sasdrich and T. Güneysu. Implementing Curve25519 for side-channel–protected elliptic curve cryptography. ACM TRETS, 9(1):3:1–3:15, Nov. 2015.</li>

      <li>[72] S. Setty, A. J. Blumberg, and M. Walfish. Toward practical and unconditional verification of remote computations. In HotOS, May 2011.</li>

      <li>[73] S. Setty, B. Braun, V. Vu, A. J. Blumberg, B. Parno, and M. Walfish. Resolving the conflict between generality and plausibility in verified computation. In EuroSys, Apr. 2013.</li>

      <li>[74] S. Setty, R. McPherson, A. J. Blumberg, and M. Walfish. Making argument systems for outsourced computation practical (sometimes). In NDSS, Feb. 2012.</li>

      <li>[75] S. Setty, V. Vu, N. Panpalia, B. Braun, A. J. Blumberg, and M. Walfish. Taking proof-based verified computation a few steps closer to practicality. In USENIX Security, Aug. 2012.</li>

      <li>[76] A. Shamir. IP = PSPACE. J. ACM, 39(4):869–877, Oct. 1992.</li>

      <li>[77] J. Thaler. Time-optimal interactive proofs for circuit evaluation. In CRYPTO, Aug. 2013. Citations refer to full version: https://arxiv.org/abs/1304.3812.</li>

      <li>[78] J. Thaler. A note on the GKR protocol. http://people.seas.harvard.edu/ jthaler/GKRNote.pdf, 2015.</li>

      <li>[79] J. Thaler, M. Roberts, M. Mitzenmacher, and H. Pfister. Verifiable computation with massively parallel interactive proofs. In USENIX HotCloud Workshop, June 2012.</li>

      <li>[80] Victor Vu. Personal communication, 2013.</li>

      <li>[81] V. Vu, S. Setty, A. J. Blumberg, and M. Walfish. A hybrid architecture for interactive verifiable computation. In IEEE S&P, May 2013.</li>

      <li>[82] R. S. Wahby, M. Howald, S. Garg, abhi shelat, and M. Walfish. Verifiable ASICs. In IEEE S&P, May 2016.</li>

      <li>[83] R. S. Wahby, S. Setty, Z. Ren, A. J. Blumberg, and M. Walfish. Efficient RAM and control flow in verifiable outsourced computation. In NDSS, Feb. 2015.</li>

      <li>[84] M. Walfish and A. J. Blumberg. Verifying computations without reexecuting them: from theoretical possibility to near practicality. Communications of the ACM, 58(2):74–84, Feb. 2015.</li>

      <li>[85] S. Williams. Icarus Verilog. http://iverilog.icarus.com.</li>

      <li>[86] S. Zahur and D. Evans. Circuit structures for improved efficiency of security and privacy tools. In IEEE S&P, pages 493–507, May 2013.</li>

      <li>[87] Y. Zhang, D. Genkin, J. Katz, D. Papadopoulos, and C. Papamanthou. vSQL: Verifying arbitrary SQL queries over dynamic outsourced databases. In IEEE S&P, May 2017.</li>

    </ul>

    <p class="text-gray-300">1: function VERIFY(ArithCircuit c, input  <span class="math">x</span> , output  <span class="math">y</span> ) 2:  <span class="math">(q_0&#x27;, q_0) \\xleftarrow{R} \\mathbb{P}^{\\log N} \\times \\mathbb{P}^{\\log G}</span> 3:  <span class="math">a_0 \\gets \\tilde{V}_y(q_0&#x27;, q_0) // \\tilde{V}_y</span>  is the multilin. ext. of the output  <span class="math">y</span> 4: SendToProver  <span class="math">(q_0&#x27;, q_0)</span> 5:  <span class="math">d \\gets \\mathrm{c.depth}</span> 6: 7: for  <span class="math">i = 1, \\dots, d</span>  do 8: // Reduce  <span class="math">\\tilde{V}_{i-1}(q_{i-1}&#x27;, q_{i-1}) \\stackrel{?}{=} a_{i-1}</span>  to  <span class="math">P_{q,i}(r_0, r_1, r&#x27;) \\stackrel{?}{=} e</span> 9:  <span class="math">(e, r&#x27;, r_0, r_1) \\gets \\text{SUMCHECKV}(i, a_{i-1})</span> 10: 11: // Below,  <span class="math">\\mathcal{P}</span>  describes a univariate polynomial  <span class="math">H(t)</span> , 12: // of degree  <span class="math">\\log G</span> , claimed to be  <span class="math">\\tilde{V}_i(r&#x27;, (r_1 - r_0)t + r_0)</span> 13:  <span class="math">H \\gets</span>  ReceiveFromProver() // see line 47 of Figure 14 14:  <span class="math">v_0 \\gets H(0)</span> 15:  <span class="math">v_1 \\gets H(1)</span> 16: 17: // Reduce  <span class="math">P_{q,i}(r_0, r_1, r&#x27;) \\stackrel{?}{=} e</span>  to two questions: 18: //  <span class="math">\\tilde{V}_i(r&#x27;, r_0) \\stackrel{?}{=} v_0</span>  and  <span class="math">\\tilde{V}_i(r&#x27;, r_1) \\stackrel{?}{=} v_1</span> 19: 20: if  <span class="math">e \\neq \\widehat{\\mathrm{eq}}(q_{i-1}&#x27;, r&#x27;) \\cdot \\left[ \\mathrm{add}_i(q_{i-1}, r_0, r_1) \\cdot (v_0 + v_1) + \\mathrm{mult}_i(q_{i-1}, r_0, r_1) \\cdot v_0 \\cdot v_1 \\right]</span>  then 21: return reject 22: 23: // Reduce the two  <span class="math">v_0, v_1</span>  questions to  <span class="math">\\tilde{V}_i(q_i&#x27;, q_i) \\stackrel{?}{=} a_i</span> 24:  <span class="math">\\tau_i \\xleftarrow{R} \\mathbb{P}</span> 25:  <span class="math">a_i \\gets H(\\tau_i)</span> 26:  <span class="math">(q_i&#x27;, q_i) \\gets (r&#x27;, (r_1 - r_0) \\cdot \\tau_i + r_0)</span> 27: 28: SendToProver( <span class="math">\\tau_i</span> ) 29: 30: //  <span class="math">\\tilde{V}_d(\\cdot)</span>  is the multilinear extension of the input  <span class="math">x</span> 31: if  <span class="math">\\tilde{V}_d(q_d&#x27;, q_d) = a_d</span>  then 32: return accept 33: return reject</p>

    <p class="text-gray-300">FIGURE 10—(Copy of Fig. 1.)  <span class="math">\\mathcal{V}</span> 's side of T13 [77, §7] and Giraffe.</p>

    <p class="text-gray-300">Recall from Section 2.2 that the starting point of Giraffe's back-end is  <span class="math">T13</span>  [77, §7], with an optimization [78]. A complete description of the verifier's work in this protocol can be found in Figures 10 and 11. A complete description of the prover's work can be found in Figures 12 and 14.</p>

    <p class="text-gray-300">The following theorem restates the relevant properties of this protocol (§2.2): completeness, soundness,  <span class="math">\\mathcal{V}</span> 's runtime, and  <span class="math">\\mathcal{P}</span> 's runtime. The proof of this theorem is omitted for brevity; it essentially follows the analysis of [77, §7], as the only difference between the protocol of [77, §7] and the protocol of this section is the inclusion of the optimization of [78]. We do, however, provide a detailed proof of the claim about  <span class="math">\\mathcal{V}</span> 's runtime, as Giraffe's verifier is implemented in a similar manner.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Theorem A.1. Consider the protocol with verifier described in Figures 10 and 11, and prover described in Figure 14. When applied to a circuit  <span class="math">\\mathcal{C}</span>  as in Section 2.2, the protocol satisfies completeness, and satisfies soundness with  $\\epsilon = (\\lceil \\log</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">y</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\rceil + 6d \\log (G \\cdot N)) /</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{P}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> .  </span>\\mathcal{V}<span class="math">  requires precomputation that is  </span>O(d \\cdot G)<span class="math"> . Then, to validate all inputs and outputs,  </span>\\mathcal{V}<span class="math">  incurs cost  </span>O(d \\cdot \\log (N \\cdot G) +</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">+</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">y</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math"> .  </span>\\mathcal{P}<span class="math"> &#x27;s running time is  </span>O(d \\cdot G \\cdot N \\cdot \\log G)$ .</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">1: function SumCHECKV(layer  <span class="math">i</span> ,  <span class="math">a_{i-1}</span> ) 2:  <span class="math">e\\gets a_{i - 1}</span> 3: 4:  <span class="math">(r&#x27;, r_0, r_1) \\stackrel{R}{\\leftarrow} \\mathbb{F}^{b_N} \\times \\mathbb{F}^{b_G} \\times \\mathbb{F}^{b_G}</span> 5:  <span class="math">r\\gets (r&#x27;,r_0,r_1)</span>  // variable order is from §3.1 6: // For the protocol of Theorem A.1 (variable order of §2.2) 7: // replace the above line with  <span class="math">r\\gets (r_0,r_1,r&#x27;)</span> 8: 9: for  <span class="math">j = 1,2,\\ldots ,(b_N + 2b_G)</span>  do 10: 11: //  <span class="math">F_{j}</span>  is a degree-2 or degree-3 polynomial 12:  <span class="math">F_{j}\\gets</span>  ReceiveFromProver()//see lines 18,46 of Fig. 15 13: // For the protocol of Theorem A.1, 14: // see lines 22 and 41 of Figure 14 15: 16: if  <span class="math">F_{j}(0) + F_{j}(1)\\neq e</span>  then 17: return reject 18: 19: SendToProver(r[j]) 20: 21:  <span class="math">e\\gets F_j(r[j])</span> 22: 23: return  <span class="math">(e,r^{\\prime},r_{0},r_{1})</span></p>

    <p class="text-gray-300">FIGURE 11— <span class="math">\\mathcal{V}</span> 's side of the sum-check protocol in T13 and Giraffe. This protocol reduces the claim that  <span class="math">a_{i}</span>  equals the sum  <span class="math">\\sum_{n,h_0,h_1}P_{q,i}^<em> (n,h_0,h_1)</span>  (this sum equals  <span class="math">\\tilde{V}_{i - 1}(q_{i - 1}&#x27;,q_{i - 1})</span> , per Equation (2)) to the claim  <span class="math">e = P_{q,i}^{</em>}(r^{\\prime},r_{0},r_{1})</span> . The depiction here follows Section 3.1:  <span class="math">r^\\prime</span>  comes before  <span class="math">r_0,r_1</span>  in the variable order, and the polynomial is  <span class="math">P_{q,i}^{*}(n,h_{0},h_{1})</span> , not  <span class="math">P_{q,i}(h_0,h_1,n)</span> .</p>

    <p class="text-gray-300">It will be convenient to have the following expression for the multilinear extension. For a function  <span class="math">f \\colon \\{0,1\\}^{\\gamma} \\to \\mathbb{F}</span> , the multilinear extension  <span class="math">\\tilde{f}</span>  of  <span class="math">f</span>  is given by:</p>

    <div class="my-4 text-center"><span class="math-block">\\tilde {f} = \\sum_ {s \\in \\{0, 1 \\} ^ {\\gamma}} f (s) \\cdot \\chi_ {s}. \\tag {7}</span></div>

    <p class="text-gray-300">This follows because both sides of the equality are multilinear polynomials that agree at all Boolean inputs, and hence must be equal as formal polynomials.</p>

    <p class="text-gray-300">Remark.  <span class="math">\\tilde{f}</span>  can be viewed as an encoding of a table of  <span class="math">f</span> 's values. Specifically, let us view  <span class="math">f(\\cdot)</span>  as a function table with  <span class="math">2^{\\gamma}</span>  entries, where each  <span class="math">s \\in \\{0,1\\}^{\\gamma}</span>  is an index into that table. Notice that every point in the domain of  <span class="math">\\tilde{f}</span>  is a linear combination of all  <span class="math">2^{\\gamma}</span>  entries in this table.</p>

    <p class="text-gray-300"><span class="math">\\mathcal{V}</span> 's Precomputation.  <span class="math">\\mathcal{V}</span> 's precomputation evaluates  <span class="math">\\mathrm{add}_i(q_{i-1}, r_0, r_1)</span>  and  <span class="math">\\mathrm{mult}_i(q_{i-1}, r_0, r_1)</span>  for each  <span class="math">i = 1, \\ldots, d</span> , and all points  <span class="math">(q_{i-1}, r_0, r_1) \\in \\mathbb{F}^{\\log G} \\times \\mathbb{F}^{\\log G} \\times \\mathbb{F}^{\\log G}</span>  encountered in Lines 20 and 21 of Figure 10 over the course of the protocol execution. <span class="math">^{11}</span></p>

    <p class="text-gray-300">Hence, to show that  <span class="math">\\mathcal{V}</span> 's precomputation work is  <span class="math">O(d \\cdot G)</span> , it suffices to show that for each  <span class="math">i</span> ,  <span class="math">\\mathrm{add}_i(q_{i-1}, r_0, r_1)</span>  and  <span class="math">\\mathrm{mult}_i(q_{i-1}, r_0, r_1)</span>  can be evaluated in  <span class="math">O(G)</span>  time. An algorithm for achieving this was claimed by Vu et al. [81]; we present the details of such an algorithm.</p>

    <p class="text-gray-300">30</p>

    <p class="text-gray-300">1: function PROVE(ArithCircuit c, input <span class="math">x</span>) 2: <span class="math">(q_{0}^{\\prime},q_{0})\\leftarrow</span> ReceiveFromVerifier() // see line 4 3: <span class="math">d\\leftarrow</span> c.depth 4: 5: // each circuit layer induces one sumcheck invocation 6: for <span class="math">i=1,\\ldots,d</span> do 7: <span class="math">r^{\\prime},r_{0},r_{1}\\leftarrow</span> SumCheckP(<span class="math">c</span>, <span class="math">i</span>, <span class="math">(q_{i-1}^{\\prime},q_{i-1})</span>) 8: <span class="math">\\tau_{i}\\leftarrow</span> ReceiveFromVerifier() // see line 29 of Figure 1 9: <span class="math">(q_{i}^{\\prime},q_{i})\\leftarrow(r^{\\prime},(r_{1}-r_{0})\\cdot\\tau_{i}+r_{0})</span></p>

    <p class="text-gray-300">FIGURE 12—Pseudocode for <span class="math">\\mathcal{P}</span> in T13 and Giraffe. SumCheckP is defined in Figures 14 and 15 for T13 and Giraffe, respectively.</p>

    <p class="text-gray-300">Let <span class="math">S_{\\mathrm{add},i}\\subseteq S_{\\mathrm{all},i}\\subseteq\\{0,1\\}^{3b_{G}}</span> denote the set of all addition gates at layer-<span class="math">(i-1)</span>, with their layer-<span class="math">i</span> neighbors, and similarly for <span class="math">S_{\\mathrm{mult},i}</span>. By Equation (7),</p>

    <p class="text-gray-300"><span class="math">\\tilde{\\mathrm{add}}_{i}=\\sum_{u\\in\\{0,1\\}^{3b_{G}}}\\mathrm{add}_{i}(u)\\cdot\\chi_{u}=\\sum_{u\\in S_{\\mathrm{add},i}}\\chi_{u}</span> <span class="math">=\\sum_{(g,g_{L},g_{R})\\in S_{\\mathrm{add},i}}\\chi_{g}\\cdot\\chi_{g_{L}}\\cdot\\chi_{g_{R}}</span> (8)</p>

    <p class="text-gray-300">Likewise,</p>

    <p class="text-gray-300"><span class="math">\\mathrm{mult}_{i}=\\sum_{(g,g_{L},g_{R})\\in S_{\\mathrm{mult},i}}\\chi_{g}\\cdot\\chi_{g_{L}}\\cdot\\chi_{g_{R}}</span> (9)</p>

    <p class="text-gray-300">Hence, <span class="math">\\mathcal{V}</span>’s algorithm for evaluating <span class="math">\\tilde{\\mathrm{add}}_{i}(q_{i-1},r_{0},r_{1})</span> and <span class="math">\\tilde{\\mathrm{mult}}_{i}(q_{i-1},r_{0},r_{1})</span> first constructs three zero-indexed arrays, each with <span class="math">G</span> elements:</p>

    <p class="text-gray-300"><span class="math">A_{q}</span> <span class="math">=\\{\\chi_{0}(q),\\ldots,\\chi_{G-1}(q)\\}</span> <span class="math">A_{r_{0}}</span> <span class="math">=\\{\\chi_{0}(r_{0}),\\ldots,\\chi_{G-1}(r_{0})\\}</span> <span class="math">A_{r_{1}}</span> <span class="math">=\\{\\chi_{0}(r_{1}),\\ldots,\\chi_{G-1}(r_{1})\\}\\,.</span></p>

    <p class="text-gray-300">To construct each array, consider the algorithm in Figure 13. This algorithm uses dynamic programming to avoid recomputing suffixes. For example, notice that for all even <span class="math">h\\in[0,G-1]</span>, <span class="math">\\chi_{h}(q)=(1-q[1])\\cdot L</span> and <span class="math">\\chi_{h+1}(q)=q[1]\\cdot L</span>, where <span class="math">L=\\prod_{\\ell=2}^{b_{G}}\\chi_{h[\\ell]}(q[\\ell])</span>; the algorithm computes <span class="math">L</span> only once. Constructing an array takes <span class="math">O(G)</span> time because for each iteration of the outer loop, the number of iterations in the inner loop ascends as <span class="math">2^{1},2^{2},\\ldots,2^{b_{G}-1}</span>, making the total number of inner loop iterations <span class="math">\\sum_{i=1}^{b_{G}-1}2^{i}&lt;2^{b_{G}}=G</span>. Moreover, each inner loop iteration requires 2 field multiplications, so constructing all 3 arrays requires at most <span class="math">6\\cdot G</span> multiplications.</p>

    <p class="text-gray-300">Once the three arrays are computed, <span class="math">\\mathcal{V}</span> computes the right hand sides of Equations (8) and Equation (9) by iterating over each gate <span class="math">s=(g,g_{L},g_{R})</span>; looking up the three quantities <span class="math">A_{q}[g]</span>, <span class="math">A_{r_{0}}[g_{L}]</span>, <span class="math">A_{r_{1}}[g_{R}]</span>; multiplying them; and adding this product to a running sum for <span class="math">\\tilde{\\mathrm{add}}_{i}(q,r_{0},r_{1})</span> or <span class="math">\\tilde{\\mathrm{mult}}_{i}(q,r_{0},r_{1})</span>, depending on whether the gate is an addition or multiplication gate. This requires an additional <span class="math">2G</span> multiplications, and <span class="math">G</span> additions.</p>

    <p class="text-gray-300">In summary, the above shows that both <span class="math">\\tilde{\\mathrm{add}}_{i}(q_{i-1},r_{0},r_{1})</span> and <span class="math">\\tilde{\\mathrm{mult}}_{i}(q_{i-1},r_{0},r_{1})</span> can be computed in <span class="math">O(G)</span> time, with at most <span class="math">8G</span> field multiplications in total.</p>

    <p class="text-gray-300">Finally, <span class="math">\\mathcal{V}</span> must evaluate <span class="math">H(\\tau_{i})</span> (line 26, Fig. 10). Prior work has the prover specify the univariate polynomial <span class="math">H</span> appearing in Figure 10 by specifying its evaluations at <span class="math">b_{G}+1</span> inputs (§3.3). In some prior work, <span class="math">\\mathcal{V}</span> evaluates Lagrange basis polynomials at various points in precomputation, in <span class="math">O(d\\log^{2}G)</span> time <em>[81, 82]</em>. This allows <span class="math">\\mathcal{V}</span> to evaluate <span class="math">H(\\tau_{i})</span> online in <span class="math">O(\\log G)</span> time per evaluation.</p>

    <p class="text-gray-300">1: //  <span class="math">t[\\ell] \\in \\mathbb{F}</span>  are elements of vector  <span class="math">t</span> , 2: // which is indexed  <span class="math">1, \\ldots, b_{G}</span>  from LSB to MSB 3: //  <span class="math">A</span>  is an array of length  <span class="math">G</span> 4: 5:  <span class="math">A[0] \\gets 1 - t[b_G]</span> 6:  <span class="math">A[1] \\gets t[b_G]</span> 7: for  <span class="math">\\ell = b_{G} - 1, b_{G} - 2, \\ldots, 1</span>  do 8: for  <span class="math">k = 2^{b_G - \\ell} - 1, 2^{b_G - \\ell} - 2, \\ldots, 0</span>  do 9:  <span class="math">A[2k] \\gets (1 - t[\\ell]) \\cdot A[k]</span> 10:  <span class="math">A[2k + 1] \\gets t[\\ell] \\cdot A[k]</span></p>

    <p class="text-gray-300">FIGURE 13—Pseudocode for computing  <span class="math">A_{t} = \\{\\chi_{0}(t),\\dots ,\\chi_{G - 1}(t)\\}</span>  in time  <span class="math">O(G)</span> .  <span class="math">\\mathcal{V}</span>  needs to compute  <span class="math">A_{q},A_{r_{0}}</span>  and  <span class="math">A_{r_1}</span> . Each of  <span class="math">q,r_0,r_1</span>  is in  <span class="math">\\mathbb{F}^{b_G}</span> .</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math">\\mathcal{V}</span> 's remaining costs. Given the results of  <span class="math">\\mathcal{V}</span> 's precomputation, inspection of Figures 10 and 11 indicates that  <span class="math">\\mathcal{V}</span>  runs in  $O(d \\cdot \\log (N \\cdot G) +</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">+</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">y</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math">  time, provided that  </span>\\mathcal{V}$  can accomplish the following tasks in the following time bounds:</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">- For any point  <span class="math">(q_{d}^{\\prime}, q_{d}) \\in \\mathbb{F}^{b_{N}} \\times \\mathbb{F}^{b_{G}}</span> , evaluate  <span class="math">\\tilde{V}_{0}(q_{0^{\\prime}}^{\\prime}, q_{0})</span>  in time  $O(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">y</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)$ .</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>For any point  <span class="math">(q_{t - 1}^{\\prime},r^{\\prime})\\in \\mathbb{F}^{b_{N}}\\times \\mathbb{F}^{b_{N}}</span>  , evaluate  <span class="math">\\widetilde{\\mathrm{eq}} (q_{t - 1}^{\\prime},r^{\\prime})</span>  in time  <span class="math">O(b_{N})</span></li>

    </ul>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">- For any point  <span class="math">(q_{d}^{\\prime}, q_{d}) \\in \\mathbb{F}^{b_{N}} \\times \\mathbb{F}^{b_{G}}</span> , evaluate  <span class="math">\\tilde{V}_{d}(q_{d}^{\\prime}, q_{d})</span>  in time  $O(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)$ .</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">The first and third bullets are handled as in Section 3.3 (cf. the paragraph on multilinear extensions of I/O). To establish the second bullet, note that  <span class="math">\\widetilde{\\mathrm{eq}}\\colon \\mathbb{F}^{2b_N}\\to \\mathbb{F}</span>  has the following form [69, Prop. 3.2.1] (see also [81, Appx. A.1]):</p>

    <div class="my-4 text-center"><span class="math-block">\\widetilde {\\mathrm {e q}} \\left(q ^ {\\prime}, r ^ {\\prime}\\right) = \\prod_ {\\ell = 1} ^ {b _ {N}} \\left(q ^ {\\prime} [ \\ell ] \\cdot r ^ {\\prime} [ \\ell ] + \\left(1 - q ^ {\\prime} [ \\ell ]\\right) \\cdot \\left(1 - r ^ {\\prime} [ \\ell ]\\right)\\right) \\tag {10}</span></div>

    <p class="text-gray-300">Each term simplifies to  <span class="math">2q&#x27;[\\ell] \\cdot r&#x27;[\\ell] + 1 - (q&#x27;[\\ell] + r&#x27;[\\ell])</span> , which can be computed with one multiplication and four additions. Thus the whole computation requires  <span class="math">4b_{N}</span>  additions and  <span class="math">2b_{N} - 1</span>  multiplications.</p>

    <h2 id="sec-35" class="text-2xl font-bold">B Details of Giraffe's back-end</h2>

    <p class="text-gray-300">As stated in Section 3.1, Giraffe's back-end differs from T13 (the protocol of Appx. A) in that Giraffe changes the order in which variables are bound within each invocation of the sum-check protocol, and exploits that order to simplify  <span class="math">\\mathcal{P}</span> 's work.</p>

    <p class="text-gray-300">A complete description of the verifier's work in Giraffe can be found in Figures 10 and 11. A complete description of the prover's work can be found in Figures 12 and 15. The following theorem states the relevant properties of this protocol.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Theorem B.1. Consider the protocol with verifier described in Figures 10 and 11, and prover described in Figure 15. When applied to a circuit  <span class="math">\\mathcal{C}</span>  as in Section 2.2, the protocol satisfies completeness, and satisfies soundness with  $\\epsilon = (\\lceil \\log</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">y</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\rceil + 6d \\log (G \\cdot N)) /</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> .  </span>\\mathcal{V}<span class="math">  requires precomputation that is  </span>O(d \\cdot G)<span class="math"> . Then, to validate all inputs and outputs,  </span>\\mathcal{V}<span class="math">  incurs cost  </span>O(d \\cdot \\log (N \\cdot G) +</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">+</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">y</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math"> .  </span>\\mathcal{P}<span class="math"> &#x27;s running time is  </span>O(d \\cdot (G \\cdot N + G \\cdot \\log G))$ .</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">1: function SumCHECKP(ArithCircuit c, layer  <span class="math">i</span> ,  <span class="math">q_{i-1}^{\\prime}</span> ,  <span class="math">q_{i-1}</span> ) 2: for  <span class="math">j = 1,\\dots ,2b_{G}</span>  do 3: // In these rounds, prover sends degree-2 polynomial  <span class="math">F_{j}</span> . Does this by computing and sending  <span class="math">F_{j}(0), F_{j}(1), F_{j}(2)</span> . 4: 5: for all  <span class="math">\\sigma \\in \\{0,1\\}^{b_N}</span>  and all  <span class="math">g \\in \\{0,1\\}^{b_G}</span>  and  <span class="math">k \\in \\{0,1,2\\}</span>  do 6:  <span class="math">s \\gets (g, g_L, g_R) // g_L, g_R</span>  are labels of  <span class="math">g</span> 's layer- <span class="math">i</span>  inputs in subcircuit 7:  <span class="math">u_k \\gets (q_{i-1}[1], \\ldots, q_{i-1}[b_G], r[1], \\ldots, r[j-1], k)</span> 8: termP  <span class="math">\\leftarrow \\widehat{\\mathrm{eq}}(q_{i-1}&#x27;, \\sigma) \\cdot \\prod_{\\ell=1}^{b_G+j} \\chi_{s[\\ell]}(u_k[\\ell])</span> 9: 10: if  <span class="math">j \\leq b_G</span>  then 11: termL  <span class="math">\\leftarrow \\tilde{V}_i(\\sigma, r[1], \\ldots, r[j-1], k, g_L[j+1], \\ldots, g_L[b_G])</span> 12: termR  <span class="math">\\leftarrow V_i(\\sigma, g_R) // V_i = \\tilde{V}_i</span>  on gate labels 13: else //  <span class="math">b_G &amp;lt; j \\leq 2b_G</span> 14: termL  <span class="math">\\leftarrow \\tilde{V}_i(\\sigma, r[1], \\ldots, r[b_G])</span> 15: termR  <span class="math">\\leftarrow \\tilde{V}_i(\\sigma, r[b_G+1], \\ldots, r[j-1], k, g_R[j-b_G+1], \\ldots, g_R[b_G])</span> 16: 17: if  <span class="math">g</span>  is an add gate then  <span class="math">F_j[\\sigma, g][k] \\gets \\text{termP} \\cdot (\\text{termL} + \\text{termR})</span> 18: else if  <span class="math">g</span>  is a mult gate then  <span class="math">F_j[\\sigma, g][k] \\gets \\text{termP} \\cdot \\text{termL} \\cdot \\text{termR}</span> 19: 20: for  <span class="math">k \\in \\{0,1,2\\}</span>  do 21:  <span class="math">F_j[k] \\gets \\sum_{\\sigma \\in \\{0,1\\}^{b_N}} \\sum_{g \\in \\{0,1\\}^{b_G}} F_j[\\sigma, g][k]</span> 22: SendToVerifier( <span class="math">F_j</span> , 2) 23:  <span class="math">r[j] \\gets \\text{ReceiveFromVerifier()}</span>  // see line 19 of Figure 11 24: 25:  <span class="math">r_0 \\gets (r[1], \\ldots, r[b_G])</span>  // notation 26:  <span class="math">r_1 \\gets (r[b_G + 1], \\ldots, r[2b_G])</span>  // notation 27: 28: for  <span class="math">j = 1,\\dots ,b_N</span>  do 29: // In these rounds, prover sends degree-3 polynomial  <span class="math">F_{2b_G + j}</span> , so computes  <span class="math">F_{2b_G + j}(0), \\ldots, F_{2b_G + j}(3)</span> 30: 31: for all  <span class="math">\\sigma \\in \\{0,1\\}^{b_N - j}</span>  and  <span class="math">k \\in \\{0,1,2,3\\}</span>  do 32: termP  <span class="math">\\leftarrow \\widehat{\\mathrm{eq}}(q_{i-1}&#x27;, r&#x27;[1], \\ldots, r&#x27;[j-1], k, \\sigma[1], \\ldots, \\sigma[b_N - j])</span> 33: termL  <span class="math">\\leftarrow \\tilde{V}_i(r&#x27;[1], \\ldots, r&#x27;[j-1], k, \\sigma[1], \\ldots, \\sigma[b_N - j], r_0)</span> 34: termR  <span class="math">\\leftarrow \\tilde{V}_i(r&#x27;[1], \\ldots, r&#x27;[j-1], k, \\sigma[1], \\ldots, \\sigma[b_N - j], r_1)</span> 35: 36: // See text for computation of  <span class="math">\\widehat{\\mathrm{add}}(q_{i-1}, r_0, r_1)</span>  and  <span class="math">\\widehat{\\mathrm{mult}}(q_{i-1}, r_0, r_1)</span> 37:  <span class="math">F_{2b_G + j}[\\sigma][k] \\gets \\text{termP} \\cdot (\\widehat{\\mathrm{add}}(q_{i-1}, r_0, r_1) \\cdot (\\text{termL} + \\text{termR}) + \\widehat{\\mathrm{mult}}(q_{i-1}, r_0, r_1) \\cdot \\text{termL} \\cdot \\text{termR})</span> 38: 39: for  <span class="math">k \\in \\{0,1,2,3\\}</span>  do 40:  <span class="math">F_{2b_G + j}[k] \\gets \\sum_{\\sigma \\in \\{0,1\\}^{b_N - j}} F_{2b_G + j}[\\sigma][k]</span> 41: SendToVerifier( <span class="math">F_{2b_G + j}</span> , 3) 42:  <span class="math">r&#x27;[j] \\gets \\text{ReceiveFromVerifier()}</span>  // see line 19 of Figure 11 43: 44:  <span class="math">r&#x27; \\gets (r&#x27;[1], \\ldots, r&#x27;[b_N])</span>  // notation 45: 46: for  <span class="math">\\ell = \\{0,\\dots ,b_G\\}</span> ,  <span class="math">H[\\ell ]\\gets \\tilde{V}_i(r&#x27;,(r_1 - r_0)\\cdot \\ell +r_0)</span> 47: SendToVerifier( <span class="math">H, b_G</span> ) 48: 49: return  <span class="math">(r&#x27;, r_0, r_1)</span></p>

    <p class="text-gray-300">FIGURE 14— <span class="math">\\mathcal{P}</span>  pseudocode in T13 [77, §7] (with optimization [78]) for the layer- <span class="math">i</span>  sum-check invocation.</p>

    <p class="text-gray-300">1: function SumCHECKP(ArithCircuit c, layer  <span class="math">i</span> ,  <span class="math">q_{i-1}^{\\prime}</span> ,  <span class="math">q_{i-1}</span> ) 2: for  <span class="math">j = 1,\\dots ,b_{N}</span>  do 3: // Prover sends degree-3 polynomial  <span class="math">F_{j}</span> . Does this by computing  <span class="math">F_{j}(-1), F_{j}(0), F_{j}(1), F_{j}(2)</span>  and then interpolating. 4: 5: for all  <span class="math">\\sigma \\in \\{0,1\\}^{b_N - j}</span>  and  <span class="math">g\\in \\{0,1\\}^{b_G}</span>  and  <span class="math">k\\in \\{-1,0,1,2\\}</span>  do 6:  <span class="math">s\\gets (g,g_L,g_R) / / g_L,g_R</span>  are labels of  <span class="math">g</span>  's layer-  <span class="math">i</span>  inputs in sub-circuit. 7: 8: termP  <span class="math">\\leftarrow \\widetilde{\\mathrm{eq}} (q_{i - 1}^{\\prime},r^{\\prime}[1],\\ldots ,r^{\\prime}[j - 1],k,\\sigma [1],\\ldots ,\\sigma [b_{N} - j])\\cdot \\chi_{g}(q_{i - 1})</span> 9: termL  <span class="math">\\leftarrow \\tilde{V}_i(r&#x27;[1],\\ldots ,r&#x27;[j - 1],k,\\sigma [1],\\ldots ,\\sigma [b_N - j],g_L)</span> 10: termR  <span class="math">\\leftarrow \\tilde{V}_i(r&#x27;[1],\\ldots ,r&#x27;[j - 1],k,\\sigma [1],\\ldots ,\\sigma [b_N - j],g_R)</span> 11: 12: if  <span class="math">g</span>  is an add gate then  <span class="math">F_{j}[\\sigma ,g][k]\\gets \\mathrm{termP}\\cdot (\\mathrm{termL} + \\mathrm{termR})</span> 13: else if  <span class="math">g</span>  is a mult gate then  <span class="math">F_{j}[\\sigma ,g][k]\\gets \\mathrm{termP}\\cdot \\mathrm{termL}\\cdot \\mathrm{termR}</span> 14: 15: for  <span class="math">k\\in \\{-1,0,1,2\\}</span>  do 16:  <span class="math">F_{j}[k]\\gets \\sum_{\\sigma \\in \\{0,1\\}^{b_{N} - j}}\\sum_{g\\in \\{0,1\\}^{b_{G}}}F_{j}[\\sigma ,g][k]</span> 17: // Use Lagrange interpolation to compute coefficients of  <span class="math">F_{j}</span>  and send them to  <span class="math">\\mathcal{V}</span> 18: SendToVerifier  <span class="math">(F_j,3)</span> 19:  <span class="math">r^{\\prime}[j]\\gets</span>  ReceiveFromVerifier() // see line 19 of Figure 11 20: 21:  <span class="math">r^{\\prime}\\gets (r^{\\prime}[1],\\ldots ,r^{\\prime}[b_{N}])</span>  // notation 22: 23: for  <span class="math">j = 1,\\dots ,2b_{G}</span>  do 24: // In these rounds, prover sends degree-2 polynomial  <span class="math">F_{b_N + j}</span> 25: for all gates  <span class="math">g\\in \\{0,1\\}^{b_G}</span>  and  <span class="math">k\\in \\{-1,0,1\\}</span>  do 26: 27:  <span class="math">s\\gets (g,g_L,g_R)</span>  //  <span class="math">g_{L},g_{R}</span>  are labels of  <span class="math">g</span>  's layer-  <span class="math">i</span>  inputs in subcircuit 28:  <span class="math">u_{k}\\gets (q_{i - 1}[1],\\ldots ,q_{i - 1}[b_{G}],r[1],\\ldots ,r[j - 1],k)</span> 29: termP  <span class="math">\\leftarrow \\widetilde{\\mathrm{eq}} (q_{i - 1}^{\\prime},r^{\\prime})\\cdot \\prod_{\\ell = 1}^{b_{G} + j}\\chi_{s[\\ell ]}(u_{k}[\\ell ])</span> 30: 31: if  <span class="math">j\\leq b_G</span>  then 32: termL  <span class="math">\\leftarrow \\tilde{V}_i(r&#x27;,r[1],\\ldots ,r[j - 1],k,g_L[j + 1],\\ldots ,g_L[b_G])</span> 33: termR  <span class="math">\\leftarrow \\tilde{V}_i(r&#x27;,g_R)</span> 34: else //  <span class="math">b_{G} &amp;lt;   j\\leq 2b_{G}</span> 35: termL  <span class="math">\\leftarrow \\tilde{V}_i(r&#x27;,r[1],\\ldots ,r[b_G])</span> 36: termR  <span class="math">\\leftarrow \\tilde{V}_i(r&#x27;,r[b_G + 1],\\ldots ,r[j - 1],k,g_R[j - b_G + 1],\\ldots ,g_R[b_G])</span> 37: 38: if  <span class="math">g</span>  is an add gate then 39:  <span class="math">F_{b_N + j}[g][k]\\gets \\mathrm{termP}\\cdot (\\mathrm{termL} + \\mathrm{termR})</span> 40: else if  <span class="math">g</span>  is a mult gate then 41:  <span class="math">F_{b_N + j}[g][k]\\gets \\mathrm{termP}\\cdot \\mathrm{termL}\\cdot \\mathrm{termR}</span> 42: 43: for  <span class="math">k\\in \\{-1,0,1\\}</span>  do 44:  <span class="math">F_{b_N + j}[k]\\gets \\sum_{g\\in \\{0,1\\}^{b_G}}F_{b_N + j}[g][k]</span> 45: // Use Lagrange interpolation to compute coefficients of  <span class="math">F_{b_N + j}</span>  and send them to verifier 46: SendToVerifier  <span class="math">(F_{b_N + j},2)</span> 47:  <span class="math">r[j]\\gets</span>  ReceiveFromVerifier() // see line 19 of Figure 11 48: 49:  <span class="math">r_0\\gets (r[1],\\ldots ,r[b_G])</span>  // notation 50:  <span class="math">r_1\\gets (r[b_G + 1],\\ldots ,r[2b_G])</span>  // notation 51: 52: for  <span class="math">\\ell = \\{0,\\dots ,b_G\\}</span>  ，  <span class="math">H[\\ell ]\\gets \\tilde{V}_i(r&#x27;,(r_1 - r_0)\\cdot \\ell +r_0)</span> 53: // Use Lagrange interpolation to compute coefficients of  <span class="math">H</span>  and send them to  <span class="math">\\mathcal{V}</span> 54: SendToVerifier  <span class="math">(H,b_{G})</span> 55: 56: return  <span class="math">(r^{\\prime},r_{0},r_{1})</span></p>

    <p class="text-gray-300">FIGURE 15— <span class="math">\\mathcal{P}</span>  pseudocode in Giraffe for the layer-  <span class="math">i</span>  sum-check invocation.</p>

    <p class="text-gray-300">The conclusion of Theorem B.1 is identical to that of Theorem A.1, except for the improvement in <span class="math">\\mathcal{P}</span>’s runtime.</p>

    <h6 id="sec-36" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">Completeness, soundness, and the bound on <span class="math">\\mathcal{V}</span>’s runtime are established via the analyses in <em>[77, §7]</em> and Appendix A. This is because the principal difference between Giraffe and T13 is the order in which variables are bound, which does not affect completeness, soundness, or <span class="math">\\mathcal{V}</span>’s runtime.</p>

    <p class="text-gray-300">The remainder of the proof is devoted to bounding <span class="math">\\mathcal{P}</span>’s runtime. From inspection of Figure 12, the claim about <span class="math">\\mathcal{P}</span>’s runtime is true as long as each of the <span class="math">d</span> calls to SumCheckP (cf. line 7 of Fig. 12) can be implemented in time <span class="math">O(G\\cdot N+G\\cdot\\log G)</span>.</p>

    <p class="text-gray-300">To show this, we begin by explaining how the first for loop of the SumCheckP function (lines 2–19 in Fig. 15) can be implemented to run in time <span class="math">O(G\\cdot N)</span>. As in Section 3.1, we call this part of the protocol “phase 1”.</p>

    <p class="text-gray-300">We begin with the inner for loop of phase 1 (lines 5-13 in Fig. 15). This loop has <span class="math">4G\\cdot N/2^{j}</span> iterations. Lines 12 and 13 each take constant time per iteration, leading to a contribution of <span class="math">O(\\sum_{j=1}^{b_{N}}G\\cdot N/2^{j})=O(G\\cdot N)</span>. Next, consider the computation of termL and termR in lines 9 and 10. Section 3.2 (see the “algorithm” paragraph) explained how to compute, in iteration <span class="math">j</span>, all required values of termL and termR (across <span class="math">\\sigma</span>, <span class="math">g</span>, <span class="math">k</span>) in total time <span class="math">O(G\\cdot N/2^{j})</span>, leading to another contribution of <span class="math">O(\\sum_{j=1}^{b_{N}}G\\cdot N/2^{j})=O(G\\cdot N)</span>.</p>

    <p class="text-gray-300">The bulk of our attention on the inner loop is on computing all required values of termP in line 8 in <span class="math">O(G+N)</span> time across all iterations <span class="math">j=1,\\ldots,b_{N}</span>. This decomposes into (a) computing <span class="math">\\chi_{g}(q_{i-1})</span> for all <span class="math">g\\in\\{0,1\\}^{b_{G}}</span>, and (b) computing <span class="math">\\widetilde{\\mathrm{eq}}(q^{\\prime}_{i-1},r^{\\prime}[1],\\ldots,r^{\\prime}[j-1],\\ k,\\ \\sigma[1],\\ldots,\\sigma[b_{N}-j])</span>, where <span class="math">j</span> ranges from 1 up to <span class="math">b_{N}</span>, <span class="math">\\sigma</span> ranges over <span class="math">\\{0,1\\}^{b_{N}-j}</span>, and <span class="math">k</span> ranges over <span class="math">\\{-1,0,1,2\\}</span>. For (a), Appx A (the “precomputation” paragraph) explained precisely how to compute the <span class="math">\\chi_{g}(q_{i-1})</span> in time <span class="math">O(G)</span>.</p>

    <p class="text-gray-300">To achieve (b) in time <span class="math">O(N)</span>, consider the function <span class="math">Z\\colon\\{0,1\\}^{b_{N}}\\to\\mathbb{F}</span> given by <span class="math">Z(\\cdot)=\\widetilde{\\mathrm{eq}}(q^{\\prime}_{i-1},\\cdot)</span>. Observe that <span class="math">\\tilde{Z}(\\cdot)</span> and <span class="math">\\widetilde{\\mathrm{eq}}(q^{\\prime}_{i-1},\\cdot)</span> are equal as formal polynomials, because they are both multilinear and agree at all Boolean inputs. Hence, task (b) is equivalent to evaluating <span class="math">\\tilde{Z}</span> at all points of the form <span class="math">(r^{\\prime}[1],\\ldots,r^{\\prime}[j-1],\\ k,\\ \\sigma[1],\\ldots,\\sigma[b_{N}-j])</span>.</p>

    <p class="text-gray-300"><span class="math">\\mathcal{P}</span> can achieve this in two steps. In the first step, prior to round <span class="math">j=1</span>, <span class="math">\\mathcal{P}</span> evaluates <span class="math">Z</span> on all Boolean inputs as follows. Observe that for any <span class="math">\\sigma\\in\\{0,1\\}^{b_{N}}</span>, Equation (10) implies that <span class="math">Z(\\sigma)=\\widetilde{\\mathrm{eq}}(q^{\\prime}_{i-1},\\sigma)=\\chi_{\\sigma}(q^{\\prime}_{i-1})</span>. <span class="math">\\mathcal{P}</span> can again use the technique from Appendix A, this time to build an array containing <span class="math">\\chi_{\\sigma}(q^{\\prime}_{i-1})</span> for all <span class="math">\\sigma\\in\\{0,1\\}^{b_{N}}</span> in time <span class="math">O(N)</span>.</p>

    <p class="text-gray-300">In the second step, <span class="math">\\mathcal{P}</span> evaluates <span class="math">\\tilde{Z}</span> at all of the necessary points using the following method. Notice that when explaining how to efficiently compute termL and termR (§3.2), we more generally showed that the following is true. For any <span class="math">b</span>-variate function <span class="math">f\\colon\\{0,1\\}^{b}\\to\\mathbb{F}</span> and any <span class="math">k\\in\\mathbb{F}</span>, given <span class="math">f</span>’s values on all Boolean inputs, one can, in total time <span class="math">O(2^{b})</span>, evaluate <span class="math">\\tilde{f}</span> at all points of the form <span class="math">(r^{\\prime}[1],\\ldots,r^{\\prime}[j-1],k,\\ \\sigma[1],\\ldots,\\sigma[b-j])</span>, where <span class="math">j</span> ranges from 1 up to <span class="math">b</span>, and <span class="math">\\sigma</span> ranges over <span class="math">\\{0,1\\}^{b-j}</span>. Hence, once <span class="math">\\mathcal{P}</span> has evaluated <span class="math">Z</span> on all Boolean inputs, <span class="math">\\mathcal{P}</span> can apply the aforementioned result to <span class="math">f=Z</span> in order to evaluate <span class="math">\\tilde{Z}</span> at the necessary points in time <span class="math">O(2^{b_{N}})=O(N)</span>.</p>

    <p class="text-gray-300">In total, both steps of task (b) are dispatched in <span class="math">O(N)</span> time.</p>

    <p class="text-gray-300">By inspection, lines 15–19 of Figure 15 can be dispatched in <span class="math">\\sum_{j=1}^{b_{N}}O(G\\cdot N/2^{j})=O(G\\cdot N)</span> time. Hence, phase 1 of the protocol can be dispatched in <span class="math">O(G\\cdot N)</span> time in total.</p>

    <p class="text-gray-300">The next cost to <span class="math">\\mathcal{P}</span> to account for is the for loop with <span class="math">2b_{G}</span> iterations (cf. line 23); as in Section 3.1, we refer to this as “phase 2”. This for loop can be dispatched in <span class="math">O(G)</span> time per iteration (hence, <span class="math">O(G\\log G)</span> time in total), in a manner analogous to the prover implementation of CMT <em>[36]</em> (indeed, the pseudocode of Fig. 15 already incorporates key insights from *[36]</p>

    <p class="text-gray-300">In more detail, it is enough to show that for each iteration <span class="math">j\\in[1,\\,2b_{G}]</span> of this for loop, all <span class="math">3G</span> iterations of the inner for loop in line 25 of Figure 15 can be dispatched in <span class="math">O(G)</span> total time, as this will yield a time bound of <span class="math">O(G\\cdot b_{G})=O(G\\log G)</span>. The dominant cost of these iterations is in computing the termP, termL, and termR values. The termL and termR values are handled via essentially the same method as in phase 1, requiring <span class="math">O(\\sum_{j=b_{N}+1}^{b_{N}+b_{G}}G/2^{j})=O(G)</span> time in total (across all <span class="math">2b_{G}</span> iterations <span class="math">j</span>).</p>

    <p class="text-gray-300">The bottleneck for phase 2 is the time required to compute termP (cf. line 29). The prover stores, at all iterations <span class="math">j\\in[1,\\,2b_{G}]</span> of the outer loop, and for each gate <span class="math">g\\in\\{0,1\\}^{b_{G}}</span> and <span class="math">k=0</span>, the value <span class="math">U[g]=\\widetilde{\\mathrm{eq}}(q^{\\prime}_{i-1},r^{\\prime})\\cdot\\prod_{\\ell=1}^{b_{G}+j-1}\\chi_{s[\\ell]}(u_{k}[\\ell])</span> (see lines 27 and 28 for the definition of <span class="math">s</span> and <span class="math">u</span>). Given these <span class="math">U[g]</span> quantities, in each iteration <span class="math">j</span> of the outer loop, <span class="math">\\mathcal{P}</span> can compute each value of termP and update <span class="math">U[g]</span> in constant time. This means that for each iteration <span class="math">j</span>, all <span class="math">O(G)</span> values of termP can be computed in <span class="math">O(G)</span> total time, resulting in the claimed <span class="math">O(G\\log G)</span> time bound across all <span class="math">2b_{G}</span> iterations of the outer loop.</p>

    <p class="text-gray-300">The final cost to account for in <span class="math">\\mathcal{P}</span>’s work is evaluating the degree-<span class="math">b_{G}</span> univariate polynomial <span class="math">H=\\tilde{V}_{i}(r^{\\prime},(r_{1}-r_{0})\\cdot\\ell+r_{0})</span> at <span class="math">b_{G}+1</span> values of <span class="math">\\ell</span> (see lines 52 and 53 of Fig. 15). Consider the function <span class="math">Q\\colon\\{0,1\\}^{b_{G}}\\to\\mathbb{F}</span>, defined as <span class="math">Q(\\cdot)=\\tilde{V}_{i}(r^{\\prime},\\cdot)</span>. Then <span class="math">\\tilde{Q}(\\cdot)</span> and <span class="math">\\tilde{V}_{i}(r^{\\prime},\\cdot)</span> are equal as formal polynomials, because the right- and left-hand sides are multilinear polynomials that agree at all Boolean inputs.</p>

    <p class="text-gray-300">Hence, <span class="math">\\mathcal{P}</span> must compute <span class="math">\\tilde{Q}((r_{1}-r_{0})\\cdot\\ell+r_{0})</span> for <span class="math">\\ell\\in\\{0,\\ldots,b_{G}\\}</span>. For this purpose, we use the following result, which is a variant of the one given earlier (in reference to task (b)): given the evaluations of a <span class="math">b_{G}</span>-variate function <span class="math">Q</span> on all Boolean inputs, one can evaluate <span class="math">\\tilde{Q}</span> at any point in time <span class="math">O(2^{b_{G}})=O(G)</span>. This follows from again applying the technique from Section 3.2 used to compute all of the termL and termR values (and is described in Section 3.3, the paragraph on multilinear extensions of I/O).</p>

    <p class="text-gray-300">To get the evaluations of <span class="math">Q(\\cdot)</span> on all Boolean inputs, we need <span class="math">\\tilde{V}_{i}(r^{\\prime},\\sigma)</span> for <span class="math">\\sigma\\in\\{0,1\\}^{b_{G}}</span>. These evaluations can be computed in time <span class="math">O(N\\cdot G)</span>, again using the Section 3.2 technique. Then, we apply the previous paragraph to the <span class="math">b_{G}+1</span> points <span class="math">\\{(r_{1}-r_{0})\\cdot\\ell+r_{0}\\mid\\ell=0,\\ldots,b_{G}\\}</span>, yielding additional computational cost of <span class="math">O(G\\cdot b_{G})</span>.</p>

    <p class="text-gray-300">In summary, lines 52 and 53 of Figure 15 together can be dispatched in time <span class="math">O(N\\cdot G+G\\cdot\\log G)</span>. ∎</p>

    <h3 id="sec-37" class="text-xl font-semibold mt-8">B.1 Recursive expression for <span class="math">\\tilde{V}_{i}</span></h3>

    <p class="text-gray-300">The Equation (5) recurrence in Section 3.2 is derived as follows:</p>

    <p class="text-gray-300"><span class="math">\\tilde{V}_{i}\\left(r^{\\prime}[1..j],\\sigma,h\\right)</span> <span class="math">=\\sum_{s\\in\\{0,1\\}^{b_{N}+b_{G}}}V_{i}(s)\\cdot\\chi_{s}\\left(r^{\\prime}[1..j],\\sigma,h\\right)</span> <span class="math">=\\sum_{s\\in\\{0,1\\}^{b_{N}+b_{G}}:s_{j}=0}V_{i}(s)\\cdot\\chi_{s}\\left(r^{\\prime}[1..j],\\sigma,h\\right)</span> <span class="math">\\quad+\\sum_{s\\in\\{0,1\\}^{b_{N}+b_{G}}:s_{j}=1}V_{i}(s)\\cdot\\chi_{s}\\left(r^{\\prime}[1..j],\\sigma,h\\right)</span> <span class="math">=(1-r^{\\prime}[j])\\cdot\\tilde{V}_{i}\\left(r^{\\prime}[1..j-1],0,\\sigma,h\\right)</span> <span class="math">\\quad+r^{\\prime}[j]\\cdot\\tilde{V}_{i}\\left(r^{\\prime}[1..j-1],1,\\sigma,h\\right).</span></p>

    <p class="text-gray-300">The first and last equalities apply Equation (7).</p>

    <h3 id="sec-38" class="text-xl font-semibold mt-8">B.2 Other implementation considerations</h3>

    <h4 id="sec-39" class="text-lg font-semibold mt-6">The choice of values <span class="math">k\\in\\{-1,0,1,2\\}</span>.</h4>

    <p class="text-gray-300">In phase 1, Giraffe’s <span class="math">\\mathcal{P}</span> evaluates <span class="math">F_{j}(k)</span>, <span class="math">k\\in\\{-1,0,1,2\\}</span>. This is a small optimization compared to, e.g., <span class="math">k\\in\\{0,1,2,3\\}</span>. Recall from Section 3.2 that for <span class="math">k=-1</span>,</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">cost</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">verifier</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">prover</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">|  energy  |   |   |</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">compute</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">G(8d+4N)Emul,t+G(d+2N)Eadd,t</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">d[G(6N+8log G)Emul,u+G(11N+8log G)Eadd,u+4G(N+log G)⟨Eg,u⟩]</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">store</td>

            <td class="px-3 py-2 border-b border-gray-700">—</td>

            <td class="px-3 py-2 border-b border-gray-700">dNG·Esto,u</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">PRNG</td>

            <td class="px-3 py-2 border-b border-gray-700">d(2log G+log N)Eprng,t</td>

            <td class="px-3 py-2 border-b border-gray-700">—</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">V I/O</td>

            <td class="px-3 py-2 border-b border-gray-700">2NG·Eio,t</td>

            <td class="px-3 py-2 border-b border-gray-700">—</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">|  area  |   |   |</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">compute</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">nV,sc(4Amul,t+3Aadd,t)+2nV,io(2Amul,t+Aadd,t)</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">nP,sc[(4GnP,ea+N/2+2nP,γlog G)Amul,u+(4GnP,ea+N/2+nP,γlog G)Aadd,u]</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">store</td>

            <td class="px-3 py-2 border-b border-gray-700">—</td>

            <td class="px-3 py-2 border-b border-gray-700">dNGnP,pl·Asto,u</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">PRNG</td>

            <td class="px-3 py-2 border-b border-gray-700">d(2log G+log N)Aprng,t</td>

            <td class="px-3 py-2 border-b border-gray-700">—</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">V I/O</td>

            <td class="px-3 py-2 border-b border-gray-700">2NG·Aio,t</td>

            <td class="px-3 py-2 border-b border-gray-700">—</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">delay: Giraffe's overall throughput is 1/max ( <span class="math">\\mathcal{V}</span>  delay,  <span class="math">\\mathcal{P}</span>  delay); the expressions for  <span class="math">\\mathcal{V}</span>  and  <span class="math">\\mathcal{P}</span>  delay are given immediately below:</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">max(dG/nV,sc(3λmul,t+λadd,t),NG/nV,io(λmul,t+λadd,t))</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">d/nP,sc[(3C/nP,ea+G/nP,γ)(λmul,u+λadd,u)]</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">nV,io: V template argument; trades area vs I/O delay</td>

            <td class="px-3 py-2 border-b border-gray-700">nV,sc: V template argument; trades area vs sumcheck delay</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">nP,pl: P template argument; # in-flight runs</td>

            <td class="px-3 py-2 border-b border-gray-700">nP,ea: P template argument; parallelism in phase 1 (§3.2)</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">nP,sc: P template argument; trades area vs delay</td>

            <td class="px-3 py-2 border-b border-gray-700">nP,γ: P template argument; parallelism for final V(z3,·) eval</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">⟨Eg,u⟩: mean per-gate energy of C, untrusted</td>

            <td class="px-3 py-2 border-b border-gray-700">d,G,N: depth, width, and number of copies of arithmetic circuit C</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">|  E{add,mul,tx,sto,prng,io},{t,u}: energy cost in {trusted, untrusted} technology node for {+,×,V-P interaction, store, PRNG, V I/O} |   |</p>

    <p class="text-gray-300">|  A{add,mul,tx,sto,prng,io},{t,u}: area cost in {trusted, untrusted} technology node for {+,×,V-P interaction, store, PRNG, V I/O} |   |</p>

    <p class="text-gray-300">|  λ{add,mul},{t,u}: delay in {trusted, untrusted} technology node for {+,×} |   |</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">FIGURE 16— <span class="math">\\mathcal{V}</span>  and  <span class="math">\\mathcal{P}</span>  costs as a function of AC parameters and technology nodes (simplified model; low-order terms discarded). We assume  $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">=</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">y</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">= N \\cdot G<span class="math"> . Energy and area constants for interaction, store, PRNG, and I/O indicate costs for a single element of  </span>\\mathbb{F}_p<span class="math"> .  </span>\\mathcal{V} - \\mathcal{P}<span class="math">  tx is the cost of interaction between  </span>\\mathcal{V}<span class="math">  and  </span>\\mathcal{P}<span class="math"> ;  </span>\\mathcal{V}<span class="math">  I/O is the cost for the operator to communicate with Giraffe. For  </span>\\mathcal{P}$ , store is the cost of buffering pipelined computations. Transmit, store, and PRNG occur in parallel with execution, so their delay is not included under the assumption that the corresponding circuits execute quickly enough.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><span class="math">\\mathrm{termL}_{j,n,g_L, - 1} = 2\\cdot \\mathrm{termL}_{j,n,g_L,0} + (-1)\\cdot \\mathrm{termL}_{j,n,g_L,1}</span> . Multiplication by 2 and by  <span class="math">-1</span>  can both be implemented as an addition rather than a multiplication, while  <span class="math">k = 3</span>  requires either two additions or a multiplication. A further slight optimization arises in  <span class="math">\\mathcal{P}</span> 's work interpolating  <span class="math">F_{j}</span> : interpolating a third-degree polynomial for evaluations at the chosen points allows a few more multiplications to be traded for additions. In phase 2, Giraffe uses  <span class="math">k\\in \\{-1,0,1\\}</span>  (Fig. 15) for the same reason.</p>

    <p class="text-gray-300"><span class="math">\\mathcal{V}</span> 's precomputation hardware.  <span class="math">\\mathcal{V}</span>  implements the dynamic programming algorithm of Figure 13 using an approach similar to the one described in Section 3.2. In brief, the access pattern of the algorithm is read one, write two, read one, and so forth.  <span class="math">\\mathcal{V}</span>  instantiates two multipliers, one for each of the products in the innermost loop of Figure 13, and uses a variant of the RWSR design to store operands and results.</p>

    <p class="text-gray-300">Figure 16 presents a simplified cost model for Giraffe's operating cost (energy), manufacturing cost (chip area), and performance (delay, i.e., inverse throughput). Roughly speaking, energy captures the number of operations executed, area corresponds to parallelism, and throughput represents the time spent on the critical path of execution.</p>

    <p class="text-gray-300">Both  <span class="math">\\mathcal{P}</span>  and  <span class="math">\\mathcal{V}</span>  are designed to allow the designer to trade chip area for throughput. Section 3.2 describes one such tradeoff; Giraffe applies similar techniques in other parts of both  <span class="math">\\mathcal{P}</span>  and  <span class="math">\\mathcal{V}</span> . In addition, Giraffe's protocol requires computations expressed as layered arithmetic circuits (§2), and</p>

    <p class="text-gray-300">!<a href="img-15.jpeg">img-15.jpeg</a> FIGURE 17—Effect of Giraffe's protocol optimizations (§3.1). We compare the performance of Giraffe's prover to T13's prover by counting field multiplications for a generic AC (§6.1) with number of sub-AC copies  <span class="math">N = 2^{10}</span> , fraction of multipliers  <span class="math">\\delta = 0.5</span> , and depth  <span class="math">d = 20</span> . Giraffe's protocol improves over T13 by about a factor of  <span class="math">\\log G</span> . This improvement is essentially insensitive to  <span class="math">N</span> ,  <span class="math">\\delta</span> , and  <span class="math">d</span> . Comparing field additions gives a similar result.</p>

    <p class="text-gray-300">as with prior work [82, §3.2], Giraffe can take advantage of this requirement using pipelining. In this arrangement,  <span class="math">\\mathcal{P}</span>  and  <span class="math">\\mathcal{V}</span>  comprise a number of submodules, all running in parallel and executing different instances of the proof protocol.</p>

    <p class="text-gray-300">To control area and throughput, Giraffe's  <span class="math">\\mathcal{P}</span>  and  <span class="math">\\mathcal{V}</span>  design templates each take several arguments. For  <span class="math">\\mathcal{V}</span> , the arguments are  <span class="math">n_{\\mathcal{V},\\mathrm{io}}</span> , the chip area dedicated to computing the multilinear extension of inputs and outputs; and  <span class="math">n_{\\mathcal{V},\\mathrm{sc}}</span> , the number of sum-check instances  <span class="math">\\mathcal{V}</span>  executes simultaneously. For  <span class="math">\\mathcal{P}</span> , the arguments are  <span class="math">n_{\\mathcal{P},\\mathrm{pl}}</span> , the number of in-flight computations in the pipeline;  <span class="math">n_{\\mathcal{P},\\mathrm{sc}}</span> , the number of sum-check instances  <span class="math">\\mathcal{P}</span>  executes simultaneously;  <span class="math">n_{\\mathcal{P},\\mathrm{ea}}</span> ,  <span class="math">\\mathcal{P}</span> 's parallelism in the early rounds of the sum-check (§3.2); and  <span class="math">n_{\\mathcal{P},\\tilde{\\mathcal{V}}}</span> ,  <span class="math">\\mathcal{P}</span> 's parallelism in the final  <span class="math">\\tilde{V}</span>  evaluation (Fig. 15, line 52).</p>

    <p class="text-gray-300">To measure the effect of Giraffe's protocol improvements (§3.1) versus T13, we compare the number of field multiplications that each requires when proving correct execution of a generic AC (§6.1). Here, we vary  <span class="math">G</span>  and fix  <span class="math">N = 2^{10}</span> ,  <span class="math">\\delta = 0.5</span> , and  <span class="math">d = 20</span> .</p>

    <p class="text-gray-300">Figure 17 shows the ratio of T13's cost to Giraffe's (1 is equal performance; higher is better). Giraffe's prover is a factor of  <span class="math">\\log G</span>  less expensive in terms of number of field multiplications than T13. We have confirmed that this result is similar when counting field additions and that it is insensitive to  <span class="math">N</span> ,  <span class="math">\\delta</span> , and  <span class="math">d</span> .</p>`;
---

<BaseLayout title="Full accounting for verifiable outsourcing (2017/242)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2017 &middot; eprint 2017/242
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <p class="mt-4 text-xs text-gray-500">
        All content below belongs to the original authors. This page
        reproduces the paper for educational purposes. Always
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >cite the original</a>.
      </p>
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <Fragment set:html={CONTENT} />

  </article>
</BaseLayout>
