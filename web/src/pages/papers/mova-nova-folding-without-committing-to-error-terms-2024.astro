---
import BaseLayout from '../../layouts/BaseLayout.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2024/1220';
const CRAWLER = 'mistral';
const CONVERTED_DATE = '2026-02-16';
const TITLE_HTML = 'Mova: Nova folding without committing to error terms';
const AUTHORS_HTML = 'Nikolaos Dimitriou, Albert Garreta, Ignacio Manzur, Ilia Vlasov';

const CONTENT = `    <p class="text-gray-300">Mova: Nova folding without committing to error terms</p>

    <p class="text-gray-300">Nikolaos Dimitriou^{1}, Albert Garreta^{1}, Ignacio Manzur^{1}, and Ilia Vlasov^{1}</p>

    <p class="text-gray-300">^{1}Nethermind Research, {nikolaos,albert,ignacio,ilia}@nethermind.io</p>

    <h6 id="sec-1" class="text-base font-medium mt-4">Abstract</h6>

    <p class="text-gray-300">We present Mova, a folding scheme for R1CS instances that does not require committing to error or cross terms, nor makes use of the sumcheck protocol. We compute concrete costs and provide benchmarks showing that, for reasonable parameter choices, Mova’s Prover is about 5 to 10 times faster than Nova’s Prover, and between 1.05 to 1.3 times faster than Hypernova’s Prover (applied to R1CS instances) – assuming the R1CS witness vector contains only small elements. Mova’s Verifier has a similar cost as Hypernova’s Verifier, but Mova has the advantage of having only 3 rounds of communication, while Hypernova has a logarithmic number of rounds.</p>

    <p class="text-gray-300">Mova, which is based on the Nova folding scheme, manages to avoid committing to Nova’s so-called error term <span class="math">\\mathbf{E}</span> and cross term <span class="math">\\mathbf{T}</span> by replacing said commitments with evaluations of the Multilinear Extension (MLE) of <span class="math">\\mathbf{E}</span> and <span class="math">\\mathbf{T}</span> at a random point sampled by the Verifier. A key observation used in Mova’s soundness proofs is that <span class="math">\\mathbf{E}</span> is implicitly committed by a commitment to the input-witness vector <span class="math">\\mathbf{Z}</span>, since <span class="math">\\mathbf{E}=(A\\cdot\\mathbf{Z})\\circ(B\\cdot\\mathbf{Z})-u(C\\cdot\\mathbf{Z})</span>.</p>

    <p class="text-gray-300">We also note that ProtoGalaxy <em>[x10]</em> can be specialised to an R1CS folding scheme with similar properties. Some of our further contributions are that 1) Mova is described with a language that sheds new insights into the topic of “Nova-style folding”; 2) we provide concrete costs, benchmarks, and optimisations for the Prover; 3) we describe how to fold two accumulated instances (which is important for applications in Proof Carrying Data); and 4) provide non-trivial knowledge soundness proofs in the context of multilinear polynomials.</p>

    <h6 id="sec-2" class="text-base font-medium mt-4">Contents</h6>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>1 Introduction</li>

      <li>1.1 Mova in a nutshell</li>

      <li>1.2 Mova compared to Nova and Hypernova</li>

      <li>2 Techniques</li>

      <li>2.1 The Mova folding scheme</li>

      <li>2.2 Knowledge soundness proof of Protocol 2</li>

      <li>3 Preliminaries</li>

      <li>3.1 Multilinear polynomials</li>

      <li>3.2 Commitment schemes</li>

      <li>3.3 Reductions of knowledge</li>

    </ul>

    <p class="text-gray-300">4 The Mova folding scheme</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>4.1 From <span class="math">\\mathbf{R_{R1CS}}</span> to <span class="math">\\mathbf{R_{acc}}</span></li>

      <li>4.2 Reduction to common evaluation point</li>

      <li>4.3 Folding two instances in <span class="math">\\mathbf{R_{acc}}</span> with the same evaluation point</li>

      <li>4.4 Putting everything together</li>

      <li>5 Performance</li>

      <li>5.1 Experimental evaluation</li>

      <li>5.2 Computing the concrete costs of Mova, Nova, and Hypernova</li>

      <li>6 An efficient algorithm for composing a multilinear polynomial with a line</li>

      <li>7 Deferred proofs</li>

      <li>7.1 Proof of Lemma 4.4</li>

      <li>8 Acknowledgements</li>

      <li>9 References</li>

    </ul>

    <h2 id="sec-3" class="text-2xl font-bold">1 Introduction</h2>

    <p class="text-gray-300">Folding schemes have been an active area of research in the last years <em>[x1, x2, BCL^{+}20, x11, x13, x12, x14, x15]</em>. Informally, these schemes can be described as interactive proofs in which a Prover and Verifier create a new instance-witness pair in a certain relation <span class="math">\\mathbf{R}_{2}</span> from two instance-witness pairs in relations <span class="math">\\mathbf{R}_{1},\\mathbf{R}_{2}</span>. The validity of the newly created instance-witness pair implies the validity of the original instance-witness pair, except with negligible probability. The idea is that if this combination process is less expensive than proving an individual instance-witness pair (in prover time, memory requirements, or proof size), one can save costs by reducing the task of proving many instance-witness pairs to proving a single pair. Initially, folding schemes were created with the intention of improving the construction of primitives like Incrementally Verifiable Computation (IVC) <em>[x20]</em> and Proof-Carrying-Data (PCD) <em>[x7]</em>.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Nova <em>[x11]</em> is a celebrated folding scheme due to Kothapalli, Setty, and Tzialla where <span class="math">\\mathbf{R}_{1}</span> is the R1CS relation, and <span class="math">\\mathbf{R}_{2}</span> is the committed relaxed R1CS relation. Nova features only one round of communication, a very lean Verifier with <span class="math">O(1)</span> complexity, and a relatively simple and efficient Prover. For the purposes of this paper, the main drawback of Nova is the need for the Prover to compute a commitment to a vector <span class="math">\\mathbf{T}</span> with arbitrarily large entries, i.e. a vector whose entries have roughly $\\log(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math"> bits, where </span>\\mathbb{F}<span class="math"> is the underlying finite field used in </span>\\mathbf{R}_{1}<span class="math"> and </span>\\mathbf{R}_{2}<span class="math">. Nova requires using a homomorphic commitment scheme, which in practical scenarios is typically chosen to be an elliptic curve-based scheme such as Pedersen or KZG. Then, to compute the commitment to </span>\\mathbf{T}$, the Prover ultimately has to perform a Multiscalar Multiplication (MSM) of a vector with large entries. This is a costly operation that dominates by far the rest of the Prover’s costs (assuming the R1CS matrices are reasonably sparse, and not counting the cost of committing to the R1CS witness – see below for a discussion on this). For reference, over the Pallas curve, Table 1 in <em>[x10]</em></th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">estimates the cost to be, roughly, $349</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{T}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> field operations when </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{T}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">=2^{16}<span class="math">, and </span>256</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{T}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> field operations when </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{T}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">=2^{20}$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Notice that, in all folding schemes discussed in this work, the witness in <span class="math">(\\mathbf{x},\\mathbf{w})\\in\\mathbf{R}_{1}</span> always needs to be committed. In many practical scenarios, the witness is a vector that contains small entries, i.e. much smaller than the size of the underlying field <span class="math">\\mathbb{F}</span>. In that case, as observed in <em>[x21]</em> and in our benchmarks in Table 7, an MSM for such a vector can easily be an order of magnitude cheaper than an MSM with a vector containing arbitrarily large entries. Hence, for such applications, removing or lowering the cost of committing to <span class="math">\\mathbf{T}</span> in Nova can lead to dramatic efficiency improvements of the system where Nova is being used. Otherwise, in an application where witnesses have arbitrarily sized entries, removing the commitment to <span class="math">\\mathbf{T}</span> in Nova has less of an impact (still significant though, cf. Tables 2 and 8), as already the overall system is paying for expensive commitments (cf. Section 5).</p>

    <p class="text-gray-300">Hypernova <em>[x13]</em> is a folding scheme due to Kothapalli and Setty where, unlike in Nova, the Prover does not commit to any vectors (within the folding scheme). However, Hypernova performs <span class="math">\\log(m)</span> rounds of communication, as opposed to Nova, which performs only 1. This difference is significant when using folding schemes to enable IVC or PCD. This is because in that scenario, at each folding step, besides running the folding protocol, the Prover also has to create a proof that the folding is done correctly. Then, each round of communication of the folding scheme translates into the need to create a proof of correct hashing (where the hash is used as a replacement of the Verifier’s randomness). Thus, having a large number of communication rounds can introduce significant overheads when using folding in IVC/PCD.</p>

    <p class="text-gray-300">We remark that Hypernova has the benefit of being usable also for folding instance-witness pairs from the CCS <em>[x21]</em> relation, which is a high degree generalisation of the R1CS relation. Nova cannot efficiently handle this relation when the degree <span class="math">d</span> is large, due to certain costs scaling by a quadratic factor on <span class="math">d</span>.</p>

    <p class="text-gray-300">Since their publication, many subsequent works have built upon Nova and Hypernova <em>[x1, NDC^{+}24, x20, x1, x2, x12, x11]</em>. We refer to <em>[x1]</em> for a vaired colletion of resources on the topic of folding.</p>

    <p class="text-gray-300">In this work we investigate the the following (loosely formulated) question:</p>

    <p class="text-gray-300">Does there exist efficient folding schemes for the R1CS relation where the Prover does not commit to any vector besides the commitment to the R1CS witness, and with <span class="math">O(1)</span> rounds of communication?</p>

    <p class="text-gray-300">We describe and benchmark a scheme that answers this question affirmatively, and which we call Mova. We also identify ProtoGalaxy as a scheme that can be specialised so as to solve the above question (see below).</p>

    <h4 id="sec-4" class="text-lg font-semibold mt-6">ProtoGalaxy</h4>

    <p class="text-gray-300">Upon completion of this work, the authors of <em>[x10]</em> brought to our attention that the ProtoGalaxy scheme can be specialised in a way that resolves the above</p>

    <p class="text-gray-300">question affirmatively as well. In fact, our Mova scheme can be seen as a variation of this specialisation, in which, among others, the multilinear Lagrange base is used, as opposed to the monomial base. To obtain such specialisation, one should apply ProtoGalaxy on the trivial special sound protocol for the R1CS relation where the Prover simply sends the witness to the Verifier, and the Verifier checks whether the witness is valid. Then, the <span class="math">\\mathsf{pow}_{i}(\\mathbf{X})</span> polynomials from <em>[x10]</em> play the role of our <span class="math">\\widehat{\\mathsf{eq}}(i;\\mathbf{X})</span> polynomials.</p>

    <p class="text-gray-300">In <em>[x10]</em>, ProtoGalaxy is presented with a very different language and with different goals than ours. Indeed, the scheme in <em>[x10]</em> is described as being a Protostar-based <em>[x3]</em> folding scheme for special sound protocols, featuring an efficient Verifier, and with capacity for folding multiple instances at once.</p>

    <p class="text-gray-300">We believe that our description and benchmarking of Mova both provides new insights into the topic of “Nova-style folding” (cf. Section 2), and highlights further attractive properties and applications of ProtoGalaxy. These insights are essentially that the commitment to the error term in Nova is redundant in many steps of the scheme, and that, whenever this commitment is needed, one can use alternative, less expensive, methods.</p>

    <p class="text-gray-300">We also: 1) provide specific Prover optimisations and cost counts for our use-case (cf. Section 5.2), resulting in a especially lean protocol for folding R1CS instances (cf. Section 5); 2) allow for efficient R1CS-based Proof Carrying Data (PCD) by describing how to fold two accumulated instances – this is necessary in PCD due to its inherent non-linear recursion topology (we believe that similar methods can be used to extend <em>[x10]</em>); and 3) provide non-trivial knowledge soundness proofs in the context of the multilinear Lagrange basis, whose principles may be applicable in other similar proofs (cf. Remark 4.3).</p>

    <h3 id="sec-5" class="text-xl font-semibold mt-8">1.1 Mova in a nutshell</h3>

    <p class="text-gray-300">In Mova, <span class="math">\\mathbf{R}_{1}=\\mathbf{R}_{\\mathsf{R1CS}}</span> is the R1CS relation (cf. Eq. (1)) and <span class="math">\\mathbf{R}_{2}=\\mathbf{R}_{\\mathsf{acc}}</span> is, essentially, the committed relaxed R1CS relation <span class="math">\\mathbf{R}_{\\mathsf{rcR1CS}}</span> of Nova (cf. Eq. (2)), except that the commitment to the error term <span class="math">\\mathbf{E}</span> is replaced by an evaluation <span class="math">v</span> of the MLE <span class="math">\\mathsf{mle}\\mathbf{E}</span> of <span class="math">\\mathbf{E}</span> at a point <span class="math">\\mathbf{r}</span>, i.e. the constraint <span class="math">\\mathsf{Com}(\\mathbf{E})=\\bar{\\mathbf{E}}</span> from Nova’s committed relaxed R1CS relation is replaced by the constraint <span class="math">\\mathsf{mle}\\mathbf{E}=v</span>. Here <span class="math">\\mathbf{r},v</span> are part of the public instance, see Eq. (3) for a full description of <span class="math">\\mathbf{R}_{\\mathsf{acc}}</span>.</p>

    <p class="text-gray-300">At a high level, the reason why Mova does not require commitments to the error term <span class="math">\\mathbf{E}</span> is because, intuitively speaking, <span class="math">\\mathbf{E}</span> is implicitly committed by the commitment to the instance-witness vector <span class="math">\\mathbf{Z}</span> (or to the underlying witness). This is because <span class="math">\\mathbf{E}</span> is uniquely determined by such vector, indeed: <span class="math">\\mathbf{E}=A\\cdot\\mathbf{Z}\\circ B\\cdot\\mathbf{Z}-uC\\cdot\\mathbf{Z}</span> (see Eq. (2) for the details). Hence, one can see that, when it comes to folding instance-witness pairs of the committed relaxed R1CS relation, it is actually possible to remove the commitments to <span class="math">\\mathbf{E}</span> altogether, without any further modification. This is explained in more detail in the first part of Section 2.1.</p>

    <p class="text-gray-300">However, this is not enough when aiming to fold instance-witness pairs for the R1CS relation <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}</span> with instance-witness pairs for the relaxed R1CS relation <span class="math">\\mathbf{R}_{\\mathsf{rcR1CS}}</span>. This is because the first instance-witness pair, say <span class="math">(\\mathtt{x},\\mathtt{w})</span>, needs to first be transformed into an instance-witness pair from <span class="math">\\mathbf{R}_{\\mathsf{rcR1CS}}</span>. In Nova, this is achieved by simply adding an error term <span class="math">\\mathbf{E}</span> which equals zero, and adding the constraint that <span class="math">\\mathsf{Com}(\\mathbf{E})</span> is a commitment to the zero vector. Since our goal is to not use commitments to <span class="math">\\mathbf{E}</span>, we instead have the Verifier send a</p>

    <p class="text-gray-300">random point  <span class="math">\\mathbf{r} \\in \\mathbb{F}^{\\log m}</span> , and add the constraint that  <span class="math">\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}</a> = 0</span> . This is the reason why our accumulated relation  <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span>  is precisely the committed relaxed R1CS relation  <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}}</span> , after removing all mentions of a commitment to  <span class="math">\\mathbf{E}</span> , and adding the constraint  <span class="math">\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}</a> = v</span>  for some public values  <span class="math">\\mathbf{r}, v</span> . Precisely (we use blue to highlight the differences with the standard committed relaxed R1CS relation),</p>

    <div class="my-4 text-center"><span class="math-block"></span></div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf {R} _ {\\mathrm {a c c}} = \\left\\{(x; w) = (\\mathbf {x}, \\overline {{\\mathbf {W}}}, u, \\ell_ {\\mathbf {E}}, \\mathbf {r}; \\mathbf {W}, \\mathbf {E}) \\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\begin{array}{l} (A \\cdot \\mathbf {Z}) \\circ (B \\cdot \\mathbf {Z}) = u \\cdot (C \\cdot \\mathbf {Z}) + \\mathbf {E}, \\\\ \\mathbf {Z} = (\\mathbf {W}, \\mathbf {x}, 1), \\\\ \\operatorname {C o m} (\\mathbf {W}) = \\overline {{\\mathbf {W}}}, \\operatorname {m l e} [ \\mathbf {E} ] (\\mathbf {r}) = \\ell_ {\\mathbf {E}}, \\\\ \\mathbf {x} \\in \\mathbb {F} ^ {\\ell}, \\mathbf {W} \\in \\mathbb {F} ^ {m - \\ell - 1}, \\mathbf {r} \\in \\mathbb {F} ^ {\\log n} \\end{array} \\right. \\right\\}.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="my-4 text-center"><span class="math-block"></span></div>

    <p class="text-gray-300">This aspect of Mova is explained in more detail in our technical overview (cf. Protocol 3), or in full formality in Section 4.1.</p>

    <p class="text-gray-300">We are almost done now. It remains to design a way of folding two instance-witness pairs from  <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span> , say  <span class="math">(\\mathbf{x}_1; \\mathbf{w}_1)</span>  and  <span class="math">(\\mathbf{x}_2; \\mathbf{w}_2)</span> . Say the MLE evaluation point in these instances is  <span class="math">\\mathbf{r}_1</span>  and  <span class="math">\\mathbf{r}_2</span> , respectively. It is relatively simple to see that, if  <span class="math">\\mathbf{r}_1 = \\mathbf{r}_2</span> , then Nova's scheme works as is, replacing any operation with the commitments to errors or cross terms with the evaluation of the MLE of these terms at  <span class="math">\\mathbf{r}_1 = \\mathbf{r}_2</span> . See Protocol 2 for further details, or Section 4.3.</p>

    <p class="text-gray-300">Finally, if  <span class="math">\\mathbf{r}_1 \\neq \\mathbf{r}_2</span>  (which, in general, will always be the case), we employ a reduction of knowledge whereby the two claims  <span class="math">\\mathsf{mle}<a href="\\mathbf{r}_1">\\mathbf{E}_1</a> = v_1</span>  and  <span class="math">\\mathsf{mle}<a href="\\mathbf{r}_2">\\mathbf{E}_2</a> = v_2</span>  are transformed into two claims of the form  <span class="math">\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}_1</a> = v_1&#x27;</span>  and  <span class="math">\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}_2</a> = v_2&#x27;</span>  for some new  <span class="math">\\mathbf{r}&#x27;, v_1&#x27;, v_2&#x27;</span> . At this point, we have reached the case above, where  <span class="math">\\mathbf{r}_1 = \\mathbf{r}_2</span> , and we can proceed accordingly. This reduction of knowledge is based on a technique used in GKR, in which the polynomials  <span class="math">\\mathsf{mle}[\\mathbf{E}_i]</span>  are composed with a line passing through the points  <span class="math">\\mathbf{r}_1</span>  and  <span class="math">\\mathbf{r}_2</span> . This is explained in more detail in Protocol 4 from our technical overview, and in full formality in Section 4.2.</p>

    <h2 id="sec-6" class="text-2xl font-bold">1.2 Mova compared to Nova and Hypernova</h2>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">In Table 1 we provide the concrete dominating costs of Mova, Nova, and Hypernova when used on the same R1CS structure (cf. Table 3 for more detailed costs). Later in the paper, in Table 4, we see that, for reasonable concrete parameter choices, Mova's Prover is roughly 5 to 10 times more efficient than Nova's Prover, and roughly 1.2 to 1.4 times more efficient than Hypernova's, assuming the entries in the R1CS witness vector  <span class="math">\\mathbf{W}</span>  are small, by which we mean they all belong to the range  $\\{0,\\dots ,</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{W}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">- 1\\}<span class="math"> . If the entries of  </span>\\mathbf{W}$  are arbitrarily large, then the improvement is of a factor between 1.8 to 2.8 in Nova, and of about 1.02 to 1.1 in Hypernova (cf. Table 4 for some details).</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">On the other hand, Mova's Verifier concrete cost is similar to Hypernova's, but Mova has only 3 rounds of communication, as opposed to Hypernova, which has  <span class="math">\\log(m)</span> .</p>

    <p class="text-gray-300">We implemented Mova and benchmarked its Prover against Nova and Hypernova's Prover, corroborating the above findings for the case where the R1CS matrices  <span class="math">A, B, C</span></p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">P</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">V</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Round(s)</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Nova [KST21]</td>

            <td class="px-3 py-2 border-b border-gray-700">3n + 5m F 2 G ops., 2 G exp. Com. vector of m elements in F Com. W</td>

            <td class="px-3 py-2 border-b border-gray-700">2ℓ F 2 G ops., 2 G exp.</td>

            <td class="px-3 py-2 border-b border-gray-700">1</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Hypernova [KS23b]</td>

            <td class="px-3 py-2 border-b border-gray-700">6n + 14m F 1 G op., 1 G exp. Com. W</td>

            <td class="px-3 py-2 border-b border-gray-700">2ℓ + O(log(m)) F 1 G op., 1 G exp.</td>

            <td class="px-3 py-2 border-b border-gray-700">log(m) + O(1)</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Mova (this work)</td>

            <td class="px-3 py-2 border-b border-gray-700">3n + 12m F 1 G op., 1 G exp. Com. W</td>

            <td class="px-3 py-2 border-b border-gray-700">2ℓ + 7 log(m) + 5 F 1 G op., 1 G exp.</td>

            <td class="px-3 py-2 border-b border-gray-700">3</td>

          </tr>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Table 1: Dominating concrete costs of Mova, Nova, and Hypernova. Here  <span class="math">m</span>  is the size of the vectors used in the various R1CS relations. In particular, the R1CS matrices  <span class="math">A, B, C</span>  are  <span class="math">m \\times m</span>  matrices.  <span class="math">n</span>  denotes the number of nonzero entries in each of these matrices.  <span class="math">\\ell</span>  is the size of the public input vector. The  <span class="math">\\mathbb{F}</span>  symbol indicates the number of field multiplications in column 1, and the number of field additions and multiplications in column  <span class="math">2^3</span> . The  <span class="math">\\mathbb{G}</span>  symbol indicates the number of group operations or exponentiations. We write "Com.  <span class="math">a</span>  elements in  <span class="math">\\mathbb{F}</span> " to mean that a commitment to  <span class="math">a</span>  field elements of bit-size roughly  $\\log(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math">  must be made. &quot;Com.  </span>\\mathbf{W}<span class="math"> &quot; means that the witness vector  </span>\\mathbf{W}$  of the R1CS relation has to be committed.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">are the identity matrix. We remark that the performance improvement is expected to be less steep when  <span class="math">A, B, C</span>  are more complicated matrices (cf. Table 4 and Remark 5.1). Running benchmarks for more general matrices is currently work-in-progress of ours. Further work-in-progress is to implement the optimised method for computing the cross-term  <span class="math">\\mathbf{T}</span> , described in Section 5.2.</p>

    <p class="text-gray-300">A simplified report of the results is provided in Table 2. Further details can be found in Section 5.1. We have made the code publicly available <span class="math">^4</span> .</p>

    <p class="text-gray-300">In this section we provide a technical overview of how and why Mova works.</p>

    <p class="text-gray-300">Fix a finite field  <span class="math">\\mathbb{F}</span>  and three  <span class="math">m\\times m</span>  matrices  <span class="math">A,B,C</span>  with  <span class="math">n = \\Omega (m)</span>  nonzero entries in  <span class="math">\\mathbb{F}</span> . Let  <span class="math">\\mathsf{Com}</span>  be an additively homomorphic vector commitment scheme. Define the R1CS relation as</p>

    <div class="my-4 text-center"><span class="math-block"></span></div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf {R} _ {\\mathrm {c R 1 C S}} = \\left\\{\\left(\\mathbf {x}; \\mathbf {w}\\right) = \\left(\\mathbf {x}; \\mathbf {W}\\right) \\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\begin{array}{l} (A \\cdot \\mathbf {Z}) \\circ (B \\cdot \\mathbf {Z}) = (C \\cdot \\mathbf {Z}), \\\\ \\mathbf {Z} = (\\mathbf {W}, \\mathbf {x}, 1), \\\\ \\mathbf {x} \\in \\mathbb {F} ^ {\\ell}, \\mathbf {W} \\in \\mathbb {F} ^ {m - \\ell - 1} \\end{array} \\right. \\right\\}, \\tag {1}</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="my-4 text-center"><span class="math-block"></span></div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">m</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">W entries</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Mova</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Nova</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Hypernova</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">216</td>

            <td class="px-3 py-2 border-b border-gray-700">∅</td>

            <td class="px-3 py-2 border-b border-gray-700">36.0216 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">400.4406 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">150.0280 ms</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">216</td>

            <td class="px-3 py-2 border-b border-gray-700">Small</td>

            <td class="px-3 py-2 border-b border-gray-700">64.5093 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">428.9283 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">178.5157 ms</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">216</td>

            <td class="px-3 py-2 border-b border-gray-700">Large</td>

            <td class="px-3 py-2 border-b border-gray-700">411.6056 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">776.0246 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">525.6120 ms</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">220</td>

            <td class="px-3 py-2 border-b border-gray-700">∅</td>

            <td class="px-3 py-2 border-b border-gray-700">761.5190 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">5470.6681 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">3195.2789 ms</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">220</td>

            <td class="px-3 py-2 border-b border-gray-700">Small</td>

            <td class="px-3 py-2 border-b border-gray-700">1316.0204 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">6025.1695 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">3749.7803 ms</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">220</td>

            <td class="px-3 py-2 border-b border-gray-700">Large</td>

            <td class="px-3 py-2 border-b border-gray-700">5675.1007 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">10384.2498 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">8108.8606 ms</td>

          </tr>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Table 2: Benchmarks of Mova, Nova, and Hypernova's Prover runtime on a single core, when the R1CS matrices are the identity matrix, using an unoptimised way of computing the cross term  <span class="math">\\mathbf{T}</span>  (see Section 5.2 for the optimised method). As in Table 1,  <span class="math">m</span>  denotes the size of the R1CS vectors. The column  <span class="math">\\mathbf{W}</span>  indicates whether the entries in the R1CS witness are "small" (in the range  $\\{0,\\dots ,</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{W}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">- 1\\}<span class="math"> ) or &quot;large&quot; (sampled randomly in  </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> ). The  </span>\\emptyset<span class="math">  symbol indicates that the time to commit to  </span>\\mathbf{W}$  was not included. See Section 5.1 for further details. Hypernova's implementation does not use certain known optimizations and thus its runtime does not correspond to the concrete costs from Table 1, cf. Remark 5.2.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">where  <span class="math">\\ell \\geq 0</span>  is fixed,  <span class="math">\\circ</span>  is the Hadamard product, and  <span class="math">A\\cdot \\mathbf{Z}</span>  denotes matrix-vector multiplication (and similarly for  <span class="math">B\\cdot \\mathbf{Z},C\\cdot \\mathbf{Z}</span> ). For the purposes of this overview, we omit several formalities; for example we fix the public parameters  <span class="math">\\mathbb{F},A,B,C,n,m,\\ell</span>  and omit referring to them as such. We also omit referring to the parameters and keys used in  <span class="math">\\mathsf{Com}</span> . We refer to the beginning of Section 3 for an explanation of the basic notation used in this section and throughout the paper.</p>

    <p class="text-gray-300">In the Nova paper [KST21], Kothapalli, Setty and Tzialla introduce a new relation called relaxed committed R1CS relation. This is defined as</p>

    <div class="my-4 text-center"><span class="math-block"></span></div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf {R} _ {\\mathrm {r c R 1 C S}} = \\left\\{\\left(\\mathbf {x}; \\mathbf {w}\\right) = \\left(\\mathbf {x}, \\bar {\\mathbf {W}}, u, \\bar {\\mathbf {E}}; \\mathbf {W}, \\mathbf {E}\\right) \\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\begin{array}{l} (A \\cdot \\mathbf {Z}) \\circ (B \\cdot \\mathbf {Z}) = u \\cdot (C \\cdot \\mathbf {Z}) + \\mathbf {E}, \\\\ \\mathbf {Z} = (\\mathbf {W}, \\mathbf {x}, 1), \\\\ \\operatorname {C o m} (\\mathbf {W}) = \\bar {\\mathbf {W}}, \\operatorname {C o m} (\\mathbf {E}) = \\bar {\\mathbf {E}}, \\\\ \\mathbf {x} \\in \\mathbb {F} ^ {\\ell}, \\mathbf {W} \\in \\mathbb {F} ^ {m - \\ell - 1} \\end{array} \\right. \\right\\}, \\tag {2}</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="my-4 text-center"><span class="math-block"></span></div>

    <p class="text-gray-300">The Nova authors go on to describe a reduction of knowledge [KP23] (i.e. a folding scheme) from  <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}} \\times \\mathbf{R}_{\\mathrm{rcR1CS}}</span>  to  <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}}</span> . We reproduce the protocol in Protocol 1. Since it is required to understand why our scheme Mova is sound, in Section 2.2 we provide a quick overview of its knowledge soundness proof.</p>

    <p class="text-gray-300">Suppose, for illustrative purposes, that in  <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}}</span>  we use two commitments schemes,  <span class="math">\\mathsf{Com}_1</span>  and  <span class="math">\\mathsf{Com}_2</span> , so that  <span class="math">\\mathbf{W}</span>  is committed with  <span class="math">\\mathsf{Com}_1</span>  and  <span class="math">\\mathbf{E}</span>  is committed with  <span class="math">\\mathsf{Com}_2</span> . Then suppose we modify Protocol 1 so that  <span class="math">\\mathsf{P}</span>  uses  <span class="math">\\mathsf{Com}_2</span>  to commit to  <span class="math">\\mathbf{T}</span> . It is clear that the resulting scheme works in the same way as the original one.</p>

    <p class="text-gray-300">Our first observation is that, perhaps surprisingly, one does not need  <span class="math">\\mathsf{Com}_2</span>  to be binding for Protocol 1 to be knowledge sound. This is readily apparent from our overview in Section 2.2 of the knowledge soundness proof of Protocol 1. Indeed, the only property used from  <span class="math">\\mathsf{Com}_2</span>  is its additivity. With this observation in mind, it is not difficult to see that one can replace  <span class="math">\\mathsf{Com}_2</span>  in  <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}}</span>  and in Protocol 1 by any linear map  <span class="math">\\mathcal{L}</span> , and still obtain a complete, publicly reducible, and knowledge sound folding scheme. Looking ahead, for us  <span class="math">\\mathcal{L}</span></p>

    <p class="text-gray-300">Protocol 1 Nova [KST21] folding scheme  <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}} \\times \\mathbf{R}_{\\mathrm{rcR1CS}} \\rightarrow \\mathbf{R}_{\\mathrm{rcR1CS}}</span></p>

    <p class="text-gray-300">Input: Let  <span class="math">(\\mathbf{x}_i;\\mathbf{w}_i) = (\\mathbf{x}_i,\\overline{\\mathbf{W}}_i,u_i,\\overline{\\mathbf{E}}_i;\\mathbf{W}_i,\\mathbf{E}_i)\\in \\mathbf{R}_{\\mathrm{rcR1CS}}</span> <span class="math">(i = 1,2)</span></p>

    <p class="text-gray-300"><span class="math">\\mathsf{P}</span>  receives  <span class="math">(\\mathbf{x}_i;\\mathbf{w}_i)</span>  as input  <span class="math">(i = 1,2)</span> .</p>

    <p class="text-gray-300">V receives  <span class="math">\\mathbf{x}_i</span>  as input  <span class="math">(i = 1,2)</span></p>

    <p class="text-gray-300">1: First,  <span class="math">\\mathsf{P}</span>  computes  <span class="math">\\mathbf{T} = (A\\cdot \\mathbf{Z}_1)\\circ (B\\cdot \\mathbf{Z}_2) + (A\\cdot \\mathbf{Z}_2)\\circ (B\\cdot \\mathbf{Z}_1) - u_1\\cdot (C\\cdot \\mathbf{Z}_2) - u_2\\cdot (C\\cdot \\mathbf{Z}_1)</span> , and sends  <span class="math">\\mathsf{V}</span>  a commitment  <span class="math">\\bar{\\mathbf{T}}</span>  to  <span class="math">\\mathbf{T}</span> . 2: V replies with a uniformly sampled challenge  <span class="math">\\alpha \\gets \\mathbb{F}</span> 3:  <span class="math">\\mathsf{P}</span>  and  <span class="math">\\mathsf{V}</span>  both output  <span class="math">\\mathbf{x} = (\\mathbf{x},\\overline{\\mathbf{W}},u,\\bar{\\mathbf{E}})</span>  where  <span class="math">\\mathbf{x} = \\mathbf{x}_1 + \\alpha \\mathbf{x}_2</span> <span class="math">\\overline{\\mathbf{W}} = \\overline{\\mathbf{W}}_1 + \\alpha \\overline{\\mathbf{W}}_2</span> <span class="math">u = u_{1} + \\alpha u_{2}</span> , and  <span class="math">\\bar{\\mathbf{E}} = \\bar{\\mathbf{E}}_1 + \\alpha \\bar{\\mathbf{T}} +\\alpha^2\\bar{\\mathbf{E}}_2</span> 4: Additionally,  <span class="math">\\mathsf{P}</span>  outputs  <span class="math">\\mathbf{w} = (\\mathbf{W},\\mathbf{E})</span>  , where  <span class="math">\\mathbf{W} = \\mathbf{W}_1 + \\alpha \\mathbf{W}_2</span>  and  <span class="math">\\mathbf{E} = \\mathbf{E}_1 + \\alpha \\mathbf{T} + \\alpha^2\\mathbf{E}_2</span></p>

    <p class="text-gray-300">will be the map that assigns, to a vector  <span class="math">\\mathbf{E}</span> , the evaluation of its Multilinear Extension (MLE) at a fixed point  <span class="math">\\mathbf{r}</span>  (with  <span class="math">\\mathbf{r}</span>  being the same each time  <span class="math">\\mathcal{L}</span>  is applied), i.e.  <span class="math">\\mathcal{L}(\\mathbf{E}) = \\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}</a></span> .</p>

    <p class="text-gray-300">More precisely, let  <span class="math">\\mathcal{L}:\\mathbb{F}^n\\to \\mathbb{G}</span>  be an additive homomorphism from  <span class="math">\\mathbb{F}^n</span>  to some group  <span class="math">\\mathbb{G}</span> . Define:</p>

    <div class="my-4 text-center"><span class="math-block"></span></div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf {R} _ {\\mathrm {r c R 1 C S}} ^ {\\mathcal {L}} = \\left\\{(x; w) = (x, \\overline {{W}}, u, \\ell_ {E}; W, E) \\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\begin{array}{l} (A \\cdot \\mathbf {Z}) \\circ (B \\cdot \\mathbf {Z}) = u \\cdot (C \\cdot \\mathbf {Z}) + E, \\\\ \\mathbf {Z} = (\\mathbf {W}, x, 1), \\\\ \\operatorname {C o m} (\\mathbf {W}) = \\overline {{\\mathbf {W}}}, \\mathcal {L} (\\mathbf {E}) = \\ell_ {E}, \\\\ x \\in \\mathbb {F} ^ {\\ell}, W \\in \\mathbb {F} ^ {m - \\ell - 1} \\end{array} \\right. \\right\\}.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="my-4 text-center"><span class="math-block"></span></div>

    <p class="text-gray-300">Then the same proof as in Section 2.2 (i.e. the knowledge soundness proof of Nova) can be adapted to show that Protocol 2 is a reduction of knowledge from  <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}}^{\\mathcal{L}} \\times \\mathbf{R}_{\\mathrm{rcR1CS}}^{\\mathcal{L}}</span>  to  <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}}</span> . We use the color blue to highlight the differences between  <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}}</span>  and  <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}}^{\\mathcal{L}}</span> , and between Protocol 1 and Protocol 2.</p>

    <p class="text-gray-300">Protocol 2 Mova folding scheme  <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}}^{\\mathcal{L}} \\times \\mathbf{R}_{\\mathrm{rcR1CS}}^{\\mathcal{L}} \\rightarrow \\mathbf{R}_{\\mathrm{rcR1CS}}^{\\mathcal{L}}</span></p>

    <p class="text-gray-300">Input: Let  <span class="math">(\\mathbf{x}_i;\\mathbf{w}_i) = (\\mathbf{x}_i,\\overline{\\mathbf{W}}_i,u_i,\\mathcal{L}(\\mathbf{E}_i);\\mathbf{W}_i,\\mathbf{E}_i)\\in \\mathbf{R}_{\\mathrm{rcR1CS}}^{\\mathcal{L}}(i = 1,2).</span></p>

    <p class="text-gray-300"><span class="math">\\mathsf{P}</span>  receives  <span class="math">(\\mathbf{x}_i;\\mathbf{w}_i)</span>  as input  <span class="math">(i = 1,2)</span> .</p>

    <p class="text-gray-300">V receives  <span class="math">\\mathbf{x}_i</span>  as input  <span class="math">(i = 1,2)</span></p>

    <p class="text-gray-300">1: First,  <span class="math">\\mathsf{P}</span>  computes  <span class="math">\\mathbf{T} = (A\\cdot \\mathbf{Z}_1)\\circ (B\\cdot \\mathbf{Z}_2) + (A\\cdot \\mathbf{Z}_2)\\circ (B\\cdot \\mathbf{Z}_1) - u_1\\cdot (C\\cdot \\mathbf{Z}_2) - u_2\\cdot (C\\cdot \\mathbf{Z}_1)</span> , and sends  <span class="math">\\mathcal{L}(T)</span>  to  <span class="math">\\mathsf{V}</span> . 2: V replies with a uniformly sampled challenge  <span class="math">\\alpha \\gets \\mathbb{F}</span> 3:  <span class="math">\\mathsf{P}</span>  and  <span class="math">\\mathsf{V}</span>  both output  <span class="math">\\mathbf{x} = (\\mathbf{x},\\overline{\\mathbf{W}},u,\\mathcal{L}(\\mathbf{E}))</span>  where  <span class="math">\\mathbf{x} = \\mathbf{x}_1 + \\alpha \\mathbf{x}_2</span> ,  <span class="math">\\overline{\\mathbf{W}} = \\overline{\\mathbf{W}}_1 + \\alpha \\overline{\\mathbf{W}}_2</span> ,  <span class="math">u = u_{1} + \\alpha u_{2}</span> , and  <span class="math">\\mathcal{L}(\\mathbf{E}) = \\mathcal{L}(\\mathbf{E}_1) + \\alpha \\mathcal{L}(\\mathbf{T}) + \\alpha^2\\mathcal{L}(\\mathbf{E}_2)</span> . 4: Additionally,  <span class="math">\\mathsf{P}</span>  outputs  <span class="math">\\mathbf{w} = (\\mathbf{W},\\mathbf{E})</span>  , where  <span class="math">\\mathbf{W} = \\mathbf{W}_1 + \\alpha W_2</span>  and  <span class="math">\\mathbf{E} = \\mathbf{E}_1 + \\alpha T + \\alpha^2\\mathbf{E}_2</span></p>

    <p class="text-gray-300">In fact, when looking only at folding schemes for  <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}} \\times \\mathbf{R}_{\\mathrm{rcR1CS}} \\to \\mathbf{R}_{\\mathrm{rcR1CS}}</span> , it is even possible to forget altogether about the commitments to  <span class="math">\\mathbf{E}</span>  and the linear maps  <span class="math">\\mathcal{L}</span> . Another way of seeing this is that, when Protocol 2 is instantiated with  <span class="math">\\mathcal{L}: \\mathbb{F}^m \\to \\{0\\}</span>  being a degenerate map that maps all vectors to 0, then Protocol 2 is still a reduction of knowledge from  <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}}^{\\mathcal{L}} \\times \\mathbf{R}_{\\mathrm{rcR1CS}}^{\\mathcal{L}}</span>  to  <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}}^{\\mathcal{L}}</span> . We remark, however, that for such degenerate maps, the relation  <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}}^{\\mathcal{L}}</span>  is equally degenerate. Indeed, for any  <span class="math">\\mathbf{x}</span> ,  <span class="math">\\overline{\\mathbf{W}} = \\operatorname{Com}(\\mathbf{W})</span> ,  <span class="math">u, \\mathbf{W}</span> , there exists  <span class="math">\\mathbf{E}</span>  such that  <span class="math">(\\mathbf{x}; \\mathbf{w}) = (\\mathbf{x}, \\overline{\\mathbf{W}}, u, 0; \\mathbf{W}, \\mathbf{E}) \\in \\mathbf{R}_{\\mathrm{rcR1CS}}^{\\mathcal{L}}</span> . It suffices to take  <span class="math">\\mathbf{E} = (A \\cdot \\mathbf{Z}) \\circ (B \\cdot \\mathbf{Z}) - u(C \\cdot \\mathbf{Z})</span> , where  <span class="math">\\mathbf{Z} = (\\mathbf{W}, \\mathbf{x}, 1)</span> .</p>

    <p class="text-gray-300">The situation is more involved if one wishes to fold an instance-witness pair from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}</span> with an instance-witness pair from <span class="math">\\mathbf{R}_{\\mathsf{rcR1CS}}</span>, i.e. if one wants to build a reduction of knowledge from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}\\times\\mathbf{R}_{\\mathsf{rcR1CS}}</span> to <span class="math">\\mathbf{R}_{\\mathsf{rcR1CS}}</span>. One can easily construct one as follows: first, design a reduction of knowledge <span class="math">\\Pi_{1}</span> from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}</span> to <span class="math">\\mathbf{R}_{\\mathsf{rcR1CS}}</span>:</p>

    <p class="text-gray-300"><span class="math">\\Pi_{1}:\\mathbf{R}_{\\mathsf{R1CS}}\\to\\mathbf{R}_{\\mathsf{rcR1CS}}.</span></p>

    <p class="text-gray-300">Then, build a reduction of knowledge <span class="math">\\Pi_{2}</span> from <span class="math">\\mathbf{R}_{\\mathsf{rcR1CS}}\\times\\mathbf{R}_{\\mathsf{rcR1CS}}</span> to <span class="math">\\mathbf{R}_{\\mathsf{rcR1CS}}</span>:</p>

    <p class="text-gray-300"><span class="math">\\Pi_{2}:\\mathbf{R}_{\\mathsf{rcR1CS}}\\times\\mathbf{R}_{\\mathsf{rcR1CS}}\\to\\mathbf{R}_{\\mathsf{rcR1CS}}.</span></p>

    <p class="text-gray-300">Once this is done, use the parallel composition theorem from <em>[x10]</em> (cf. Theorem 3.4) to obtain a reduction of knowledge <span class="math">\\widetilde{\\Pi}_{1}</span> from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}\\times\\mathbf{R}_{\\mathsf{rcR1CS}}</span> to <span class="math">\\mathbf{R}_{\\mathsf{rcR1CS}}\\times\\mathbf{R}_{\\mathsf{rcR1CS}}</span> (here we compose in parallel <span class="math">\\Pi_{1}</span> and the trivial reduction of knowledge from <span class="math">\\mathbf{R}_{\\mathsf{rcR1CS}}</span> to <span class="math">\\mathbf{R}_{\\mathsf{rcR1CS}}</span> where the instance-witness pair from <span class="math">\\mathbf{R}_{\\mathsf{rcR1CS}}</span> remains unchanged). Then, use the sequential composition theorem from <em>[x10]</em> (cf. Theorem 3.3) to obtain the desired reduction of knowledge (here we compose <span class="math">\\widetilde{\\Pi}_{1}</span> with <span class="math">\\Pi_{2}</span>).</p>

    <p class="text-gray-300">We next describe a simple way to obtain a reduction of knowledge <span class="math">\\Pi_{1}</span> from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}</span> to <span class="math">\\mathbf{R}_{\\mathsf{rcR1CS}}</span>. This construction is implicit in the Nova paper <em>[x11]</em>. Let <span class="math">(\\mathbf{x};\\mathbf{w})=(\\mathbf{x};\\mathbf{W})\\in\\mathbf{R}_{\\mathsf{cR1CS}}</span>. Then <span class="math">\\mathsf{P}</span> sends a commitment <span class="math">\\bar{\\mathbf{W}}</span> to <span class="math">\\mathbf{W}</span>, and <span class="math">\\mathsf{P}</span> and <span class="math">\\mathsf{V}</span> output <span class="math">\\mathbf{x}^{\\prime}=(\\mathbf{x},\\bar{\\mathbf{W}},u,\\bar{\\mathbf{E}})</span>. <span class="math">\\mathsf{P}</span> additionally outputs <span class="math">\\mathbf{w}^{\\prime}=(\\mathbf{W},\\mathbf{E})</span>, where <span class="math">u=1</span> and <span class="math">\\mathbf{E}=\\mathbf{0}</span>. Here, we assume <span class="math">\\mathsf{V}</span> has precomputed <span class="math">\\mathsf{Com}_{2}(\\mathbf{0})</span> (with a fixed commitment randomness that is reused every time this reduction of knowledge is applied).</p>

    <p class="text-gray-300">Thus, even though we concluded above that the commitments to <span class="math">\\mathbf{E}</span> are not necessary for the Nova folding scheme, one does in principle need to include them when reducing from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}</span> to <span class="math">\\mathbf{R}_{\\mathsf{rcR1CS}}</span>, and afterwards carry them over to the reduction <span class="math">\\mathbf{R}_{\\mathsf{rcR1CS}}\\times\\mathbf{R}_{\\mathsf{rcR1CS}}\\to\\mathbf{R}_{\\mathsf{rcR1CS}}</span> (Protocol 1).</p>

    <p class="text-gray-300">Our second main observation is the following: assume one has designed a reduction of knowledge from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}</span> to <span class="math">\\mathbf{R}^{\\mathcal{L}}_{\\mathsf{rcR1CS}}</span>, for some efficient linear function <span class="math">\\mathcal{L}</span>, in particular, for <span class="math">\\mathcal{L}</span> not requiring committing to the error vector. Then, following the same strategy as above, we obtain a reduction of knowledge from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}\\times\\mathbf{R}^{\\mathcal{L}}_{\\mathsf{rcR1CS}}</span> to <span class="math">\\mathbf{R}^{\\mathcal{L}}_{\\mathsf{rcR1CS}}</span>. In particular, the resulting scheme does not require committing to the error vector <span class="math">\\mathbf{E}</span>, instead, it requires computing <span class="math">\\mathcal{L}(\\mathbf{E})</span>. Again, looking ahead, and informally speaking, we will take <span class="math">\\mathcal{L}</span> to be the evaluation of <span class="math">\\mathsf{mle}[\\mathbf{E}]</span> at a random point sampled by the Verifier.</p>

    <p class="text-gray-300">As we will see next, our final Mova folding scheme almost follows the blueprint above. The difference is that, instead of using a reduction from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}</span> to <span class="math">\\mathbf{R}^{\\mathcal{L}}_{\\mathsf{rcR1CS}}</span> for a suitable map <span class="math">\\mathcal{L}</span>, we describe a reduction from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}</span> to a union of relations of the form <span class="math">\\mathbf{R}^{\\mathcal{L}}_{\\mathsf{rcR1CS}}</span>, for different maps <span class="math">\\mathcal{L}</span>.</p>

    <p class="text-gray-300">More precisely, define</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\[ \\mathbf{R}_{\\mathsf{acc}}=\\left\\{(\\mathbf{x};\\mathbf{w})=(\\mathbf{x},\\bar{\\mathbf{W}},u,\\ell_{\\mathbf{E}},\\mathbf{r};\\mathbf{W},\\mathbf{E})\\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\begin{array}[]{l}(A\\cdot\\mathbf{Z})\\circ(B\\cdot\\mathbf{Z})=u\\cdot(C\\cdot\\mathbf{Z})+\\mathbf{E},\\\\</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">\\mathbf{Z}=(\\mathbf{W},\\mathbf{x},1),\\\\ \\mathsf{Com}(\\mathbf{W})=\\bar{\\mathbf{W}},\\ \\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}</a>=\\ell_{\\mathbf{E}},\\\\ \\mathbf{x}\\in\\mathbb{F}^{\\ell},\\mathbf{W}\\in\\mathbb{F}^{m-\\ell-1},\\mathbf{r}\\in\\mathbb{F}^{\\log n}\\end{array}\\right.\\right\\}.\\end{array} \\] (3)</p>

    <p class="text-gray-300">The</p>

    <p class="text-gray-300">Notice that, for a fixed <span class="math">\\mathbf{r} \\in \\mathbb{F}^{\\log n}</span>, this relation becomes <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}}^{\\mathcal{L}_{\\mathbf{r}}}</span>, where <span class="math">\\mathcal{L}_r</span> maps vectors <span class="math">\\mathbf{E}</span> from <span class="math">\\mathbb{F}^n</span> to the value <span class="math">\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}</a></span>, i.e. <span class="math">\\mathcal{L}_{\\mathbf{r}}(\\mathbf{E}) = \\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}</a></span>. Indeed,</p>

    <div class="my-4 text-center"><span class="math-block">\\mathbf {R} _ {\\mathrm {a c c}} = \\bigcup_ {\\mathbf {r} \\in \\mathbb {F} ^ {\\log n}} \\mathbf {R} _ {\\mathrm {r c R 1 C S}} ^ {\\mathcal {L} _ {\\mathbf {r}}}.</span></div>

    <p class="text-gray-300">In Protocol 3 we describe our reduction of knowledge from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span>. The main idea is that the constraint <span class="math">\\mathsf{Com}_2(\\mathbf{E}) = \\mathsf{Com}_2(\\mathbf{0})</span> in Nova's reduction of knowledge, is replaced by the constraint <span class="math">\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}</a> = 0</span> for a point <span class="math">\\mathbf{r} \\in \\mathbb{F}^{\\log n}</span> randomly sampled by <span class="math">\\mathsf{V}</span>. Loosely speaking, as we see later in the paper, by the Schwartz-Zippel lemma, this forces <span class="math">\\mathbf{E}</span> to be the zero vector with high probability, and allows the extractor to obtain a satisfying witness for the initial instance from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}</span>. We remark that the knowledge soundness proof of this reduction of knowledge is not trivial (cf. Lemma 4.1 and Remark 4.3). One of the main conceptual points in the proof is that the vector <span class="math">\\mathbf{E}</span> is implicitly committed by the commitment to the witness <span class="math">\\mathbf{W}</span>, because <span class="math">\\mathbf{E} = (A \\cdot \\mathbf{Z}) \\circ (B \\cdot \\mathbf{Z}) - u(C \\cdot \\mathbf{Z})</span>. This comes up in a few other spots throughout our proofs.</p>

    <p class="text-gray-300"><strong>Protocol 3</strong> Mova's reduction of knowledge <span class="math">\\mathbf{R}_{\\mathsf{R1CS}} \\to \\mathbf{R}_{\\mathrm{acc}}</span></p>

    <p class="text-gray-300"><strong>Input:</strong> Let <span class="math">(\\mathbf{x};\\mathbf{w}) = (\\mathbf{x};\\mathbf{W}) \\in \\mathbf{R}_{\\mathsf{R1CS}}</span>. <span class="math">\\mathsf{P}</span> receives <span class="math">(\\mathbf{x};\\mathbf{w})</span> as input. <span class="math">\\mathsf{V}</span> receives <span class="math">\\mathbf{x}</span>.</p>

    <p class="text-gray-300">1: <span class="math">\\mathsf{P}</span> computes a commitment <span class="math">\\overline{\\mathbf{W}} = \\mathsf{Com}(\\mathbf{W})</span> to <span class="math">\\mathbf{W}</span>, and sends <span class="math">\\overline{\\mathbf{W}}</span> to <span class="math">\\mathsf{V}</span>. 2: <span class="math">\\mathsf{V}</span> samples <span class="math">\\mathbf{r} \\gets \\mathbb{F}^{\\log n}</span>. 3: <span class="math">\\mathsf{P}</span> and <span class="math">\\mathsf{V}</span> both output <span class="math">\\mathbf{x}&#x27; = (\\mathbf{x}, \\overline{\\mathbf{W}}, u, \\ell_{\\mathbf{E}}, \\mathbf{r})</span> where <span class="math">u = 1</span> and <span class="math">\\ell_{\\mathbf{E}} = 0</span>. 4: Additionally, <span class="math">\\mathsf{P}</span> outputs <span class="math">\\mathbf{w}&#x27; = (\\mathbf{W}, \\mathbf{E})</span>, where <span class="math">\\mathbf{E} = \\mathbf{0}</span>.</p>

    <p class="text-gray-300">Are we done? Not yet, since we have not described a reduction from <span class="math">\\mathbf{R}_{\\mathrm{CR1CS}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}}^{\\mathcal{L}}</span> for a suitable <span class="math">\\mathcal{L}</span>, and thus we cannot use our reduction from <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}}^{\\mathcal{L}} \\times \\mathbf{R}_{\\mathrm{rcR1CS}}^{\\mathcal{L}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}}^{\\mathcal{L}}</span> from Protocol 2. We need to build instead a reduction of knowledge from <span class="math">\\mathbf{R}_{\\mathrm{acc}} \\times \\mathbf{R}_{\\mathrm{acc}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span>. We do this in two steps:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>First, we describe a reduction of knowledge from <span class="math">\\mathbf{R}_{\\mathrm{acc}} \\times \\mathbf{R}_{\\mathrm{acc}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{equal}}</span>, where <span class="math">\\mathbf{R}_{\\mathrm{equal}}</span> consists of pairs <span class="math">(\\mathbf{x}_1, \\mathbf{w}_1), (\\mathbf{x}_2, \\mathbf{w}_2)</span> from <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span> with the same evaluation point <span class="math">\\mathbf{r}</span> both in <span class="math">\\mathbf{x}_1</span> and <span class="math">\\mathbf{x}_2</span>. Precisely,</li>

    </ul>

    <div class="my-4 text-center"><span class="math-block">\\mathbf {R} _ {\\text {e q u a l}} = \\left\\{\\left(\\mathbf {x} _ {1}, \\mathbf {x} _ {2}; \\mathbf {w} _ {1}, \\mathbf {w} _ {2}\\right) \\mid \\text {E x i s t s} \\mathbf {r} \\in \\mathbb {F} ^ {\\log n} \\text {s u c h t h a t} \\left(\\mathbf {x} _ {i}; \\mathbf {w} _ {i}\\right) \\in \\mathbf {R} _ {\\mathrm {r c R 1 C S}} ^ {\\mathcal {L} _ {\\mathbf {r}}}, i = 1, 2 \\right\\}</span></div>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Observe that an instance-witness pair from <span class="math">\\mathbf{R}_{\\mathrm{equal}}</span> consists of two instance-witness pairs each from <span class="math">\\mathbf{R}_{\\mathrm{rcR1CS}}^{\\mathcal{L}_{\\mathbf{r}}}</span> for some <span class="math">\\mathbf{r} \\in \\mathbb{F}^{\\log n}</span>. Hence, here we can use our reduction from <span class="math">\\mathbf{R}_{\\mathrm{acc}}^{\\mathcal{L}_{\\mathbf{r}}} \\times \\mathbf{R}_{\\mathrm{acc}}^{\\mathcal{L}_{\\mathbf{r}}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{acc}}^{\\mathcal{L}_{\\mathbf{r}}}</span> in Protocol 2.</li>

    </ul>

    <p class="text-gray-300">Once this is done, it suffices to use the sequential and parallel composition theorems from [KP23] (cf. Theorems 3.3 and 3.4) so as to obtain our desired reduction from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}} \\times \\mathbf{R}_{\\mathrm{acc}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span>. Precisely, the proof workflow is as follows: First, use the parallel composition theorem to obtain a reduction from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}} \\times \\mathbf{R}_{\\mathrm{acc}}</span> to <span class="math">\\mathbf{R}_{\\mathsf{R1CS}} \\times \\mathbf{R}_{\\mathrm{acc}}</span> (composing Protocol 3 and the trivial reduction from <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span>). Then, use the reduction from <span class="math">\\mathbf{R}_{\\mathrm{acc}} \\times \\mathbf{R}_{\\mathrm{acc}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{equal}}</span> from Protocol 4. Finally, Protocol 2 is naturally adapted to become a reduction of knowledge from <span class="math">\\mathbf{R}_{\\mathrm{equal}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span>.</p>

    <p class="text-gray-300">10</p>

    <p class="text-gray-300">In Protocol 4 we describe our reduction from <span class="math">\\mathbf{R}_{\\text{acc}}\\times\\mathbf{R}_{\\text{acc}}</span> to <span class="math">\\mathbf{R}_{\\text{equal}}</span>. The scheme is based on what we call the “point-vs-line” argument (cf. Section 4.5.2 from Thaler’s book <em>[x21]</em>).</p>

    <p class="text-gray-300">The point-vs-line argument allows to take two evaluation claims for multilinear polynomials (at different points) and turn them into evaluation two evaluation claims at the same point. The argument works as follows. Say we have two claims of the form <span class="math">f_{1}(\\mathbf{r}_{1})=c_{1}</span> and <span class="math">f_{2}(\\mathbf{r}_{2})=c_{2}</span> for some multilinear polynomials <span class="math">f_{1},f_{2}</span> in <span class="math">n</span> variables, some <span class="math">\\mathbf{r}_{1},\\mathbf{r}_{2}\\in\\mathbb{F}^{n}</span>, and some <span class="math">c_{1},c_{2}\\in\\mathbb{F}</span>. Let <span class="math">\\ell:\\mathbb{F}\\to\\mathbb{F}^{n}</span> be the parameterised line such that <span class="math">\\ell(0)=\\mathbf{r}_{1}</span> and <span class="math">\\ell(1)=\\mathbf{r}_{2}</span>, and set <span class="math">h_{i}:=f_{i}\\circ\\ell</span> for <span class="math">i=1,2</span>. Then, we have that the <span class="math">h_{i}</span> are univariate polynomials of degree at most <span class="math">n</span>, and <span class="math">h_{1}(0)=c_{1}</span>, <span class="math">h_{2}(1)=c_{2}</span>. Once this has been checked, we ask for the Verifier to pick a random uniform <span class="math">\\beta\\in\\mathbb{F}</span>, and the new claims are <span class="math">f_{i}(\\mathbf{r}^{\\prime})=c_{i}^{\\prime}</span>, where <span class="math">\\mathbf{r}^{\\prime}:=\\ell(\\beta)</span> and <span class="math">c_{i}^{\\prime}:=h_{i}(\\beta)</span> for <span class="math">i=1,2</span>. Importantly, the new claims are about the evaluations of the <span class="math">f_{i}</span> at the same point <span class="math">\\mathbf{r}^{\\prime}</span>. We will show that this is a knowledge sound reduction of knowledge from the initial evaluation claims to the new evaluation claims at the same point (see Section 7.1).</p>

    <p class="text-gray-300">[Proof of 4] Mova’s reduction of knowledge <span class="math">\\mathbf{R}_{\\text{acc}}\\times\\mathbf{R}_{\\text{acc}}\\to\\mathbf{R}_{\\text{equal}}</span> Input: <span class="math">\\mathsf{P}</span> receives <span class="math">(\\mathbf{x}_{i};\\mathbf{w}_{i})=(\\mathbf{x}_{i},\\overline{\\mathbf{W}}_{i},u_{i},\\ell_{\\mathbf{E}_{i}},\\mathbf{r}_{i};\\mathbf{W}_{i},\\mathbf{E}_{i})\\in\\mathbf{R}_{\\text{acc}}</span> as input, for <span class="math">i=1,2</span>. <span class="math">\\mathsf{V}</span> receives <span class="math">\\mathbf{x}_{1},\\mathbf{x}_{2}</span> as input.</p>

    <p class="text-gray-300">1: If <span class="math">\\mathbf{r}_{1}=\\mathbf{r}_{2}</span>, then <span class="math">\\mathsf{P}</span> outputs <span class="math">(\\mathsf{pp},\\mathbf{x}_{1},\\mathbf{x}_{2};\\mathbf{w}_{1},\\mathbf{w}_{2})</span>, and <span class="math">\\mathsf{V}</span> outputs <span class="math">(\\mathsf{pp},\\mathbf{x}_{1},\\mathbf{x}_{2})</span>.</p>

    <p class="text-gray-300">Otherwise, let <span class="math">\\ell:\\mathbb{F}\\to\\mathbb{F}^{\\log(m)}</span> be the linear function satisfying <span class="math">\\ell(0)=\\mathbf{r}_{1},\\ell(1)=\\mathbf{r}_{2}</span>. <span class="math">\\mathsf{P}</span> sends <span class="math">\\mathsf{V}</span> polynomials <span class="math">h_{1},h_{2}</span> of degree at most <span class="math">\\log(m)</span>. Supposedly,</p>

    <p class="text-gray-300"><span class="math">h_{i}(X):=\\mathsf{mle}[\\mathbf{E}_{i}]\\circ\\ell(\\mathbf{X}),\\quad i=1,2.</span> 2: <span class="math">\\mathsf{V}</span> checks that <span class="math">h_{1}(0)=v_{1}</span> and <span class="math">h_{2}(1)=v_{2}</span>, and aborts if this check fails. <span class="math">\\mathsf{V}</span> samples a random <span class="math">\\beta\\leftarrow\\mathbb{F}</span>. 3: <span class="math">\\mathsf{P}</span> and <span class="math">\\mathsf{V}</span> set <span class="math">\\ell^{\\prime}_{\\mathbf{E}_{i}}:=h_{i}(\\beta)</span> and <span class="math">\\mathbf{r}^{\\prime}:=\\ell(\\beta)</span>. 4: <span class="math">\\mathsf{P}</span> and <span class="math">\\mathsf{V}</span> output <span class="math">\\mathbf{x}^{\\prime}_{i}=(\\mathbf{x}_{i},\\overline{\\mathbf{W}}_{i},u_{i},\\ell^{\\prime}_{\\mathbf{E}_{i}},\\mathbf{r}^{\\prime})</span> for <span class="math">i=1,2</span>. 5: Additionally, <span class="math">\\mathsf{P}</span> outputs <span class="math">\\mathbf{w}^{\\prime}_{i}=(\\mathbf{W}_{i},\\mathbf{E}_{i})</span>, for <span class="math">i=1,2</span>.</p>

    <h3 id="sec-9" class="text-xl font-semibold mt-8">2.2 Knowledge soundness proof of Protocol 2</h3>

    <p class="text-gray-300">For completeness, in this section we provide a brief overview of the knowledge soundness proof of our reduction of knowledge from <span class="math">\\mathbf{R}^{\\mathcal{L}}_{\\mathsf{rcR1CS}}\\times\\mathbf{R}^{\\mathcal{L}}_{\\mathsf{rcR1CS}}</span> to <span class="math">\\mathbf{R}^{\\mathcal{L}}_{\\mathsf{rcR1CS}}</span> cf. (Protocol 2). The knowledge soundness proof of Nova <em>[x15]</em> (i.e. Protocol 1) can be recovered by replacing <span class="math">\\mathcal{L}</span> with any homomorphic commitment <span class="math">\\mathsf{Com}_{2}</span> (typically, <span class="math">\\mathsf{Com}_{2}=\\mathsf{Com}</span>).</p>

    <p class="text-gray-300">The proof uses the forking lemma for reductions of knowledge <em>[x14, x15, x1]</em>. This lemma states that, to prove that a reduction of knowledge is knowledge sound, it suffices to describe a PPT extractor that outputs valid witnesses, given a tree of accepting transcripts.</p>

    <p class="text-gray-300">When applied to Protocol 1, the forking lemma can be applied in the following way. Let <span class="math">\\mathbf{x}_{1},\\mathbf{x}_{2}</span> be a pair of instances for the relation <span class="math">\\mathbf{R}^{\\mathcal{L}}_{\\mathsf{rcR1CS}}</span>, let</p>

    <p class="text-gray-300"><span class="math">\\mathsf{tr}^{(i)}=(\\ell^{(i)}_{\\mathbf{T}},\\alpha^{(i)}),\\quad i=1,2,3</span></p>

    <p class="text-gray-300">be three accepting transcripts for Protocol 1, and let <span class="math">(\\mathbf{x}^{(i)},\\mathbf{w}^{(i)})</span> be instance-witness pairs in <span class="math">\\mathbf{R}_{\\mathsf{reRICS}}^{\\mathcal{L}}</span> output after each of the transcripts <span class="math">\\mathsf{tr}^{(i)}</span>, <span class="math">i=1,2,3</span>. Assume the challenges <span class="math">\\alpha^{(i)}</span> are pairwise different, and that <span class="math">\\ell_{\\mathbf{T}}^{(1)}=\\ell_{\\mathbf{T}}^{(2)}=\\ell_{\\mathbf{T}}^{(3)}</span>, thus we can omit the superscript and write <span class="math">\\ell_{\\mathbf{T}}</span>. Now the forking lemma states that knowledge soundness of Protocol 1 is guaranteed by the existence of a PPT extractor <span class="math">\\mathsf{Ext}</span> that outputs valid witnesses <span class="math">\\mathbf{w}_{1},\\mathbf{w}_{2}</span>, when given <span class="math">\\mathbf{x}_{1},\\mathbf{x}_{2},\\tau^{(i)}</span> (<span class="math">i=1,2,3</span>) as input.</p>

    <p class="text-gray-300">Let <span class="math">(\\mathbf{x}^{(i)};\\mathbf{w}^{(i)})=(\\mathbf{x}^{(i)},\\overline{\\mathbf{W}}^{(i)},u^{(i)},\\bar{\\mathbf{E}}^{(i)};\\mathbf{W}^{(i)},\\mathbf{E}^{(i)})</span>. By correctness of transcripts, it holds that</p>

    <p class="text-gray-300"><span class="math">\\overline{\\mathbf{W}}^{(i)}=\\overline{\\mathbf{W}}_{1}+\\alpha^{(i)}\\overline{\\mathbf{W}}_{2},\\quad i=1,2,3.</span> (4)</p>

    <p class="text-gray-300">Using interpolation for <span class="math">i=1,2</span>, <span class="math">\\mathsf{Ext}</span> constructs vectors <span class="math">\\mathbf{W}_{1}^{<em>},\\mathbf{W}_{2}^{</em>}</span> such that <span class="math">\\mathbf{W}^{(i)}=\\mathbf{W}_{1}^{<em>}+\\alpha^{(i)}\\mathbf{W}_{2}^{</em>}</span> for <span class="math">i=1,2</span>. Using the linear properties of <span class="math">\\mathsf{Com}</span> together with Eq. (4), one deduces that</p>

    <p class="text-gray-300"><span class="math">\\mathsf{Com}(\\mathbf{W}_{1}^{<em>})+\\alpha^{(i)}\\mathsf{Com}(\\mathbf{W}_{2}^{</em>})=\\mathsf{Com}(\\mathbf{W}^{(i)})=\\overline{\\mathbf{W}}^{(i)}=\\overline{\\mathbf{W}}_{1}+\\alpha^{(i)}\\overline{\\mathbf{W}}_{2},\\quad i=1,2.</span></p>

    <p class="text-gray-300">Then, again by interpolation, <span class="math">\\mathsf{Com}(\\mathbf{W}_{1}^{<em>})=\\overline{\\mathbf{W}}_{1}</span> and <span class="math">\\mathsf{Com}(\\mathbf{W}_{2}^{</em>})=\\overline{\\mathbf{W}}_{2}</span>. Now</p>

    <p class="text-gray-300"><span class="math">\\mathsf{Com}(\\mathbf{W}^{(3)})=\\overline{\\mathbf{W}}^{(3)}=\\overline{\\mathbf{W}}_{1}+\\alpha^{(3)}\\overline{\\mathbf{W}}_{2}=\\mathsf{Com}(\\mathbf{W}_{1}^{<em>}+\\alpha^{(3)}\\mathbf{W}_{2}^{</em>}),</span></p>

    <p class="text-gray-300">and hence, by the binding property of <span class="math">\\mathsf{Com}</span>, we have <span class="math">\\mathbf{W}^{(3)}=\\mathbf{W}_{1}^{<em>}+\\alpha^{(3)}\\mathbf{W}_{2}^{</em>}</span>, e.w.n.p.</p>

    <p class="text-gray-300">Next, by correctness of transcripts,</p>

    <p class="text-gray-300"><span class="math">\\ell_{\\mathbf{E}^{(i)}}=\\ell_{\\mathbf{E}_{1}}+\\alpha^{(i)}\\ell_{\\mathbf{T}}+(\\alpha^{(i)})^{2}\\ell_{\\mathbf{E}_{2}},\\quad i=1,2,3.</span> (5)</p>

    <p class="text-gray-300">Using interpolation, <span class="math">\\mathsf{Ext}</span> constructs vectors <span class="math">\\mathbf{E}_{1}^{<em>},\\mathbf{E}_{2}^{</em>},\\mathbf{T}^{<em>}</span> such that <span class="math">\\mathbf{E}^{(i)}=\\mathbf{E}_{1}^{</em>}+\\alpha^{(i)}\\mathbf{T}^{<em>}+(\\alpha^{(i)})^{2}\\mathbf{E}_{2}^{</em>}</span> for <span class="math">i=1,2,3</span>. Now, similarly, as above, using the linearity of <span class="math">\\mathcal{L}</span>, together with Eq. (5), one obtains</p>

    <p class="text-gray-300"><span class="math">\\mathcal{L}(\\mathbf{E}_{1}^{<em>})+\\alpha^{(i)}\\mathcal{L}(\\mathbf{T}^{</em>})+(\\alpha^{(i)})^{2}\\mathcal{L}(\\mathbf{E}_{2}^{*})=\\ell_{\\mathbf{E}_{1}}+\\alpha^{(i)}\\ell_{\\mathbf{T}}+(\\alpha^{(i)})^{2}\\ell_{\\mathbf{E}_{2}}.</span></p>

    <p class="text-gray-300">Again by interpolation, <span class="math">\\mathcal{L}(\\mathbf{E}_{1}^{<em>})=\\ell_{\\mathbf{E}_{1}},\\mathcal{L}(\\mathbf{E}_{2}^{</em>})=\\ell_{\\mathbf{E}_{2}},\\mathcal{L}(\\mathbf{T}^{*})=\\ell_{\\mathbf{T}}</span>.</p>

    <p class="text-gray-300">Now <span class="math">\\mathsf{Ext}</span> outputs <span class="math">\\mathbf{w}_{1}=(\\mathbf{W}_{1}^{<em>},\\mathbf{E}_{1}^{</em>})</span> and <span class="math">\\mathbf{w}_{2}=(\\mathbf{W}_{2}^{<em>},\\mathbf{E}_{2}^{</em>})</span>. We are left to show that <span class="math">(A\\cdot\\mathbf{Z}_{j})\\circ(B\\cdot\\mathbf{Z}_{j})=u_{j}\\cdot(C\\cdot\\mathbf{Z}_{j})+\\mathbf{E}_{j}</span> for <span class="math">\\mathbf{Z}_{j}=(\\mathbf{W}_{j},\\mathbf{x}_{j},1)</span>, <span class="math">j=1,2</span>. This follows in the same exact way as in the Nova paper <em>[x10]</em>, and does not require using any properties of <span class="math">\\mathsf{Com}</span> or <span class="math">\\mathcal{L}</span>.</p>

    <h2 id="sec-10" class="text-2xl font-bold">3 Preliminaries</h2>

    <p class="text-gray-300">Throughout the article we fix a finite field <span class="math">\\mathbb{F}</span>. Given an integer <span class="math">k\\geq 1</span> we denote <span class="math">[k]:=\\{1,\\ldots,n\\}</span>. We let <span class="math">\\mathbb{B}:=\\{0,1\\}</span>. Similarly</p>

    <p class="text-gray-300"><span class="math">\\mathbb{B}^{k}=\\{0,1\\}^{k}:=\\{(b_{1},\\ldots,b_{k})\\mid b_{i}\\in\\mathbb{B},\\text{ for all }i\\in[k]\\}</span></p>

    <p class="text-gray-300">is the hypercube of dimension <span class="math">k</span>, or, in other words, the set of all sequences of <span class="math">k</span> bits.</p>

    <p class="text-gray-300">For <span class="math">n\\geq 1</span> and <span class="math">d\\geq 0</span>, we let <span class="math">\\mathbb{F}^{\\leq d}[\\mathbf{X}]</span> be the set of multivariate polynomials in the variables <span class="math">\\mathbf{X}=(X_{1},\\ldots,X_{n})</span> with degree in each variable at most <span class="math">d</span>.</p>

    <p class="text-gray-300">Given an <span class="math">m_{1}\\times m_{2}</span> matrix <span class="math">A</span> and a vector <span class="math">\\mathbf{Z}</span> of size <span class="math">m_{2}</span>, we write <span class="math">A\\cdot\\mathbf{Z}</span> to denote the multiplication of the matrix <span class="math">A</span> with <span class="math">\\mathbf{Z}</span> in column form.</p>

    <p class="text-gray-300">Given two interactive algorithms <span class="math">\\mathcal{A}_{1},\\mathcal{A}_{2}</span>, we let</p>

    <p class="text-gray-300"><span class="math">\\langle\\mathcal{A}_{1}(\\mathsf{inp}_{1}),\\mathcal{A}_{2}(\\mathsf{inp}_{2})\\rangle(\\mathsf{inp}_{3})</span></p>

    <p class="text-gray-300">be the random variable whose outcome is the output of the interaction of <span class="math">\\mathcal{A}_{1}</span> and <span class="math">\\mathcal{A}_{2}</span> on inputs <span class="math">(\\mathsf{inp}_{1},\\mathsf{inp}_{3})</span> and <span class="math">(\\mathsf{inp}_{2},\\mathsf{inp}_{3})</span>, respectively.</p>

    <h4 id="sec-11" class="text-lg font-semibold mt-6">Negligible functions</h4>

    <p class="text-gray-300">We use <span class="math">\\lambda</span> to denote the security parameter. A function <span class="math">f(\\lambda)</span> is said to be <em>negligible</em> if for all <span class="math">c\\in\\mathbb{N}</span> and <span class="math">k\\in\\mathbb{R}</span> with <span class="math">k&gt;0</span>, there exists <span class="math">\\lambda_{0}</span> such that <span class="math">f(\\lambda)&lt;k\\lambda^{-c}</span> for all <span class="math">\\lambda\\geq\\lambda_{0}</span>. In that case we write <span class="math">f(\\lambda)=\\mathsf{negl}(\\lambda)</span>. Whenever an event occurs with probability <span class="math">1-\\mathsf{negl}(\\lambda)</span> we will say that it holds <em>except with negligible probability</em>, and we abbreviate this as <em>e.w.n.p.</em></p>

    <h6 id="sec-12" class="text-base font-medium mt-4">Remark 3.1.</h6>

    <p class="text-gray-300">Let <span class="math">f(\\lambda),g(\\lambda)</span> be two negligible functions and <span class="math">g(\\lambda)\\neq 1</span> for all <span class="math">\\lambda</span>. Define <span class="math">h(\\lambda)=f(\\lambda)/(1-g(\\lambda))</span>. Then <span class="math">h(\\lambda)</span> is also negligible. Indeed, let <span class="math">\\lambda_{0}</span> be such that <span class="math">g(\\lambda)&lt;\\frac{1}{2}\\cdot 1</span> for all <span class="math">\\lambda\\geq\\lambda_{0}</span>. Thus for such <span class="math">\\lambda</span> we have <span class="math">1-g(\\lambda)&gt;\\frac{1}{2}</span>. Thus</p>

    <p class="text-gray-300"><span class="math">h(\\lambda)=\\frac{f(\\lambda)}{1-g(\\lambda)}&lt;2\\cdot f(\\lambda),\\text{ for all }\\lambda\\geq\\lambda_{0}.</span></p>

    <p class="text-gray-300">It is clear that the function <span class="math">2\\cdot f(\\lambda)</span> is negligible and a function dominated by a negligible function is again negligible.</p>

    <h4 id="sec-13" class="text-lg font-semibold mt-6">Indexed relations</h4>

    <p class="text-gray-300">An <em>indexed relation</em> is a subset <span class="math">\\mathbf{R}\\subseteq\\{0,1\\}^{<em>}\\times\\{0,1\\}^{</em>}\\times\\{0,1\\}^{<em>}</span>. Given <span class="math">(\\mathsf{pp},\\mathtt{x};\\mathtt{w})\\in\\mathbf{R}</span>, the string <span class="math">\\mathsf{pp}</span> are the </em>public parameters<em> (sometimes referred to as </em>index<em> in the literature); <span class="math">\\mathtt{x}</span> is called an </em>instance<em>, and <span class="math">\\mathtt{w}</span> a </em>witness*. In this work we often interpret instances and witnesses as vectors of field elements, natural numbers, and field descriptions.</p>

    <p class="text-gray-300">All relations considered in this work are indexed relations. We always use the syntax <span class="math">(\\mathsf{pp},\\mathtt{x};\\mathtt{w})</span> to denote a triple in <span class="math">\\mathbf{R}</span>. The usage of ; denotes a separation between public and private data.</p>

    <h3 id="sec-14" class="text-xl font-semibold mt-8">3.1 Multilinear polynomials</h3>

    <p class="text-gray-300">Let <span class="math">n\\geq 1</span> and let <span class="math">\\mathbf{X}=(X_{1},\\ldots,X_{n})</span> be a tuple of variables. It is well-known that a multilinear polynomial <span class="math">f(\\mathbf{X})\\in\\mathbb{F}^{\\leq 1}[\\mathbf{X}]</span> is uniquely determined by the values it takes on <span class="math">\\mathbb{B}^{n}</span>, i.e. its restriction to <span class="math">\\mathbb{B}^{n}</span>. In other words, for any two polynomials <span class="math">f,g\\in\\mathbb{F}^{\\leq 1}[\\mathbf{X}]</span> the following holds</p>

    <p class="text-gray-300"><span class="math">f(\\mathbf{x})=g(\\mathbf{x})\\text{ for all }\\mathbf{x}\\in\\mathbb{B}^{n}\\implies f=g.</span></p>

    <p class="text-gray-300">Further, given a map <span class="math">f:\\mathbb{B}^{n}\\to\\mathbb{F}</span>, there always exists a unique multilinear polynomial in <span class="math">n</span> variables, denoted <span class="math">\\mathsf{mle}<a href="\\mathbf{X}">f</a></span>, such that <span class="math">\\mathsf{mle}<a href="\\mathbf{x}">f</a>=f(\\mathbf{x})</span> for all <span class="math">\\mathbf{x}\\in\\mathbb{B}^{n}</span>. It is given by the expression</p>

    <p class="text-gray-300"><span class="math">\\mathsf{mle}<a href="\\mathbf{X}">f</a>:=\\sum_{\\mathbf{x}\\in\\mathbb{B}^{n}}f(\\mathbf{x})\\widetilde{\\mathsf{eq}}(\\mathbf{x};\\mathbf{X})</span> (6)</p>

    <p class="text-gray-300">where <span class="math">\\widetilde{\\mathsf{eq}}(\\mathbf{x};\\mathbf{X})</span> is the unique multilinear polynomial in <span class="math">n</span> variables that takes the value <span class="math">0</span> on all points of the hypercube <span class="math">\\mathbb{B}^{n}</span>, except at <span class="math">\\mathbf{x}</span> where it takes the value <span class="math">1</span>. Precisely,</p>

    <p class="text-gray-300"><span class="math">\\widetilde{\\mathsf{eq}}(\\mathbf{x};\\mathbf{X}):=\\prod_{i\\in[n]}\\left(x_{i}X_{i}-(1-x_{i})(1-X_{i})\\right).</span></p>

    <p class="text-gray-300">This unique multilinear polynomial <span class="math">\\mathsf{mle}<a href="\\mathbf{X}">f</a></span> is called the multilinear extension (MLE) of <span class="math">f</span>. Given a vector <span class="math">\\mathbf{v} = (v_{1},\\ldots ,v_{N})\\in \\mathbb{F}^{N}</span>, where <span class="math">N = 2^n</span>, we define the MLE of <span class="math">\\mathbf{v}</span> as the MLE of the map <span class="math">\\mathbf{v}:\\mathbb{B}^n\\to \\mathbb{F}</span> assigning to each element <span class="math">\\mathbf{x}\\in \\mathbb{B}^n</span> the element <span class="math">v_{\\mathbf{x}}</span>, where here we interpret <span class="math">\\mathbf{x}</span> as the natural number whose binary representation is <span class="math">\\mathbf{x}</span>. We denote the MLE of <span class="math">\\mathbf{v}</span> by <span class="math">\\mathsf{mle}<a href="\\mathbf{X}">\\mathbf{v}</a></span>.</p>

    <p class="text-gray-300">Throughout the paper we use the following observation without further reference:</p>

    <p class="text-gray-300"><strong>Lemma 3.2.</strong> Let <span class="math">\\mathbf{v},\\mathbf{u}\\in \\mathbb{F}^N</span> be two vectors, and let <span class="math">\\mathbf{r}\\in \\mathbb{F}^n</span>. Then</p>

    <div class="my-4 text-center"><span class="math-block">\\mathsf{mle}[\\mathbf{v} + \\mathbf{u}](\\mathbf{r}) = \\mathsf{mle}[\\mathbf{v}](\\mathbf{r}) + \\mathsf{mle}[\\mathbf{u}](\\mathbf{r})</span></div>

    <p class="text-gray-300"><em>Proof.</em> This follows immediately from Eq. (6).</p>

    <h2 id="sec-15" class="text-2xl font-bold">3.2 Commitment schemes</h2>

    <p class="text-gray-300">Throughout this section we follow [KST21].</p>

    <p class="text-gray-300"><strong>Definition 3.1.</strong> A commitment scheme for vectors in <span class="math">\\mathbb{F}^k</span> is a tuple of three protocols</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\operatorname{Gen}(1^{\\lambda}, k) \\to \\mathfrak{pp}_{\\mathrm{Com}}</span>: Takes a security parameter <span class="math">1^{\\lambda}</span> and a length parameter <span class="math">k</span>. Outputs public parameters <span class="math">\\mathfrak{pp}_{\\mathrm{Com}}</span>.</li>

      <li><span class="math">\\operatorname{Com}(\\mathfrak{pp}_{\\mathrm{Com}}, \\mathbf{W}, s) \\to C</span>: Takes as input <span class="math">\\mathfrak{pp}_{\\mathrm{Com}}</span>, a vector <span class="math">\\mathbf{W} \\in \\mathbb{F}^k</span> and <span class="math">s \\in \\mathbb{F}</span>. Outputs a commitment <span class="math">C</span>.</li>

      <li><span class="math">\\operatorname{Open}(\\mathfrak{pp}_{\\mathrm{Com}}, C, \\mathbf{W}, s) \\to \\{0, 1\\}</span>: Checks whether <span class="math">C = \\operatorname{Com}(\\mathfrak{pp}_{\\mathrm{Com}}, \\mathbf{W}, s)</span>.</li>

    </ul>

    <p class="text-gray-300">The scheme is required to be binding. This means that for any PPT adversary <span class="math">\\mathcal{A}</span>, the following probability is <span class="math">\\mathrm{negl}(\\lambda)</span>:</p>

    <div class="my-4 text-center"><span class="math-block">\\Pr \\left[ \\begin{array}{l l} &amp;amp; \\mathfrak{pp}_{\\mathrm{Com}} \\leftarrow \\operatorname{Gen}(1^{\\lambda}, k), \\\\ b_{0} = b_{1} = 1, &amp;amp; (C, \\mathbf{W}_{0} \\in \\mathbb{F}^{k}, \\mathbf{W}_{1} \\in \\mathbb{F}^{k}, s_{0} \\in \\mathbb{F}, s_{1} \\in \\mathbb{F}) \\leftarrow \\mathcal{A}(\\mathfrak{pp}_{\\mathrm{Com}}), \\\\ \\mathbf{W}_{0} \\neq \\mathbf{W}_{1} &amp;amp; b_{0} \\leftarrow \\operatorname{Open}(\\mathfrak{pp}_{\\mathrm{Com}}, C, \\mathbf{W}_{0}, s_{0}), \\\\ &amp;amp; b_{1} \\leftarrow \\operatorname{Open}(\\mathfrak{pp}_{\\mathrm{Com}}, C, \\mathbf{W}_{1}, s_{1}) \\end{array} \\right]</span></div>

    <p class="text-gray-300">Throughout the paper we fix a commitment scheme that is succinct and additively homomorphic, as defined below.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><strong>Definition 3.2 (Succinctness).</strong> A commitment scheme for vectors in <span class="math">\\mathbb{F}^k</span>, <span class="math">(\\mathrm{Gen},\\mathrm{Com},\\mathrm{Open})</span>, is succinct if for all <span class="math">\\mathfrak{pp}_{\\mathrm{Com}} \\gets \\mathrm{Gen}(1^{\\lambda}, k)</span>, and for any <span class="math">\\mathbf{W} \\in \\mathbb{F}^k</span>, <span class="math">s \\in \\mathbb{F}</span>, $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathrm{Com}(\\mathfrak{pp}_{\\mathrm{Com}}, \\mathbf{W}, s)</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">= O_{\\lambda}(\\mathrm{polylog}(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{W}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">))$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><strong>Definition 3.3 (Additively Homomorphic).</strong> A commitment scheme for vectors in <span class="math">\\mathbb{F}^k</span>, <span class="math">(\\mathrm{Gen},\\mathrm{Com},\\mathrm{Open})</span>, is additively homomorphic if for all <span class="math">\\mathfrak{pp}_{\\mathrm{Com}} \\gets \\mathrm{Gen}(1^{\\lambda},k)</span>, and for any <span class="math">\\mathbf{W}_1, \\mathbf{W}_2 \\in \\mathbb{F}^k</span>, <span class="math">s_1, s_2 \\in \\mathbb{F}</span>, it holds that <span class="math">\\mathrm{Com}(\\mathfrak{pp}_{\\mathrm{Com}}, \\mathbf{W}_1, s_1) + \\mathrm{Com}(\\mathfrak{pp}_{\\mathrm{Com}}, \\mathbf{W}_2, s_2) = \\mathrm{Com}(\\mathfrak{pp}_{\\mathrm{Com}}, \\mathbf{W}_1 + \\mathbf{W}_2, s_1 + s_2)</span>.</p>

    <p class="text-gray-300">14</p>

    <p class="text-gray-300">3.3 Reductions of knowledge</p>

    <p class="text-gray-300">Reductions of knowledge were formally defined in <em>[x10]</em>. We reproduce the main concepts from <em>[x10]</em> that will be used in this paper.</p>

    <h6 id="sec-16" class="text-base font-medium mt-4">Definition 3.4 (Reduction of Knowledge).</h6>

    <p class="text-gray-300">Consider indexed relations <span class="math">\\mathbf{R}_{1}</span> and <span class="math">\\mathbf{R}_{2}</span> consisting of public parameters, input, witness tuples. A reduction of knowledge from <span class="math">\\mathbf{R}_{1}</span> to <span class="math">\\mathbf{R}_{2}</span> is defined by PPT algorithms <span class="math">(\\mathsf{Gen},\\mathsf{P},\\mathsf{V})</span> denoting the generator, the Prover, and the Verifier, respectively, with the following interface.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathsf{Gen}(1^{\\lambda})\\to\\mathsf{pp}</span>: Takes security parameter <span class="math">\\lambda</span>. Outputs public parameters <span class="math">\\mathsf{pp}</span>.</li>

      <li><span class="math">\\mathsf{P}(\\mathsf{pp},\\mathbf{x}_{1},\\mathbf{w}_{1})\\to(\\mathbf{x}_{2};\\mathbf{w}_{2})</span>: Takes as input public parameters <span class="math">\\mathsf{pp}</span>, and input-witness pair <span class="math">(\\mathbf{x}_{1};\\mathbf{w}_{1})</span>. Interactively reduces the input <span class="math">(\\mathsf{pp},\\mathbf{x}_{1};\\mathbf{w}_{1})\\in\\mathbf{R}_{1}</span> to a new input <span class="math">(\\mathsf{pp},\\mathbf{x}_{2};\\mathbf{w}_{2})\\in\\mathbf{R}_{2}</span>.</li>

      <li><span class="math">V(\\mathsf{pp},\\mathbf{x}_{1})\\to\\mathbf{x}_{2}</span>: Takes as input public parameters <span class="math">\\mathsf{pp}</span>, and input <span class="math">\\mathbf{x}_{1}</span> associated with <span class="math">\\mathbf{R}_{1}</span>. Interactively reduces the task of checking <span class="math">\\mathbf{x}_{1}</span> to the task of checking a new input <span class="math">\\mathbf{x}_{2}</span> associated with <span class="math">\\mathbf{R}_{2}</span>.</li>

    </ul>

    <p class="text-gray-300">Let <span class="math">\\langle\\mathsf{P},\\mathsf{V}\\rangle</span> denote the interaction between <span class="math">\\mathsf{P}</span> and <span class="math">\\mathsf{V}</span>. We treat <span class="math">\\langle\\mathsf{P},\\mathsf{V}\\rangle</span> as a function that takes as input <span class="math">(\\mathsf{pp},\\mathbf{x}_{1},\\mathbf{w}_{1})</span> and runs the interaction on Prover’s input <span class="math">(\\mathsf{pp},\\mathbf{x}_{1},\\mathbf{w}_{1})</span> and Verifier’s input <span class="math">(\\mathsf{pp},\\mathbf{x}_{1})</span>. At the end of the interaction, <span class="math">\\langle\\mathsf{P},\\mathsf{V}\\rangle</span> outputs the Verifier’s input <span class="math">\\mathbf{x}_{2}</span> and the Prover’s witness <span class="math">\\mathbf{w}_{2}</span>. A reduction of knowledge <span class="math">(\\mathsf{Gen},\\mathsf{P},\\mathsf{V})</span> satisfies the following conditions.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Perfect Completeness: For any PPT adversary <span class="math">\\mathcal{A}</span>, given <span class="math">\\mathsf{pp}\\leftarrow\\mathsf{Gen}(1^{\\lambda})</span> and <span class="math">(\\mathbf{x}_{1};\\mathbf{w}_{1})\\leftarrow\\mathcal{A}(\\mathsf{pp})</span> such that <span class="math">(\\mathsf{pp},\\mathbf{x}_{1};\\mathbf{w}_{1})\\in\\mathbf{R}_{1}</span>, we have that the Verifier accepts at the end of the protocol, the Prover’s output is equal to the Verifier’s output, and</li>

    </ol>

    <p class="text-gray-300"><span class="math">(\\mathsf{pp},\\langle\\mathsf{P},\\mathsf{V}\\rangle(\\mathsf{pp},\\mathbf{x}_{1},\\mathbf{w}_{1}))\\in\\mathbf{R}_{2}.</span></p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Knowledge Soundness: For any expected polynomial-time adversaries <span class="math">\\mathcal{A}</span> and <span class="math">\\mathsf{P}^{*}</span>, there exists an expected polynomial-time extractor <span class="math">\\mathsf{Ext}</span> such that given <span class="math">\\mathsf{pp}\\leftarrow\\mathsf{Gen}(1^{\\lambda})</span> and <span class="math">(\\mathbf{x}_{1},\\mathsf{st})\\leftarrow\\mathcal{A}(\\mathsf{pp})</span>, we have that</li>

    </ol>

    <p class="text-gray-300"><span class="math">\\Pr[(\\mathsf{pp},\\mathbf{x}_{1},\\mathsf{Ext}(\\mathsf{pp},\\mathbf{x}_{1},\\mathsf{st}))\\in\\mathbf{R}_{1}]\\approx\\Pr[(\\mathsf{pp},\\langle\\mathsf{P}^{*},\\mathsf{V}\\rangle(\\mathsf{pp},\\mathbf{x}_{1},\\mathsf{st}))\\in\\mathbf{R}_{2}].</span></p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Public Reducibility: There exists a deterministic polynomial-time function <span class="math">\\varphi</span> such that for any PPT adversary <span class="math">\\mathcal{A}</span> and expected polynomial-time adversary <span class="math">\\mathsf{P}^{<em>}</span>, given <span class="math">\\mathsf{pp}\\leftarrow\\mathsf{Gen}(1^{\\lambda})</span>, <span class="math">(\\mathbf{x}_{1},\\mathsf{st})\\leftarrow\\mathcal{A}(\\mathsf{pp})</span>, and <span class="math">(\\mathbf{x}_{2};\\mathbf{w}_{2})\\leftarrow\\langle\\mathsf{P}^{</em>},\\mathsf{V}\\rangle(\\mathsf{pp},\\mathbf{x}_{1},\\mathsf{st})</span> with interaction transcript <span class="math">\\tau</span>, we have that <span class="math">\\varphi(\\mathsf{pp},\\mathbf{x}_{1},\\tau)=\\mathbf{x}_{2}</span>.</li>

    </ol>

    <p class="text-gray-300">We write <span class="math">\\Pi:\\mathbf{R}_{1}\\to\\mathbf{R}_{2}</span> to denote that protocol <span class="math">\\Pi</span> is a reduction of knowledge from relation <span class="math">\\mathbf{R}_{1}</span> to relation <span class="math">\\mathbf{R}_{2}</span>.</p>

    <h6 id="sec-17" class="text-base font-medium mt-4">Theorem 3.3 (Sequential Composition, Theorem 5 of <em>[x10]</em>).</h6>

    <p class="text-gray-300">Consider ternary relations <span class="math">\\mathbf{R}_{1}</span>, <span class="math">\\mathbf{R}_{2}</span>, and <span class="math">\\mathbf{R}_{3}</span>. For reductions of knowledge <span class="math">\\Pi_{1}=(\\mathsf{Gen},\\mathsf{P}_{1},V_{1}):\\mathbf{R}_{1}\\to\\mathbf{R}_{2}</span> and <span class="math">\\Pi_{2}=(\\mathsf{Gen},\\mathsf{P}_{2},\\mathsf{V}_{2}):\\mathbf{R}_{2}\\to\\mathbf{R}_{3}</span>, we have that <span class="math">\\Pi_{2}\\circ\\Pi_{1}=(\\mathsf{Gen},\\mathsf{P},\\mathsf{V})</span> is a reduction of knowledge from <span class="math">\\mathbf{R}_{1}</span> to <span class="math">\\mathbf{R}_{3}</span> where</p>

    <p class="text-gray-300"><span class="math">\\mathsf{P}(\\mathsf{pp},\\mathbf{x}_{1},\\mathbf{w}_{1})</span> <span class="math">=\\mathsf{P}_{2}(\\mathsf{pp},\\mathsf{P}_{1}(\\mathsf{pp},\\mathbf{x}_{1},\\mathbf{w}_{1}))</span> <span class="math">\\mathsf{V}(\\mathsf{pp},\\mathbf{x}_{1})</span> <span class="math">=\\mathsf{V}_{2}(\\mathsf{pp},\\mathsf{V}_{1}(\\mathsf{pp},\\mathbf{x}_{1},\\mathbf{w}_{1})).</span></p>

    <h6 id="sec-18" class="text-base font-medium mt-4">Theorem 3.4 (Parallel Composition, Theorem 6 of <em>[x10]</em>).</h6>

    <p class="text-gray-300">Consider ternary relations <span class="math">\\mathbf{R}_{1}</span>, <span class="math">\\mathbf{R}_{2}</span>, <span class="math">\\mathbf{R}_{3}</span>, and <span class="math">\\mathbf{R}_{4}</span>. For reductions of knowledge <span class="math">\\Pi_{1}=(\\mathsf{Gen},\\mathsf{P}_{1},\\mathsf{V}_{1}):\\mathbf{R}_{1}\\to\\mathbf{R}_{2}</span> and <span class="math">\\Pi_{2}=(\\mathsf{Gen},\\mathsf{P}_{2},\\mathsf{V}_{2}):\\mathbf{R}_{3}\\to\\mathbf{R}_{4}</span>, we have that <span class="math">\\Pi_{1}\\times\\Pi_{2}=(\\mathsf{Gen},\\mathsf{P},\\mathsf{V})</span> is a reduction of knowledge from <span class="math">\\mathbf{R}_{1}\\times\\mathbf{R}_{3}</span> to <span class="math">\\mathbf{R}_{2}\\times\\mathbf{R}_{4}</span> where</p>

    <p class="text-gray-300"><span class="math">\\mathsf{P}(\\mathsf{pp},(\\mathtt{x}_{1},\\mathtt{x}_{3}),(\\mathtt{w}_{1},\\mathtt{w}_{3}))</span> <span class="math">=(\\mathsf{P}_{1}(\\mathsf{pp},\\mathtt{x}_{1},\\mathtt{w}_{1}),\\mathsf{P}_{2}(\\mathsf{pp},\\mathtt{x}_{3},\\mathtt{w}_{3}))</span> <span class="math">\\mathsf{V}(\\mathsf{pp},(\\mathtt{x}_{1},\\mathtt{x}_{3}))</span> <span class="math">=(\\mathsf{V}_{1}(\\mathsf{pp},\\mathtt{x}_{1}),\\mathsf{V}_{2}(\\mathsf{pp},\\mathtt{x}_{3})).</span></p>

    <h6 id="sec-19" class="text-base font-medium mt-4">Definition 3.5 (Tree of transcripts).</h6>

    <p class="text-gray-300">Consider an <span class="math">m</span>-round public coin interactive protocol <span class="math">(\\mathsf{Gen},\\mathsf{P},\\mathsf{V})</span> satisfying Definition 3.4. A <span class="math">(n_{1},\\ldots,n_{m})</span>-tree of accepting transcripts, for an input <span class="math">\\mathtt{x}</span>, is a rooted tree of depth <span class="math">m</span>, with each node of depth <span class="math">i</span> having <span class="math">n_{i}</span> descendants, such that:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>each vertex of layer <span class="math">i</span> is a Prover’s message of round <span class="math">i</span>;</li>

      <li>each edge connecting a node of layer <span class="math">i</span> to a node of layer <span class="math">i+1</span> is labeled by a different Verifier’s challenge from round <span class="math">i</span>;</li>

      <li>each leaf of layer <span class="math">m</span> is labeled with an accepting instance-witness pair output, that corresponds to the interaction along the path.</li>

    </ul>

    <h6 id="sec-20" class="text-base font-medium mt-4">Lemma 3.5 (Tree Extraction Lemma).</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Consider an <span class="math">m</span>-round public-coin interactive protocol <span class="math">(\\mathsf{Gen},\\mathsf{P},\\mathsf{V})</span> that satisfies the interface described in Definition 3.4 and satisfies perfect completeness. Then <span class="math">(\\mathsf{Gen},\\mathsf{P},\\mathsf{V})</span> is a reduction of knowledge if there exists a PPT extractor <span class="math">\\chi</span> such that for all instances <span class="math">\\mathtt{x}</span>, outputs a satisfying witness <span class="math">\\mathtt{w}</span> with probability <span class="math">1-\\mathsf{negl}(\\lambda)</span>, given a <span class="math">(n_{1},\\ldots,n_{m})</span>-tree of accepting transcripts for <span class="math">\\mathtt{x}</span> where the Verifier’s challenges are sampled from a space <span class="math">Q</span> such that $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Q</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">=O(2^{\\lambda})<span class="math">, and </span>\\prod_{i}n_{i}=poly(\\lambda)$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h2 id="sec-21" class="text-2xl font-bold">4 The Mova folding scheme</h2>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Throughout the rest of the paper we fix a security parameter <span class="math">\\lambda</span> and a finite field <span class="math">\\mathbb{F}</span> with $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">^{-1}=\\mathsf{negl}(\\lambda)<span class="math">. Further, we let </span>\\mathsf{pp}<span class="math"> denote public parameters </span>\\mathsf{pp}_{\\mathsf{R1CS}}=(\\mathbb{F},m,n,\\ell,A,B,C)<span class="math">, where </span>m,n,\\ell\\geq 0<span class="math"> are nonnegative integers with </span>m\\leq</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math">; </span>A,B,C<span class="math"> are </span>m\\times m<span class="math"> matrices each with at most </span>\\Omega(n)<span class="math"> nonzero entries in </span>\\mathbb{F}<span class="math">. The parameter </span>\\ell<span class="math"> denotes the size of public input vectors. We fix a vector commitment scheme </span>(\\mathsf{Gen}_{\\mathsf{com}},\\mathsf{Com},\\mathsf{Open})<span class="math"> for vectors in </span>\\mathbb{F}^{m}<span class="math">, and public parameters </span>\\mathsf{pp}_{\\mathsf{Com}}\\leftarrow\\mathsf{Gen}_{\\mathsf{com}}(1^{\\lambda},m)<span class="math">. Then, we let </span>\\mathsf{pp}=(\\mathsf{pp}_{\\mathsf{R1CS}},\\mathsf{pp}_{\\mathsf{com}})$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">In this section we describe a reduction of knowledge from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}\\times\\mathbf{R}_{\\mathsf{acc}}</span> to <span class="math">\\mathbf{R}_{\\mathsf{acc}}</span>. Along the way, we also construct a reduction of knowledge from <span class="math">\\mathbf{R}_{\\mathsf{acc}}\\times\\mathbf{R}_{\\mathsf{acc}}</span> to <span class="math">\\mathbf{R}_{\\mathsf{acc}}</span>. Here <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}</span> denotes the R1CS relation:</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\[ \\mathbf{R}_{\\mathsf{R1CS}}:=\\left\\{\\left(\\mathsf{pp}_{\\mathsf{R1CS}},\\mathtt{x}=\\mathbf{x};\\mathtt{w}=\\mathbf{W}\\right)\\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\begin{array}[]{l}\\mathbf{x}\\in\\mathbb{F}^{\\ell},\\ \\mathbf{W}\\in\\mathbb{F}^{m-\\ell-1}\\\\</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">(A\\cdot\\mathbf{Z})\\circ(B\\cdot\\mathbf{Z})=C\\cdot\\mathbf{Z}\\\\ Z=(\\mathbf{W},\\mathbf{x},1)\\end{array}\\right.\\right\\}. \\] (7)</p>

    <p class="text-gray-300">###</p>

    <p class="text-gray-300">The relation <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span> is defined as</p>

    <div class="my-4 text-center"><span class="math-block"></span></div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf {R} _ {\\mathrm {a c c}} := \\left\\{ \\begin{array}{l l} \\left( \\begin{array}{l} \\mathrm {p p}, \\\\ \\mathrm {x} = (\\mathbf {x}, v, u, \\overline {{\\mathbf {W}}}, \\mathbf {r}); \\\\ \\mathrm {w} = (\\mathbf {W}, \\mathbf {E}, s) \\end{array} \\right) &amp; \\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\begin{array}{l} \\mathbf {x} \\in \\mathbb {F} ^ {\\ell}, \\mathbf {E} \\in \\mathbb {F} ^ {m}, \\mathbf {r} \\in \\mathbb {F} ^ {\\log m}, \\\\ v, s \\in \\mathbb {F}, \\mathbf {W} \\in \\mathbb {F} ^ {m - \\ell - 1} \\\\ (A \\cdot \\mathbf {Z}) \\circ (B \\cdot \\mathbf {Z}) = u \\cdot (C \\cdot \\mathbf {Z}) + \\mathbf {E} \\\\ \\mathbf {Z} = (\\mathbf {W}, \\mathbf {x}, u) \\\\ \\operatorname {C o m} (\\mathrm {p p} _ {\\mathrm {C o m}}, \\mathbf {W}, s) = \\overline {{\\mathbf {W}}}, \\\\ \\operatorname {m l e} [ \\mathbf {E} ] (\\mathbf {r}) = v \\end{array} \\right. \\end{array} \\right\\}. \\tag {8}</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="my-4 text-center"><span class="math-block"></span></div>

    <p class="text-gray-300">To construct our reduction of knowledge from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}} \\times \\mathbf{R}_{\\mathrm{acc}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span> we describe three separate reductions of knowledge, and then we apply the sequential and parallel composition theorems from [KP23] (cf. Theorems 3.3 and 3.4) to obtain our desired protocol. Precisely, we describe reductions of knowledge for:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathbf{R}_{\\mathsf{R1CS}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span>. This reduction transforms an instance-witness pair from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}</span> into an instance-witness pair from <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span>.</li>

      <li><span class="math">\\mathbf{R}_{\\mathrm{acc}} \\times \\mathbf{R}_{\\mathrm{acc}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{equal}}</span>. Here we define <span class="math">\\mathbf{R}_{\\mathrm{equal}}</span> as follows:</li>

    </ul>

    <div class="my-4 text-center"><span class="math-block"></span></div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf {R} _ {\\mathrm {e q u a l}} = \\left\\{(\\mathsf {p p}, (\\mathbf {x} _ {1}, \\mathbf {x} _ {2}); (\\mathbf {w} _ {1}; \\mathbf {w} _ {2})) \\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\begin{array}{l} (\\mathsf {p p}, \\mathbf {x} _ {i}; \\mathbf {w} _ {i}) \\in \\mathbf {R} _ {\\mathrm {a c c}}, i = 1, 2, \\\\ \\mathbf {x} _ {1}. \\mathbf {r} = \\mathbf {x} _ {2}. \\mathbf {r} \\end{array} \\right. \\right\\}.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="my-4 text-center"><span class="math-block"></span></div>

    <p class="text-gray-300">In words, <span class="math">\\mathbf{R}_{\\mathrm{equal}}</span> is the set of pairs of instance-witness tuples from <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span> that have the same evaluation point.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathbf{R}_{\\mathrm{equal}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span>. Intuitively, this reduction transforms two instance-witness pairs <span class="math">(\\mathsf{pp}, \\mathbf{x}_1; \\mathbf{w}_1), (\\mathsf{pp}, \\mathbf{x}_2; \\mathbf{w}_2)</span> from <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span> with <span class="math">\\mathbf{x}_1 \\cdot \\mathbf{r} = \\mathbf{x}_2 \\cdot \\mathbf{r}</span>, into an instance-witness pair from <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span>.</li>

    </ul>

    <p class="text-gray-300">In Section 4.4 we put together these reductions of knowledge and describe the full Mova folding scheme.</p>

    <h2 id="sec-22" class="text-2xl font-bold">4.1 From <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span></h2>

    <p class="text-gray-300">In Protocol 5 we describe Mova's reduction of knowledge from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span>. In Lemma 4.1 we prove that the protocol is, indeed, a reduction of knowledge. In Remark 4.3 we explain why the proof of Lemma 4.1 is more involved than one would expect at first.</p>

    <p class="text-gray-300"><strong>Protocol 5</strong> Mova's reduction of knowledge <span class="math">\\mathbf{R}_{\\mathsf{R1CS}} \\rightarrow \\mathbf{R}_{\\mathrm{acc}}</span></p>

    <p class="text-gray-300"><strong>Input:</strong> <span class="math">\\mathsf{P}</span> receives <span class="math">(\\mathsf{pp}, \\mathbf{x}; \\mathbf{W}) \\in \\mathbf{R}_{\\mathsf{R1CS}}</span>. <span class="math">\\mathsf{V}</span> receives <span class="math">(\\mathsf{pp}, \\mathbf{x})</span>.</p>

    <p class="text-gray-300">1: <span class="math">\\mathsf{P}</span> samples <span class="math">s \\gets \\mathbb{F}</span>, computes the commitment <span class="math">\\overline{\\mathbf{W}} := \\operatorname{Com}(\\mathsf{pp}_{\\mathrm{Com}}, \\mathbf{W}, s)</span> and sends it to <span class="math">\\mathsf{V}</span>. 2: <span class="math">\\mathsf{V}</span> samples a random vector <span class="math">\\mathbf{r} \\gets \\mathbb{F}^{\\log m}</span> and sends <span class="math">\\mathbf{r}</span> to <span class="math">\\mathsf{P}</span>. 3: <span class="math">\\mathsf{P}</span> and <span class="math">\\mathsf{V}</span> set <span class="math">v = 0, u = 1</span>, and output <span class="math">\\mathbf{x}_{\\mathrm{acc}}</span> where <span class="math">\\mathbf{x}_{\\mathrm{acc}} = (\\mathbf{x}, v, u, \\overline{\\mathbf{W}}, \\mathbf{r})</span>. Additionally, <span class="math">\\mathsf{P}</span> outputs <span class="math">\\mathbf{w}_{\\mathrm{acc}} = (\\mathbf{W}, \\mathbf{E} = \\mathbf{0}, s)</span>.</p>

    <p class="text-gray-300"><strong>Lemma 4.1.</strong> Protocol 5 is a reduction of knowledge from <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span>.</p>

    <p class="text-gray-300">17</p>

    <h6 id="sec-23" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">Public reducibility. We construct a deterministic function <span class="math">\\varphi</span>, with input <span class="math">(\\mathsf{pp},\\mathtt{x},\\tau)</span>, that outputs <span class="math">\\mathtt{x}^{\\prime}</span>. Suppose <span class="math">\\mathtt{x}=(\\mathbf{x})</span> and <span class="math">\\tau=(\\overline{\\mathbf{W}},\\mathbf{r})</span>, otherwise the function <span class="math">\\varphi</span> aborts. The function <span class="math">\\varphi</span> takes the values <span class="math">\\overline{\\mathbf{W}}</span> and <span class="math">\\mathbf{r}</span> from the transcript <span class="math">\\tau</span>, then outputs <span class="math">(\\mathbf{x},v=0,u=0,\\overline{\\mathbf{W}},\\mathbf{r})</span>.</p>

    <p class="text-gray-300">Let <span class="math">\\mathcal{A},\\mathsf{P}^{<em>}</span> be PPT adversaries and let <span class="math">(\\mathtt{x},\\mathtt{st})\\leftarrow\\mathcal{A}(\\mathsf{pp})</span>. Let <span class="math">\\tau</span> be a transcript of the interaction between <span class="math">\\mathsf{P}^{</em>}</span> and <span class="math">\\mathsf{V}</span>, with input <span class="math">(\\mathsf{pp},\\mathtt{x},\\mathtt{st})</span>, following Protocol 5. Let <span class="math">(\\mathtt{x}^{\\prime};\\mathtt{w}^{\\prime})</span> be the output of this interaction. It is not hard to see that <span class="math">\\varphi(\\mathsf{pp},\\mathtt{x},\\tau)</span> is the output that <span class="math">\\mathsf{V}</span> obtains from the interaction with <span class="math">\\mathsf{P}^{*}</span>, following Protocol 5, after having received <span class="math">\\overline{\\mathbf{W}}</span> and sampled <span class="math">\\mathbf{r}</span>.</p>

    <p class="text-gray-300">Perfect completeness. Let <span class="math">\\mathcal{A}</span> be a PPT adversary and let <span class="math">(\\mathtt{x},\\mathtt{w})\\leftarrow\\mathcal{A}(\\mathsf{pp})</span> be such that <span class="math">(\\mathsf{pp},\\mathtt{x};\\mathtt{w})\\in\\mathbf{R}_{\\mathtt{R1CS}}</span>. Write <span class="math">\\mathtt{x}=(\\mathbf{x})</span> and <span class="math">\\mathtt{w}=(\\mathbf{W})</span>. Let <span class="math">\\mathtt{x}^{\\prime}=(\\mathbf{x},v=0,u=0,\\overline{\\mathbf{W}},\\mathbf{r})</span> and <span class="math">\\mathtt{w}^{\\prime}=(\\mathbf{W},\\mathbf{E}=\\mathbf{0},s)</span> be the outputs of <span class="math">\\mathsf{V}</span> and <span class="math">\\mathsf{P}</span> after honestly executing Protocol 5 with inputs <span class="math">(\\mathsf{pp},\\mathtt{x};\\mathtt{w})</span>. We have that <span class="math">(\\mathsf{pp},\\mathtt{x}^{\\prime};\\mathtt{w}^{\\prime})</span> is in <span class="math">\\mathbf{R}_{\\mathtt{acc}}</span>. Indeed, the relation <span class="math">(A\\cdot\\mathbf{Z})\\circ(B\\cdot\\mathbf{Z})=u\\cdot(C\\cdot\\mathbf{Z})+\\mathbf{E}</span> is satisfied by the choice of <span class="math">\\mathbf{E}=\\mathbf{0}</span>, <span class="math">u=1</span>, and by the fact that <span class="math">(\\mathsf{pp},\\mathtt{x};\\mathtt{w})\\in\\mathbf{R}_{\\mathtt{R1CS}}</span>. Moreover, since <span class="math">\\mathbf{E}=0</span>, also <span class="math">\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}</a>=v=0</span>.</p>

    <p class="text-gray-300">Knowledge soundness. Let <span class="math">\\mathcal{A}</span> and <span class="math">\\mathsf{P}^{*}</span> be expected polynomial-time adversaries. Let <span class="math">(\\mathtt{x},\\mathtt{st})\\leftarrow\\mathcal{A}(\\mathsf{pp})</span>, with <span class="math">\\mathtt{x}=(\\mathbf{x})</span>.</p>

    <p class="text-gray-300">Fix the notation <span class="math">\\langle\\mathsf{P}^{<em>},\\mathsf{V}\\rangle=\\langle\\mathsf{P}^{</em>},\\mathsf{V}\\rangle(\\mathsf{pp},\\mathtt{x},\\mathtt{st})</span>, i.e. <span class="math">\\langle\\mathsf{P}^{<em>},\\mathsf{V}\\rangle</span> is interactive protocol in which <span class="math">\\mathsf{P}^{</em>}</span> and <span class="math">\\mathsf{V}</span> interact following Protocol 3, with inputs <span class="math">\\mathsf{pp},\\mathtt{x}</span> and <span class="math">\\mathtt{st}</span>. We also look at <span class="math">\\langle\\mathsf{P}^{<em>},\\mathsf{V}\\rangle</span> as a random variable modelling the output of such interaction. Let <span class="math">\\varepsilon_{\\mathsf{total}}=\\mathsf{Pr}[(\\mathsf{pp},\\langle\\mathsf{P}^{</em>},\\mathsf{V}\\rangle(\\mathsf{pp},\\mathtt{x},\\mathtt{st}))\\in\\mathbf{R}_{\\mathtt{acc}}]</span>.</p>

    <p class="text-gray-300">The extractor <span class="math">\\mathsf{Ext}</span> proceeds as follows. <span class="math">\\mathsf{Ext}</span> receives <span class="math">(\\mathsf{pp},\\mathtt{x},\\mathtt{st})</span> as inputs. Then:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Step 1. <span class="math">\\mathsf{Ext}</span> runs the protocol <span class="math">\\langle\\mathsf{P}^{*},\\mathsf{V}\\rangle</span> once. Let <span class="math">(\\mathsf{pp},\\mathtt{x}^{(1)};\\mathtt{w}^{(1)})</span> be the output of this interaction. If <span class="math">(\\mathsf{pp},\\mathtt{x}^{(1)};\\mathtt{w}^{(1)})\\not\\in\\mathbf{R}_{\\mathtt{acc}}</span>, then <span class="math">\\mathsf{Ext}</span> aborts. Otherwise, <span class="math">(\\mathsf{pp},\\mathtt{x}^{(1)};\\mathtt{w}^{(1)})\\in\\mathbf{R}_{\\mathtt{acc}}</span>. Say we have</li>

    </ul>

    <p class="text-gray-300"><span class="math">\\mathtt{x}^{(1)}=(\\mathbf{x},v^{(1)}=0,u^{(1)}=1,\\overline{\\mathbf{W}},\\mathbf{r}^{(1)}),\\quad\\mathtt{w}^{(1)}=(\\mathbf{W}^{(1)},\\mathbf{E}^{(1)},s^{(1)}).</span></p>

    <p class="text-gray-300">If <span class="math">\\mathbf{E}^{(1)}=\\mathbf{0}</span>, then <span class="math">\\mathsf{Ext}</span> terminates and outputs <span class="math">\\mathbf{W}^{(1)}</span>.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Step 2. Next, <span class="math">\\mathsf{Ext}</span> repeatedly runs <span class="math">\\langle\\mathsf{P}^{<em>},\\mathsf{V}\\rangle</span>, keeping always the same first message sent by <span class="math">\\mathsf{P}^{</em>}</span> to be <span class="math">\\overline{\\mathbf{W}}</span>. To do so, <span class="math">\\mathsf{Ext}</span> rewinds <span class="math">\\mathsf{P}^{<em>}</span> only to the point where <span class="math">\\mathsf{P}^{</em>}</span> has already sent <span class="math">\\overline{\\mathbf{W}}</span>.</li>

    </ul>

    <p class="text-gray-300">As soon as <span class="math">\\mathsf{Ext}</span> obtains an output <span class="math">(\\mathsf{pp},\\mathtt{x}^{(2)};\\mathtt{w}^{(2)})\\in\\mathbf{R}_{\\mathtt{acc}}</span>, <span class="math">\\mathsf{Ext}</span> terminates and outputs <span class="math">\\mathbf{W}^{(1)}</span>.</p>

    <p class="text-gray-300">Let <span class="math">\\mathcal{E}_{\\overline{\\mathbf{W}}}</span> be the event that <span class="math">\\mathsf{P}^{<em>}</span>’s first message in Step 1 of <span class="math">\\mathsf{Ext}</span> is <span class="math">\\overline{\\mathbf{W}}</span>. Fix one such first message <span class="math">\\overline{\\mathbf{W}}</span>, and let <span class="math">\\varepsilon</span> be the probability that <span class="math">(\\mathsf{pp},\\langle\\mathsf{P}^{</em>},\\mathsf{V}\\rangle(\\mathsf{pp},\\mathtt{x},\\mathtt{st}))\\in\\mathbf{R}_{\\mathtt{acc}}</span>. We next prove that, conditioned on <span class="math">\\mathcal{E}_{\\overline{\\mathbf{W}}}</span>, <span class="math">\\mathsf{Ext}</span> runs in expected polynomial time and outputs a valid witness for (<span class="math">\\mathtt{x}</span>) with probability <span class="math">\\varepsilon-\\mathsf{negl}(\\lambda)</span>. For readability purposes, we avoid for now referring to <span class="math">\\overline{\\mathbf{W}}</span> in our notation. In what follows, unless stated otherwise, we consider all probabilities and events referring to <span class="math">\\mathsf{Ext}</span> as conditioned on <span class="math">\\mathcal{E}_{\\overline{\\mathbf{W}}}</span>.</p>

    <p class="text-gray-300">First of all, we prove that <span class="math">\\mathsf{Ext}</span> terminates in expected polynomial time. Notice that <span class="math">\\mathsf{Ext}</span> always runs <span class="math">\\langle\\mathsf{P}^{*},\\mathsf{V}\\rangle</span> once in Step 1. Let <span class="math">\\mathcal{E}</span> be the event that <span class="math">\\mathsf{Ext}</span> does not abort in</p>

    <p class="text-gray-300">Step 1, which happens with probability <span class="math">\\varepsilon</span>. Denote by <span class="math">Z</span> the random variable (over <span class="math">\\mathsf{Ext}</span>’s random coins) representing the number of times <span class="math">\\mathsf{Ext}</span> runs the interaction <span class="math">\\langle\\mathsf{P}^{<em>},\\mathsf{V}\\rangle</span> in Step 2 (if <span class="math">\\mathbf{E}^{(1)}=\\mathbf{0}</span> and, thus, <span class="math">\\mathsf{Ext}</span> terminated, then <span class="math">Z=0</span>). Let <span class="math">Z_{\\mathsf{total}}</span> be the random variable (over <span class="math">\\mathsf{Ext}</span>’s random coins) representing the total number of times <span class="math">\\langle\\mathsf{P}^{</em>},\\mathsf{V}\\rangle</span> is run when executing <span class="math">\\mathsf{Ext}</span>. We have</p>

    <p class="text-gray-300"><span class="math">\\mathbb{E}[Z_{\\mathsf{total}}]=1+\\Pr[\\mathcal{E}]\\cdot\\mathbb{E}[Z\\mid\\mathcal{E}]\\leq 1+\\varepsilon\\cdot\\frac{1}{\\varepsilon}=2.</span> (9)</p>

    <p class="text-gray-300">Hence, <span class="math">\\mathsf{Ext}</span> runs in expected polynomial time, and it does not abort with probability at least <span class="math">\\varepsilon</span>.</p>

    <p class="text-gray-300">Let <span class="math">\\mathcal{E}_{\\mathsf{binding}}</span> be the event that <span class="math">\\mathsf{Ext}</span>, on inputs <span class="math">(\\mathsf{pp}_{\\mathsf{Com}},\\mathtt{x},\\mathtt{st})</span> is not able to break the binding property of the commitment scheme <span class="math">\\mathsf{Com}</span>, i.e. <span class="math">\\mathcal{E}_{\\mathsf{binding}}</span> is the event that at no point <span class="math">\\mathsf{Ext}</span> has computed two vectors <span class="math">\\mathbf{U}_{1},\\mathbf{U}_{2}</span> and elements <span class="math">s_{1},s_{2}</span> such that <span class="math">\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{U}_{1},s_{1})=\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{U}_{2},s_{2})</span>. We have <span class="math">\\Pr[\\mathcal{E}_{\\mathsf{binding}}]=1-\\mathsf{negl}(\\lambda)</span>. Let <span class="math">\\mathcal{E}_{\\mathsf{zero}}</span> be the event that both <span class="math">\\mathcal{E}</span> occurs and the vector <span class="math">\\mathbf{E}^{(1)}</span> output at the end of Step 1 satisfies <span class="math">\\mathbf{E}^{(1)}=\\mathbf{0}</span>. Similarly, let <span class="math">\\mathcal{E}_{\\mathsf{nonzero}}</span> be the event that <span class="math">\\mathcal{E}</span> occurs and <span class="math">\\mathbf{E}^{(1)}\\neq\\mathbf{0}</span>. Now, conditioning on <span class="math">\\mathcal{E}</span> occurring, these two events are complementary and mutually exclusive. Hence, by the law of total expectation,</p>

    <p class="text-gray-300"><span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}]=\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{nonzero}}\\text{ or }\\mathcal{E}_{\\mathsf{zero}}]</span> (10) <span class="math">=</span> <span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{nonzero}}]\\Pr[\\mathcal{E}_{\\mathsf{nonzero}}]+\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{zero}}]\\Pr[\\mathcal{E}_{\\mathsf{zero}}]</span> <span class="math">=</span> <span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{nonzero}}]\\varepsilon_{1}+\\mathbb{E}<a href="1-\\varepsilon_{1}">Z\\mid\\mathcal{E}_{\\mathsf{zero}}</a>,</span></p>

    <p class="text-gray-300">where <span class="math">\\varepsilon_{1}=\\Pr[\\mathcal{E}_{\\mathsf{nonzero}}]</span>. Note that <span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{zero}}]=0</span>, because if, at the end of Step 1, <span class="math">\\mathbf{E}^{(1)}=\\mathbf{0}</span>, then the extractor terminates. We now make the following claim:</p>

    <h6 id="sec-24" class="text-base font-medium mt-4">Claim 4.2.</h6>

    <p class="text-gray-300"><span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{nonzero}},\\mathcal{E}_{\\mathsf{binding}}]^{-1}=\\mathsf{negl}(\\lambda)</span>.</p>

    <p class="text-gray-300">Assume Claim 4.2 is true for now. We next argue that <span class="math">\\varepsilon_{1}=\\mathsf{negl}(\\lambda)</span>. Note that this will complete the proof (barring the proof of Claim 4.2)that <span class="math">\\mathsf{Ext}</span> runs in PPT time and outputs a valid witness with probability <span class="math">\\varepsilon-\\mathsf{negl}(\\lambda)</span>, conditioned on <span class="math">\\mathsf{P}^{*}</span>’s first message being <span class="math">\\bar{\\mathbf{W}}</span>, i.e. conditioned on the event <span class="math">\\mathcal{E}_{\\mathbf{W}}</span>. Indeed, if <span class="math">\\mathsf{Ext}</span> does not abort and <span class="math">\\mathbf{E}^{(1)}=\\mathbf{0}</span>, then clearly <span class="math">(\\mathsf{pp},\\mathtt{x}^{(1)};\\mathtt{w}^{(1)})\\in\\mathbf{R}_{\\mathsf{RICS}}</span>. Further, we have already argued that <span class="math">\\mathsf{Ext}</span> runs in expected polynomial time, and that it does not abort with probability at least <span class="math">\\varepsilon</span>. If, additionally, the probability that <span class="math">\\mathsf{Ext}</span> does not abort and <span class="math">\\mathbf{E}^{(1)}\\neq\\mathbf{0}</span> is negligible, i.e. <span class="math">\\varepsilon_{1}=\\mathsf{negl}(\\lambda)</span>, then we conclude that <span class="math">\\mathsf{Ext}</span> is a PPT algorithm that outputs a valid witness for <span class="math">\\mathtt{x}</span> with probability <span class="math">\\varepsilon-\\mathsf{negl}(\\lambda)</span>.</p>

    <p class="text-gray-300">We now prove that <span class="math">\\varepsilon_{1}=\\mathsf{negl}(\\lambda)</span>, assuming that Claim 4.2 is true. Indeed, plugging (10) into (9), and using <span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{zero}}]=0</span>, we obtain</p>

    <p class="text-gray-300"><span class="math">2\\geq\\mathbb{E}[Z_{\\mathsf{total}}]=1+\\varepsilon\\mathbb{E}[Z\\mid\\mathcal{E}]=1+\\varepsilon\\varepsilon_{1}\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{nonzero}}]</span> (11)</p>

    <p class="text-gray-300">Using again the law of total expectation,</p>

    <p class="text-gray-300"><span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{nonzero}}]</span> (12) <span class="math">=</span> <span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{nonzero}},\\mathcal{E}_{\\mathsf{binding}}]\\Pr[\\mathcal{E}_{\\mathsf{binding}}]+\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{nonzero}},\\neg\\mathcal{E}_{\\mathsf{binding}}]\\Pr[\\neg\\mathcal{E}_{\\mathsf{binding}}]</span> <span class="math">\\geq</span> <span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{nonzero}},\\mathcal{E}_{\\mathsf{binding}}]\\Pr[\\mathcal{E}_{\\mathsf{binding}}].</span></p>

    <p class="text-gray-300">######</p>

    <p class="text-gray-300">Note that <span class="math">\\varepsilon_{1}\\leq\\varepsilon</span>, since <span class="math">\\varepsilon_{1}</span> is the probability that <span class="math">\\mathsf{Ext}</span> does not abort at Step 1, which occurs with probability <span class="math">\\varepsilon</span>, and, additionally, <span class="math">\\mathbf{E}^{(1)}\\neq 0</span>. Hence from (11), (12), and Claim 4.2 we obtain</p>

    <p class="text-gray-300"><span class="math">\\varepsilon_{1}^{2}\\leq\\varepsilon_{1}\\varepsilon\\leq\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{nonzero}},\\mathcal{E}_{\\mathsf{binding}}]^{-1}\\mathsf{Pr}[\\mathcal{E}_{\\mathsf{binding}}]^{-1}=\\mathsf{negl}(\\lambda),</span></p>

    <p class="text-gray-300">where the last equality follows from Remark 3.1 and the fact that <span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{nonzero}},\\mathcal{E}_{\\mathsf{binding}}]^{-1}=\\mathsf{negl}(\\lambda)</span> and <span class="math">\\mathsf{Pr}[\\mathcal{E}_{\\mathsf{binding}}]=1-\\mathsf{negl}(\\lambda)</span>. This implies that <span class="math">\\varepsilon_{1}=\\mathsf{negl}(\\lambda)</span>, as needed. It only remains to prove Claim 4.2.</p>

    <h6 id="sec-25" class="text-base font-medium mt-4">Proof of Claim 4.2.</h6>

    <p class="text-gray-300">Assume <span class="math">\\mathcal{E}_{\\mathsf{binding}}</span> holds, i.e. <span class="math">\\mathsf{Ext}</span> does not break the binding property of the commitment scheme <span class="math">\\mathsf{Com}</span>. Assume further we run <span class="math">\\mathsf{Ext}</span> up to Step 1 and <span class="math">\\mathcal{E}_{\\mathsf{nonzero}}</span> holds. Then <span class="math">\\mathsf{Ext}</span> does not abort. Let <span class="math">\\mathbf{\\Xi}^{(1)}=(\\mathbf{x},v^{(1)}=0,u^{(1)}=1,\\overline{\\mathbf{W}},\\mathbf{r}^{(1)})</span>, <span class="math">\\mathbf{w}^{(1)}=(\\mathbf{W}^{(1)},\\mathbf{E}^{(1)},s^{(1)})</span> be the output of <span class="math">\\langle\\mathsf{P}^{*},\\mathsf{V}\\rangle</span> at the end of Step 1. By assumption <span class="math">\\mathbf{E}^{(1)}\\neq\\mathbf{0}</span>.</p>

    <p class="text-gray-300">By construction of <span class="math">\\mathsf{Ext}</span>, during Step 2, <span class="math">\\mathsf{Ext}</span> successively repeats an experiment <span class="math">\\Xi</span>, until <span class="math">\\Xi</span> is successful. The experiment <span class="math">\\Xi</span> consists in running <span class="math">\\langle\\mathsf{P}^{<em>},\\mathsf{V}\\rangle</span>, and <span class="math">\\Xi</span> is successful if the output <span class="math">(\\mathbf{\\Xi}^{(2)},\\mathbf{w}^{(2)})</span> of <span class="math">\\langle\\mathsf{P}^{</em>},\\mathsf{V}\\rangle</span> is in <span class="math">\\mathbf{R}_{\\mathsf{acc}}</span>. Importantly, note that the random challenge <span class="math">\\mathbf{r}^{(2)}</span> sent by <span class="math">\\mathsf{V}</span> during this experiment is uniformly random and independent of <span class="math">\\mathbf{E}^{(1)}</span>.</p>

    <p class="text-gray-300">Let <span class="math">\\mathbf{\\Xi}^{(2)}=(\\mathbf{x},v^{(2)}=0,u^{(2)}=1,\\overline{\\mathbf{W}},\\mathbf{r}^{(2)})</span>, <span class="math">\\mathbf{w}^{(2)}=(\\mathbf{W}^{(2)},\\mathbf{E}^{(2)},s^{(2)})</span> be an output of <span class="math">\\langle\\mathsf{P}^{*},\\mathsf{V}\\rangle</span> obtained after running the experiment <span class="math">\\Xi</span>, not necessarily successfully. We argue that if <span class="math">(\\mathsf{pp},\\mathbf{\\Xi}^{(2)},\\mathbf{w}^{(2)})\\in\\mathbf{R}_{\\mathsf{acc}}</span>, then <span class="math">\\mathsf{mle}<a href="\\mathbf{r}^{(2">\\mathbf{E}^{(1)}</a>})=0</span>.</p>

    <p class="text-gray-300">Indeed, assume <span class="math">(\\mathsf{pp},\\mathbf{\\Xi}^{(2)},\\mathbf{w}^{(2)})\\in\\mathbf{R}_{\\mathsf{acc}}</span>, and set <span class="math">\\mathbf{Z}^{(i)}=(\\mathbf{W}^{(i)},\\mathbf{x},1)</span> for <span class="math">i=1,2</span>. Let <span class="math">\\mathbf{E}^{(i)}=(A\\cdot\\mathbf{Z}^{(i)})\\circ(B\\cdot\\mathbf{Z}^{(i)})-(C\\cdot\\mathbf{Z}^{(i)})</span>. Since <span class="math">(\\mathsf{pp},\\mathbf{\\Xi}^{(1)};\\mathbf{w}^{(1)})</span> and <span class="math">(\\mathsf{pp},\\mathbf{\\Xi}^{(2)};\\mathbf{w}^{(2)})</span> are in <span class="math">\\mathbf{R}_{\\mathsf{acc}}</span>, we have</p>

    <p class="text-gray-300"><span class="math">\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{W}^{(1)},s^{(1)})=\\overline{\\mathbf{W}}=\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{W}^{(2)},s^{(2)}).</span></p>

    <p class="text-gray-300">Since we assumed <span class="math">\\mathcal{E}_{\\mathsf{binding}}</span> holds, it must hold that <span class="math">\\mathbf{W}^{(1)}=\\mathbf{W}^{(2)}</span>, and hence also <span class="math">\\mathbf{Z}^{(1)}=\\mathbf{Z}^{(2)}</span>. Using again that <span class="math">(\\mathsf{pp},\\mathbf{\\Xi}^{(1)};\\mathbf{w}^{(1)})</span> and <span class="math">(\\mathsf{pp},\\mathbf{\\Xi}^{(2)};\\mathbf{w}^{(2)})</span> are in <span class="math">\\mathbf{R}_{\\mathsf{acc}}</span>, we have that, for <span class="math">i=1,2</span>,</p>

    <p class="text-gray-300"><span class="math">\\mathbf{E}^{(i)}=(A\\cdot\\mathbf{Z}^{(i)})\\circ(B\\cdot\\mathbf{Z}^{(i)})-(C\\cdot\\mathbf{Z}^{(i)}).</span> (13)</p>

    <p class="text-gray-300">Using twice Eq. (13), the fact that <span class="math">\\mathbf{Z}^{(1)}=\\mathbf{Z}^{(2)}</span>, and that <span class="math">\\mathsf{mle}<a href="\\mathbf{r}^{(2">\\mathbf{E}^{(2)}</a>})=0</span>, we obtain</p>

    <p class="text-gray-300">\\<a href="\\mathbf{r}^{(2"> \\begin{split}\\mathsf{mle}[\\mathbf{E}^{(1)}</a>})&=\\mathsf{mle}<a href="\\mathbf{r}^{(2">(A\\cdot\\mathbf{Z}^{(1)})\\circ(B\\cdot\\mathbf{Z}^{(1)})-(C\\cdot\\mathbf{Z}^{(1)})</a>})\\\\ &=\\mathsf{mle}<a href="\\mathbf{r}^{(2">(A\\cdot\\mathbf{Z}^{(2)})\\circ(B\\cdot\\mathbf{Z}^{(2)})-(C\\cdot\\mathbf{Z}^{(2)})</a>})=\\mathsf{mle}<a href="\\mathbf{r}^{(2">\\mathbf{E}^{(2)}</a>})=0,\\end{split} \\] (14)</p>

    <p class="text-gray-300">as required.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Hence, for <span class="math">\\Xi</span> to be successful, it is necessary that <span class="math">\\mathbf{r}^{(2)}</span> is a root of <span class="math">\\mathsf{mle}<a href="X">\\mathbf{E}^{(1)}</a></span>. Since <span class="math">\\mathbf{E}^{(1)}\\neq\\mathbf{0}</span>, we have <span class="math">\\mathsf{mle}<a href="X">\\mathbf{E}^{(1)}</a>\\neq 0</span>. Then, by Schwartz-Zippel lemma, and because <span class="math">\\mathbf{r}^{(2)}</span> is sampled uniformly at random after <span class="math">\\mathbf{E}^{(1)}</span> is determined, the probability that <span class="math">\\Xi</span> is successful is at most $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">^{-1}=\\mathsf{negl}(\\lambda)<span class="math">. Since </span>\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{nonzero}}]=\\mathsf{Pr}[\\Xi\\text{ is successful}]^{-1}<span class="math">, we conclude that </span>\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{nonzero}}]^{-1}=\\mathsf{negl}(\\lambda)$. This completes the proof of Claim 4.2. ∎</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Recall that <span class="math">\\mathcal{E}_{\\overline{\\mathbf{W}}}</span> denotes the event that <span class="math">\\mathsf{P}^{<em>}</span>’s first message at Step 1 of <span class="math">\\mathsf{Ext}</span> is <span class="math">\\overline{\\mathbf{W}}</span>. We have so far proved that, conditioned on <span class="math">\\mathcal{E}_{\\overline{\\mathbf{W}}}</span>, <span class="math">\\mathsf{Ext}</span> runs in expected polynomial time and outputs a valid witness for <span class="math">\\mathbf{\\Xi}</span> with probability <span class="math">\\varepsilon-\\mathsf{negl}(\\lambda)</span>, where <span class="math">\\varepsilon</span> is the probability that <span class="math">(\\mathsf{pp},\\langle\\mathsf{P}^{</em>},\\mathsf{V}\\rangle)\\in\\mathbf{R}_{\\mathsf{R1CS}}</span>, conditioned on <span class="math">\\mathsf{P}^{*}</span>’s first message being <span class="math">\\overline{\\mathbf{W}}</span>. Let us denote now <span class="math">\\varepsilon</span> by <span class="math">\\varepsilon_{\\overline{\\mathbf{W}}}</span>. Since the aforementioned negligible function <span class="math">\\mathsf{negl}(\\lambda)</span> also depends on <span class="math">\\overline{\\mathbf{W}}</span>, we denote it by <span class="math">\\nu_{\\overline{\\mathbf{W}}}(\\lambda)</span>.</p>

    <p class="text-gray-300">######</p>

    <p class="text-gray-300">Clearly, it follows that <span class="math">\\mathsf{Ext}</span> runs in expected polynomial time, regardless of what is <span class="math">\\mathsf{P}^{<em>}</span>’s first message in Step 1. We prove that, also, <span class="math">\\mathsf{Ext}</span> outputs a valid witness with probability <span class="math">\\varepsilon_{\\mathsf{total}}-\\mathsf{negl}(\\lambda)</span>, no matter what is <span class="math">\\mathsf{P}^{</em>}</span>’s first message.</p>

    <p class="text-gray-300">Consider the function</p>

    <div class="my-4 text-center"><span class="math-block">\\nu(\\lambda) = \\max_{\\overline{\\mathbf{W}}} \\{\\nu_{\\overline{\\mathbf{W}}}(\\lambda)\\},</span></div>

    <p class="text-gray-300">where the maximum is taken over the (finite) set of all possible commitments <span class="math">\\overline{\\mathbf{W}}</span>. Note that <span class="math">\\nu(\\lambda) = \\mathrm{negl}(\\lambda)</span>. Now, by the law of total expectation and what we have proved so far,</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{aligned} \\varepsilon_{\\text{total}} &amp;amp;\\geq \\Pr[(\\mathsf{pp}; \\mathbf{x}, \\mathsf{Ext}(\\mathsf{pp}, \\mathbf{x}, \\mathsf{st}) \\in \\mathbf{R}_{\\mathsf{R1CS}}] \\\\ &amp;amp;= \\sum_{\\overline{\\mathbf{W}}} \\Pr[(\\mathsf{pp}; \\mathbf{x}, \\mathsf{Ext}(\\mathsf{pp}, \\mathbf{x}, \\mathsf{st})) \\in \\mathbf{R}_{\\mathsf{R1CS}} \\mid \\mathcal{E}_{\\overline{\\mathbf{W}}} ] \\cdot \\Pr[\\mathcal{E}_{\\overline{\\mathbf{W}}} ] \\\\ &amp;amp;= \\sum_{\\overline{\\mathbf{W}}} (\\varepsilon_{\\overline{\\mathbf{W}}} - \\nu_{\\overline{\\mathbf{W}}}(\\lambda)) \\cdot \\Pr[\\mathcal{E}_{\\overline{\\mathbf{W}}} ] \\geq \\sum_{\\overline{\\mathbf{W}}} (\\varepsilon_{\\overline{\\mathbf{W}}} - \\nu(\\lambda)) \\cdot \\Pr[\\mathcal{E}_{\\overline{\\mathbf{W}}} ] \\\\ &amp;amp;= \\left(\\sum_{\\overline{\\mathbf{W}}} \\varepsilon_{\\overline{\\mathbf{W}}} \\cdot \\Pr[\\mathcal{E}_{\\overline{\\mathbf{W}}} ]\\right) - \\left(\\nu(\\lambda) \\sum_{\\overline{\\mathbf{W}}} \\Pr[\\mathcal{E}_{\\overline{\\mathbf{W}}} ]\\right) \\\\ &amp;amp;= \\varepsilon_{\\text{total}} - \\nu(\\lambda) \\cdot 1 = \\varepsilon_{\\text{total}} - \\mathrm{negl}(\\lambda), \\end{aligned}</span></div>

    <p class="text-gray-300">where the summations run over all possible commitments <span class="math">\\overline{\\mathbf{W}}</span>. This completes the proof of the lemma.</p>

    <p class="text-gray-300">Remark 4.3 (On the knowledge soundness proof of Lemma 4.5). The proof of knowledge soundness in Lemma 4.1 may look at first glance as being more complex than it needs to be. Here we informally discuss some difficulties in it. First, it is intuitively convincing that the knowledge soundness of Protocol 5 follows from the Schwartz-Zippel lemma. As such, at first glance, it seems like the tree extraction lemma (Lemma 3.5) is a perfect tool for proving that Protocol 5 is knowledge sound: one would simply need to take a tree of accepting transcripts of arity large enough that would force the output error vector <span class="math">\\mathbf{E}^5</span> to be <span class="math">\\mathbf{0}</span>. However, it is not possible to make this argument work if we restrict (as we must) to polynomially sized trees.</p>

    <p class="text-gray-300">The next natural approach is to directly describe an extractor <span class="math">\\mathsf{Ext}</span> that executes a single run of <span class="math">\\langle \\mathsf{P}^<em>, \\mathsf{V} \\rangle</span>. If the output of this interaction is a valid instance-witness <span class="math">(\\mathbf{x}, v, u, \\overline{\\mathbf{W}}. \\mathbf{r}; \\mathbf{W}, \\mathbf{E})</span> for <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span>, then <span class="math">\\mathsf{Ext}</span> simply outputs <span class="math">\\mathbf{W}</span>. One hopes that Schwartz-Zippel lemma yields then that <span class="math">\\mathbf{E} = \\mathbf{0}</span> e.w.n.p. However, this argument does not work because <span class="math">\\mathbf{E}</span> is output at the end of <span class="math">\\langle \\mathsf{P}^</em>, \\mathsf{V} \\rangle</span>, once the random evaluation point <span class="math">\\mathbf{r}</span> is already known. Hence, a priori, <span class="math">\\mathbf{r}</span> is not independent of <span class="math">\\mathbf{E}</span>. It does not seem feasible to use the commitment to <span class="math">\\mathbf{W}</span> in order to argue that <span class="math">\\mathsf{P}^*</span> chose <span class="math">\\mathbf{E}</span> before knowing <span class="math">\\mathbf{r}</span>, at least if we only rely on the binding property of the commitment scheme (and we do not want to add extra assumptions).</p>

    <p class="text-gray-300">The next natural approach is to have the extractor execute <span class="math">\\langle \\mathsf{P}^<em>, \\mathsf{V} \\rangle</span> twice, and only produce an output if both interaction outputs belong to <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span>: the binding property of the commitment scheme forces <span class="math">\\mathsf{P}^</em></span> to use the same <span class="math">\\mathbf{W}</span> and <span class="math">\\mathbf{E}</span> in both outputs, so now the challenge in the second run of <span class="math">\\langle \\mathsf{P}^*, \\mathsf{V} \\rangle</span> is indeed independent of <span class="math">\\mathbf{E}</span>, e.w.n.p. This is mostly correct, but the issue is that now <span class="math">\\mathsf{Ext}</span> has a success probability of <span class="math">\\varepsilon^2</span>, which is too low.</p>

    <p class="text-gray-300">21</p>

    <p class="text-gray-300">We thus naturally arrive at the extractor as described in our proof (or a similar one). One may be tempted to use a simpler argument to the one in our proof, and say that the challenge <span class="math">\\mathbf{r}_{\\textsf{last}}</span> in the output of the last run of <span class="math">\\langle\\mathsf{P}^{<em>},\\mathsf{V}\\rangle</span> is independent of <span class="math">\\mathbf{E}</span>, and then use Schwartz-Zippel lemma to conclude that <span class="math">\\mathbf{E}=\\mathbf{0}</span>. The problem with this argument is that it is not clear whether the random variable counting the number of times that <span class="math">\\langle\\mathsf{P}^{</em>},\\mathsf{V}\\rangle</span> is run is independent of <span class="math">\\mathbf{E}</span> (here by <span class="math">\\mathbf{E}</span> we mean the error term output in Step 1 of our extractor). Since <span class="math">\\mathbf{r}_{\\textsf{last}}</span> depends on this random variable, it is unclear then whether <span class="math">\\mathbf{r}_{\\textsf{last}}</span> is independent of <span class="math">\\mathbf{E}</span>. Our proof avoids making this assumption.</p>

    <h3 id="sec-26" class="text-xl font-semibold mt-8">4.2 Reduction to common evaluation point</h3>

    <p class="text-gray-300">In this section we describe Mova’s reductions of knowledge from <span class="math">\\mathbf{R}_{\\textsf{acc}}\\times\\mathbf{R}_{\\textsf{acc}}</span> to <span class="math">\\mathbf{R}_{\\textsf{equal}}</span>. Recall that <span class="math">\\mathbf{R}_{\\textsf{equal}}</span> was defined as the set of pairs of instance-witness tuples from <span class="math">\\mathbf{R}_{\\textsf{acc}}</span> with the same evaluation point. More precisely,</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\[ \\mathbf{R}_{\\textsf{equal}}=\\left\\{\\left(\\mathsf{pp},(\\mathtt{x}_{1},\\mathtt{x}_{2});(\\mathtt{w}_{1};\\mathtt{w}_{2})\\right)\\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">{\\begin{array}[]{l}(\\mathsf{pp},\\mathtt{x}_{i};\\mathtt{w}_{i})\\in\\mathbf{R}_{\\textsf{acc}},\\ i=1,2,\\\\</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">\\mathtt{x}_{1}.\\mathbf{r}=\\mathtt{x}_{2}.\\mathbf{r}\\end{array}}\\right.\\right\\}. \\]</p>

    <p class="text-gray-300">Our reduction of knowledge (see Protocol 6) is based on a classic technique based on first computing a line <span class="math">\\ell:\\mathbb{F}\\to\\mathbb{F}^{\\log n}</span> passing through the points <span class="math">\\mathbf{x}_{1}.\\mathbf{r},\\mathbf{x}_{2}.\\mathbf{r}</span>, and then using the composition univariate polynomials <span class="math">\\mathsf{mle}[\\mathbf{E}_{1}]\\circ\\ell</span> and <span class="math">\\mathsf{mle}[\\mathbf{E}_{2}]\\circ\\ell</span> in a clever way. The technique, which we call <em>point-vs-line</em>, can be found in Section 4.5.2 of <em>[x23]</em>.</p>

    <p class="text-gray-300">Protocol 6 Mova’s reduction of knowledge <span class="math">\\mathbf{R}_{\\textsf{acc}}\\times\\mathbf{R}_{\\textsf{acc}}\\to\\mathbf{R}_{\\textsf{equal}}</span> Input: <span class="math">\\mathsf{P}</span> receives <span class="math">(\\mathsf{pp},(\\mathtt{x}_{1},\\mathtt{x}_{2});(\\mathtt{w}_{1},\\mathtt{w}_{2}))\\in\\mathbf{R}_{\\textsf{acc}}</span> as input, for <span class="math">i=1,2</span>. <span class="math">\\mathsf{V}</span> receives <span class="math">(\\mathsf{pp},(\\mathtt{x}_{1},\\mathtt{x}_{2}))</span> as input. Let <span class="math">\\mathtt{x}_{i}=(\\mathbf{x}_{i},v_{i},u_{i},\\overline{\\mathbf{W}}_{i},\\mathbf{r}_{i})</span>, <span class="math">\\mathtt{w}_{i}=(\\mathbf{W}_{i},\\mathbf{E}_{i},s_{i})</span>, <span class="math">i=1,2</span>. 1: <span class="math">\\mathsf{P}</span> and <span class="math">\\mathsf{V}</span> define the linear function <span class="math">\\ell:\\mathbb{F}\\to\\mathbb{F}^{\\log m}</span> satisfying <span class="math">\\ell(0)=\\mathbf{r}_{1}</span> and <span class="math">\\ell(1)=\\mathbf{r}_{2}</span>. Then <span class="math">\\mathsf{P}</span> sends <span class="math">\\mathsf{V}</span> the polynomials <span class="math">h_{1}(X),h_{2}(X)</span>, of degree at most <span class="math">\\log m</span>, of the form</p>

    <p class="text-gray-300"><span class="math">h_{i}(X):=\\mathsf{mle}[\\mathbf{E}_{i}]\\circ\\ell(X),\\quad i=1,2.</span> 2: <span class="math">\\mathsf{V}</span> checks that <span class="math">h_{1}(0)=v_{1}</span> and <span class="math">h_{2}(1)=v_{2}</span>, and aborts if this check fails. Then <span class="math">\\mathsf{V}</span> samples a random challenge <span class="math">\\beta\\leftarrow\\mathbb{F}</span> and sends <span class="math">\\beta</span> to <span class="math">\\mathsf{P}</span>. 3: <span class="math">\\mathsf{P}</span> and <span class="math">\\mathsf{V}</span> set <span class="math">v_{1}^{\\prime}=h_{1}(\\beta)</span>, <span class="math">v_{2}^{\\prime}=h_{2}(\\beta)</span> and <span class="math">\\mathbf{r}^{\\prime}=\\ell(\\beta)</span>. 4: <span class="math">\\mathsf{P}</span> and <span class="math">\\mathsf{V}</span> output</p>

    <p class="text-gray-300"><span class="math">\\mathtt{x}_{i}^{\\prime}=(\\mathbf{x}_{i},v_{i}^{\\prime},u_{i},\\overline{\\mathbf{W}}_{i},\\mathbf{r}^{\\prime}),\\quad i=1,2.</span></p>

    <p class="text-gray-300">In addition, <span class="math">\\mathsf{P}</span> outputs the witnesses <span class="math">\\mathtt{w}_{1}^{\\prime},\\mathtt{w}_{2}^{\\prime}</span>, that are defined as</p>

    <p class="text-gray-300"><span class="math">\\mathtt{w}_{i}^{\\prime}=(\\mathbf{W}_{i},\\mathbf{E}_{i},s_{i}),\\quad i=1,2.</span></p>

    <h6 id="sec-27" class="text-base font-medium mt-4">Lemma 4.4.</h6>

    <p class="text-gray-300">Protocol 6 is a reduction of knowledge.</p>

    <h6 id="sec-28" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">The proof is a combination of the ideas used in the proof of Lemma 4.1, together with the proof of the classic point-vs-line argument we have alluded to before. Due to its length, we defer it to Section 7.1. ∎</p>

    <p class="text-gray-300">4.3 Folding two instances in <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span> with the same evaluation point</p>

    <p class="text-gray-300">As the last piece of Mova, in Protocol 7 we describe a reduction of knowledge from <span class="math">\\mathbf{R}_{\\mathrm{equal}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span>.</p>

    <p class="text-gray-300"><strong>Protocol 7</strong> Mova's reduction of knowledge <span class="math">\\mathbf{R}_{\\mathrm{equal}} \\to \\mathbf{R}_{\\mathrm{acc}}</span></p>

    <p class="text-gray-300"><strong>Input:</strong> <span class="math">\\mathsf{P}</span> receives <span class="math">(\\mathsf{pp}, \\mathbf{z}_i; \\mathbf{w}_i) = (\\mathsf{pp}, \\mathbf{x}_i, v_i, u_i, \\overline{\\mathbf{W}}_i, \\mathbf{r}; \\mathbf{W}_i, \\mathbf{E}_i, s_i) \\in \\mathbf{R}_{\\mathrm{acc}}</span> as input, for <span class="math">i = 1, 2</span>. <span class="math">\\mathsf{V}</span> receives <span class="math">(\\mathsf{pp}, \\mathbf{z}_i)</span> as input, for <span class="math">i = 1, 2</span>.</p>

    <p class="text-gray-300">1: <span class="math">\\mathsf{P}</span> computes <span class="math">t \\coloneqq \\mathsf{mle}\\mathbf{T}</span>, where</p>

    <div class="my-4 text-center"><span class="math-block">\\mathbf{T} := (A \\cdot \\mathbf{Z}_1) \\circ (B \\cdot \\mathbf{Z}_2) + (A \\cdot \\mathbf{Z}_2) \\circ (B \\cdot \\mathbf{Z}_1) - u_1 \\cdot (C \\cdot \\mathbf{Z}_2) - u_2 \\cdot (C \\cdot \\mathbf{Z}_1),</span></div>

    <p class="text-gray-300">and <span class="math">\\mathbf{Z}_i := (\\mathbf{W}_i, \\mathbf{x}_i, u_i)</span> for <span class="math">i = 1, 2</span>, and sends it to <span class="math">\\mathsf{V}</span>.</p>

    <p class="text-gray-300">2: <span class="math">\\mathsf{V}</span> samples a random challenge <span class="math">\\alpha \\gets \\mathbb{F}</span> and sends <span class="math">\\alpha</span> to <span class="math">\\mathsf{P}</span>.</p>

    <p class="text-gray-300">3: Both <span class="math">\\mathsf{P}</span> and <span class="math">\\mathsf{V}</span> output <span class="math">\\mathbf{z}_{\\mathrm{acc}} = (\\mathbf{x}, v, u, \\overline{\\mathbf{W}}, \\mathbf{r})</span> where</p>

    <div class="my-4 text-center"><span class="math-block">\\mathbf{x} = \\mathbf{x}_1 + \\alpha \\mathbf{x}_2,</span></div>

    <div class="my-4 text-center"><span class="math-block">v = v_1 + \\alpha t + \\alpha^2 v_2, \\tag{15}</span></div>

    <div class="my-4 text-center"><span class="math-block">u = u_1 + \\alpha u_2,</span></div>

    <div class="my-4 text-center"><span class="math-block">\\overline{\\mathbf{W}} = \\overline{\\mathbf{W}}_1 + \\alpha \\overline{\\mathbf{W}}_2.</span></div>

    <p class="text-gray-300">Additionally, <span class="math">\\mathsf{P}</span> outputs <span class="math">\\mathbf{w}_{\\mathrm{acc}} = (\\mathbf{W}, \\mathbf{E}, s)</span>, where</p>

    <div class="my-4 text-center"><span class="math-block">\\mathbf{W} = \\mathbf{W}_1 + \\alpha \\mathbf{W}_2,</span></div>

    <div class="my-4 text-center"><span class="math-block">\\mathbf{E} = \\mathbf{E}_1 + \\alpha \\mathbf{T} + \\alpha^2 \\mathbf{E}_2, \\tag{16}</span></div>

    <div class="my-4 text-center"><span class="math-block">s = s_1 + \\alpha s_2.</span></div>

    <p class="text-gray-300"><strong>Lemma 4.5.</strong> Protocol 7 is a reduction of knowledge from <span class="math">\\mathbf{R}_{\\mathrm{equal}}</span> to <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span>.</p>

    <p class="text-gray-300"><strong>Proof.</strong> Public reducibility. We construct a deterministic function <span class="math">\\varphi</span>, with input <span class="math">(\\mathsf{pp}, \\mathbf{z}_1, \\mathbf{z}_2, \\tau)</span>, that outputs <span class="math">\\mathbf{z}&#x27;</span>. Let <span class="math">\\mathbf{z}_i = (\\mathbf{x}_i, v_i, u_i, \\overline{\\mathbf{W}}_i, \\mathbf{r})</span>, <span class="math">i = 1, 2</span>, and let <span class="math">\\tau = (t, \\alpha)</span> (if the input has another form the function aborts). The function <span class="math">\\varphi</span> simply outputs <span class="math">(\\mathbf{x}, v, u, \\overline{\\mathbf{W}}, \\mathbf{r})</span>, where <span class="math">\\mathbf{x}, v, u, \\overline{\\mathbf{W}}</span> are computed as in Eq. (15).</p>

    <p class="text-gray-300">Let <span class="math">\\mathcal{A}^<em>, \\mathsf{P}^</em></span> be PPT adversaries and let <span class="math">(\\mathbf{z}_1, \\mathbf{z}_2, \\mathsf{st}) \\gets \\mathcal{A}(\\mathsf{pp})</span>. Let <span class="math">\\tau</span> be a transcript of the interaction between <span class="math">\\mathsf{P}^*</span> and <span class="math">\\mathsf{V}</span>, with input <span class="math">(\\mathsf{pp}, \\mathbf{z}_1, \\mathbf{z}_2, \\mathsf{st})</span>, following Protocol 7. Let <span class="math">(\\mathbf{z}&#x27;, \\mathbf{w}&#x27;)</span> be the output of this interaction. It is not hard to see that the output of <span class="math">\\varphi(\\mathsf{pp}, \\mathbf{z}_1, \\mathbf{z}_2, \\tau)</span> coincides with <span class="math">(\\mathbf{z}&#x27;, \\mathbf{w}&#x27;)</span>.</p>

    <p class="text-gray-300"><strong>Perfect completeness.</strong> Let <span class="math">\\mathcal{A}</span> be a PPT adversary <span class="math">\\mathcal{A}</span> and let <span class="math">(\\mathbf{z}_1, \\mathbf{z}_2; \\mathbf{w}_1, \\mathbf{w}_2), \\leftarrow \\mathcal{A}(\\mathsf{pp})</span> such that <span class="math">(\\mathsf{pp}, \\mathbf{z}_1, \\mathbf{z}_2; \\mathbf{w}_1, \\mathbf{w}_2) \\in \\mathbf{R}_{\\mathrm{equal}}</span>. Write <span class="math">\\mathbf{z}_i = (\\mathbf{x}_i, v_i, u_i, \\overline{\\mathbf{W}}_i, \\mathbf{r})</span>, <span class="math">\\mathbf{w}_i = (\\mathbf{W}_i, \\mathbf{E}_i, s_i)</span>, for <span class="math">i = 1, 2</span>. Set <span class="math">\\mathbf{Z}_i = (\\mathbf{W}_i, \\mathbf{x}_i, u_i)</span> for <span class="math">i = 1, 2</span>.</p>

    <p class="text-gray-300">Let <span class="math">\\mathbf{z} = (\\mathbf{x}, v, u, \\overline{\\mathbf{W}}, \\mathbf{r})</span> and <span class="math">\\mathbf{w} = (\\mathbf{W}, \\mathbf{E}, s)</span> be the outputs of <span class="math">\\mathsf{P}</span> and <span class="math">\\mathsf{V}</span> after honestly following Protocol 7 with inputs <span class="math">(\\mathsf{pp}, \\mathbf{z}_1, \\mathbf{z}_2; \\mathbf{w}_1, \\mathbf{w}_2)</span>. To prove that <span class="math">(\\mathsf{pp}, \\mathbf{z}; \\mathbf{w}) \\in \\mathbf{R}_{\\mathrm{acc}}</span> it suffices to check three properties. First of all that <span class="math">\\mathsf{Com}(\\mathsf{pp}_{\\mathrm{Com}}, \\mathbf{W}, s) = \\overline{\\mathbf{W}}</span>, then that <span class="math">\\mathsf{mle}\\mathbf{E} = v</span> and finally that</p>

    <div class="my-4 text-center"><span class="math-block">(A \\cdot \\mathbf{Z}) \\circ (B \\cdot \\mathbf{Z}) = u \\cdot (C \\cdot \\mathbf{Z}) + \\mathbf{E}. \\tag{17}</span></div>

    <p class="text-gray-300">The first property is immediate by the definition of <span class="math">\\overline{\\mathbf{W}}</span> and <span class="math">\\mathbf{W}</span> in Eqs. (15) and (16) respectively, and using that the commitment scheme <span class="math">\\mathsf{Com}</span> is additively homomorphic. Similarly, using that <span class="math">\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}_{i}</a>=v_{i}</span> for <span class="math">i=1,2</span>, that <span class="math">t=\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{T}</a></span> and the definition of <span class="math">\\mathbf{E}</span> in Eq. (16), we directly obtain that <span class="math">\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}</a>=v</span>.</p>

    <p class="text-gray-300">We now want to prove Eq. (17). The proof of this equality is analogous to the proof of Lemma 6 of <em>[x10]</em>. Expanding the terms of Eq. (17), we obtain</p>

    <p class="text-gray-300"><span class="math">(A\\cdot(\\mathbf{Z}_{1}+\\alpha\\mathbf{Z}_{2}))\\circ(B\\cdot(\\mathbf{Z}_{1}+\\alpha\\mathbf{Z}_{2}))=(u_{1}+\\alpha u_{2})\\cdot(C\\cdot(\\mathbf{Z}_{1}+\\alpha\\mathbf{Z}_{2}))+\\mathbf{E}</span></p>

    <p class="text-gray-300">and by distributing and reordering we obtain</p>

    <p class="text-gray-300"><span class="math">\\mathbf{E}=</span> <span class="math">(A\\cdot\\mathbf{Z}_{1})\\circ(B\\cdot\\mathbf{Z}_{1})+\\alpha[(A\\cdot\\mathbf{Z}_{1})\\circ(B\\cdot\\mathbf{Z}_{2})+(A\\cdot\\mathbf{Z}_{2})\\circ(B\\cdot\\mathbf{Z}_{1})]</span> <span class="math">+\\alpha^{2}(A\\cdot\\mathbf{Z}_{2})\\circ(B\\cdot\\mathbf{Z}_{2})-u_{1}\\cdot(C\\cdot\\mathbf{Z}_{1})-\\alpha(u_{1}\\cdot(C\\cdot\\mathbf{Z}_{2})+u_{2}\\cdot(C\\cdot\\mathbf{Z}_{1}))</span> (18) <span class="math">-\\alpha^{2}u_{2}\\cdot(C\\cdot\\mathbf{Z}_{2}).</span></p>

    <p class="text-gray-300">By construction, we have the equality</p>

    <p class="text-gray-300"><span class="math">\\mathbf{T}=(A\\cdot\\mathbf{Z}_{1})\\circ(B\\cdot\\mathbf{Z}_{2})+(A\\cdot\\mathbf{Z}_{2})\\circ(B\\cdot\\mathbf{Z}_{1})-u_{1}\\cdot(C\\cdot\\mathbf{Z}_{2})-u_{2}\\cdot(C\\cdot\\mathbf{Z}_{1}).</span></p>

    <p class="text-gray-300">Now by hypothesis <span class="math">((\\mathsf{pp},\\mathbbm{x}_{1};\\mathbbm{w}_{1}),(\\mathsf{pp},\\mathbbm{x}_{2};\\mathbbm{w}_{2}))\\in\\mathbf{R}_{\\mathsf{equal}}</span>, and in particular</p>

    <p class="text-gray-300"><span class="math">(A\\cdot\\mathbf{Z}_{1})\\circ(B\\cdot\\mathbf{Z}_{1})-u_{1}\\cdot(C\\cdot\\mathbf{Z}_{1})</span> <span class="math">=\\mathbf{E}_{1},</span> <span class="math">(A\\cdot\\mathbf{Z}_{2})\\circ(B\\cdot\\mathbf{Z}_{2})-u_{2}\\cdot(C\\cdot\\mathbf{Z}_{2})</span> <span class="math">=\\mathbf{E}_{2}.</span></p>

    <p class="text-gray-300">Using these equalities, via direct substitution in Eq. (18) we have that Eq. (17) is true if and only if</p>

    <p class="text-gray-300"><span class="math">\\mathbf{E}=\\mathbf{E}_{1}+\\alpha\\mathbf{T}+\\alpha^{2}\\mathbf{E}_{2},</span></p>

    <p class="text-gray-300">which is satisfied by the definition of <span class="math">\\mathbf{E}</span> in Eq. (16).</p>

    <h4 id="sec-29" class="text-lg font-semibold mt-6">Knowledge soundness.</h4>

    <p class="text-gray-300">The proof is similar to the knowledge soundness proof of Nova <em>[x10]</em>. We prove knowledge soundness by means of Lemma 3.5.</p>

    <p class="text-gray-300">Precisely, let <span class="math">(\\mathbbm{x}_{1},\\mathbbm{x}_{2})</span> be an instance, where <span class="math">\\mathbbm{x}_{1}=(\\mathbf{x}_{1},v_{1},u_{1},\\overline{\\mathbf{W}}_{1},\\mathbf{r})</span> and <span class="math">\\mathbbm{x}_{2}=(\\mathbf{x}_{2},v_{2},u_{2},\\overline{\\mathbf{W}}_{2},\\mathbf{r})</span>. Suppose we are given three transcripts <span class="math">\\tau^{(1)},\\tau^{(2)},\\tau^{(3)}</span> and corresponding outputs</p>

    <p class="text-gray-300"><span class="math">(\\mathbbm{x}^{(1)},\\mathbbm{w}^{(1)}),(\\mathbbm{x}^{(2)},\\mathbbm{w}^{(2)}),(\\mathbbm{x}^{(3)},\\mathbbm{w}^{(3)})</span></p>

    <p class="text-gray-300">of the interaction between <span class="math">\\mathsf{P}</span> and <span class="math">\\mathsf{V}</span> for the input <span class="math">(\\mathsf{pp},\\mathbbm{x}_{1},\\mathbbm{x}_{2})</span>. Assume the three transcripts all share the same first message <span class="math">t</span>, so that, for <span class="math">i=1,2,3</span>, <span class="math">\\tau^{(i)}=(t,\\alpha^{(i)})</span>. For each <span class="math">i=1,2,3</span>, assume <span class="math">(\\mathsf{pp},\\mathbbm{x}^{(i)};\\mathbbm{w}^{(i)})\\in\\mathbf{R}_{\\mathsf{acc}}</span>, and let</p>

    <p class="text-gray-300"><span class="math">(\\mathbbm{x}^{(i)},\\mathbbm{w}^{(i)})=(\\mathbf{x}^{(i)},v^{(i)},u^{(i)},\\overline{\\mathbf{W}}^{(i)},\\mathbf{r};\\mathbf{W}^{(i)},\\mathbf{E}^{(i)},s^{(i)})</span></p>

    <p class="text-gray-300">be the output of the interaction between <span class="math">\\mathsf{P}</span> and <span class="math">\\mathsf{V}</span> at the end of</p>

    <p class="text-gray-300">Assume further that the three challenges <span class="math">\\alpha^{(1)},\\alpha^{(2)},\\alpha^{(3)}</span> are pairwise different. These three trancripts and outputs form a tree of depth <span class="math">2</span> and arity <span class="math">3</span> (in the sense of Definition 3.5). We describe a PPT extractor <span class="math">\\mathsf{Ext}</span> that, for any such tree, outputs a valid witness <span class="math">\\mathbbm{w}_{1},\\mathbbm{w}_{2}</span> for <span class="math">\\mathbbm{x}_{1},\\mathbbm{x}_{2}</span>, e.w.n.p.</p>

    <p class="text-gray-300">Before we proceed, we introduce the following terminology: given <span class="math">k,r\\geq 0</span>, vectors <span class="math">\\mathbf{U}^{(1)},\\ldots,\\mathbf{U}^{(k)}\\in\\mathbb{F}^{r}</span>, and pairwise field elements <span class="math">\\beta_{1},\\ldots,\\beta_{k}</span>, by <em>interpolation</em> of the tuples <span class="math">(\\beta^{(1)},\\mathbf{U}^{(1)}),\\ldots,(\\beta^{(k)},\\mathbf{U}^{(k)})</span> we mean the process of finding vectors <span class="math">\\mathbf{V}_{1},\\ldots,\\mathbf{V}_{k}\\in\\mathbb{F}^{r}</span> such that <span class="math">\\mathbf{V}_{1}+\\beta_{i}\\mathbf{V}_{2}+\\ldots+\\beta_{i}^{k-1}\\mathbf{V}_{k}=\\mathbf{U}^{(i)}</span> for all <span class="math">i\\in[k]</span>. This is achieved by, for each coordinate <span class="math">j\\in[r]</span>, using standard interpolation on the points <span class="math">(\\beta^{(1)},\\mathbf{U}^{(1)}[j]),\\ldots,(\\beta^{(k)},\\mathbf{U}^{(k)}[j])</span>, and then taking <span class="math">\\mathbf{V}_{1}[j],\\ldots\\mathbf{V}_{k}[j]</span> be the coefficients of the resulting interpolating polynomial.</p>

    <p class="text-gray-300">We are ready to describe the extractor. First, by interpolating <span class="math">(\\alpha^{(1)},\\mathbf{W}^{(1)}),(\\alpha^{(2)},\\mathbf{W}^{(2)})</span>, <span class="math">\\mathsf{Ext}</span> finds two vectors <span class="math">\\mathbf{W}_{1}^{<em>},\\mathbf{W}_{2}^{</em>}</span> such that</p>

    <p class="text-gray-300"><span class="math">\\mathbf{W}_{1}^{<em>}+\\alpha^{(i)}\\mathbf{W}_{2}^{</em>}=\\mathbf{W}^{(i)}\\quad\\text{ for }\\quad i=1,2.</span> (19)</p>

    <p class="text-gray-300">(note that here we use <span class="math">\\alpha^{(1)}\\neq\\alpha^{(2)}</span>). Similarly, by interpolating the points <span class="math">(\\alpha^{(1)},s^{(1)}),(\\alpha^{(2)},s^{(2)})</span>, <span class="math">\\mathsf{Ext}</span> constructs <span class="math">s_{1}^{<em>},s_{2}^{</em>}</span> satisfying</p>

    <p class="text-gray-300"><span class="math">s_{1}^{<em>}+\\alpha^{(i)}s_{2}^{</em>}=s^{(i)}</span> (20)</p>

    <p class="text-gray-300">for <span class="math">i=1,2</span>. Again via interpolation, but this time using</p>

    <p class="text-gray-300"><span class="math">(\\alpha^{(1)},\\mathbf{E}^{(1)}),(\\alpha^{(2)},\\mathbf{E}^{(2)}),(\\alpha^{(3)},\\mathbf{E}^{(3)}),</span></p>

    <p class="text-gray-300"><span class="math">\\mathsf{Ext}</span> obtains vectors <span class="math">\\mathbf{E}_{1}^{<em>},\\mathbf{T}^{</em>}</span> and <span class="math">\\mathbf{E}_{2}^{*}</span> satisfying</p>

    <p class="text-gray-300"><span class="math">\\mathbf{E}_{1}^{<em>}+\\alpha^{(i)}\\mathbf{T}^{</em>}+(\\alpha^{(i)})^{2}\\mathbf{E}_{2}^{*}=\\mathbf{E}^{(i)}\\quad\\text{ for }\\quad i=1,2,3.</span> (21)</p>

    <p class="text-gray-300">(Similarly as before, here we have used that the challenges <span class="math">\\alpha^{(i)}</span> are pairwise different). The extractor algorithm outputs <span class="math">\\mathtt{w}_{1}^{<em>}=(\\mathbf{W}_{1}^{</em>},\\mathbf{E}_{1}^{<em>},s_{1}^{</em>})</span> and <span class="math">\\mathtt{w}_{2}^{<em>}=(\\mathbf{W}_{2}^{</em>},\\mathbf{E}_{2}^{<em>},s_{2}^{</em>})</span>. To complete the proof, it suffices to check that <span class="math">(\\mathsf{pp},\\mathtt{x}_{1},\\mathtt{x}_{2};\\mathtt{w}_{1}^{<em>},\\mathtt{w}_{2}^{</em>})\\in\\mathbf{R}_{\\mathsf{equal}}</span>.</p>

    <p class="text-gray-300">We claim that <span class="math">\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{W}_{i}^{<em>},s_{i}^{</em>})=\\overline{\\mathbf{W}}_{i}</span> for <span class="math">i=1,2</span>. Indeed, by Eqs. (19) and (20) and the fact that the final output of the transcripts <span class="math">\\tau^{(1)},\\tau^{(2)}</span> is in <span class="math">\\mathbf{R}_{\\mathsf{equal}}</span>, we have for <span class="math">i=1,2</span> that</p>

    <p class="text-gray-300"><span class="math">\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{W}_{1}^{<em>},s_{1}^{</em>})+\\alpha^{(i)}\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{W}_{2}^{<em>},s_{2}^{</em>})</span> <span class="math">=\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{W}_{1}^{<em>}+\\alpha^{(i)}\\mathbf{W}_{2}^{</em>},s_{1}^{<em>}+\\alpha^{(i)}s_{2}^{</em>})</span> <span class="math">=\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{W}^{(i)},s^{(i)})=\\overline{\\mathbf{W}}^{(i)}</span> <span class="math">=\\overline{\\mathbf{W}}_{1}+\\alpha^{(i)}\\overline{\\mathbf{W}}_{2}.</span></p>

    <p class="text-gray-300">The linear polynomials <span class="math">\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{W}_{1}^{<em>},s_{1}^{</em>})+X\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{W}_{2}^{<em>},s_{2}^{</em>})</span> and <span class="math">\\overline{\\mathbf{W}}_{1}+X\\overline{\\mathbf{W}}_{2}</span> take the same value on the two distinct points <span class="math">\\alpha^{(1)}</span> and <span class="math">\\alpha^{(2)}</span>. Therefore they must be the same polynomial, and so</p>

    <p class="text-gray-300">\\[ \\begin{split}\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{W}_{1}^{<em>},s_{1}^{</em>})&=\\overline{\\mathbf{W}}_{1},\\\\ \\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{W}_{2}^{<em>},s_{2}^{</em>})&=\\overline{\\mathbf{W}}_{2}.\\end{split} \\] (22)</p>

    <p class="text-gray-300">Using Eq. (22) we prove that Eq. (19) is valid also for <span class="math">i=3</span>. Indeed</p>

    <p class="text-gray-300"><span class="math">\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{W}_{1}^{<em>}+\\alpha^{(3)}\\mathbf{W}_{2}^{</em>},s_{1}^{<em>}+\\alpha^{(3)}s_{2}^{</em>})</span> <span class="math">=\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{W}_{1}^{<em>},s_{1}^{</em>})+\\alpha^{(3)}\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{W}_{2}^{<em>},s_{2}^{</em>})</span></p>

    <p class="text-gray-300"><span class="math">\\mathbf{W}_{1}</span> <span class="math">=\\mathbf{W}_{1}+\\alpha^{(3)}\\mathbf{W}_{2}</span> <span class="math">=\\mathbf{W}^{(3)}</span> <span class="math">=\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{W}^{(3)},s^{(3)}).</span></p>

    <p class="text-gray-300">By the binding property of <span class="math">\\mathsf{Com}</span>, e.w.n.p., we have</p>

    <p class="text-gray-300"><span class="math">\\mathbf{W}_{1}^{<em>}+\\alpha^{(3)}\\mathbf{W}_{2}^{</em>}=\\mathbf{W}^{(3)}.</span> (23)</p>

    <p class="text-gray-300">We now prove that <span class="math">\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}_{1}^{<em>}</a>=v_{1}</span> and <span class="math">\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}_{2}^{</em>}</a>=v_{2}</span>. Indeed, using Eq. (21) and the fact that the outputs in <span class="math">(\\mathsf{pp};\\mathbf{z}^{(i)},\\mathbf{w}^{(i)})\\in\\mathbf{R}_{\\mathsf{acc}}</span> for <span class="math">i=1,2,3</span>, we have that</p>

    <p class="text-gray-300"><span class="math">\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}_{1}^{<em>}</a>+\\alpha^{(i)}\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{T}^{</em>}</a>+(\\alpha^{(i)})^{2}\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}_{2}^{<em>}</a></span> <span class="math">=\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}_{1}^{</em>}+\\alpha^{(i)}\\mathbf{T}^{<em>}+(\\alpha^{(i)})^{2}\\mathbf{E}_{2}^{</em>}</a></span> <span class="math">=\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}^{(i)}</a>=v^{(i)}=v_{1}+\\alpha^{(i)}t+(\\alpha^{(i)})^{2}v_{2}</span></p>

    <p class="text-gray-300">for <span class="math">i=1,2,3</span>. Again, we look at both the left and right-hand side above as two polynomials of degree <span class="math">2</span> evaluated at <span class="math">\\alpha^{(i)}</span>. Since these polynomials take the same values evaluated at the three distinct values <span class="math">\\alpha^{(1)},\\alpha^{(2)},\\alpha^{(3)}</span>, we obtain that they are the same polynomial, and in particular <span class="math">\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}_{1}^{<em>}</a>=v_{1}</span> and <span class="math">\\mathsf{mle}<a href="\\mathbf{r}">\\mathbf{E}_{2}^{</em>}</a>=v_{2}</span>.</p>

    <p class="text-gray-300">It remains to prove that <span class="math">(A\\cdot\\mathbf{Z}_{i}^{<em>})\\circ(B\\cdot\\mathbf{Z}_{i}^{</em>})-u\\cdot(C\\cdot\\mathbf{Z}_{i}^{<em>})=\\mathbf{E}_{i}^{</em>}</span>, where <span class="math">\\mathbf{Z}_{i}^{<em>}:=(\\mathbf{W}_{i}^{</em>},\\mathbf{x}_{i},u_{i})</span> for <span class="math">i=1,2</span>. Set <span class="math">\\mathbf{Z}^{(i)}:=(\\mathbf{W}^{(i)},\\mathbf{x}^{(i)},u^{(i)})</span> for <span class="math">i=1,2,3</span>. Since <span class="math">(\\mathsf{pp},\\mathbf{z}^{(i)};\\mathbf{w}^{(i)})\\in\\mathbf{R}_{\\mathsf{acc}}</span>, we have that for <span class="math">i=1,2,3</span>,</p>

    <p class="text-gray-300"><span class="math">(A\\cdot\\mathbf{Z}^{(i)})\\circ(B\\cdot\\mathbf{Z}^{(i)})-u\\cdot(C\\cdot\\mathbf{Z}^{(i)})=\\mathbf{E}^{(i)}.</span> (24)</p>

    <p class="text-gray-300">By definition, <span class="math">\\mathbf{x}^{(i)}=\\mathbf{x}_{1}+\\alpha^{(i)}\\mathbf{x}_{2}</span> and <span class="math">u^{(i)}=u_{1}+\\alpha^{(i)}u_{2}</span> for <span class="math">i=1,2,3</span>. Together with Eqs. (19) and (23) we obtain that <span class="math">\\mathbf{Z}^{(i)}=\\mathbf{Z}_{1}^{<em>}+\\alpha^{(i)}\\mathbf{Z}_{2}^{</em>}</span> for <span class="math">i=1,2,3</span>, e.w.n.p. Expanding Eq. (24), and using Eq. (21), we obtain for <span class="math">i=1,2,3</span>, e.w.n.p.,</p>

    <p class="text-gray-300"><span class="math">(A\\cdot\\mathbf{Z}_{1}^{<em>})\\circ(B\\cdot\\mathbf{Z}_{1}^{</em>})+\\alpha^{(i)}[(A\\cdot\\mathbf{Z}_{1}^{<em>})\\circ(B\\cdot\\mathbf{Z}_{2}^{</em>})+(A\\cdot\\mathbf{Z}_{2}^{<em>})\\circ(B\\cdot\\mathbf{Z}_{1}^{</em>})]</span> <span class="math">+(\\alpha^{(i)})^{2}(A\\cdot\\mathbf{Z}_{2}^{<em>})\\circ(B\\cdot\\mathbf{Z}_{2}^{</em>})-u_{1}\\cdot(C\\cdot\\mathbf{Z}_{1}^{<em>})-\\alpha^{(i)}[u_{1}\\cdot(C\\cdot\\mathbf{Z}_{2}^{</em>})+u_{2}\\cdot(C\\cdot\\mathbf{Z}_{1}^{<em>})]</span> <span class="math">-(\\alpha^{(i)})^{2}u_{2}\\cdot(C\\cdot\\mathbf{Z}_{2}^{</em>})=\\mathbf{E}_{1}^{<em>}+\\alpha^{(i)}\\mathbf{T}^{</em>}+(\\alpha^{(i)})^{2}\\mathbf{E}_{2}^{*}.</span></p>

    <p class="text-gray-300">Again, we see both the left hand side and the right hand side of the equality as two polynomials of degree evaluated at <span class="math">\\alpha^{(i)}</span>. Since <span class="math">\\alpha^{(1)},\\alpha^{(2)},\\alpha^{(3)}</span> are pairwise different, the two polynomials must coincide coefficient-wise. Hence, e.w.n.p.,</p>

    <p class="text-gray-300"><span class="math">(A\\cdot\\mathbf{Z}_{1}^{<em>})\\circ(B\\cdot\\mathbf{Z}_{1}^{</em>})-u\\cdot(C\\cdot\\mathbf{Z}_{1}^{<em>})=\\mathbf{E}_{1}^{</em>}</span> <span class="math">(A\\cdot\\mathbf{Z}_{2}^{<em>})\\circ(B\\cdot\\mathbf{Z}_{2}^{</em>})-u\\cdot(C\\cdot\\mathbf{Z}_{2}^{<em>})=\\mathbf{E}_{2}^{</em>}</span></p>

    <p class="text-gray-300">This concludes the proof that <span class="math">(\\mathsf{pp},\\mathbf{z}_{1},\\mathbf{w}_{1}^{<em>}),(\\mathsf{pp},\\mathbf{z}_{2},\\mathbf{w}_{2}^{</em>})\\in\\mathbf{R}_{\\mathsf{acc}}</span>, e.w.n.p. ∎</p>

    <h3 id="sec-30" class="text-xl font-semibold mt-8">4.4 Putting everything together</h3>

    <p class="text-gray-300">Protocol 8 describes the Mova folding scheme in its entirety. It is a simple composition of the previous schemes Protocols 5 to 7.</p>

    <p class="text-gray-300">Input:  <span class="math">\\mathsf{P}</span>  receives  <span class="math">(\\mathsf{pp},\\mathbf{x}_1;\\mathbf{w}_1)\\in \\mathbf{R}_{\\mathsf{R1CS}}</span>  and  <span class="math">(\\mathsf{pp},\\mathbf{x}_2;\\mathbf{w}_2)\\in \\mathbf{R}_{\\mathsf{acc}}</span>  as input.  <span class="math">\\mathsf{V}</span>  receives  <span class="math">(\\mathsf{pp},\\mathbf{x}_1,\\mathbf{x}_2)</span>  as input.</p>

    <p class="text-gray-300">1:  <span class="math">\\mathsf{P}</span>  and  <span class="math">\\mathsf{V}</span>  follow Protocol 5 with input  <span class="math">(\\mathsf{pp},\\mathbf{x}_1;\\mathbf{w}_1)</span>  and  <span class="math">(\\mathsf{pp},\\mathbf{x}_1)</span> , respectively. Let  <span class="math">(\\mathbf{x}_1&#x27;;\\mathbf{w}_1&#x27;)</span>  be the output of this interaction. 2:  <span class="math">\\mathsf{P}</span>  and  <span class="math">\\mathsf{V}</span>  follow Protocol 6 with input  <span class="math">(\\mathsf{pp},\\mathbf{x}_1&#x27;,\\mathbf{x}_2;\\mathbf{w}_1&#x27;,\\mathbf{w}_2)</span> . Let  <span class="math">(\\mathbf{x}_1^{(2)};\\mathbf{w}_1^{(2)}),(\\mathbf{x}_2^{(2)};\\mathbf{w}_2^{(2)})</span>  be the output of this interaction. 3:  <span class="math">\\mathsf{P}</span>  and  <span class="math">\\mathsf{V}</span>  follow Protocol 7, with input  <span class="math">(\\mathsf{pp},\\mathbf{x}_1^{(2)};\\mathbf{w}_1^{(2)}),(\\mathsf{pp},\\mathbf{x}_2^{(2)};\\mathbf{w}_2^{(2)})</span> . Let  <span class="math">(\\mathbf{x}^{(3)};\\mathbf{w}^{(3)})</span>  be the output of this interaction.</p>

    <p class="text-gray-300">Finally,  <span class="math">\\mathsf{P}</span>  and  <span class="math">\\mathsf{V}</span>  output  <span class="math">\\mathbf{x}^{(3)}</span> , and  <span class="math">\\mathsf{P}</span>  additionally outputs  <span class="math">\\mathbf{w}^{(3)}</span> .</p>

    <p class="text-gray-300">Proof. Consider the trivial reduction of knowledge  <span class="math">\\Pi_{\\text{trivial}}</span>  from  <span class="math">\\mathbf{R}_{\\text{acc}}</span>  to  <span class="math">\\mathbf{R}_{\\text{acc}}</span>  in which  <span class="math">\\mathsf{P}</span>  and  <span class="math">\\mathsf{V}</span>  perform no rounds of interaction and simply output their inputs.</p>

    <p class="text-gray-300">By Lemma 4.1, Step 1 is a reduction of knowledge  <span class="math">\\Pi_1</span>  from  <span class="math">\\mathbf{R}_{\\mathsf{R1CS}}</span>  to  <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span> . Step 1 followed by Step 2 is equivalent to the protocol one obtains by composing  <span class="math">\\Pi_1</span>  and  <span class="math">\\Pi_{\\mathrm{trivial}}</span>  in parallel, yielding a reduction of knowledge  <span class="math">\\mathbf{R}_{\\mathsf{R1CS}} \\times \\mathbf{R}_{\\mathrm{acc}} \\rightarrow \\mathbf{R}_{\\mathrm{acc}} \\times \\mathbf{R}_{\\mathrm{acc}}</span> , and then running Step 2, which is a reduction of knowledge from  <span class="math">\\mathbf{R}_{\\mathrm{acc}} \\times \\mathbf{R}_{\\mathrm{acc}}</span>  to  <span class="math">\\mathbf{R}_{\\mathrm{equal}}</span> . By the Parallel and Sequential Composition Theorems 3.3 and 3.4, and by Lemmas 4.1 and 4.4, we obtain that Step 1 followed by Step 2 is a reduction of knowledge from  <span class="math">\\mathbf{R}_{\\mathsf{R1CS}} \\times \\mathbf{R}_{\\mathrm{acc}}</span>  to  <span class="math">\\mathbf{R}_{\\mathrm{equal}}</span> .</p>

    <p class="text-gray-300">Finally, using again the Sequential Composition Theorem 3.3 and Lemma 4.6, we obtain that sequentially performing the above reduction of knowledge, and then running Step 3 results in a reduction of knowledge from  <span class="math">\\mathbf{R}_{\\mathsf{R1CS}} \\times \\mathbf{R}_{\\mathsf{acc}}</span>  to  <span class="math">\\mathbf{R}_{\\mathsf{acc}}</span> .</p>

    <p class="text-gray-300">In this section we present and compare the concrete costs of Mova, Nova [KST21], and Hypernova [KS23b] when applied to the same R1CS structure. The costs are displayed in Table 3. We defer the explanation of how these costs were derived to Section 5.2.</p>

    <p class="text-gray-300">We evaluate the scenario in which the Prover and Verifier are folding an instance-witness pair from  <span class="math">\\mathbf{R}_{\\mathrm{R1CS}}</span>  and an instance-witness pair from  <span class="math">\\mathbf{R}_{\\mathrm{acc}}</span> .</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">We consider different scenarios depending on whether the R1CS witness vector  <span class="math">\\mathbf{W}</span>  has "small" or "large" entries. Here, by "small" we mean that entries in  <span class="math">\\mathbf{W}</span>  all belong to the range  $\\{0,\\dots ,</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{W}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">- 1\\}<span class="math"> . By &quot;large&quot; we mean that  </span>\\mathbf{W}<span class="math">  has entries sampled randomly in  </span>\\mathbb{F}<span class="math"> . In many practical applications,  </span>\\mathbf{W}<span class="math">  does contain small entries, and in this case the cost of committing to it is much lower than if  </span>\\mathbf{W}<span class="math">  contains large entries. Namely, if  </span>\\mathbf{W}<span class="math">  contains small entries, then the cost of committing to  </span>\\mathbf{W}<span class="math">  is roughly equivalent to the cost of computing  </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{W}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math">  group operations [STW23b], which in turn is roughly equivalent to the cost of  </span>15</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{W}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$  field multiplications [zka]. In Table 7 we provide benchmarks that show that this is, roughly, indeed the case. We refer to [STW23b] for further discussion on this topic.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">P</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">V</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Round(s)</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Nova [KST21]</td>

            <td class="px-3 py-2 border-b border-gray-700">3n + 5m F 2 G ops., 2 G exp. Com. vector of m F-elements Com. W</td>

            <td class="px-3 py-2 border-b border-gray-700">2ℓ F 2 G ops., 2 G exp.</td>

            <td class="px-3 py-2 border-b border-gray-700">1</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Hypernova [KS23b]</td>

            <td class="px-3 py-2 border-b border-gray-700">6n + 14m + O(√m) F 1 G op., 1 G exp. Com. W</td>

            <td class="px-3 py-2 border-b border-gray-700">2ℓ + O(log(m)) F 1 G op., 1 G exp.</td>

            <td class="px-3 py-2 border-b border-gray-700">log(m) + O(1)</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Mova (this work)</td>

            <td class="px-3 py-2 border-b border-gray-700">3n + 12m + 3 log(m) F 1 G op., 1 G exp. Com. W</td>

            <td class="px-3 py-2 border-b border-gray-700">2ℓ + 7 log(m) + 5 F 1 G op., 1 G exp.</td>

            <td class="px-3 py-2 border-b border-gray-700">3</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Table 3: Comparison of the concrete costs of Mova, Nova, and Hypernova. In all cases, we consider the same size bounds  <span class="math">m,n,\\ell</span> , meaning that R1CS matrices are in  <span class="math">\\mathbb{F}^{m\\times m}</span>  with  <span class="math">n = \\Omega (m)</span>  nonzero entries, and witnesses are in  <span class="math">\\mathbb{F}^{m - \\ell -1}</span> .</p>

    <p class="text-gray-300">The symbol  <span class="math">\\mathbb{F}</span>  in the Prover  <span class="math">\\mathsf{P}</span>  column indicates the number of field multiplications. In the Verifier  <span class="math">\\mathsf{V}</span>  column,  <span class="math">\\mathbb{F}</span>  indicates the number of field multiplications and additions <span class="math">^6</span> . We write  <span class="math">n</span>  group operations (resp. exponentiations) as  <span class="math">n\\mathbb{G}</span>  ops. (resp.  <span class="math">n\\mathbb{G}</span>  exp.). "Com. vector  <span class="math">m</span>  elements in  <span class="math">\\mathbb{F}</span> " indicates that a vector of size  <span class="math">m</span>  with arbitrarily sized entries in  <span class="math">\\mathbb{F}</span>  must be committed. "Com.  <span class="math">\\mathbf{W}</span> " means that the R1CS witness vector  <span class="math">\\mathbf{W}</span>  must be committed.</p>

    <p class="text-gray-300">In Mova and Nova, the cost of computing  <span class="math">\\mathbf{T}</span>  is of  <span class="math">3n + 2m</span>  field multiplications plus some field additions (see Eq. (25)). If  <span class="math">A, B, C</span>  are binary matrices, this cost can be replaced by  <span class="math">2m</span>  (cf. Section 5.2). In Hypernova, computing all the necessary matrix-vector products costs  <span class="math">6n + m</span>  field multiplications plus some field additions (cf. Section 5.2). If  <span class="math">A, B, C</span>  are binary, then this cost is only  <span class="math">m</span> .</p>

    <p class="text-gray-300">In Table 4 we display the relative speed-up of Mova's Prover with respect to Nova's and Hypernova's Prover. We make this comparison by taking Table 3 and setting different specific choices of parameters.</p>

    <p class="text-gray-300">We assume the commitment scheme is the Pedersen or the KZG scheme over the Pallas curve. We estimate the costs of computing a vector with arbitrarily large entries (such as Nova's cross term  <span class="math">\\mathbf{T}</span> ) using Table 1 in [Hab22]. For example, the cost of committing to a vector of size  <span class="math">m = 2^{16}</span>  is estimated to be  <span class="math">349m</span>  field multiplications in [Hab22]. When  <span class="math">m = 2^{20}</span> , the same source estimates the cost to be roughly  <span class="math">2^8 m</span>  field multiplications. When  <span class="math">\\mathbf{W}</span>  has small entries, we estimate the cost of committing to  <span class="math">\\mathbf{W}</span>  as explained above.</p>

    <p class="text-gray-300">As we see in Section 5.2, each major step in Mova's Prover has a cost of no more than, roughly,  <span class="math">2^{3}m</span>  field multiplications (assuming the matrices  <span class="math">A, B, C</span>  are not very complex), not counting the cost of committing to the R1CS witness  <span class="math">\\mathbf{W}</span> . The dominating costs of Mova are no longer the commitment to  <span class="math">\\mathbf{T}</span>  as in Nova, but either the execution of the point-vs-line reduction of knowledge (Protocol 6), which costs  <span class="math">\\approx 4m</span>  multiplications (Section 5.2), or the computation of  <span class="math">\\mathbf{T}</span>  itself, which costs  <span class="math">\\approx 3n</span>  multiplications, where  <span class="math">n = \\Omega(m)</span>  is a bound on the number of nonzero entries in each matrix  <span class="math">A, B, C</span>  (cf. Eq. (25)).</p>

    <p class="text-gray-300">In Section 5.1 we present our benchmarks for the Prover's runtime in the three schemes. Then, in Section 5.2 we describe how the costs in Table 3 were computed.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">m</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">n</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Binary R1CS matrices?</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">W entries</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Nova Prover Mova Prover</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Hypernova Prover Mova Prover</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">216</td>

            <td class="px-3 py-2 border-b border-gray-700">m</td>

            <td class="px-3 py-2 border-b border-gray-700">Yes</td>

            <td class="px-3 py-2 border-b border-gray-700">Small</td>

            <td class="px-3 py-2 border-b border-gray-700">13.667</td>

            <td class="px-3 py-2 border-b border-gray-700">1.074</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">216</td>

            <td class="px-3 py-2 border-b border-gray-700">m</td>

            <td class="px-3 py-2 border-b border-gray-700">No</td>

            <td class="px-3 py-2 border-b border-gray-700">Small</td>

            <td class="px-3 py-2 border-b border-gray-700">12.400</td>

            <td class="px-3 py-2 border-b border-gray-700">1.167</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">216</td>

            <td class="px-3 py-2 border-b border-gray-700">3m</td>

            <td class="px-3 py-2 border-b border-gray-700">Yes</td>

            <td class="px-3 py-2 border-b border-gray-700">Small</td>

            <td class="px-3 py-2 border-b border-gray-700">13.667</td>

            <td class="px-3 py-2 border-b border-gray-700">1.074</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">216</td>

            <td class="px-3 py-2 border-b border-gray-700">3m</td>

            <td class="px-3 py-2 border-b border-gray-700">No</td>

            <td class="px-3 py-2 border-b border-gray-700">Small</td>

            <td class="px-3 py-2 border-b border-gray-700">10.500</td>

            <td class="px-3 py-2 border-b border-gray-700">1.306</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">220</td>

            <td class="px-3 py-2 border-b border-gray-700">m</td>

            <td class="px-3 py-2 border-b border-gray-700">Yes</td>

            <td class="px-3 py-2 border-b border-gray-700">Small</td>

            <td class="px-3 py-2 border-b border-gray-700">10.222</td>

            <td class="px-3 py-2 border-b border-gray-700">1.074</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">220</td>

            <td class="px-3 py-2 border-b border-gray-700">m</td>

            <td class="px-3 py-2 border-b border-gray-700">No</td>

            <td class="px-3 py-2 border-b border-gray-700">Small</td>

            <td class="px-3 py-2 border-b border-gray-700">9.300</td>

            <td class="px-3 py-2 border-b border-gray-700">1.167</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">220</td>

            <td class="px-3 py-2 border-b border-gray-700">3m</td>

            <td class="px-3 py-2 border-b border-gray-700">Yes</td>

            <td class="px-3 py-2 border-b border-gray-700">Small</td>

            <td class="px-3 py-2 border-b border-gray-700">10.222</td>

            <td class="px-3 py-2 border-b border-gray-700">1.074</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">220</td>

            <td class="px-3 py-2 border-b border-gray-700">3m</td>

            <td class="px-3 py-2 border-b border-gray-700">No</td>

            <td class="px-3 py-2 border-b border-gray-700">Small</td>

            <td class="px-3 py-2 border-b border-gray-700">7.917</td>

            <td class="px-3 py-2 border-b border-gray-700">1.306</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">216</td>

            <td class="px-3 py-2 border-b border-gray-700">m</td>

            <td class="px-3 py-2 border-b border-gray-700">Yes</td>

            <td class="px-3 py-2 border-b border-gray-700">Large</td>

            <td class="px-3 py-2 border-b border-gray-700">1.864</td>

            <td class="px-3 py-2 border-b border-gray-700">1.005</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">216</td>

            <td class="px-3 py-2 border-b border-gray-700">m</td>

            <td class="px-3 py-2 border-b border-gray-700">No</td>

            <td class="px-3 py-2 border-b border-gray-700">Large</td>

            <td class="px-3 py-2 border-b border-gray-700">1.857</td>

            <td class="px-3 py-2 border-b border-gray-700">1.013</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">216</td>

            <td class="px-3 py-2 border-b border-gray-700">3m</td>

            <td class="px-3 py-2 border-b border-gray-700">Yes</td>

            <td class="px-3 py-2 border-b border-gray-700">Large</td>

            <td class="px-3 py-2 border-b border-gray-700">1.864</td>

            <td class="px-3 py-2 border-b border-gray-700">1.005</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">216</td>

            <td class="px-3 py-2 border-b border-gray-700">3m</td>

            <td class="px-3 py-2 border-b border-gray-700">No</td>

            <td class="px-3 py-2 border-b border-gray-700">Large</td>

            <td class="px-3 py-2 border-b border-gray-700">1.844</td>

            <td class="px-3 py-2 border-b border-gray-700">1.027</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">220</td>

            <td class="px-3 py-2 border-b border-gray-700">m</td>

            <td class="px-3 py-2 border-b border-gray-700">Yes</td>

            <td class="px-3 py-2 border-b border-gray-700">Large</td>

            <td class="px-3 py-2 border-b border-gray-700">2.779</td>

            <td class="px-3 py-2 border-b border-gray-700">1.014</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">220</td>

            <td class="px-3 py-2 border-b border-gray-700">m</td>

            <td class="px-3 py-2 border-b border-gray-700">No</td>

            <td class="px-3 py-2 border-b border-gray-700">Large</td>

            <td class="px-3 py-2 border-b border-gray-700">2.741</td>

            <td class="px-3 py-2 border-b border-gray-700">1.035</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">220</td>

            <td class="px-3 py-2 border-b border-gray-700">3m</td>

            <td class="px-3 py-2 border-b border-gray-700">Yes</td>

            <td class="px-3 py-2 border-b border-gray-700">Large</td>

            <td class="px-3 py-2 border-b border-gray-700">2.779</td>

            <td class="px-3 py-2 border-b border-gray-700">1.014</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">220</td>

            <td class="px-3 py-2 border-b border-gray-700">3m</td>

            <td class="px-3 py-2 border-b border-gray-700">No</td>

            <td class="px-3 py-2 border-b border-gray-700">Large</td>

            <td class="px-3 py-2 border-b border-gray-700">2.671</td>

            <td class="px-3 py-2 border-b border-gray-700">1.074</td>

          </tr>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Table 4: Concrete comparison of Mova's Prover with Nova and Hypernova's Prover. The two last columns compare the approximate equivalent field multiplication work of the Provers of both protocols. Larger numbers are better. The third column indicates whether the R1CS  <span class="math">m \\times m</span>  matrices  <span class="math">A, B, C</span>  are binary or not, i.e. whether all their entries belong to  <span class="math">\\{0,1\\}</span> . The 4th column indicates whether  <span class="math">\\mathbf{W}</span>  contains small or large entries. By small and large we mean that all entries are sampled randomly in the sets  $\\{0,\\dots,</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{W}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">- 1\\}<span class="math">  or  </span>\\mathbb{F}<span class="math"> , respectively. As commitment scheme we use the KZG commitment scheme over the Pallas curve. As per Table 1 in [Hab22], the cost of committing to a witness  </span>\\mathbf{W}<span class="math">  with large entries, or to the cross-term  </span>\\mathbf{T}<span class="math"> , is estimated as approximately  </span>349m<span class="math">  and  </span>2^{8}m<span class="math">  field multiplications, for  </span>m = 2^{16}<span class="math">  and  </span>m = 2^{20}<span class="math"> , respectively. When  </span>\\mathbf{W}<span class="math">  contains small entries, we estimate the cost of committing to  </span>\\mathbf{W}<span class="math">  as  </span>15</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{W}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math">  field multiplications. This is equivalent to the cost of 1 group multiplication per entry in  </span>\\mathbf{W}$  [zka, STW23b].</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">In this section we evaluate the performance of our Mova implementation against the Nova and Hypernova implementations in the Sonobe [St] framework. For cryptographic operations, we used Pedersen commitments [Ped91] over the Pallas curve [Hop], and Poseidon [Arka] for cryptographic hashing.</p>

    <p class="text-gray-300">We set the R1CS matrices  <span class="math">A, B, C</span>  as the identity matrices. We populated the vector  <span class="math">\\mathbf{Z}_1</span>  with random numbers of size from 1 to 384 bits, and  <span class="math">\\mathbf{Z}_2</span>  with random numbers of size from 1 to 20 bits. For Nova and Mova, we chose  <span class="math">u</span>  to be a random field element. We computed  <span class="math">\\mathbf{T}</span>  naively through the formula  <span class="math">\\mathbf{T} = A \\cdot \\mathbf{Z}_2 \\circ B \\cdot \\mathbf{Z}_1 + A \\cdot \\mathbf{Z}_1 \\circ B \\cdot \\mathbf{Z}_2 - u_1 C \\mathbf{Z}_2 - u_2 C \\mathbf{Z}_1</span> . In</p>

    <p class="text-gray-300">Section 5.2 we describe a more efficient approach which is roughly twice faster than the naive method. In the previous Tables 3 and 4 we used the optimised method.</p>

    <p class="text-gray-300">The concluding numbers are the results of the average time of 100 runs measured on a 16 GB memory, 16-core Intel i7-11800H laptop CPU restricted to a single core. Table 5 presents the benchmarking results, not including the cost of committing to the R1CS witness vector  <span class="math">\\mathbf{W}</span> . Within parentheses we indicate the costs of the main steps that are unique to each folding scheme. In Table 6 we display the costs of the main common operations in the three schemes.</p>

    <p class="text-gray-300">Table 7 shows the costs of committing to a vector  <span class="math">\\mathbf{V}</span>  of different sizes and with either small entries, or large entries. Finally, Table 8 displays the benchmark results for Mova, Nova, and Hypernova, including the costs of committing to the R1CS witness  <span class="math">\\mathbf{W}</span> . We consider two cases, depending on whether  <span class="math">\\mathbf{W}</span>  contains small or large entries.</p>

    <p class="text-gray-300">The code used for the experiments is publicly available on GitHub.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">m</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Mova (pt-vs-line + MLE eval.)</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Nova (Commit to T)</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Hypernova (Sumcheck)</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">216</td>

            <td class="px-3 py-2 border-b border-gray-700">36.0216 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">400.4406 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">150.0280 ms</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">(10.4999 ms)</td>

            <td class="px-3 py-2 border-b border-gray-700">(375.5840 ms)</td>

            <td class="px-3 py-2 border-b border-gray-700">(106.7538 ms)</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">220</td>

            <td class="px-3 py-2 border-b border-gray-700">761.5190 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">5470.6681 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">3195.2789 ms</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">(223.4527 ms)</td>

            <td class="px-3 py-2 border-b border-gray-700">(4913.5817 ms)</td>

            <td class="px-3 py-2 border-b border-gray-700">(1975.6348 ms)</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Table 5: Comparison of Prover runtimes for a single fold across Nova, Mova, and Hypernova, not including the cost of committing to the witness vector (cf. Table 8 for those costs). In all cells, the time without parentheses indicates the total Proving runtime. In Mova, the time within parentheses indicates the duration of the point-vs-line step (Protocol 6) plus the evaluation time of  <span class="math">\\mathsf{mle}[\\mathbf{T}]</span>  at a point  <span class="math">\\mathbf{r}</span> . For Nova, it shows the time taken to commit to  <span class="math">\\mathbf{T}</span> , and in Hypernova, it reflects the duration of the sumcheck. We remark that Hypernova's implementation does not leverage certain optimizations and thus its runtime does not reflect the concrete costs from Table 3 (cf. Remark 5.2)</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">m</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Computing T</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Miscellaneous</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">216</td>

            <td class="px-3 py-2 border-b border-gray-700">20.7591 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">4.0797 ms</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">220</td>

            <td class="px-3 py-2 border-b border-gray-700">416.5537 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">84.1495 ms</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Table 6: The two main operations that Nova and Mova Provers have in common (besides committing to  <span class="math">\\mathbf{W}</span> ), namely computing the cross-term  <span class="math">\\mathbf{T}</span> , and performing miscellaneous operations such as computing the resulting folded witness and error vectors, and computing their commitments by using the homomorphicity of the commitment scheme.</p>

    <p class="text-gray-300">Remark 5.1 (On the complexity of the R1CS matrices  <span class="math">A, B, C</span> ). The benchmarks were run taking the R1CS matrices  <span class="math">A, B, C</span>  to be the identity matrix. The complexity of these matrices affects the cost of computing the cross-term  <span class="math">\\mathbf{T}</span> , which is a common cost to Mova and Nova, and also affects Hypernova's costs (see Section 5.2). However, unlike in Nova and Hypernova, this computation represents the dominating cost of Mova's Prover work, and so</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">m</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Entries in {0,...,m-1}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Entries in F</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">216</td>

            <td class="px-3 py-2 border-b border-gray-700">28.4877 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">375.5840 ms</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">220</td>

            <td class="px-3 py-2 border-b border-gray-700">554.5014 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">4913.5817 ms</td>

          </tr>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Table 7: Time taken to commit a vector  <span class="math">\\mathbf{V}</span>  with size  $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{V}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">= m<span class="math">  where  </span>m = 2^{16}<span class="math">  or  </span>m = 2^{20}<span class="math"> , and so that either  </span>\\mathbf{V}<span class="math">  contains random entries within the range  </span>\\{0, \\dots,</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{V}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">- 1\\}<span class="math"> , or random entries in  </span>\\mathbb{F}$ . Here we used the Pedersen commitment over the Pallas curve. The Hypernova library we used [St] does not make use of known optimisations such as [DT24].</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">m</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">W entries</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Mova</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Nova</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Hypernova</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">216</td>

            <td class="px-3 py-2 border-b border-gray-700">Small</td>

            <td class="px-3 py-2 border-b border-gray-700">64.5093 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">428.9283 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">178.5157 ms</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">216</td>

            <td class="px-3 py-2 border-b border-gray-700">Large</td>

            <td class="px-3 py-2 border-b border-gray-700">411.6056 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">776.0246 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">525.6120 ms</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">220</td>

            <td class="px-3 py-2 border-b border-gray-700">Small</td>

            <td class="px-3 py-2 border-b border-gray-700">1316.0204 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">6025.1695 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">3749.7803 ms</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">220</td>

            <td class="px-3 py-2 border-b border-gray-700">Large</td>

            <td class="px-3 py-2 border-b border-gray-700">5675.1007 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">10384.2498 ms</td>

            <td class="px-3 py-2 border-b border-gray-700">8108.8606 ms</td>

          </tr>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Table 8: Benchmark of Mova, Nova and Hypernova's Prover runtimes including the cost of committing to the R1CS witness  <span class="math">\\mathbf{W}</span> . We consider two scenarios, one where the entries in  <span class="math">\\mathbf{W}</span>  are "small", and one where the entries are "large". As usual, by "small" we mean that the entries are randomly sampled in the range  $\\{0,\\dots ,</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{W}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">- 1\\}<span class="math"> , and by &quot;large&quot; that the entries are sampled in  </span>\\mathbb{F}$ . The benchmarks follow the same configuration as those in Table 5.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">using more complex matrices  <span class="math">A, B, C</span>  will result in a less favorable comparison for Mova. Table 4 presents cost comparisons for matrices  <span class="math">A, B, C</span>  of varying sparseness.</p>

    <p class="text-gray-300">Remark 5.2 (On Hypernova's benchmark results). As we discuss later in Section 5.2, when estimating the concrete costs of Hypernova's Prover in Table 3, we use [DT24] to bound the cost of the sumcheck protocol as  <span class="math">10m</span> . However, for our benchmarks, we used the less optimised framework [St]. This explains that the benchmarked Hypernova Prover is significantly more expensive than what Table 3 suggest.</p>

    <p class="text-gray-300">In this section we detail how the concrete costs in Table 3 were derived. We first look at the concrete field operation cost in Nova and Mova, then we analyse the concrete field operation cost of the point-vs-line reduction (Protocol 6), and finally we explain how we obtained the concrete field operation cost of the Hypernova protocol applied to R1CS.</p>

    <p class="text-gray-300">Concrete costs of Mova and Nova In both Nova and Mova, the Prover has to compute the cross term</p>

    <div class="my-4 text-center"><span class="math-block">\\mathbf {T} = A \\mathbf {Z} _ {1} \\circ B \\mathbf {Z} _ {2} + A \\mathbf {Z} _ {2} \\circ B \\mathbf {Z} _ {1} - u _ {1} C \\mathbf {Z} _ {2} - u _ {2} C \\mathbf {Z} _ {1}</span></div>

    <p class="text-gray-300">However, note the equality:</p>

    <div class="my-4 text-center"><span class="math-block">A (\\mathbf {Z} _ {1} + \\mathbf {Z} _ {2}) \\circ B (\\mathbf {Z} _ {1} + \\mathbf {Z} _ {2}) - (u _ {1} + u _ {2}) C (\\mathbf {Z} _ {1} + \\mathbf {Z} _ {2}) = \\mathbf {E} _ {1} + \\mathbf {T} + \\mathbf {E} _ {2}</span></div>

    <p class="text-gray-300">This means that the Prover can compute the expression on the left hand side of this equality, and then subtract the error terms. In general, for an  <span class="math">m \\times m</span>  matrix  <span class="math">M</span>  with non-zero rows,</p>

    <p class="text-gray-300">the multiplication of <span class="math">M</span> with a vector in <span class="math">\\mathbb{F}^{m}</span> costs <span class="math">\\sum_{i=1}^{m}(k_{i}^{M}-1)=n-m</span> field additions and <span class="math">\\sum_{i=1}^{m}k_{i}^{M}=n</span> field multiplications, where for all <span class="math">i\\in[m]</span>, <span class="math">k_{i}^{M}&gt;0</span> is the number of non-zero entries of row <span class="math">i</span>, and <span class="math">n=\\Omega(m)</span> is the number of non-zero entries of <span class="math">M</span>. This is because each entry in the matrix-vector product is the result of <span class="math">k_{i}^{M}</span> multiplications, and <span class="math">k_{i}^{M}-1</span> additions. To compute the cross-term <span class="math">\\mathbf{T}</span>, apart from the matrix-vector products, the Prover needs to perform the following operations: compute the vector <span class="math">\\mathbf{Z}_{1}+\\mathbf{Z}_{2}</span>; perform a Hadamard product between vectors of length <span class="math">m</span>; perform a multiplication of a vector of length <span class="math">m</span> by the constant <span class="math">u_{1}+u_{2}</span>; and perform three subtractions of a vector of length <span class="math">m</span>. Therefore, the computation of the cross-term can be done in:</p>

    <p class="text-gray-300">\\[ \\begin{split}4m+\\sum_{i=1}^{m}((k_{i}^{A}-1)+(k_{i}^{B}-1)+(k_{i}^{C}-1))=3n+m,&\\text{field additions, and}\\\\ 2m+\\sum_{i=1}^{m}(k_{i}^{A}+k_{i}^{B}+k_{i}^{C})=3n+2m,&\\text{field multiplications,}\\end{split} \\] (25)</p>

    <p class="text-gray-300">where <span class="math">k_{i}^{A}</span> is the number of non-zero entries in the <span class="math">i</span>-th row of <span class="math">A</span> (similarly for <span class="math">k_{i}^{B},k_{i}^{C}</span>). If all entries in <span class="math">A,B,C</span> are either <span class="math">0</span> or <span class="math">1</span>, then the cost of the matrix-vector products is reduced (there is no need for multiplications), and the total cost is reduced to</p>

    <p class="text-gray-300"><span class="math">3n+m\\text{ additions,}\\quad 2m\\text{ multiplications.}</span> (26)</p>

    <p class="text-gray-300">Additionally, both Mova and Nova’s Prover has to update the witness and error vectors in the last step of the protocol. This costs <span class="math">3m</span> field multiplications (required to compute <span class="math">\\alpha\\mathbf{W}_{2},\\alpha\\mathbf{T},\\alpha^{2}\\mathbf{E}_{2}</span>) and <span class="math">3m</span> field additions. We do not count other negligible costs.</p>

    <p class="text-gray-300">This suffices to count the cost of Nova’s Prover and Verifier. The additional field operation work for Mova’s Prover and Verifier consists of:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>The field operation work in the reduction <span class="math">\\mathbf{R_{acc}}\\times\\mathbf{R_{acc}}\\rightarrow\\mathbf{R_{equal}}</span>. This is the point-vs-line reduction of Section 4.2. The costs are computed below.</li>

      <li>The evaluation of the MLE of the cross-term at a random vector of field elements. This can be done in at most <span class="math">2m</span> field additions and <span class="math">3m</span> field multiplications (see Lemma 3.8 in <em>[x28]</em>).</li>

    </ul>

    <h4 id="sec-36" class="text-lg font-semibold mt-6">Concrete costs of the point-vs-line reduction of knowledge (Protocol 6)</h4>

    <p class="text-gray-300">In the point-vs-line reduction (Section 4.2), the field operation costs are as follows:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>The Prover needs to compute the polynomials <span class="math">h_{1}(X),h_{2}(X)</span>. In Section 6 we describe an algorithm that, in general, allows to compute each <span class="math">h_{i}</span> with <span class="math">4m-2\\log m-4</span> field multiplications and <span class="math">5m-2\\log m-5</span> field additions.</li>

    </ul>

    <p class="text-gray-300">In the application where the Prover and Verifier fold an instance-witness pair from <span class="math">\\mathbf{R_{R1C5}}</span> with an instance-witness pair from <span class="math">\\mathbf{R_{acc}}</span>, the Prover’s costs for computing <span class="math">h_{1}(X)</span> are <span class="math">0</span>. This is because, if <span class="math">\\mathsf{P}</span> is honest, then <span class="math">h_{1}(X)=\\mathsf{mle}[\\mathbf{E}_{1}]\\circ\\ell(X)=0</span> because <span class="math">\\mathbf{E}_{1}=\\mathbf{0}</span> is the zero vector. Hence, the total field multiplication cost of computing <span class="math">h_{1}(X),h_{2}(X)</span> in Mova (Protocol 8) is <span class="math">4m-2\\log m-4</span>.</p>

    <p class="text-gray-300">We note that if the Prover and Verifier were folding two instance-witness pairs from <span class="math">\\mathbf{R_{acc}}</span>, then this cost would double, since then both <span class="math">h_{1}(X),h_{2}(X)</span> are nonzero polynomials.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>The Verifier needs to compute  <span class="math">h_1(0), h_2(1)</span> , the first of which can be read off as the constant coefficient of  <span class="math">h_1</span> , and the second of which is the sum of the coefficients of  <span class="math">h_2</span> . In total, this costs  <span class="math">\\log(m) + 1</span>  field additions and no field multiplications.</li>

      <li>The Prover and Verifier need to compute the evaluations of  <span class="math">h_1, h_2, \\ell</span>  at  <span class="math">\\beta</span> . By using Horner's method, computing  <span class="math">h_1(\\beta), h_2(\\beta)</span>  costs at most  <span class="math">4(\\log(m) + 1)</span>  field multiplications. Computing  <span class="math">\\ell(\\beta)</span>  takes at most  <span class="math">\\log(m)</span>  field additions and  <span class="math">\\log(m)</span>  field multiplications.</li>

    </ul>

    <p class="text-gray-300">All in all, we obtain the concrete cost in Table 9 for the point-vs-line reduction.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">P</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">V</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Rounds</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Point-versus-line (Protocol 6)</td>

            <td class="px-3 py-2 border-b border-gray-700">4m + 3 log(m) F</td>

            <td class="px-3 py-2 border-b border-gray-700">7 log(m) + 5 F</td>

            <td class="px-3 py-2 border-b border-gray-700">1</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Table 9: Concrete cost of the point-versus-line method, in field multiplications for  <span class="math">\\mathsf{P}</span> , and field additions and multiplications for  <span class="math">\\mathsf{V}</span> . Rounds indicate the number of communication rounds in Protocol 6.</p>

    <p class="text-gray-300">Concrete costs of Hypernova We next discuss the costs of Hypernova. We adopt the notation from [KS23b], and we refer to this reference for a full description of the Hypernova protocol.</p>

    <p class="text-gray-300">Hypernova has the Prover perform a sumcheck on the function:</p>

    <div class="my-4 text-center"><span class="math-block">g (\\mathbf {X}) := \\left(\\sum_ {j \\in [ 3 ]} \\gamma^ {j} \\cdot L _ {j} (\\mathbf {X})\\right) + \\gamma^ {4} \\cdot Q (\\mathbf {X})</span></div>

    <p class="text-gray-300">where  <span class="math">\\gamma \\in \\mathbb{F}</span>  is random, and the  <span class="math">L_{j},Q</span>  are defined as:</p>

    <div class="my-4 text-center"><span class="math-block">L _ {1} (\\mathbf {X}) := \\tilde {\\mathbf {e q}} (\\mathbf {r} _ {x}, \\mathbf {X}) \\cdot \\left(\\sum_ {\\mathbf {y} \\in \\mathbb {B} ^ {\\log (m)}} \\widetilde {A} (\\mathbf {X}, \\mathbf {y}) \\cdot \\tilde {z} _ {1} (\\mathbf {y})\\right)</span></div>

    <div class="my-4 text-center"><span class="math-block">L _ {2} (\\mathbf {X}) := \\tilde {\\mathbf {e q}} (\\mathbf {r} _ {x}, \\mathbf {X}) \\cdot \\left(\\sum_ {\\mathbf {y} \\in \\mathbb {B} ^ {\\log (m)}} \\widetilde {B} (\\mathbf {X}, \\mathbf {y}) \\cdot \\tilde {z} _ {1} (\\mathbf {y})\\right)</span></div>

    <div class="my-4 text-center"><span class="math-block">L _ {3} (\\mathbf {X}) := \\tilde {\\mathbf {e q}} (\\mathbf {r} _ {x}, \\mathbf {X}) \\cdot \\left(\\sum_ {\\mathbf {y} \\in \\mathbb {B} ^ {\\log (m)}} \\widetilde {C} (\\mathbf {X}, \\mathbf {y}) \\cdot \\tilde {z} _ {1} (\\mathbf {y})\\right)</span></div>

    <div class="my-4 text-center"><span class="math-block">\\begin{array}{l} Q (\\mathbf {X}) := \\tilde {\\mathbf {e q}} (\\beta , \\mathbf {X}) \\cdot \\left(\\left(\\sum_ {\\mathbf {y} \\in \\mathbb {B} ^ {\\log (m)}} \\widetilde {A} (\\mathbf {X}, \\mathbf {y}) \\cdot \\tilde {z} _ {2} (\\mathbf {y})\\right) \\cdot \\left(\\sum_ {\\mathbf {y} \\in \\mathbb {B} ^ {\\log (m)}} \\widetilde {B} (\\mathbf {X}, \\mathbf {y}) \\cdot \\tilde {z} _ {2} (\\mathbf {y})\\right) \\right. \\\\ \\left. - \\left(\\sum_ {\\mathbf {y} \\in \\mathbb {B} ^ {\\log (m)}} \\widetilde {C} (\\mathbf {X}, \\mathbf {y}) \\cdot \\tilde {z} _ {2} (\\mathbf {y})\\right)\\right) \\\\ \\end{array}</span></div>

    <p class="text-gray-300"><span class="math">\\widetilde{z}_{1}=(\\mathbb{w}_{1},u,\\mathbb{x}_{1}),\\widetilde{z}_{2}=(\\mathbb{w}_{2},1,\\mathbb{x}_{2})</span> are concatenatations of the witnesses for the linearised R1CS instance and the R1CS instance (respectively) with a field element (either <span class="math">u</span> or <span class="math">1</span>), and the public inputs.</p>

    <p class="text-gray-300">In <em>[x10]</em>, Dao and Thaler bound the Prover’s number of field multiplications when performing the sumcheck on a polynomial of the form <span class="math">Q(\\mathbf{X})</span>. They bound this cost as <span class="math">5m</span>. By applying their optimisations to the remaining term <span class="math">\\gamma\\cdot L_{1}(\\mathbf{X})+\\gamma^{2}\\cdot L_{2}(\\mathbf{X})+\\gamma^{3}\\cdot L_{3}(\\mathbf{X})</span>, the Prover can perform the sumcheck on this term in <span class="math">4m+O(\\sqrt{m})</span> field multiplications, with the constant being such that the <span class="math">O(\\sqrt{m})</span> term is well below <span class="math">m</span>. All in all, we estimate that the Prover performs no more than <span class="math">10m</span> field multiplications in total for the sumcheck involving <span class="math">g</span>.</p>

    <p class="text-gray-300">The costs above assume the Prover knows all the evaluations of the sums in the definition of <span class="math">L_{1},L_{2},L_{3},Q</span>.We next explain how Prover can compute these.</p>

    <p class="text-gray-300">Note that for any <span class="math">\\mathbf{x}\\in\\mathbb{B}^{\\log(m)}</span>, the sum <span class="math">\\sum_{\\mathbf{y}\\in\\mathbb{B}^{\\log(m)}}\\widetilde{A}(\\mathbf{x},\\mathbf{y})\\cdot\\tilde{z}_{1}(\\mathbf{y})</span> is equal to the <span class="math">\\mathbf{x}</span>-th coordinate (the coordinate whose index has binary representation <span class="math">\\mathbf{x}</span>) of the product <span class="math">A\\cdot z_{1}</span>, and similarly for <span class="math">B,C</span> and <span class="math">z_{2}</span>. Therefore, the Prover needs to compute <span class="math">A\\cdot z_{1},B\\cdot z_{1},C\\cdot z_{1},A\\cdot z_{2},B\\cdot z_{2},C\\cdot z_{2}</span>. We estimate the cost of these as follows: to compute <span class="math">A\\cdot z_{1}</span>, one needs</p>

    <p class="text-gray-300"><span class="math">\\sum_{i=1}^{m}(k_{i}^{A}-1)=n-m</span></p>

    <p class="text-gray-300">field additions, where <span class="math">k_{i}^{A}</span> is the number of nonzero entries in the <span class="math">i</span>-th row of <span class="math">A</span>, and</p>

    <p class="text-gray-300"><span class="math">\\sum_{i=1}^{m}k_{i}^{A}=n</span></p>

    <p class="text-gray-300">field multiplications. Recall <span class="math">n=\\Omega(m)</span> is the number of nonzero entries in the R1CS matrices. The rest of matrix-vector multiplications can be computed similarly. Overall, the total field operations to compute the necessary matrix vector quantities is <span class="math">6(n-m)</span> field additions, and <span class="math">6n</span> field multiplications. If <span class="math">A,B,C</span> are binary matrices, then there are no multiplications.</p>

    <p class="text-gray-300">The Prover also needs the quantities:</p>

    <p class="text-gray-300"><span class="math">\\sigma_{1}</span> <span class="math">:=\\sum_{\\mathbf{y}\\in\\mathbb{B}^{\\log(m)}}\\widetilde{A}(\\mathbf{r}_{x^{\\prime}},\\mathbf{y})\\cdot\\widetilde{z}_{1}(\\mathbf{y})</span> <span class="math">\\sigma_{2}</span> <span class="math">:=\\sum_{\\mathbf{y}\\in\\mathbb{B}^{\\log(m)}}\\widetilde{B}(\\mathbf{r}_{x^{\\prime}},\\mathbf{y})\\cdot\\widetilde{z}_{1}(\\mathbf{y})</span> <span class="math">\\sigma_{3}</span> <span class="math">:=\\sum_{\\mathbf{y}\\in\\mathbb{B}^{\\log(m)}}\\widetilde{C}(\\mathbf{r}_{x^{\\prime}},\\mathbf{y})\\cdot\\widetilde{z}_{1}(\\mathbf{y})</span> <span class="math">\\theta_{1}</span> <span class="math">:=\\sum_{\\mathbf{y}\\in\\mathbb{B}^{\\log(m)}}\\widetilde{A}(\\mathbf{r}_{x^{\\prime}},\\mathbf{y})\\cdot\\widetilde{z}_{2}(\\mathbf{y})</span> <span class="math">\\theta_{2}</span> <span class="math">:=\\sum_{\\mathbf{y}\\in\\mathbb{B}^{\\log(m)}}\\widetilde{B}(\\mathbf{r}_{x^{\\prime}},\\mathbf{y})\\cdot\\widetilde{z}_{2}(\\mathbf{y})</span> <span class="math">\\theta_{3}</span> <span class="math">:=\\sum_{\\mathbf{y}\\in\\mathbb{B}^{\\log(m)}}\\widetilde{C}(\\mathbf{r}_{x^{\\prime}},\\mathbf{y})\\cdot\\widetilde{z}_{2}(\\mathbf{y})</span></p>

    <p class="text-gray-300"><span class="math">\\mathbf{r}_{x^{\\prime}}</span> is determined during the course of the sumcheck protocol. The Prover already has all <span class="math">\\sigma_{i}</span>’s at the end of sumcheck. It also knows the quantity <span class="math">\\theta_{1}\\cdot\\theta_{2}-\\theta_{3}</span>, so it may only compute two out of the three <span class="math">\\theta_{i}</span>. Because at this point, the Prover already has the products <span class="math">A\\cdot z_{2}</span> and so on, each of the two <span class="math">\\theta_{i}</span> can be obtained by computing the evaluation of the MLE of the corresponding matrix vector product at the random point <span class="math">\\mathbf{r}_{x^{\\prime}}</span>. This can be separated into computing the evaluations of <span class="math">\\widehat{\\mathbf{eq}}(\\mathbf{r}_{x^{\\prime}},\\mathbf{x})</span> for all <span class="math">\\mathbf{x}\\in\\mathbb{B}^{\\log(m)}</span>, and then performing two inner products with the dense representations of the corresponding matrix-vector products. The former costs no more than <span class="math">m</span> field multiplications, and the latter costs <span class="math">2m</span> field multiplications in total. Finally there is the added cost of updating the witness, public inputs, and commitments among other things. Barring negligible costs, this amounts to <span class="math">m</span> field multiplications and <span class="math">m</span> field additions.</p>

    <p class="text-gray-300">The Verifier needs to perform the sumcheck, which costs <span class="math">c\\log(m)</span> field operations (counting both additions and multiplications) for some very small <span class="math">c</span>. The Verifier also computes the quantity:</p>

    <p class="text-gray-300"><span class="math">\\sum_{j\\in[3]}\\gamma^{j}\\cdot e_{1}\\cdot\\sigma_{j}+\\gamma^{4}\\cdot e_{2}\\cdot(\\theta_{1}\\cdot\\theta_{2}-\\theta_{3})</span></p>

    <p class="text-gray-300">where <span class="math">e_{1}:=\\widehat{\\mathbf{eq}}(\\mathbf{r}_{x},\\mathbf{r}_{x^{\\prime}})</span> and <span class="math">e_{2}:=\\widehat{\\mathbf{eq}}(\\beta,\\mathbf{r}_{x^{\\prime}})</span>. Computing <span class="math">e_{1},e_{2}</span> costs at most <span class="math">5\\log(m)</span> field operations, so this costs at most <span class="math">5\\log(m)+13</span> field operations. Then, there is also the cost to update the commitment, and the public inputs among other things. We do not make the constant hidden in the asymptotics explicit in Table 3, since we believe the Verifier cost is comparable to Mova.</p>

    <h2 id="sec-37" class="text-2xl font-bold">6 An efficient algorithm for composing a multilinear polynomial with a line</h2>

    <p class="text-gray-300">In Section 4.2, we described a reduction of knowledge that uses the point-vs-line method as a subroutine. In the point-vs-line method, the Prover needs to compute univariate polynomials of the form <span class="math">\\mathsf{mle}[\\mathbf{E}]\\circ\\ell</span>, where <span class="math">\\mathbf{E}\\in\\mathbb{F}^{m}</span> and <span class="math">\\ell:\\mathbb{F}\\to\\mathbb{F}^{m}</span> is a line in <span class="math">\\mathbb{F}^{m}</span> (a polynomial of the form <span class="math">\\mathbf{r}_{0}+X(\\mathbf{r}_{1}-\\mathbf{r}_{0})</span> for <span class="math">\\mathbf{r}_{0},\\mathbf{r}_{1}\\in\\mathbb{F}^{m}</span>). If done naively, this could easily cost <span class="math">O(m\\mathsf{polylog}(m))</span> field multiplications. In this section we provide a cost analysis of an algorithm that allows to compose a line and a <span class="math">\\log m</span>-variate MLE over a field <span class="math">\\mathbb{F}</span> in <span class="math">O(m)</span> time and <span class="math">O(m)</span> space. We will be using the MLE point evaluation algorithm first proposed by Vu et al. in <em>[x21]</em> (cf. <em>[x20]</em> for a detailed exposition) and implemented in the arkworks algebra library <em>[x1]</em>. Our observation is that the algorithm can be used for point values from any commutative ring <span class="math">\\mathbb{F}\\subseteq\\mathcal{R}</span>. This is because using the addition, subtraction and multiplication in <span class="math">\\mathcal{R}</span> leaves the algorithm unchanged, while division is not used at all. Therefore, we can evaluate the MLE on the “point” <span class="math">\\mathbf{r}_{0}+(\\mathbf{r}_{1}-\\mathbf{r}_{0})X\\in\\mathbb{F}[X]^{\\log m}</span> using the <span class="math">O(m)</span> evaluation algorithm. The algorithm is described in full in Algorithm 1. The result is similar to Claim 4.4 in <em>[x5]</em>.</p>

    <p class="text-gray-300">We identify the boolean hypercube points <span class="math">b\\in\\mathbb{B}^{n}</span> with the binary integers they represent in little endian form, i.e. <span class="math">(1,1,0,1)</span> is equivalent to <span class="math">0b1011</span> (cf. <em>[x1]</em>).</p>

    <p class="text-gray-300">###</p>

    <p class="text-gray-300">Algorithm 1 MLE-after-line composition</p>

    <p class="text-gray-300">Input: MLE evaluations <span class="math">[E_{b}]_{b\\in\\mathbb{B}^{m}}</span> and line points <span class="math">\\mathbf{r}_{0},\\mathbf{r}_{1}\\ \\in\\ \\mathbb{F}^{\\log m}</span>.</p>

    <p class="text-gray-300">Output: The coefficients of the univariate polynomial <span class="math">\\mathsf{mle}<a href="\\mathbf{r}_{0}+(\\mathbf{r}_{1}-\\mathbf{r}_{0}">\\mathbf{E}</a>X)</span>.</p>

    <p class="text-gray-300">1: <span class="math">\\mathsf{poly}[0..(m-1)]\\leftarrow[E_{b}]_{b\\in\\mathbb{B}^{m}}</span>; // Treat each <span class="math">E_{b}\\in\\mathbb{F}</span> as a constant polynomial <span class="math">\\in\\mathbb{F}[X]</span> 2: for <span class="math">i=1</span> to <span class="math">\\log m</span> do 3: <span class="math">\\ell_{i}\\leftarrow\\mathbf{r}_{0}[i-1]+(\\mathbf{r}_{1}[i-1]-\\mathbf{r}_{0}[i-1])X</span>; // compute the line coordinate polynomial 4: for <span class="math">b=0</span> to <span class="math">2^{\\log m-i}-1</span> do 5: left <span class="math">\\leftarrow</span> <span class="math">\\mathsf{poly}[2\\cdot b]</span>; 6: right <span class="math">\\leftarrow</span> <span class="math">\\mathsf{poly}[2\\cdot b+1]</span>; 7: <span class="math">\\mathsf{poly}[b]\\leftarrow</span> left <span class="math">+\\ell_{i}\\cdot(\\text{right}-\\text{left})</span>; 8: end for 9: end for 10: return <span class="math">\\mathsf{poly}[0]</span>;</p>

    <p class="text-gray-300">A natural question is whether this algorithm is still linear in space and time. Now addition can cost up to <span class="math">O(\\log m)</span> field operations and multiplication up to <span class="math">O(\\log^{2}m)</span> field operations, assuming univariate polynomial multiplication is done naively. Will it bring a (poly)logarithmic factor to the time complexity? In addition, one atomic storage now can contain up to <span class="math">\\log m</span> field elements, will it also lead to increase in space complexity? Fortunately, the answer is no to both questions!</p>

    <h6 id="sec-38" class="text-base font-medium mt-4">Lemma 6.1.</h6>

    <p class="text-gray-300">Algorithm 1 can be executed in at most <span class="math">5m-2\\log m-5</span> field additions, <span class="math">4m-2\\log m-4</span> field multiplications, and with a total memory consumption of at most <span class="math">2m+2\\log m-2</span> field elements, given that the univariate polynomial multiplication algorithm is the naive quadratic one.</p>

    <h6 id="sec-39" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">Let us analyse the time complexity first. We will count field operations required to perform the algorithm.</p>

    <p class="text-gray-300">Let us count how many multiplications <span class="math">\\mu</span> each iteration of the loop 2-9 does and compute the degree <span class="math">d</span> of the auxiliary polynomials stored in the array <span class="math">\\mathsf{poly}</span>.</p>

    <p class="text-gray-300"><span class="math">i=1</span>: At the beginning, all the auxiliary polynomials are field elements. We do <span class="math">2^{\\log m-1}=m/2</span> (the loop 4-8) multiplications of field elements by a linear polynomial <span class="math">\\mathbf{r}_{0}[0]+(\\mathbf{r}_{1}[0]-\\mathbf{r}_{0}[0])X</span> (line 7). That is <span class="math">2\\cdot m/2=2\\cdot 1\\cdot m/2</span> multiplications. The degrees of <span class="math">m/2</span> resulting auxiliary polynomials is 1 now. So</p>

    <p class="text-gray-300"><span class="math">\\mu_{1}=2\\cdot 1\\cdot m/2\\text{ and }d_{1}=1.</span></p>

    <p class="text-gray-300"><span class="math">i=2</span>: At the beginning, all the auxiliary polynomials are degree <span class="math">d_{1}=1</span>. We do <span class="math">2^{\\log m-2}=m/4</span> multiplications of linear polynomial by a linear polynomial <span class="math">\\mathbf{r}_{0}[1]+(\\mathbf{r}_{1}[1]-\\mathbf{r}_{0}[1])X</span>. That is <span class="math">2\\cdot 2\\cdot m/4</span> multiplications. The degrees of <span class="math">m/4</span> resulting auxiliary polynomials is 2 now (linear by linear). So</p>

    <p class="text-gray-300"><span class="math">\\mu_{2}=2\\cdot 2\\cdot m/4\\text{ and }d_{2}=2.</span></p>

    <p class="text-gray-300"><span class="math">i=i</span>: At the beginning, all the auxiliary polynomials are degree <span class="math">d_{i-1}=i-1</span>. We do <span class="math">m/2^{i}</span> multiplications of degree-<span class="math">(i-1)</span> polynomials by a linear polynomial <span class="math">\\mathbf{r}_{0}[i-1]+(\\mathbf{r}_{1}[i-1]-\\mathbf{r}_{0}[1])X</span></p>

    <p class="text-gray-300"><span class="math">\\mathbf{r}_{0}[i-1])X</span>. That is <span class="math">2\\cdot i\\cdot m/2^{i}</span> multiplications. The degrees of <span class="math">m/2^{i+1}</span> resulting auxiliary polynomials is <span class="math">i</span> now (<span class="math">i-1</span> by linear). So</p>

    <p class="text-gray-300"><span class="math">\\mu_{i}=2\\cdot i\\cdot m/2^{i}\\text{ and }d_{i}=i.</span></p>

    <p class="text-gray-300">Summing up <span class="math">\\mu_{1},\\ldots,\\mu_{\\log m}</span> we get the expression for the number of multiplications</p>

    <p class="text-gray-300"><span class="math">\\sum_{i=1}^{\\log m}\\mu_{i}=\\sum_{i=1}^{\\log m}\\frac{i\\cdot m}{2^{i-1}}=m\\sum_{i=1}^{\\log m}\\frac{i}{2^{i-1}}.</span></p>

    <p class="text-gray-300">Using the method of differentiation of geometric progression we compute the sum</p>

    <p class="text-gray-300"><span class="math">m\\sum_{i=1}^{\\log m}\\frac{i}{2^{i-1}}=m\\cdot\\left(4-\\frac{2\\log m+4}{m}\\right)=4m-2\\log m-4.</span></p>

    <p class="text-gray-300">The number of additions on the <span class="math">i</span>th iteration can be derived similarly. let us focus on the line 7 of the algorithm. We do <span class="math">d_{i-1}+1</span> additions in <span class="math">\\mathsf{right-left}</span>. Then we need <span class="math">d_{i-1}</span> additions for the multiplication by the linear polynomial <span class="math">\\ell_{i}</span>. And finally to add to left we do <span class="math">d_{i-1}+1</span> additions as well. That sums up to <span class="math">3d_{i-1}+2</span>. Knowing that <span class="math">d_{i-1}=i-1</span> we get <span class="math">3i-1</span> additions. And that happens <span class="math">m/2^{i}</span> times in the loop 4-8. This finally gives <span class="math">m(3i-1)/2^{i}</span>. In addition to that, at each of the <span class="math">\\log m</span> iterations of the loop 2-9 we compute the polynomial <span class="math">\\ell_{i}</span> at line 3. The only field operation we do for that is <span class="math">\\mathbf{r}_{1}[i-1]-\\mathbf{r}_{0}[i-1]</span>, which is an addition (we do not distinguish addition and subtraction). Thus we arrive to the following sum for the number of additions</p>

    <p class="text-gray-300"><span class="math">\\sum_{i=1}^{\\log m}\\left(\\frac{m\\cdot(3i-1)}{2^{i}}+1\\right).</span></p>

    <p class="text-gray-300">Using the same summation techniques we obtain the number of additions:</p>

    <p class="text-gray-300"><span class="math">5m-2\\log m-5.</span></p>

    <p class="text-gray-300">Summing the two formulas together the number of field operations is</p>

    <p class="text-gray-300"><span class="math">9m-4\\log m-9.</span></p>

    <p class="text-gray-300">Regarding the memory, we have variables <span class="math">\\ell_{i},\\mathsf{left}</span> and <span class="math">\\mathsf{right}</span> that are always there and that occupy in total <span class="math">3\\log m</span>. The array <span class="math">\\mathsf{poly}</span> occupies by the end of the algorithm</p>

    <p class="text-gray-300"><span class="math">\\frac{m}{2}\\sum_{i=1}^{\\log m}\\frac{i}{2^{i-1}}=2m-\\log m-2</span></p>

    <p class="text-gray-300">field memory cells. Hence the total memory consumption is</p>

    <p class="text-gray-300"><span class="math">2m+2\\log m-2.</span></p>

    <p class="text-gray-300">∎</p>

    <p class="text-gray-300">##</p>

    <p class="text-gray-300">7 Deferred proofs</p>

    <p class="text-gray-300">We end the paper with a proof that Protocol 6 is a reduction of knowledge.</p>

    <h3 id="sec-40" class="text-xl font-semibold mt-8">7.1 Proof of Lemma 4.4</h3>

    <p class="text-gray-300">Public reducibility. We construct a deterministic function <span class="math">\\varphi</span> that, with input <span class="math">(\\mathsf{pp},\\mathtt{x}_{1},\\mathtt{x}_{2},\\tau)</span>, outputs <span class="math">(\\mathtt{x}_{1}^{\\prime},\\mathtt{x}_{2}^{\\prime})</span>. Let <span class="math">\\tau=((h_{1}(X),h_{2}(X)),\\beta)</span> (otherwise abort). Define <span class="math">v_{1}^{\\prime}:=h_{1}(\\beta)</span>, <span class="math">v_{2}^{\\prime}:=h_{2}(\\beta)</span> and <span class="math">\\mathbf{r}^{\\prime}=(\\mathtt{x}_{2}.\\mathbf{r}-\\mathtt{x}_{1}.\\mathbf{r})\\beta+\\mathtt{x}_{1}.\\mathbf{r}</span> and notice that these values are polynomial-time computable from <span class="math">\\mathtt{x}_{1},\\mathtt{x}_{2}</span> and <span class="math">\\tau</span>. We have <span class="math">\\varphi</span> output <span class="math">(\\mathtt{x}_{1}^{\\prime},\\mathtt{x}_{2}^{\\prime})</span>, where <span class="math">\\mathtt{x}_{1}^{\\prime}=(\\mathtt{x}_{1}.\\mathbf{x},v_{1}^{\\prime},\\mathtt{x}_{1}.u,\\mathtt{x}_{1}.\\overline{\\mathbf{W}},\\mathbf{r}^{\\prime})</span> and <span class="math">\\mathtt{x}_{2}^{\\prime}=(\\mathtt{x}_{2}.\\mathbf{x},v_{2}^{\\prime},\\mathtt{x}_{2}.u,\\mathtt{x}_{2}.\\overline{\\mathbf{W}},\\mathbf{r}^{\\prime})</span>.</p>

    <p class="text-gray-300">Let <span class="math">\\mathcal{A},\\mathsf{P}^{<em>}</span> be PPT adversaries and let <span class="math">(\\mathtt{x}_{1},\\mathtt{x}_{2},\\mathsf{st})\\leftarrow\\mathcal{A}(\\mathsf{pp})</span>. Let <span class="math">\\tau</span> be the transcript of the interaction between <span class="math">\\mathsf{P}^{</em>}</span> and <span class="math">\\mathsf{V}</span>, with input <span class="math">(\\mathsf{pp},\\mathtt{x}_{1},\\mathtt{x}_{2},\\mathsf{st})</span> following Protocol 6. Let <span class="math">(\\mathtt{x}_{1}^{\\prime},\\mathtt{x}_{2}^{\\prime};\\mathtt{w}_{1}^{\\prime},\\mathtt{w}_{2}^{\\prime})</span>, be the output of the interaction. The output of <span class="math">\\varphi(\\mathsf{pp},\\mathtt{x}_{1},\\mathtt{x}_{2})</span> is by construction equal to <span class="math">(\\mathtt{x}_{1}^{\\prime},\\mathtt{x}_{2}^{\\prime})</span>, as required.</p>

    <p class="text-gray-300">Perfect completeness. Let <span class="math">\\mathcal{A}</span> be a PPT adversary and let <span class="math">(\\mathtt{x}_{1};\\mathtt{w}_{1}),(\\mathtt{x}_{1};\\mathtt{w}_{2})\\leftarrow\\mathcal{A}(\\mathsf{pp})</span> be such that <span class="math">(\\mathsf{pp},\\mathtt{x}_{i};\\mathtt{w}_{i})\\in\\mathbf{R}_{\\mathsf{acc}}</span> for both <span class="math">i=1,2</span>.</p>

    <p class="text-gray-300">Write <span class="math">(\\mathsf{pp},\\mathtt{x}_{i};\\mathtt{w}_{i})=(\\mathsf{pp},\\mathtt{x}_{i},v_{i},u_{i},\\overline{\\mathbf{W}}_{i},\\mathbf{r}_{i};\\mathbf{W}_{i},\\mathbf{E}_{i},s_{i})\\in\\mathbf{R}_{\\mathsf{acc}}</span>, for <span class="math">i=1,2</span>. Assume <span class="math">\\mathsf{P}</span> and <span class="math">\\mathsf{V}</span> honestly follow Protocol 6 with inputs <span class="math">(\\mathsf{pp},\\mathtt{x}_{i};\\mathtt{w}_{i})</span> and <span class="math">(\\mathsf{pp},\\mathtt{x}_{i})</span>, <span class="math">i=1,2</span>. We argue that the output of the interaction <span class="math">\\langle\\mathsf{P},\\mathsf{V}\\rangle</span> is in <span class="math">\\mathbf{R}_{\\mathsf{equal}}</span> and <span class="math">\\mathsf{V}</span> accepts. The case when <span class="math">\\mathbf{r}_{1}=\\mathbf{r}_{2}</span> is immediate. Assume <span class="math">\\mathbf{r}_{1}\\neq\\mathbf{r}_{2}</span>. By definition of <span class="math">h_{1}(X),h_{2}(X)</span> (the first message sent by <span class="math">\\mathsf{P}</span>) and <span class="math">\\ell</span>, we have</p>

    <p class="text-gray-300"><span class="math">h_{1}(0)=\\mathsf{mle}[\\mathbf{E}_{1}]\\circ\\ell(0)=\\mathsf{mle}<a href="\\mathbf{r}_{1}">\\mathbf{E}_{1}</a>=v_{1}.</span></p>

    <p class="text-gray-300">In exactly the same way, we obtain <span class="math">h_{2}(1)=v_{2}</span>. Hence, <span class="math">\\mathsf{V}</span> does not abort at the end of Protocol 6. Let <span class="math">(\\mathtt{x}_{1}^{\\prime},\\mathtt{x}_{2}^{\\prime};\\mathtt{w}_{1},\\mathtt{w}_{2})</span> be the output of the interaction between <span class="math">\\mathsf{P}</span> and <span class="math">\\mathsf{V}</span>. Write <span class="math">\\mathtt{x}_{i}^{\\prime}=(\\mathtt{x}_{i},v_{i}^{\\prime},u_{i}^{\\prime},\\overline{\\mathbf{W}}_{i},\\mathbf{r}_{i}^{\\prime})</span> and <span class="math">\\mathtt{w}_{i}^{\\prime}=(\\mathbf{W}_{i},\\mathbf{E}_{i},s_{i})</span>, <span class="math">i=1,2</span>. We now argue that <span class="math">(\\mathsf{pp},\\mathtt{x}_{1}^{\\prime},\\mathtt{x}_{2}^{\\prime};\\mathtt{w}_{1}^{\\prime},\\mathtt{w}_{2}^{\\prime})\\in\\mathbf{R}_{\\mathsf{equal}}</span>. Indeed, by construction, <span class="math">\\mathbf{r}_{1}^{\\prime}=\\mathbf{r}_{2}^{\\prime}=\\mathbf{r}^{\\prime}</span> for some point <span class="math">\\mathbf{r}^{\\prime}</span>. Hence, it suffices to check that <span class="math">(\\mathsf{pp},\\mathtt{x}_{i}^{\\prime};\\mathtt{w}_{i}^{\\prime})\\in\\mathbf{R}_{\\mathsf{acc}}</span> for both <span class="math">i=1,2</span>. We show this for <span class="math">(\\mathsf{pp},\\mathtt{x}_{1}^{\\prime};\\mathtt{w}_{1}^{\\prime})</span>, the reasoning for <span class="math">(\\mathsf{pp},\\mathtt{x}_{2}^{\\prime};\\mathtt{w}_{2}^{\\prime})</span> being analogous. Since <span class="math">(\\mathsf{pp},\\mathtt{x}_{1};\\mathtt{w}_{1})\\in\\mathbf{R}_{\\mathsf{acc}}</span>, we have <span class="math">(A\\cdot\\mathbf{Z}_{1})\\circ(B\\cdot\\mathbf{Z}_{1})=u_{1}\\cdot(C\\cdot\\mathbf{Z}_{1})+\\mathbf{E}_{1}</span>, and <span class="math">\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{W}_{1},s_{1})=\\overline{\\mathbf{W}}_{1}</span>. Hence, it only remains to check that <span class="math">\\mathsf{mle}<a href="\\mathbf{r}^{\\prime}">\\mathbf{E}_{1}</a>=v_{1}^{\\prime}</span>. However, it is clear that this is the case after inspecting Step 3 in Protocol 6. More precisely, we have <span class="math">v_{1}^{\\prime}=h_{1}(\\beta)=\\mathsf{mle}[\\mathbf{E}_{1}]\\circ\\ell(\\beta)=\\mathsf{mle}<a href="\\mathbf{r}^{\\prime}">\\mathbf{E}_{1}</a></span>.</p>

    <p class="text-gray-300">Knowledge soundness. The proof follows the same structure as the knowledge soundness proof in Lemma 4.1. Some of its arguments are a word-by-word repetiton of reasonings used in said proof.</p>

    <p class="text-gray-300">Let <span class="math">\\mathcal{A}</span> and <span class="math">\\mathsf{P}^{*}</span> be expected polynomial-time adversaries. Let <span class="math">(\\mathtt{x}_{1},\\mathtt{x}_{2},\\mathsf{st})\\leftarrow\\mathcal{A}(\\mathsf{pp})</span>. Write <span class="math">(\\mathtt{x}_{i},\\mathtt{w}_{i})=(\\mathbf{x}_{i},\\overline{\\mathbf{W}}_{i},v_{i},u_{i},\\mathbf{r}_{i};\\mathbf{W}_{i},\\mathbf{E}_{i},s_{i})</span> for <span class="math">i=1,2,</span>. Assume <span class="math">\\mathbf{r}_{1}\\neq\\mathbf{r}_{2}</span>. Let <span class="math">\\ell</span> be the linear function <span class="math">\\ell:\\mathbb{F}\\rightarrow\\mathbb{F}^{\\log m}</span> satisfying <span class="math">\\ell(0)=\\mathbf{r}_{1}</span> and <span class="math">\\ell(1)=\\mathbf{r}_{2}</span>.</p>

    <p class="text-gray-300">Fix the notation <span class="math">\\langle\\mathsf{P}^{<em>},\\mathsf{V}\\rangle=\\langle\\mathsf{P}^{</em>},\\mathsf{V}\\rangle(\\mathsf{pp},\\mathtt{x}_{1},\\mathtt{x}_{2},\\mathsf{st})</span>, i.e. <span class="math">\\langle\\mathsf{P}^{<em>},\\mathsf{V}\\rangle</span> is interactive protocol in which <span class="math">\\mathsf{P}^{</em>}</span> and <span class="math">\\mathsf{V}</span> interact following Protocol 3, with inputs <span class="math">\\mathsf{pp},\\mathtt{x}_{1},\\mathtt{x}_{2}</span> and <span class="math">\\mathsf{st}</span>. We also look at <span class="math">\\langle\\mathsf{P}^{*},\\mathsf{V}\\rangle</span> as a random variable modelling the output of such interaction.</p>

    <p class="text-gray-300">Let <span class="math">\\varepsilon_{\\mathsf{total}}</span> be the probability that <span class="math">\\mathsf{P}^{<em>}</span>, with inputs <span class="math">(\\mathsf{pp},\\mathtt{x}_{1},\\mathtt{x}_{2},\\mathsf{st})</span>, succeeds in obtaining an output in <span class="math">\\mathbf{R}_{\\mathsf{equal}}</span>, i.e. <span class="math">\\varepsilon_{\\mathsf{total}}=\\mathsf{Pr}[(\\mathsf{pp},\\mathtt{x},\\langle\\mathsf{P}^{</em>},\\mathsf{V}\\rangle(\\mathsf{pp},\\mathtt{x}_{1},\\mathtt{x}_{2},\\mathsf{st}))\\in\\mathbf{R}_{\\mathsf{equal}}]</span>.</p>

    <p class="text-gray-300">We define the extractor <span class="math">\\mathsf{Ext}</span> as follows. <span class="math">\\mathsf{Ext}</span> receives <span class="math">(\\mathsf{pp},\\mathfrak{x}_{1},\\mathfrak{x}_{2},\\mathsf{st})</span> as inputs. Then:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\mathsf{Ext}</span> runs the protocol <span class="math">\\langle\\mathsf{P}^{*},\\mathsf{V}\\rangle</span> once. Let <span class="math">(\\mathsf{pp},\\mathfrak{x}_{1}^{(1)},\\mathfrak{x}_{2}^{(1)};\\mathfrak{w}_{1}^{(1)},\\mathfrak{w}_{2}^{(1)})</span> be the output of this interaction. If <span class="math">\\mathsf{V}</span> rejects the interaction, or if <span class="math">(\\mathsf{pp},\\mathfrak{x}_{1}^{(1)},\\mathfrak{x}_{2}^{(1)};\\mathfrak{w}_{1}^{(1)},\\mathfrak{w}_{2}^{(1)})\\not\\in\\mathbf{R}_{\\mathsf{equal}}</span>, then <span class="math">\\mathsf{Ext}</span> aborts.</li>

    </ol>

    <p class="text-gray-300">Otherwise, say we have</p>

    <p class="text-gray-300"><span class="math">(\\mathsf{pp},\\mathfrak{x}_{i}^{(1)};\\mathfrak{w}_{i}^{(1)})=(\\mathsf{pp},\\mathbf{x}_{i},v_{i}^{(1)},u_{i},\\widetilde{\\mathbf{W}}_{i},\\mathbf{r}^{(1)};\\mathbf{W}_{i}^{(1)},\\mathbf{E}_{i}^{(1)},s_{i}^{(1)})</span></p>

    <p class="text-gray-300">for <span class="math">i=1,2</span>, where <span class="math">\\mathbf{r}^{(1)}=\\ell(\\beta^{(1)})</span> and <span class="math">\\beta^{(1)}</span> is the Verifier’s challenge. Let <span class="math">h_{1}(X),h_{2}(X)</span> be the Prover’s first message of the interaction.</p>

    <p class="text-gray-300">Let <span class="math">\\mathcal{E}_{\\mathsf{lines}}</span> be the event that <span class="math">\\mathsf{Ext}</span> does not abort in Step 1 and both <span class="math">h_{1}(X)=\\mathsf{mle}[E_{1}^{(1)}]\\circ\\ell(X)</span> and <span class="math">h_{2}(X)=\\mathsf{mle}[E_{2}^{(1)}]\\circ\\ell(X)</span>. Let <span class="math">\\mathcal{E}_{\\mathsf{bad-lines}}</span> be the event that <span class="math">\\mathsf{Ext}</span> does not abort in Step 1 but either <span class="math">h_{1}(X)\\neq\\mathsf{mle}[E_{1}^{(1)}]\\circ\\ell(X)</span> or <span class="math">h_{2}(X)\\neq\\mathsf{mle}[E_{2}^{(1)}]\\circ\\ell(X)</span>.</p>

    <p class="text-gray-300">If <span class="math">\\mathcal{E}_{\\mathsf{lines}}</span> holds, then <span class="math">\\mathsf{Ext}</span> terminates and outputs <span class="math">\\mathfrak{w}_{i}^{(1)}=(\\mathbf{W}_{i}^{(1)},\\mathbf{E}_{i}^{(1)},s_{i}^{(1)})</span> for <span class="math">i=1,2</span>.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Next, <span class="math">\\mathsf{Ext}</span> repeatedly runs <span class="math">\\langle\\mathsf{P}^{<em>},\\mathsf{V}\\rangle</span>, keeping always the same first message sent by <span class="math">\\mathsf{P}^{</em>}</span> to be <span class="math">(h_{1}(X),h_{2}(X))</span>. To do so, <span class="math">\\mathsf{Ext}</span> rewinds <span class="math">\\mathsf{P}^{<em>}</span> only to the point where <span class="math">\\mathsf{P}^{</em>}</span> has already sent <span class="math">(h_{1}(X),h_{2}(X))</span>.</li>

    </ol>

    <p class="text-gray-300">As soon as <span class="math">\\mathsf{Ext}</span> obtains an output</p>

    <p class="text-gray-300"><span class="math">(\\mathsf{pp},\\mathfrak{x}_{1}^{(2)},\\mathfrak{x}_{2}^{(2)};\\mathfrak{w}_{1}^{(2)},\\mathfrak{w}_{2}^{(2)})\\in\\mathbf{R}_{\\mathsf{equal}}</span></p>

    <p class="text-gray-300">and <span class="math">\\mathsf{V}</span> does not reject, <span class="math">\\mathsf{Ext}</span> terminates and outputs <span class="math">\\mathfrak{w}_{i}^{(1)}=(\\mathbf{W}_{i}^{(1)},\\mathbf{E}_{i}^{(1)},s_{i}^{(1)})</span> for <span class="math">i=1,2</span>.</p>

    <p class="text-gray-300">Let <span class="math">\\mathcal{E}_{h_{1},h_{2}}</span> be the event that <span class="math">\\mathsf{P}^{<em>}</span>’s first message in Step 1 of <span class="math">\\mathsf{Ext}</span> is <span class="math">(h_{1}(X),h_{2}(X))</span>. Fix one such first message <span class="math">(h_{1}(X),h_{2}(X))</span>, and let <span class="math">\\varepsilon</span> be the probability that <span class="math">(\\mathsf{pp},\\langle\\mathsf{P}^{</em>},\\mathsf{V}\\rangle(\\mathsf{pp},\\mathfrak{x}_{1},\\mathfrak{x}_{2},\\mathsf{st}))\\in\\mathbf{R}_{\\mathsf{acc}}</span>. We next prove that, conditioned on <span class="math">\\mathcal{E}_{h_{1},h_{2}}</span>, <span class="math">\\mathsf{Ext}</span> runs in expected polynomial time and outputs a valid witness for <span class="math">(\\mathfrak{x}_{1},\\mathfrak{x}_{2})</span> with probability <span class="math">\\varepsilon-\\mathsf{negl}(\\lambda)</span>. For readability purposes, we avoid referring to <span class="math">(h_{1},h_{2})</span> in our notation. In what follows, unless stated otherwise, we consider all probabilities and events referring to <span class="math">\\mathsf{Ext}</span> as conditioned on <span class="math">\\mathcal{E}_{h_{1},h_{2}}</span>.</p>

    <p class="text-gray-300">First of all we prove that <span class="math">\\mathsf{Ext}</span> terminates in expected polynomial time. The argument is analogous to that in the proof of Lemma 4.1. Let <span class="math">\\mathcal{E}</span> be the event that <span class="math">\\mathsf{Ext}</span> does not abort in Step 1. We have <span class="math">\\mathsf{Pr}[\\mathsf{Ext}]=\\varepsilon</span>. Denote by <span class="math">Z</span> the random variable (over <span class="math">\\mathsf{Ext}</span>’s random coins) representing the number of times <span class="math">\\mathsf{Ext}</span> runs the interaction <span class="math">\\langle\\mathsf{P}^{<em>},\\mathsf{V}\\rangle</span> in Step 2 (if <span class="math">\\mathsf{Ext}</span> terminated or aborts in Step 1, then <span class="math">Z=0</span>). Let <span class="math">Z_{\\mathsf{total}}</span> be the random variable (over <span class="math">\\mathsf{Ext}</span>’s random coins) representing the total number of times <span class="math">\\langle\\mathsf{P}^{</em>},\\mathsf{V}\\rangle</span> is run when executing <span class="math">\\mathsf{Ext}</span>. We have</p>

    <p class="text-gray-300"><span class="math">\\mathbb{E}[Z_{\\mathsf{total}}]=1+\\Pr[\\mathcal{E}]\\cdot\\mathbb{E}[Z\\mid\\mathcal{E}]\\leq 1+\\varepsilon\\cdot\\frac{1}{\\varepsilon}=2.</span> (27)</p>

    <p class="text-gray-300">Hence, <span class="math">\\mathsf{Ext}</span> runs in expected polynomial time, and it does not abort with probability at least <span class="math">\\varepsilon</span>.</p>

    <p class="text-gray-300">Let <span class="math">\\mathcal{E}_{\\mathsf{binding}}</span> be the event that <span class="math">\\mathsf{Ext}</span>, on inputs <span class="math">(\\mathsf{pp}_{\\mathsf{Com}},\\mathfrak{x}_{1},\\mathfrak{x}_{2},\\mathsf{st})</span> is not able to break the binding property of the commitment scheme <span class="math">\\mathsf{Com}</span>, i.e. <span class="math">\\mathcal{E}_{\\mathsf{binding}}</span> is the event that at no point</p>

    <p class="text-gray-300"><span class="math">\\mathsf{Ext}</span> has computed two vectors <span class="math">\\mathbf{U}_{1},\\mathbf{U}_{2}</span> and elements <span class="math">s_{1},s_{2}</span> such that <span class="math">\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{U}_{1},s_{1})=\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{U}_{2},s_{2})</span>. We have <span class="math">\\mathsf{Pr}[\\mathcal{E}_{\\mathsf{binding}}]=1-\\mathsf{negl}(\\lambda)</span>.</p>

    <p class="text-gray-300">Now, conditioning on <span class="math">\\mathcal{E}</span> occurring, since the events <span class="math">\\mathcal{E}_{\\mathsf{lines}}</span> and <span class="math">\\mathcal{E}_{\\mathsf{bad-lines}}</span> are complementary and mutually exclusive, by the law of total expectation,</p>

    <p class="text-gray-300"><span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}]=\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{lines}}\\text{ or }\\mathcal{E}_{\\mathsf{bad-lines}}]</span> (28) <span class="math">=</span> <span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{lines}}]\\mathsf{Pr}[\\mathcal{E}_{\\mathsf{lines}}]+\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{bad-lines}}]\\mathsf{Pr}[\\mathcal{E}_{\\mathsf{bad-lines}}]</span> <span class="math">=</span> <span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{bad-lines}}]\\varepsilon_{1}+\\mathbb{E}<a href="1-\\varepsilon_{1}">Z\\mid\\mathcal{E}_{\\mathsf{lines}}</a>,</span></p>

    <p class="text-gray-300">where <span class="math">\\varepsilon_{1}=\\mathsf{Pr}[\\mathcal{E}_{\\mathsf{bad-lines}}]</span> is the probability that <span class="math">\\mathsf{Ext}</span> does not abort in Step 1 and <span class="math">\\mathcal{E}_{\\mathsf{lines}}</span> does not hold. Note that <span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{lines}}]=0</span>, because if, at the end of Step 1, <span class="math">\\mathcal{E}_{\\mathsf{lines}}</span> holds, then the extractor terminates. We now make the following claim:</p>

    <h6 id="sec-41" class="text-base font-medium mt-4">Claim 7.1.</h6>

    <p class="text-gray-300"><span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{bad-lines}},\\mathcal{E}_{\\mathsf{binding}}]^{-1}=\\mathsf{negl}(\\lambda)</span>.</p>

    <p class="text-gray-300">Assume Claim 7.1 is true for now. We now claim that if <span class="math">\\mathsf{Ext}</span> does not abort in Step 1 and <span class="math">\\mathcal{E}_{\\mathsf{lines}}</span> holds, then both <span class="math">(\\mathsf{pp},\\mathbbm{x}_{1};\\mathbbm{w}_{1}^{(1)})</span> and <span class="math">(\\mathsf{pp},\\mathbbm{x}_{2};\\mathbbm{w}_{2}^{(1)})</span> belong to <span class="math">\\mathbf{R}_{\\mathsf{acc}}</span>. Indeed, since <span class="math">\\mathsf{Ext}</span> does not abort, we know that <span class="math">(\\mathsf{pp},\\mathbbm{x}_{1}^{(1)},\\mathbbm{x}_{2}^{(1)};\\mathbbm{w}_{1}^{(1)},\\mathbbm{w}_{2}^{(1)})\\in\\mathbf{R}_{\\mathsf{equal}}</span>. Further, <span class="math">h_{1}(0)=v_{1}</span> and <span class="math">h_{2}(1)=v_{2}</span>, because <span class="math">\\mathsf{V}</span> did not reject. Since <span class="math">\\mathcal{E}_{\\mathsf{lines}}</span> holds, this implies that</p>

    <p class="text-gray-300"><span class="math">\\mathsf{mle}<a href="\\mathbf{r}_{1}">\\mathbf{E}_{1}^{(1)}</a></span> <span class="math">=\\mathsf{mle}[\\mathbf{E}_{1}^{(1)}]\\circ\\ell(0)=h_{1}(0)=v_{1}</span> (29) <span class="math">\\mathsf{mle}<a href="\\mathbf{r}_{2}">\\mathbf{E}_{2}^{(1)}</a></span> <span class="math">=\\mathsf{mle}[\\mathbf{E}_{2}^{(1)}]\\circ\\ell(1)=h_{2}(1)=v_{2}.</span></p>

    <p class="text-gray-300">Finally, since <span class="math">(\\mathsf{pp},\\mathbbm{x}_{i}^{(1)};\\mathbbm{w}_{i}^{(1)})\\in\\mathbf{R}_{\\mathsf{acc}}</span> for both <span class="math">i=1,2</span>, we conclude that <span class="math">(\\mathsf{pp},\\mathbbm{x}_{i};\\mathbbm{w}_{i}^{(1)})\\in\\mathbf{R}_{\\mathsf{acc}}</span> as well, due to Eq. (29) and <span class="math">\\mathbbm{x}_{i},\\mathbbm{x}_{i}^{(1)}</span> only differing in the evaluation points <span class="math">\\mathbf{r}_{i},\\mathbf{r}^{(1)}</span>, and evaluation values <span class="math">v_{i},v_{i}^{(1)}</span>.</p>

    <p class="text-gray-300">We next argue that <span class="math">\\varepsilon_{1}=\\mathsf{negl}(\\lambda)</span>. Note that this will complete the proof (barring the proof of Claim 7.1) that <span class="math">\\mathsf{Ext}</span> runs in PPT time and outputs a valid witness with probability <span class="math">\\varepsilon-\\mathsf{negl}(\\lambda)</span>, conditioned on <span class="math">\\mathsf{P}^{*}</span>’s first message being <span class="math">(h_{1}(X),h_{2}(X))</span>, i.e. conditioned on the event <span class="math">\\mathcal{E}_{h_{1},h_{2}}</span>. .</p>

    <p class="text-gray-300">Further, we have already argued that <span class="math">\\mathsf{Ext}</span> runs in expected polynomial time, and that it does not abort with probability at least <span class="math">\\varepsilon</span>. If, additionally, the probability that <span class="math">\\mathsf{Ext}</span> does not abort and <span class="math">\\neg\\mathcal{E}_{\\mathsf{lines}}</span> is negligible, then we conclude that <span class="math">\\mathsf{Ext}</span> is a PPT algorithm that outputs a valid witness for <span class="math">\\mathbbm{x}_{1},\\mathbbm{x}_{2}</span> with probability <span class="math">\\varepsilon-\\mathsf{negl}(\\lambda)</span>.</p>

    <p class="text-gray-300">We now prove that <span class="math">\\varepsilon_{1}=\\mathsf{negl}(\\lambda)</span>, assuming that Claim 7.1 is true. Indeed, plugging (28) into (27), and using both Claim 7.1 and <span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{lines}}]=0</span>, we obtain</p>

    <p class="text-gray-300"><span class="math">2\\geq\\mathbb{E}[Z_{\\mathsf{total}}]=1+\\varepsilon\\mathbb{E}[Z\\mid\\mathcal{E}]=1+\\varepsilon\\varepsilon_{1}\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{bad-lines}}]</span> (30)</p>

    <p class="text-gray-300">Using again the law of total expectation,</p>

    <p class="text-gray-300"><span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{bad-lines}}]</span> (31) <span class="math">=</span> <span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{bad-lines}},\\mathcal{E}_{\\mathsf{binding}}]\\mathsf{Pr}[\\mathcal{E}_{\\mathsf{binding}}]+\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{bad-lines}},\\neg\\mathcal{E}_{\\mathsf{binding}}]\\mathsf{Pr}[\\neg\\mathcal{E}_{\\mathsf{binding}}]</span> <span class="math">\\geq</span> <span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{bad-lines}},\\mathcal{E}_{\\mathsf{binding}}]\\mathsf{Pr}[\\mathcal{E}_{\\mathsf{binding}}].</span></p>

    <p class="text-gray-300">Note that <span class="math">\\varepsilon_{1}\\leq\\varepsilon</span>, since <span class="math">\\varepsilon_{1}</span> is the probability that <span class="math">\\mathsf{Ext}</span> does not abort at Step 1, which occurs with probability <span class="math">\\varepsilon</span>, and, additionally, <span class="math">\\mathcal{E}_{\\mathsf{bad-lines}}</span> holds. Hence from (30), (31), and Claim 7.1 we obtain</p>

    <p class="text-gray-300"><span class="math">\\varepsilon_{1}^{2}\\leq\\varepsilon_{1}\\varepsilon\\leq\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{bad-lines}},\\mathcal{E}_{\\mathsf{binding}}]^{-1}\\mathsf{Pr}[\\mathcal{E}_{\\mathsf{binding}}]^{-1}=\\mathsf{negl}(\\lambda),</span></p>

    <p class="text-gray-300">where the last equality follows from Remark 3.1 and the fact that <span class="math">\\mathbb{E}[Z\\mid\\mathcal{E}_{\\mathsf{nonzero}},\\mathcal{E}_{\\mathsf{binding}}]^{-1}=\\mathsf{negl}(\\lambda)</span> and <span class="math">\\mathsf{Pr}[\\mathcal{E}_{\\mathsf{binding}}]=1-\\mathsf{negl}(\\lambda)</span>. This implies that <span class="math">\\varepsilon_{1}=\\mathsf{negl}(\\lambda)</span>, as needed.</p>

    <p class="text-gray-300">Now it only remains to prove Claim 7.1.</p>

    <h6 id="sec-42" class="text-base font-medium mt-4">Proof of Claim 7.1.</h6>

    <p class="text-gray-300">Assume <span class="math">\\mathcal{E}_{\\mathsf{binding}}</span> holds, i.e. <span class="math">\\mathsf{Ext}</span> does not break the binding property of the commitment scheme <span class="math">\\mathsf{Com}</span>. Assume we run <span class="math">\\mathsf{Ext}</span> up to Step 1 and <span class="math">\\mathcal{E}_{\\mathsf{bad-lines}}</span> holds, so in particular <span class="math">\\mathsf{Ext}</span> does not abort. Let <span class="math">(\\mathsf{pp},\\mathbf{x}_{1}^{(1)},\\mathbf{x}_{2}^{(1)};\\mathbf{w}_{1}^{(1)},\\mathbf{w}_{2}^{(1)})</span> be the output of <span class="math">\\langle\\mathsf{P}^{*},\\mathsf{V}\\rangle</span> obtained at the end of Step 1. By definition, this output belongs to <span class="math">\\mathbf{R}_{\\mathsf{equal}}</span>.</p>

    <p class="text-gray-300">By construction of <span class="math">\\mathsf{Ext}</span>, during Step 2, <span class="math">\\mathsf{Ext}</span> successively repeats an experiment <span class="math">\\Xi</span>, until <span class="math">\\Xi</span> is successful. The experiment <span class="math">\\Xi</span> consists in running <span class="math">\\langle\\mathsf{P}^{<em>},\\mathsf{V}\\rangle</span>, and <span class="math">\\Xi</span> is successful if the output <span class="math">(\\mathsf{pp},\\mathbf{x}_{1}^{(2)},\\mathbf{x}_{2}^{(2)};\\mathbf{w}_{1}^{(2)},\\mathbf{w}_{2}^{(2)})</span> of <span class="math">\\langle\\mathsf{P}^{</em>},\\mathsf{V}\\rangle</span> is in <span class="math">\\mathbf{R}_{\\mathsf{equal}}</span>. Importantly, note that the random challenge <span class="math">\\beta^{(2)}</span> sent by <span class="math">\\mathsf{V}</span> during this experiment is uniformly random and independent of <span class="math">(\\mathsf{pp},\\mathbf{x}_{1}^{(1)},\\mathbf{x}_{2}^{(1)};\\mathbf{w}_{1}^{(1)},\\mathbf{w}_{2}^{(1)})</span>. Suppose one such output <span class="math">(\\mathsf{pp},\\mathbf{x}_{1}^{(2)},\\mathbf{x}_{2}^{(2)};\\mathbf{w}_{1}^{(2)},\\mathbf{w}_{2}^{(2)})</span> is in <span class="math">\\mathbf{R}_{\\mathsf{acc}}</span>. Write</p>

    <p class="text-gray-300"><span class="math">(\\mathsf{pp},\\mathbf{x}_{i}^{(2)};\\mathbf{w}_{i}^{(2)})=(\\mathsf{pp},\\mathbf{x}_{i},v_{i}^{(2)},u_{i},\\overline{\\mathbf{W}}_{i},\\mathbf{r}^{(2)};\\mathbf{W}_{i}^{(2)},\\mathbf{E}_{i}^{(2)},s_{i}^{(2)}),\\quad i=1,2.</span></p>

    <p class="text-gray-300">We next prove that <span class="math">\\mathbf{W}_{i}^{(1)}=\\mathbf{W}_{i}^{(2)}</span> and that <span class="math">\\mathbf{E}_{i}^{(1)}=\\mathbf{E}_{i}^{(2)}</span> for both <span class="math">i=1,2</span>. Indeed, since in particular <span class="math">(\\mathsf{pp},\\mathbf{x}_{i}^{(1)};\\mathbf{w}_{i}^{(1)})</span> and <span class="math">(\\mathsf{pp},\\mathbf{x}_{i}^{(2)};\\mathbf{w}_{i}^{(2)})</span> are in <span class="math">\\mathbf{R}_{\\mathsf{acc}}</span>, we have</p>

    <p class="text-gray-300"><span class="math">\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{W}_{i}^{(1)},s_{i}^{(1)})=\\overline{\\mathbf{W}}_{i}=\\mathsf{Com}(\\mathsf{pp}_{\\mathsf{Com}},\\mathbf{W}_{i}^{(2)},s_{i}^{(2)})\\quad\\text{ for }i=1,2.</span> (32)</p>

    <p class="text-gray-300">Since we assume <span class="math">\\mathcal{E}_{\\mathsf{binding}}</span> holds, we obtain <span class="math">\\mathbf{W}_{1}^{(1)}=\\mathbf{W}_{1}^{(2)}</span> and <span class="math">\\mathbf{W}_{2}^{(1)}=\\mathbf{W}_{2}^{(2)}</span>. Setting <span class="math">\\mathbf{Z}_{i}^{(1)}=(\\mathbf{W}_{i}^{(1)},\\mathbf{x}_{i},1)</span> and <span class="math">\\mathbf{Z}_{i}^{(2)}=(\\mathbf{W}_{i}^{(2)},\\mathbf{x}_{i},1)</span>, it also follows that <span class="math">\\mathbf{Z}_{i}^{(1)}=\\mathbf{Z}_{i}^{(2)}</span> for both <span class="math">i=1,2</span>. To show that <span class="math">\\mathbf{E}_{i}^{(1)}=\\mathbf{E}_{i}^{(2)}</span> for both <span class="math">i=1,2</span>, we use that <span class="math">(\\mathsf{pp},\\mathbf{x}_{1}^{(1)};\\mathbf{w}_{1}^{(1)})</span> and <span class="math">(\\mathsf{pp},\\mathbf{x}_{1}^{(2)};\\mathbf{w}_{1}^{(2)})</span> are in <span class="math">\\mathbf{R}_{\\mathsf{acc}}</span>, and that <span class="math">\\mathbf{Z}_{1}^{(1)}=\\mathbf{Z}_{1}^{(2)}</span>:</p>

    <p class="text-gray-300"><span class="math">\\mathbf{E}_{1}^{(1)}</span> <span class="math">=(A\\cdot\\mathbf{Z}_{1}^{(1)})\\circ(B\\cdot\\mathbf{Z}_{1}^{(1)})-u_{1}\\cdot(C\\cdot\\mathbf{Z}_{1}^{(1)})</span> (33) <span class="math">=(A\\cdot\\mathbf{Z}_{1}^{(2)})\\circ(B\\cdot\\mathbf{Z}_{1}^{(2)})-u_{1}\\cdot(C\\cdot\\mathbf{Z}_{1}^{(2)})=\\mathbf{E}_{1}^{(2)}.</span></p>

    <p class="text-gray-300">An analogous argument shows that <span class="math">\\mathbf{E}_{2}^{(1)}=\\mathbf{E}_{2}^{(2)}</span>.</p>

    <p class="text-gray-300">We now prove that <span class="math">\\beta^{(2)}</span> is a root of <span class="math">h_{1}(X)-\\mathsf{mle}[\\mathbf{E}_{1}^{(1)}]\\circ\\ell(X)</span> and of <span class="math">h_{2}(X)-\\mathsf{mle}[\\mathbf{E}_{2}^{(1)}]\\circ\\ell(X)</span>. Recall that the univariate polynomials <span class="math">h_{1}(X),h_{2}(X)</span> constitute <span class="math">\\mathsf{P}^{<em>}</span>’s first message in all runs of <span class="math">\\langle\\mathsf{P}^{</em>},\\mathsf{V}\\rangle</span>. Since we assumed that</p>

    <p class="text-gray-300"><span class="math">(\\mathsf{pp},\\mathbf{x}_{1}^{(2)},\\mathbf{x}_{2}^{(2)};\\mathbf{w}_{1}^{(2)},\\mathbf{w}_{2}^{(2)})\\in\\mathbf{R}_{\\mathsf{equal}}</span> (34)</p>

    <p class="text-gray-300">and <span class="math">\\mathsf{V}</span> did not abort, we have <span class="math">h_{1}(0)=v_{1}</span> and <span class="math">h_{2}(1)=v_{2}</span>, and <span class="math">v_{1}^{(2)}=h_{1}(\\beta^{(2)})</span>, <span class="math">v_{2}^{(2)}=h_{2}(\\beta^{(2)})</span>. Further, due to (34), <span class="math">v_{i}^{(2)}=\\mathsf{mle}<a href="\\mathbf{r}^{(2">\\mathbf{E}_{i}^{(2)}</a>})</span> for <span class="math">i=1,2</span>. Hence, using that <span class="math">\\mathbf{E}_{1}^{(1)}=\\mathbf{E}_{1}^{(2)}</span>, and <span class="math">\\mathbf{E}_{2}^{(1)}=\\mathbf{E}_{2}^{(2)}</span> we have that that</p>

    <p class="text-gray-300"><span class="math">h_{i}(\\beta^{(2)})</span> <span class="math">=v_{i}^{(2)}=\\mathsf{mle}<a href="\\mathbf{r}^{(2">\\mathbf{E}_{i}^{(2)}</a>})=\\mathsf{mle}<a href="\\mathbf{r}^{(2">\\mathbf{E}_{i}^{(1)}</a>})</span> (35) <span class="math">=\\mathsf{mle}<a href="\\ell(\\beta^{(2">\\mathbf{E}_{i}^{(1)}</a>}))=(\\mathsf{mle}[\\mathbf{E}_{i}^{(1)}]\\circ\\ell)(\\beta^{(2)})</span></p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">for <span class="math">i=1,2</span>, as claimed. Hence, for <span class="math">\\Xi</span> to be successful, it is necessary that <span class="math">\\beta^{(2)}</span> is a root of both degree <span class="math">\\leq\\log(m)</span> univariate polynomials <span class="math">h_{1}(X)-\\mathsf{mle}[\\mathbf{E}_{1}^{(1)}]\\circ\\ell(X)</span> and <span class="math">h_{2}(X)-\\mathsf{mle}[\\mathbf{E}_{2}^{(1)}]\\circ\\ell(X)</span>. Since we assume <span class="math">\\mathcal{E}_{\\mathsf{bad-lines}}</span> holds, we have that at least one of these polynomials is not the zero polynomial. Then, by Schwartz-Zippel lemma, and because <span class="math">\\beta^{(2)}</span> is sampled uniformly at random after <span class="math">\\mathbf{E}^{(1)}</span>, <span class="math">\\ell(X)</span>, and <span class="math">h_{1}(X),h_{2}(X)</span> are determined, the probability that <span class="math">\\Xi</span> is successful is at most $\\log m/</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbb{F}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">=\\mathsf{negl}(\\lambda)$. ∎</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Recall we have only proved so far that, conditioned on <span class="math">\\mathsf{P}^{<em>}</span>’s first message in Step 1 of <span class="math">\\mathsf{Ext}</span> being <span class="math">(h_{1}(X),h_{2}(X))</span>, we have that <span class="math">\\mathsf{Ext}</span> runs in expected polynomial time and outputs a valid witness for <span class="math">\\mathtt{x}</span> with probability <span class="math">\\varepsilon-\\mathsf{negl}(\\lambda)</span>, where <span class="math">\\varepsilon</span> is the probability that <span class="math">(\\mathsf{pp},\\langle\\mathsf{P}^{</em>},\\mathsf{V}\\rangle)\\in\\mathbf{R}_{\\mathsf{R1CS}}</span>, conditioned on <span class="math">\\mathsf{P}^{*}</span>’s first message being <span class="math">(h_{1}(X),h_{2}(X))</span>.</p>

    <p class="text-gray-300">We have to prove that, without conditioning on any <span class="math">\\mathsf{P}^{*}</span>’s first message, <span class="math">\\mathsf{Ext}</span> is a PPT algorithm that outputs a valid witness with probability <span class="math">\\varepsilon_{\\mathsf{total}}-\\mathsf{negl}(\\lambda)</span>. At this point, this follows word by word as the last part of the proof of Lemma 4.1.</p>

    <p class="text-gray-300">Finally, recall we assumed that the evaluation points <span class="math">\\mathbf{r}_{1},\\mathbf{r}_{2}</span> were different. If <span class="math">\\mathbf{r}_{1}=\\mathbf{r}_{2}</span>, then it is trivial to build a PPT extractor that outputs a valid witness with probability <span class="math">\\varepsilon_{\\mathsf{total}}</span>. ∎</p>

    <h2 id="sec-43" class="text-2xl font-bold">8 Acknowledgements</h2>

    <p class="text-gray-300">We thank Ariel Gabizon, Matteo Pintonello, Srinath Setty, Lev Soukhanov, and Justin Thaler for very helpful discussions, and Togzhan Barakbayeva and Matthew Klein for invaluable help in preparing the experimental evaluations of Mova.</p>

    <p class="text-gray-300">This work was supported by the Ethereum Foundation ZK grant FY24-1491.</p>

    <h2 id="sec-44" class="text-2xl font-bold">9 References</h2>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[Arka] Arkworks_contributors. Ark-crypto-primitives. URL: https://github.com/arkworks-rs/crypto-primitives.</li>

      <li>[Arkb] Arkworks_contributors. Arkworks zkSNARK ecosystem/algebra. URL: https://github.com/arkworks-rs/algebra/tree/dcf73a5f9610ba9d16a3c8e0de0b3835e5e5d5e4.</li>

      <li>[Awe] Awesome_folding_contributors. Awesome-folding. URL: https://github.com/lurk-lab/awesome-folding?tab=readme-ov-file#other-resources-podcasts-etc.</li>

      <li>[BC23] Benedikt Bünz and Binyi Chen. Protostar: Generic efficient accumulation/folding for special sound protocols. Cryptology ePrint Archive, Paper 2023/620, 2023. https://eprint.iacr.org/2023/620. URL: https://eprint.iacr.org/2023/620.</li>

      <li>[BC24a] Dan Boneh and Binyi Chen. Latticefold: A lattice-based folding scheme and its applications to succinct proof systems. Cryptology ePrint Archive, Paper</li>

    </ul>

    <p class="text-gray-300">2024/257, 2024. https://eprint.iacr.org/2024/257. URL: https://eprint.iacr.org/2024/257.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[BC24b] Benedikt Bünz and Jessica Chen. Proofs for deep thought: Accumulation for large memories and deterministic computations. Cryptology ePrint Archive, Paper 2024/325, 2024. URL: https://eprint.iacr.org/2024/325.</li>

      <li>[BCL^{+}20] Benedikt Bünz, Alessandro Chiesa, William Lin, Pratyush Mishra, and Nicholas Spooner. Proof-carrying data without succinct arguments. Cryptology ePrint Archive, Paper 2020/1618, 2020. URL: https://eprint.iacr.org/2020/1618.</li>

      <li>[BCMS20] Benedikt Bünz, Alessandro Chiesa, Pratyush Mishra, and Nicholas Spooner. Proof-carrying data from accumulation schemes. Cryptology ePrint Archive, Paper 2020/499, 2020. URL: https://eprint.iacr.org/2020/499.</li>

      <li>[BCS21] Jonathan Bootle, Alessandro Chiesa, and Katerina Sotiraki. Sumcheck arguments and their applications. In Advances in Cryptology–CRYPTO 2021: 41st Annual International Cryptology Conference, CRYPTO 2021, Virtual Event, August 16–20, 2021, Proceedings, Part I 41, pages 742–773. Springer, 2021.</li>

      <li>[BGH19] Sean Bowe, Jack Grigg, and Daira Hopwood. Recursive proof composition without a trusted setup. Cryptology ePrint Archive, Paper 2019/1021, 2019. URL: https://eprint.iacr.org/2019/1021.</li>

      <li>[CT10] Alessandro Chiesa and Eran Tromer. Proof-carrying data and hearsay arguments from signature cards. In ICS, pages 310–331. Tsinghua University Press, 2010.</li>

      <li>[DT24] Quang Dao and Justin Thaler. More optimizations to sum-check proving. Cryptology ePrint Archive, Paper 2024/1210, 2024. https://eprint.iacr.org/2024/1210. URL: https://eprint.iacr.org/2024/1210.</li>

      <li>[EG23] Liam Eagen and Ariel Gabizon. Protogalaxy: Efficient protostar-style folding of multiple instances. Cryptology ePrint Archive, Paper 2023/1106, 2023. https://eprint.iacr.org/2023/1106. URL: https://eprint.iacr.org/2023/1106.</li>

      <li>[Hab22] Ulrich Haböck. Multivariate lookups based on logarithmic derivatives. Cryptology ePrint Archive, Paper 2022/1530, 2022. https://eprint.iacr.org/2022/1530. URL: https://eprint.iacr.org/2022/1530.</li>

      <li>[Hop] Daira Hopwood. Crate ark_pallas. URL: https://docs.rs/ark-pallas/latest/ark_pallas/.</li>

      <li>[KP23] Abhiram Kothapalli and Bryan Parno. Algebraic reductions of knowledge. In Annual International Cryptology Conference, pages 669–701. Springer, 2023.</li>

      <li>[KS23a] Abhiram Kothapalli and Srinath Setty. Cyclefold: Folding-scheme-based recursive arguments over a cycle of elliptic curves. Cryptology ePrint Archive, 2023.</li>

    </ul>

    <p class="text-gray-300">[KS23b] Abhiram Kothapalli and Srinath Setty. Hypernova: Recursive arguments for customizable constraint systems. Cryptology ePrint Archive, Paper 2023/573, 2023. https://eprint.iacr.org/2023/573. URL: https://eprint.iacr.org/2023/573.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[KST21] Abhiram Kothapalli, Srinath Setty, and Ioanna Tzialla. Nova: Recursive zero-knowledge arguments from folding schemes. Cryptology ePrint Archive, Paper 2021/370, 2021. https://eprint.iacr.org/2021/370. URL: https://eprint.iacr.org/2021/370.</li>

      <li>[NDC^{+}24] Wilson Nguyen, Trisha Datta, Binyi Chen, Nirvan Tyagi, and Dan Boneh. Mangrove: A scalable framework for folding-based snarks. Cryptology ePrint Archive, 2024.</li>

      <li>[Ped91] Torben Pryds Pedersen. Non-interactive and information-theoretic secure verifiable secret sharing. In Annual international cryptology conference, pages 129–140. Springer, 1991.</li>

      <li>[Sou24] Lev Soukhanov. Warpfold: Wrongfield arithmetic for protostar folding. Cryptology ePrint Archive, 2024.</li>

      <li>[St] Sonobe-team. Sonobe. URL: https://github.com/privacy-scaling-explorations/sonobe.</li>

      <li>[STW23a] Srinath Setty, Justin Thaler, and Riad Wahby. Customizable constraint systems for succinct arguments. Cryptology ePrint Archive, 2023.</li>

      <li>[STW23b] Srinath Setty, Justin Thaler, and Riad Wahby. Unlocking the lookup singularity with lasso. Cryptology ePrint Archive, Paper 2023/1216, 2023. https://eprint.iacr.org/2023/1216. URL: https://eprint.iacr.org/2023/1216.</li>

      <li>[Tha22] Justin Thaler. Proofs, arguments, and zero-knowledge. Foundations and Trends® in Privacy and Security, 4(2–4):117–660, 2022.</li>

      <li>[Val08] Paul Valiant. Incrementally verifiable computation or proofs of knowledge imply time/space efficiency. pages 1–18, 03 2008. doi:10.1007/978-3-540-78524-8_1.</li>

      <li>[VSBW13] Victor Vu, Srinath Setty, Andrew J. Blumberg, and Michael Walfish. A hybrid architecture for interactive verifiable computation. In 2013 IEEE Symposium on Security and Privacy, pages 223–237, 2013. doi:10.1109/SP.2013.48.</li>

      <li>[ZGGX23] Tianyu Zheng, Shang Gao, Yu Guo, and Bin Xiao. Kilonova: Non-uniform pcd with zero-knowledge property from generic folding schemes. Cryptology ePrint Archive, 2023.</li>

      <li>[zka] zkalc. zkalc is a cryptographic calculator! URL: https://zka.lc/.</li>

      <li>[ZZD23] Zibo Zhou, Zongyang Zhang, and Jin Dong. Proof-carrying data from multi-folding schemes. Cryptology ePrint Archive, 2023.</li>

    </ul>`;
---

<BaseLayout title="Mova: Nova folding without committing to error terms (2024/1220)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2024 &middot; eprint 2024/1220
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <p class="mt-4 text-xs text-gray-500">
        All content below belongs to the original authors. This page
        reproduces the paper for educational purposes. Always
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >cite the original</a>.
      </p>
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <Fragment set:html={CONTENT} />

  </article>
</BaseLayout>
