---
import BaseLayout from '../../layouts/BaseLayout.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2012/549';
const CRAWLER = 'mistral';
const CONVERTED_DATE = '2026-02-16';
const TITLE_HTML = 'Faster batch forgery identification';
const AUTHORS_HTML = 'Daniel J.  Bernstein, Jeroen Doumen, Tanja Lange, Jan-Jaap Oosterwijk';

const CONTENT = `    <p class="text-gray-300">Daniel J. Bernstein<span class="math">^{1,3}</span>, Jeroen Doumen<span class="math">^{2}</span>, Tanja Lange<span class="math">^{3}</span>, and Jan-Jaap Oosterwijk<span class="math">^{3}</span></p>

    <p class="text-gray-300"><span class="math">^{1}</span> Department of Computer Science University of Illinois at Chicago, Chicago, IL 60607-7053, USA djb@cr.yp.to</p>

    <p class="text-gray-300"><span class="math">^{2}</span> Irdeto, CTO Research Group, Taurus Avenue 105, 2132 LS, Hoofddorp, The Netherlands jdoumen@irdeto.com</p>

    <p class="text-gray-300"><span class="math">^{3}</span> Department of Mathematics and Computer Science Technische Universiteit Eindhoven, P.O. Box 513, 5600 MB Eindhoven, The Netherlands tanja@hyperelliptic.org, j.oosterwijk@tue.nl</p>

    <p class="text-gray-300">Abstract. Batch signature verification detects whether a batch of signatures contains any forgeries. Batch forgery identification pinpoints the location of each forgery. Existing forgery-identification schemes vary in their strategies for selecting subbatches to verify (individual checks, binary search, combinatorial designs, etc.) and in their strategies for verifying subbatches. This paper exploits synergies between these two levels of strategies, reducing the cost of batch forgery identification for elliptic-curve signatures.</p>

    <p class="text-gray-300">Keywords: Signatures, batch verification, elliptic curves, scalar multiplication</p>

    <p class="text-gray-300">Our goal in this paper is to minimize the cost of elliptic-curve signature verification. As an illustration of our results, one of our algorithms verifies a sequence of 64 elliptic-curve signatures (from 64 different signers) at a <span class="math">2^{128}</span> security level using</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>a total of <span class="math">0.9 \\cdot 64 \\cdot 128</span> additions if all signatures turn out to be valid,</li>

      <li>a total of <span class="math">1.3 \\cdot 64 \\cdot 128</span> additions if 2 signatures turn out to be invalid,</li>

      <li>a total of <span class="math">2.3 \\cdot 64 \\cdot 128</span> additions if 10 signatures turn out to be invalid, and</li>

      <li>a total of <span class="math">3.6 \\cdot 64 \\cdot 128</span> additions if all 64 signatures turn out to be invalid.</li>

    </ul>

    <p class="text-gray-300">This work was supported by the National Science Foundation under grant 1018836, by the Netherlands Organisation for Scientific Research (NWO) under grant 639.073.005, by the Dutch Technology Foundation STW (which is part of NWO, and which is partly funded by the Ministry of Economic Affairs, Agriculture and Innovation) under grant 10518, and by the European Commission under Contract ICT-2007-216676 ECRYPT II. Permanent ID of this document: 3bde3ab884b9aa2995cb5589e3037232. Date: 2012.09.19.</p>

    <p class="text-gray-300">For comparison, we use a total of <span class="math">2.8\\cdot 64\\cdot 128</span> additions to separately verify the same 64 signatures.</p>

    <p class="text-gray-300">We emphasize that our algorithms pinpoint the forgeries. These algorithms are not merely “batch signature verification” algorithms, saying yes if and only if all of the signatures are valid; these algorithms are “batch forgery identification” algorithms, telling the user separately for each signature whether that signature is valid. The main challenge we address is to locate each forgery as efficiently as possible.</p>

    <p class="text-gray-300">Cost metric. We systematically report the costs of our algorithms in group operations: the total number of elliptic-curve doublings, additions, and subtractions. For conciseness we write “additions” rather than “group operations”, but readers evaluating costs in more detail should be aware that doublings are less expensive than additions in typical elliptic-curve coordinate systems, that “mixed additions” save time, etc.</p>

    <p class="text-gray-300">We also caution the reader that elliptic-curve computations often involve significant overhead beyond group operations. For example, the CHES 2011 elliptic-curve-signatures paper <em>[4]</em> by Bernstein, Duif, Lange, Schwabe, and Yang reports quite noticeable time, even after various speedups, for decompressing points and for manipulating a priority queue of scalars. We would expect our algorithms to use the same amount of time for decompression and less time for manipulating scalars, but properly verifying these predictions would require an optimized assembly-language implementation at the level of <em>[4]</em>.</p>

    <p class="text-gray-300">Our verification algorithms are randomized. Performance depends somewhat on these random choices, but our experiments indicate that the variance in performance (for any particular number of forgeries) is quite small.</p>

    <p class="text-gray-300">The total cost of separately verifying <span class="math">n</span> signatures at a <span class="math">2^{b}</span> security level scales linearly in <span class="math">n</span> and almost linearly in <span class="math">b</span>: it has the form <span class="math">\\alpha nb</span> where <span class="math">\\alpha</span> is independent of <span class="math">n</span> and nearly independent of <span class="math">b</span>. This paper’s batch-forgery-identification algorithms use <span class="math">\\alpha nb</span> additions where <span class="math">\\alpha</span> is a more complicated function of <span class="math">n</span>, <span class="math">b</span>, the number of forgeries, and various algorithm parameters. We systematically report the number of additions in the form <span class="math">\\alpha nb</span>, as illustrated by the <span class="math">0.9nb</span> example above with <span class="math">n=64</span> and <span class="math">b=128</span>.</p>

    <p class="text-gray-300">Choice of signature system. We focus on the EdDSA signature system proposed in <em>[4]</em>. This system is a tweaked version of the classic Schnorr signature system <em>[35]</em>; one of the tweaks allows much faster batch verification.</p>

    <p class="text-gray-300">In EdDSA, verifying a signature <span class="math">(R,S)</span> on a message <span class="math">M</span> under a public key <span class="math">A</span> means verifying an equation of the form <span class="math">SB=R+hA</span>. Here <span class="math">B</span> is a standard elliptic-curve point, <span class="math">R</span> and <span class="math">A</span> are elliptic-curve points, <span class="math">S</span> is a scalar, and the scalar <span class="math">h</span> is a hash of <span class="math">R</span>, <span class="math">A</span>, and <span class="math">M</span>.</p>

    <p class="text-gray-300">For comparison, in Schnorr’s system, the signature is <span class="math">(h,S)</span> rather than <span class="math">(R,S)</span>. The verifier recomputes <span class="math">R=SB-hA</span> and then checks that the hash matches <span class="math">h</span>. This is not compatible with our verification algorithms: our algorithms require <span class="math">R</span> as input.</p>

    <p class="text-gray-300">An analogous tweak for DSA (and the general idea of sending <span class="math">R</span> instead of <span class="math">h</span>) was introduced much earlier by Naccache, M’Raïhi, Vaudenay, and Raphaeli</p>

    <p class="text-gray-300">Faster batch forgery identification</p>

    <p class="text-gray-300">in <em>[23]</em>. We prefer Schnorr to ECDSA (and prefer EdDSA to tweaked ECDSA) for several reasons: Schnorr eliminates inversions, for example, and is resilient to hash-function collisions.</p>

    <p class="text-gray-300">For elliptic-curve signatures at a <span class="math">2^{b}</span> security level it is standard practice to use about <span class="math">2b</span> bits for hashes, scalars, and field elements, and to compress points to single coordinates. EdDSA and Schnorr’s system then have the same signature size, about <span class="math">4b</span> bits. Additions require uncompressed points, so the standard way to verify a signature in Schnorr’s system is to decompress the public key <span class="math">A</span>, compute <span class="math">SB-hA</span>, compress the result to obtain <span class="math">R</span>, compute the hash, and check for a match with <span class="math">h</span>. We emphasize that the same operations, in a different order, verify a signature in EdDSA: compute the hash <span class="math">h</span>, decompress the public key <span class="math">A</span>, compute <span class="math">SB-hA</span>, compress the result, and check for a match with <span class="math">R</span>. The advantage of EdDSA is that it allows further choices for the verifier: fast batch verification, as discussed in <em>[4]</em>, and fast batch forgery identification, as discussed in this paper. These algorithms require decompression of both <span class="math">A</span> and <span class="math">R</span> for each signature, but amply compensate for the extra decompression (an extra square-root computation) by eliminating a large fraction of the subsequent elliptic-curve operations.</p>

    <p class="text-gray-300">One can merge EdDSA with Schnorr’s system, simultaneously allowing signatures of the form <span class="math">(h,S)</span> and signatures of the form <span class="math">(R,S)</span>. The first step in verifying an EdDSA signature computes, as a side effect, a Schnorr signature for the same message; similarly, one of the (later) steps in verifying a Schnorr signature computes, as a side effect, an EdDSA signature. It is not commonly appreciated that Schnorr’s system actually allows hashes as short as <span class="math">b</span> bits (as pointed out by Schnorr), reducing a signature to about <span class="math">3b</span> bits; users then have the flexibility to convert signatures from EdDSA format to Schnorr format to save space, and to convert signatures from Schnorr format to EdDSA format for fast batch forgery identification. One can of course also save decompression time by transmitting uncompressed signatures and uncompressed public keys.</p>

    <p class="text-gray-300">Pairing-based signatures allow shorter signatures, about <span class="math">2b</span> bits, but pairing-based verification is an order of magnitude slower than elliptic-curve verification. Consider, for example, [[21, Figures 1(a), 2(a), 3(a), 4(a)]]: batch verification of pairing-based signatures with <span class="math">b=80</span> costs about <span class="math">2^{14}</span> field multiplications per signature, i.e., about <span class="math">200nb</span> field multiplications. This is the cost in the best case, when there are no forgeries; the cost increases rapidly with the number of forgeries. For comparison, Hisil et al. showed in <em>[12]</em> how to reduce the cost of an elliptic-curve addition to at most <span class="math">8</span> field multiplications; we never use more than <span class="math">4nb</span> additions, i.e., <span class="math">32nb</span> field multiplications.</p>

    <p class="text-gray-300">Previous work on elliptic-curve signature verification. There is an extensive literature analyzing and optimizing various techniques to verify <em>one</em> elliptic-curve signature. The main bottleneck here is double-scalar multiplication, computing an expression of the form <span class="math">\\ell P+mQ</span> where <span class="math">\\ell</span> and <span class="math">m</span> are scalars (typically <span class="math">256</span> bits) and <span class="math">P</span> and <span class="math">Q</span> are elliptic-curve points. Typical speedups include signed digits, windows, sliding windows, fractional windows, and merged doublings; combining these speedups typically reduces the number of additions by a fac</p>

    <p class="text-gray-300">Bernstein, Doumen, Lange, Oosterwijk</p>

    <p class="text-gray-300">tor between 2 and 3 compared to the simplest binary methods of computing <span class="math">\\ell P+mQ</span>. There are also many lower-level speedups inside elliptic-curve additions, field arithmetic, etc., but these speedups have no effect on the number-of-additions metric used for the rest of this paper.</p>

    <p class="text-gray-300">There are, as mentioned above, some papers proposing batch verification of elliptic-curve signatures. The central idea is to check that several quantities <span class="math">V_{1}=R_{1}+h_{1}A_{1}-S_{1}B</span>, <span class="math">V_{2}=R_{2}+h_{2}A_{2}-S_{2}B</span>, etc. are all 0 by checking whether a random linear combination</p>

    <p class="text-gray-300"><span class="math">V=z_{1}R_{1}+z_{2}R_{2}+\\cdots+(z_{1}h_{1})A_{1}+(z_{2}h_{2})A_{2}+\\cdots-(z_{1}S_{1}+z_{2}S_{2}+\\cdots)B</span></p>

    <p class="text-gray-300">is 0. If the verifier chooses the “randomizers” <span class="math">z_{1},z_{2},\\ldots</span> as independent uniform random 128-bit integers then this test cannot be fooled with probability above <span class="math">2^{-128}</span>. We emphasize the importance of including these randomizers; in Section 2 we explain how to break the non-randomized batch-verification system from a very recent paper.</p>

    <p class="text-gray-300">This linear-combination idea was proposed in <em>[23]</em> for (tweaked) DSA, in the simpler (and faster but obviously less useful) case of verifying multiple signatures of the same user, i.e. <span class="math">A_{1}=A_{2}=\\cdots</span>. The speedup in <em>[23]</em> was only a small constant for high security levels, because <em>[23]</em> computed <span class="math">V</span> using only very simple techniques for multi-scalar multiplication, but <em>[4]</em> showed that the Bos–Coster multi-scalar multiplication method produced a much larger speedup. It is easy to see that the speedup here is asymptotically <span class="math">\\Theta(\\lg n)</span> for a batch of <span class="math">n</span> signatures. The first paper to point out a non-constant speedup was <em>[2]</em> by Bellare, Garay, and Rabin, using a different technique that does not appear to be competitive with advanced multi-scalar multiplication methods.</p>

    <p class="text-gray-300">What is missing from all of these papers is an efficient way to handle forgeries. Consider, for example, the following quote from <em>[4]</em>:</p>

    <p class="text-gray-300">&gt; If verification fails then there must be at least one invalid signature. We then fall back to verifying each signature separately. There are several techniques to identify a <em>small</em> number of invalid signatures in a batch, but all known techniques become slower than separate verification as the number of invalid signatures increases; separate verification provides the best defense against denial-of-service attacks.</p>

    <p class="text-gray-300">This strategy means that an attacker sending a low volume of forgeries, enough to have one forgery in each batch, causes a severe slowdown in the software from <em>[4]</em>: each signature ends up being verified separately. It is of course desirable to reduce this damage, if that can be done without compromising performance under heavier denial-of-service floods; what is most desirable is to simultaneously reduce the cost of handling a few forgeries, the cost of handling many forgeries, and every case in between.</p>

    <p class="text-gray-300">Previous work on forgery identification. Pastuszak, Michalek, Pieprzyk, and Seberry in <em>[25]</em> proposed a binary-splitting method of identifying forgeries: if a batch is bad (i.e., fails verification), split it into two halves and apply the same algorithm to each half separately. It is easy to see that this algorithm rapidly</p>

    <p class="text-gray-300">becomes slower than separate verification as the number of forgeries increases; however, this algorithm is the foundation for several improved algorithms discussed below.</p>

    <p class="text-gray-300">If one measures algorithm speed by simply counting the <em>number</em> of batch verifications then the binary-splitting method seems quite fast, identifying each forgery in <span class="math">\\lg n</span> batch verifications where <span class="math">n</span> is the batch size; this is optimal for a single forgery, and diverges only slowly from optimality as the number of forgeries grows. However, the number of batch verifications is not a good measure for the actual amount of time needed to identify the forgeries. Not all verifications require the same amount of time: a larger batch takes longer. Counting additions is a much more realistic cost measure and shows that the binary-splitting method of <em>[25]</em> is actually quite slow.</p>

    <p class="text-gray-300">Pastuszak, Pieprzyk, and Seberry in <em>[26]</em> considered the possibility of <em>non-adaptively</em> choosing subbatches to verify. All available evidence suggests that this non-adaptivity restriction compromises performance even when the number of forgeries is somehow known in advance, and it certainly does not improve performance. Furthermore, non-adaptivity is clearly a disaster when the approximate number of forgeries is not known in advance. We therefore focus on the more flexible adaptive case.</p>

    <p class="text-gray-300">Zaverucha and Stinson in <em>[39]</em> pointed out that there was already a long literature on the number of tests required by adaptive and non-adaptive “group testing” algorithms. Aside from terminology, a “group testing” algorithm is precisely a forgery-identification algorithm built on top of batch verification; in particular, both <em>[25]</em> and <em>[26]</em> fit into this framework. However, the following papers (some of which predate <em>[39]</em>) do not fit into this framework.</p>

    <p class="text-gray-300">Law and Matt in <em>[18]</em> were the first to point out, in the context of pairing-based signatures, that batch verification is providing more information than a simple “yes” or “no”. The most important idea, transported to the elliptic-curve case discussed in this paper, is that one can reuse the randomizers <span class="math">z_{1},\\ldots,z_{n}</span> from <span class="math">V=z_{1}V_{1}+\\cdots+z_{n}V_{n}</span>. If <span class="math">V\\neq 0</span> then the binary-splitting method begins with a half-size multi-scalar multiplication to compute a left-half sum <span class="math">z_{1}V_{1}+\\cdots+z_{n/2}V_{n/2}</span>; and then the right-half sum <span class="math">z_{n/2+1}V_{n/2+1}+\\cdots+z_{n}V_{n}</span> is trivially computed with a single subtraction, rather than another half-size multi-scalar multiplication.</p>

    <p class="text-gray-300">Law and Matt also suggested computing <span class="math">V^{\\prime}=z_{1}V_{1}+2z_{2}V_{2}+\\cdots+nz_{n}V_{n}</span>. If there is just one invalid signature, say <span class="math">V_{i}\\neq 0</span>, then <span class="math">V^{\\prime}=iV</span>, and one can compute <span class="math">i</span> in <span class="math">O(\\sqrt{n})</span> additions by the baby-step-giant-step method. Further development of this approach appears in <em>[18]</em>, <em>[20]</em>, and <em>[21]</em>.</p>

    <p class="text-gray-300">We start from the same ideas, move from pairing-based signatures to elliptic-curve signatures for extra speed, and then point out additional speedups. For example, we introduce two ways to drastically reduce the cost of computing the <em>left-half</em> sum described above, without penalizing other parts of the algorithm. To simplify verifiability and reuse of our results we have posted public-domain implementations of our main algorithms at http://cr.yp.to/badbatch.html.</p>

    <p class="text-gray-300">2 On the importance of being random</p>

    <p class="text-gray-300">The paper <em>[16]</em> by Karati, Das, Roychowdhury, Bellur, Bhattacharya, and Iyer, appearing at Africacrypt 2012 earlier this year, proposed a scheme for batch verification of ECDSA signatures. This section shows that the scheme is insecure. The main problem is that the scheme does not randomize the linear combination being verified.</p>

    <p class="text-gray-300">ECDSA. The basic ECDSA signature scheme works as follows. The system parameters are a prime <span class="math">\\ell</span>, a generator <span class="math">B</span> of an order-<span class="math">\\ell</span> group <span class="math">\\langle B\\rangle</span>, and a cryptographic hash function <span class="math">H</span>. The secret key of a user is a random integer <span class="math">a</span> in <span class="math">[1,\\ell]</span>; the user’s public key is <span class="math">A=aB</span>. The group is a subgroup of the set of <span class="math">\\mathbf{F}_{p}</span>-rational points on an elliptic curve given in Weierstrass form <span class="math">y^{2}=x^{3}+c_{4}x+c_{6}</span> for <span class="math">c_{4},c_{6}\\in\\mathbf{F}_{p}</span>. An affine point is a tuple <span class="math">P=(x(P),y(P))</span> satisfying the curve equation; the negative of this point is <span class="math">-P=(x(P),-y(P))</span>. The curve consists of the affine points and the point at infinity <span class="math">P_{\\infty}</span>, which is the neutral element of the group of points.</p>

    <p class="text-gray-300">A signature on message <span class="math">M</span> under public key <span class="math">A</span> is a tuple <span class="math">(r,s)</span> such that the <span class="math">x</span>-coordinate of <span class="math">(H(M)/s)B+(r/s)A</span> is congruent to <span class="math">r</span> modulo <span class="math">\\ell</span>. The standard approach to verification is to compute <span class="math">R=(H(M)/s)B+(r/s)A</span> and to check that <span class="math">x(R)</span> is congruent to <span class="math">r</span> modulo <span class="math">\\ell</span>.</p>

    <p class="text-gray-300">The scheme from <em>[16]</em> for batch ECDSA verification. The batch verification scheme described in <em>[16]</em> verifies signatures <span class="math">(r_{i},s_{i})</span> on messages <span class="math">M_{i}</span> and public keys <span class="math">A_{i}</span> for <span class="math">1\\leq i\\leq n</span> by reconstructing <span class="math">R_{i}</span> from <span class="math">r_{i}</span> and checking whether <span class="math">\\sum_{i=1}^{n}R_{i}</span> equals <span class="math">(\\sum_{i=1}^{n}H(M_{i})/s_{i})\\,B+\\sum_{i=1}^{n}(r_{i}/s_{i})A_{i}</span>.</p>

    <p class="text-gray-300">The obvious approach to reconstructing <span class="math">R_{i}</span> from <span class="math">r_{i}</span> is to first compute <span class="math">x(R_{i})</span> from <span class="math">x(R_{i})\\bmod\\ell=r_{i}</span> and then compute <span class="math">y(R_{i})</span> from the curve equation. The first step is straightforward in the common case that <span class="math">\\ell\\approx p</span>: there is almost always a unique integer <span class="math">x(R_{i})\\in\\{0,1,\\ldots,p-1\\}</span> satisfying <span class="math">x(R_{i})\\bmod\\ell=r_{i}</span>. The second step is more difficult: it seems to require a square-root computation, and furthermore can at best determine <span class="math">\\pm y(R_{i})</span>; in a batch of <span class="math">n</span> signatures one needs to guess as many as <span class="math">2^{n}</span> combinations of signs. This implies that the batches need to be chosen small; in <em>[16]</em> the maximum batch size considered is 8. The paper puts the main effort into developing new techniques for computing <span class="math">\\sum R_{i}</span> from the <span class="math">x</span>-coordinates in a more efficient manner and reports a good speed-up factor compared to individual verification.</p>

    <p class="text-gray-300">First attack. A batch signature system is broken if invalid signatures pass as valid. The easiest way to break the above scheme is to submit <span class="math">(r,s)</span> as a signature on a target message <span class="math">M</span> under a target public key <span class="math">A</span> and also <span class="math">(r,-s)</span> as a signature on the same message under the same public key, where <span class="math">r</span> is any <span class="math">x</span>-coordinate of a curve point. The verification algorithm reconstructs two points <span class="math">R,-R</span> having <span class="math">x</span>-coordinate <span class="math">r</span>, and then the contributions of these signatures cancel out in both sums:</p>

    <p class="text-gray-300"><span class="math">R+(-R)=P_{\\infty}=(H(M)/s)B+(r/s)A+(H(M)/(-s))B+(r/(-s))A.</span></p>

    <p class="text-gray-300">This attack relies on the fact that <span class="math">r</span> does not pinpoint a unique <span class="math">R</span>: it can be expanded to <span class="math">R</span> and to <span class="math">-R</span>.</p>

    <p class="text-gray-300">These forgeries are easy to detect once the system is altered to check for them. Excluding a sum of <span class="math">P_{\\infty}</span> is not adequate if the batch includes other signatures along with these two forgeries, but checking for repeated <span class="math">r</span> values is adequate. However, as we will see in a moment, there are other attacks on the scheme that are much more difficult to detect.</p>

    <h3 id="sec-3" class="text-xl font-semibold mt-8">0.10 Second attack.</h3>

    <p class="text-gray-300">Assume that the attacker knows the secret key <span class="math">a_{2}</span> for a public key <span class="math">A_{2}</span>. The following attack convinces the verifier to accept a signature on any target message <span class="math">M_{1}</span> under any target public key <span class="math">A_{1}</span>, along with a signature on <span class="math">M_{2}</span> under <span class="math">A_{2}</span>.</p>

    <p class="text-gray-300">The attacker picks a random <span class="math">k_{1}</span>, and computes <span class="math">R_{1}=k_{1}B</span> and <span class="math">r_{1}=x(R_{1})</span> as in proper signature generation. He then picks a random <span class="math">s_{1}</span> and computes <span class="math">R_{2}=(r_{1}/s_{1})A_{1}</span>, <span class="math">r_{2}=x(R_{2})</span>, and <span class="math">s_{2}=(H(M_{2})+r_{2}a_{2})/(k_{1}-H(M_{1})/s_{1})</span>; the denominators are nonzero with overwhelming probability. The attacker then submits <span class="math">(r_{1},s_{1})</span> as signature on <span class="math">M_{1}</span> from <span class="math">A_{1}</span> and <span class="math">(r_{2},s_{2})</span> as signature on <span class="math">M_{2}</span> from <span class="math">A_{2}</span> to the batch system.</p>

    <p class="text-gray-300">The verifier now reconstructs the same <span class="math">R_{1}</span> and <span class="math">R_{2}</span>, and computes <span class="math">R_{1}+R_{2}</span> and <span class="math">(H(M_{1})/s_{1}+H(M_{2})/s_{2})B+(r_{1}/s_{1})A_{1}+(r_{2}/s_{2})A_{2}</span>, both of which equal <span class="math">k_{1}B+(r_{1}/s_{1})A_{1}</span>. These forgeries thus pass verification, even though neither of them is valid individually and the attacker does not know the secret key for <span class="math">A_{1}</span>. The forgeries also work if they are batched together with other signatures in the same verification.</p>

    <p class="text-gray-300">As far as we can tell, the most efficient way to distinguish <span class="math">(r_{1},s_{1})</span> and <span class="math">(r_{2},s_{2})</span> from properly formed signatures is to verify them separately. This trivial batch-verification scheme is obviously secure but also sacrifices all of the speedup reported in <em>[16]</em>.</p>

    <h3 id="sec-4" class="text-xl font-semibold mt-8">0.11 Consequences.</h3>

    <p class="text-gray-300">These attacks show that the scheme considered in <em>[16]</em> is insecure. The second attack would work even if the ECDSA signature system were replaced by a signature system such as EdDSA that transmits <span class="math">R</span> instead of <span class="math">r</span>, removing the <span class="math">\\pm R</span> ambiguity. The second attack shows that it is important to use randomness in the tests: to introduce <span class="math">n</span> sufficiently random integers <span class="math">z_{i}</span> to scale the equations and verify <span class="math">\\sum_{i=1}^{n}z_{i}R_{i}=(\\sum_{i=1}^{n}z_{i}H(M_{i})/s_{i})\\,B+\\sum_{i=1}^{n}(z_{i}r_{i}/s_{i})A_{i}</span> instead.</p>

    <p class="text-gray-300">Randomizers were used in the original batch signature scheme introduced by Naccache, M’Raïhi, Vaudenay, and Raphaeli in <em>[23]</em>. There is no discussion of randomizers in <em>[16]</em>, and in particular no explanation of why the randomizers were omitted in <em>[16]</em>, but it is clear that computing <span class="math">\\sum_{i=1}^{n}z_{i}R_{i}</span> would take much longer than computing <span class="math">\\sum_{i=1}^{n}R_{i}</span>, and it is even harder to compute its <span class="math">x</span>-coordinate from the <span class="math">r_{i}</span> without square-root computations to recover each point <span class="math">R_{i}</span> first.</p>

    <h2 id="sec-5" class="text-2xl font-bold">1</h2>

    <h2 id="sec-6" class="text-2xl font-bold">3 High level: Binary search</h2>

    <p class="text-gray-300">This section presents a family of algorithms for verifying a batch of <span class="math">n</span> EdDSA signatures. We begin with a simple binary-search algorithm and then discuss several variants of the algorithm.</p>

    <p class="text-gray-300">These algorithms rely on multi-scalar multiplication as a lower-level subroutine. Section 4 presents several multi-scalar multiplication algorithms usable in this context, pointing out new synergies between these two levels of algorithms. Section 5 analyzes the overall algorithm cost and reports the results of computer experiments with particular algorithm parameters.</p>

    <p class="text-gray-300">For simplicity we assume that the batch size <span class="math">n</span> is a power of 2. Other batch sizes can be split into power-of-2 batch sizes, or handled directly by straightforward generalizations of the algorithms here.</p>

    <p class="text-gray-300">We also assume for simplicity that <span class="math">B</span> has prime order <span class="math">\\ell</span>, and that all input points <span class="math">R_{i},A_{i}</span> are known in advance to be in the group generated by <span class="math">B</span>. For elliptic-curve groups with small cofactors the usual way to ensure this is to multiply all input points by the cofactor, such as the cofactor 8 in <em>[3]</em> and <em>[4]</em>. A closer look shows that this multiplication can safely be suppressed in the context of signature verification, but since the multiplication has very low cost we skip further discussion.</p>

    <p class="text-gray-300">Randomizers. All of our algorithms use the randomizers <span class="math">z_{i}</span> discussed in Sections 1 and 2. As precomputation we choose <span class="math">z_{1},z_{2},\\ldots,z_{n}</span> independently and uniformly at random from the set <span class="math">\\left\\{1,2,3,\\ldots,2^{b}\\right\\}</span>, where <span class="math">b</span> is the security level. There are several reasonable ways to do this: for example, generate a uniform random <span class="math">b</span>-bit integer and add 1, or generate a uniform random <span class="math">b</span>-bit integer and replace 0 with <span class="math">2^{b}</span>.</p>

    <p class="text-gray-300">Of course, it is also safe to simply generate <span class="math">z_{i}</span> as a uniform random <span class="math">b</span>-bit integer, disregarding the negligible chance that <span class="math">z_{i}=0</span>; but this requires minor technical modifications to the security guarantees stated below, so we prefer to require <span class="math">z_{i}\\neq 0</span>. It is also safe to simulate random numbers as outputs of a strong stream cipher using a long-term random secret key; this is helpful on platforms where generating randomness is expensive. Rather than maintaining stream-cipher state (e.g., the counter in the AES-CTR stream cipher) one can safely encrypt a collision-resistant hash of the input batch.</p>

    <p class="text-gray-300">We also precompute integers <span class="math">h_{1},h_{2},\\ldots,h_{n}</span> as the standard (system-specified) hashes of <span class="math">(R_{1},A_{1},M_{1}),(R_{2},A_{2},M_{2}),\\ldots,(R_{n},A_{n},M_{n})</span> respectively. By definition the <span class="math">i</span>th signature is valid if <span class="math">S_{i}B=R_{i}+h_{i}A_{i}</span>, and a forgery if <span class="math">S_{i}B\\neq R_{i}+h_{i}A_{i}</span>.</p>

    <p class="text-gray-300">Leaf randomizers. In this section we define <span class="math">V_{i}=z_{i}(R_{i}+h_{i}A_{i}-S_{i}B)</span>. Note the inclusion of <span class="math">z_{i}</span> here, deviating from Section 1. This is not merely a change of notation: to verify a single signature (when this is required), our algorithm computes this <span class="math">V_{i}</span>, whereas the standard verification approach from <em>[4]</em> is to compute <span class="math">S_{i}B-h_{i}A_{i}</span>. Note that signature <span class="math">i</span> is valid if and only if <span class="math">V_{i}=0</span>.</p>

    <p class="text-gray-300">The standard approach would seem at first glance to be more efficient: computing <span class="math">S_{i}B-h_{i}A_{i}</span> involves two full-size (<span class="math">2b</span>-bit) scalars <span class="math">S_{i},h_{i}</span>, while computing <span class="math">V_{i}</span> as <span class="math">z_{i}R_{i}+z_{i}h_{i}A_{i}-z_{i}S_{i}B</span> involves two full-size scalars <span class="math">z_{i}h_{i}</span> mod <span class="math">\\ell,z_{i}S_{i}</span> mod <span class="math">\\ell</span></p>

    <p class="text-gray-300">Faster batch forgery identification</p>

    <p class="text-gray-300">!<a href="img-0.jpeg">img-0.jpeg</a> Fig. 3.1. Tree of sums of randomized leaves  <span class="math">V_{1}, V_{2}, \\ldots, V_{n}</span>  for  <span class="math">n = 16</span> .</p>

    <p class="text-gray-300">and a half-size scalar  <span class="math">z_{i}</span> , for a total of  <span class="math">25\\%</span>  more scalar bits. However, the cost of multi-scalar multiplication (see Section 4) is affected much more by the maximum number of scalar bits than by the total number of scalar bits; the cost of computing  <span class="math">V_{i}</span>  turns out to be only slightly higher than the cost of computing  <span class="math">S_{i}B - h_{i}A_{i}</span> . This slight extra expense pays off in subsequent steps of the batch algorithm, as discussed below.</p>

    <p class="text-gray-300">Shared randomizers. Starting from these randomized quantities  <span class="math">V_{1}, \\ldots, V_{n}</span>  we draw a binary tree as illustrated in Figure 3.1, with  <span class="math">V_{1,2} = V_{1} + V_{2}</span>  and  <span class="math">V_{3,4} = V_{3} + V_{4}</span>  and so on at the second level,  <span class="math">V_{1,4} = V_{1,2} + V_{3,4}</span>  and so on at the third level, etc. In general we write  <span class="math">V_{j,k}</span>  for the sum  <span class="math">\\sum_{j \\leq i \\leq k} V_{i}</span>  of leaf nodes. If all of the signatures at positions  <span class="math">j, j + 1, \\ldots, k</span>  are valid then  <span class="math">V_{j,k} = 0</span> , while if any of the signatures are invalid then with overwhelming probability  <span class="math">V_{j,k} \\neq 0</span> . The root node  <span class="math">V_{1,n}</span>  at the top represents the randomized signature verification of the entire batch; we denote this sum by  <span class="math">V</span>  as a shorthand.</p>

    <p class="text-gray-300">The set of tree nodes actually computed by the algorithm is determined adaptively; see below.</p>

    <p class="text-gray-300">We emphasize that one sequence of randomizers is shared across all levels of the tree, including the leaf nodes. This reuse does not compromise the security of the algorithm: if signature  <span class="math">i</span>  is invalid then with overwhelming probability all of the ancestor tree nodes  <span class="math">V_{j,k}</span>  with  <span class="math">j \\leq i \\leq k</span>  are nonzero. More precisely, fix a batch of signatures, and define a randomizer sequence  <span class="math">(z_1, \\ldots, z_n)</span>  as "bad" if it produces any zeros among ancestor tree nodes of any invalid signature; then the probability of a randomizer sequence being bad is at most  <span class="math">(n - 1) / 2^b</span> . The point is that if signature  <span class="math">i</span>  is invalid (i.e.,  <span class="math">R_i + h_i A_i - S_i B \\neq 0</span> ), then any equation  <span class="math">V_{j,k} = 0</span>  for  <span class="math">j \\leq i \\leq k</span>  is equivalent to a linear equation  <span class="math">\\dots + z_i (R_i + h_i A_i - S_i B) + \\dots = 0</span> . For each choice of  <span class="math">z_1, \\ldots, z_{i-1}, z_{i+1}, \\ldots, z_n</span>  this equation is satisfied by exactly one integer  <span class="math">z_i</span>  modulo  <span class="math">\\ell</span> , and therefore at most one out of the  <span class="math">2^b</span>  permitted choices of  <span class="math">z_i</span> . A randomizer sequence is therefore " <span class="math">(j, k)</span> -bad" with probability at most  <span class="math">1 / 2^b</span>  for  <span class="math">j &amp;lt; k</span>  (i.e., non-leaf nodes), and with probability 0 for  <span class="math">j = k</span>  (i.e., leaf</p>

    <p class="text-gray-300">Bernstein, Doumen, Lange, Oosterwijk</p>

    <p class="text-gray-300">nodes). There are <span class="math">n-1</span> non-leaf nodes, so a randomizer sequence is bad with probability at most <span class="math">(n-1)/2^{b}</span>.</p>

    <p class="text-gray-300">The basic batch-forgery-identification algorithm. The following algorithm takes as input public keys <span class="math">A_{1},A_{2},\\ldots ,A_{n}</span>, signatures <span class="math">(R_{1},S_{1}),\\ldots ,(R_{n},S_{n})</span>, precomputed hashes <span class="math">h_1,h_2,\\dots ,h_n</span>, and precomputed randomizers <span class="math">z_{1},z_{2},\\ldots ,z_{n}</span>. The algorithm also takes an optional input <span class="math">V</span>; this is used when the algorithm calls itself recursively in Step 5.</p>

    <p class="text-gray-300">The algorithm provides two outputs: first, <span class="math">V</span>, whether or not <span class="math">V</span> was provided as input; second, an <span class="math">n</span>-bit string <span class="math">(b_{1}, b_{2}, \\ldots, b_{n})</span>. With overwhelming probability <span class="math">b_{i} = 1</span> if and only if signature <span class="math">i</span> is valid.</p>

    <p class="text-gray-300">The algorithm has six steps:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Batch verification: Compute <span class="math">V = \\sum_{i} z_{i} (R_{i} + h_{i} A_{i} - S_{i} B)</span>, if <span class="math">V</span> was not provided as input. Output <span class="math">V</span>. If <span class="math">V = 0</span>, output <span class="math">n</span> bits <span class="math">(1, 1, \\ldots, 1)</span> and stop.</li>

      <li>Forgery rejection: If <span class="math">n = 1</span>, output (0) and stop. (At this point <span class="math">V \\neq 0</span>, so the signature is invalid.)</li>

      <li>Left subtree: Apply the same algorithm recursively to <span class="math">A_{1}, A_{2}, \\ldots, A_{n/2}</span>; <span class="math">(R_{1}, S_{1}), \\ldots, (R_{n/2}, S_{n/2})</span>; <span class="math">h_{1}, \\ldots, h_{n/2}</span>; and <span class="math">z_{1}, \\ldots, z_{n/2}</span>; obtaining outputs <span class="math">V_{1,n/2}</span> and <span class="math">(b_{1}, \\ldots, b_{n/2})</span>.</li>

      <li>Right root: If <span class="math">V_{1,n/2} = 0</span>, set <span class="math">V_{n/2+1,n} = V</span>. If <span class="math">V_{1,n/2} = V</span>, set <span class="math">V_{n/2+1,n} = 0</span>. Otherwise compute <span class="math">V_{n/2+1,n} = V - V_{1,n/2}</span>.</li>

      <li>Right subtree: Apply the same algorithm recursively to <span class="math">A_{n/2+1}, \\ldots, A_n</span>; <span class="math">(R_{n/2+1}, S_{n/2+1}), \\ldots, (R_n, S_n)</span>; <span class="math">h_{n/2+1}, \\ldots, h_n</span>; <span class="math">z_{n/2+1}, \\ldots, z_n</span>; and <span class="math">V_{n/2+1,n}</span>; obtaining outputs <span class="math">V_{n/2+1,n}</span> and <span class="math">(b_{n/2+1}, \\ldots, b_n)</span>.</li>

      <li>Final output: Output <span class="math">(b_{1},\\ldots ,b_{n})</span></li>

    </ol>

    <p class="text-gray-300">This algorithm is optimistic, hoping that there are no forgeries: Step 1 finishes the algorithm as quickly as possible in this case. See Section 4 for details of the computation in this step. The overall binary-splitting structure of this algorithm is taken from [25]. The fast computation of <span class="math">V_{n/2+1,n}</span> in Step 4, using at most one subtraction, is taken from [18]; this is also the reason for treating <span class="math">V</span> as an output and an optional input. This fast computation means that at most <span class="math">n</span> nodes require a multi-scalar multiplication in Step 1; Figure 3.2 illustrates the worst case.</p>

    <p class="text-gray-300">Another way to organize essentially the same computation is to record a partial tree of known <span class="math">V_{j,k}</span> values, and to very quickly update the tree whenever a forgery is discovered, in effect retroactively removing the forgery from the batch. Start the computation at the root; after computing a zero node, deduce without further computation that all descendants of the node are also zero; after computing a nonzero leaf node <span class="math">V_{i} \\neq 0</span>, replace all ancestors <span class="math">V_{j,k}</span> by <span class="math">V_{j,k} - V_{i}</span>, skipping the subtraction in the common case that <span class="math">V_{j,k} = V_{i}</span>; after computing a nonzero non-leaf node, compute the left child node (and all of its descendants in order), and then simply copy this (possibly updated) node to the right child node.</p>

    <p class="text-gray-300">Leaf randomizers, continued. In the case <span class="math">n = 1</span> this algorithm computes <span class="math">V_{1} = z_{1}(R_{1} + h_{1}A_{1} - S_{1}B)</span>. As discussed above, this is only slightly more expensive</p>

    <p class="text-gray-300">Faster batch forgery identification</p>

    <p class="text-gray-300">!<a href="img-1.jpeg">img-1.jpeg</a> Fig. 3.2. Tests in worst case are depicted in order.</p>

    <p class="text-gray-300">than computing  <span class="math">S_{1}B - h_{1}A_{1}</span> . We now explain the compensating advantage of computing  <span class="math">V_{1}</span> .</p>

    <p class="text-gray-300">Consider a batch of two signatures that fails batch verification. i.e.,  <span class="math">V_{1,2} \\neq 0</span> . This algorithm computes  <span class="math">V_{1}</span>  (showing whether the first signature is valid), and then deduces  <span class="math">V_{2}</span>  (showing whether the second signature is valid) with at most one subtraction. For comparison, one could instead compare  <span class="math">S_{1}B - h_{1}A_{1}</span>  to  <span class="math">R_{1}</span>  to see whether the first signature is valid, but one then still needs to check whether the second signature is valid. One could check the second signature separately, or multiply  <span class="math">R_{1} + h_{1}A_{1} - S_{1}B</span>  by  <span class="math">z_{1}</span>  to obtain  <span class="math">V_{1}</span>  and thus  <span class="math">V_{2}</span> , but simply starting with  <span class="math">V_{1}</span>  is less expensive.</p>

    <p class="text-gray-300">Early abort. This algorithm is faster than separate verification when there are not many forgeries, but as discussed in subsequent sections it becomes noticeably slower than separate verification when there are many forgeries. The gap is not very large, but we would still like to minimize it.</p>

    <p class="text-gray-300">We thus propose (1) using the fraction of invalid signatures found so far as an estimate for the expected fraction of invalid signatures in the rest of the tree, and (2) deciding on this basis whether it is best to abort the tree structure and check individual signatures.</p>

    <p class="text-gray-300">An attacker might try to spoil the estimate by, e.g., placing several invalid signatures at the beginning of a large batch. After those signatures the algorithm will confidently, but incorrectly, estimate that the entire batch is invalid. To prevent such attacks one can simply apply a random permutation to the sequence of signatures before applying the algorithm. (One can also imagine tracking forgery percentages long term from one batch to another, but for simplicity we handle each batch separately.)</p>

    <p class="text-gray-300">There is, furthermore, no need for aborts to be permanent: one can return to binary search for the next part of the tree if the fraction of invalid signatures has become small enough again. We actually propose making a new decision</p>

    <p class="text-gray-300">Bernstein, Doumen, Lange, Oosterwijk</p>

    <p class="text-gray-300">!<a href="img-2.jpeg">img-2.jpeg</a> Fig. 3.3. Tests performed for  <span class="math">n = 16</span>  when all signatures are invalid, using the early abort. Arrows denote the test replacements and savings.</p>

    <p class="text-gray-300">whenever a node is about to be computed. In the notation of the basic algorithm above, we dynamically choose between</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>optimism: computing  <span class="math">V</span> , and then, if  <span class="math">V \\neq 0</span> , computing  <span class="math">V_{1,n/2}</span>  and deducing  <span class="math">V_{n/2+1,n} = V - V_{1,n/2}</span> ; or</li>

      <li>pessimism: computing  <span class="math">V_{1,n/2}</span>  and  <span class="math">V_{n/2+1,n}</span> , and then deducing  <span class="math">V = V_{1,n/2} + V_{n/2+1,n}</span> .</li>

    </ul>

    <p class="text-gray-300">If  <span class="math">V</span>  is provided as input then optimism is better. If  <span class="math">V</span>  is not provided as input then we use  <span class="math">(1 - p)^n</span>  as an estimate of the chance that  <span class="math">V = 0</span> , where  <span class="math">p</span>  is the fraction of invalid signatures found so far (or 0 at the beginning of the algorithm), and then compare the expected costs of optimism and pessimism, using straightforward models of the costs of computing  <span class="math">V, V_{1,n/2}, V_{n/2+1,n}</span> .</p>

    <p class="text-gray-300">When there are few forgeries, this approach performs the same computations as the basic algorithm. When there are many forgeries, this approach rapidly converges on checking each signature separately, as shown in Figure 3.3. Compared to the previous worst case, where we computed the top node of each vertical branch, we now only need to compute the top nodes of the main left diagonal branch. In all other vertical branches, the leaf node is computed directly. (One can do marginally better in this extreme case by immediately updating  <span class="math">p</span>  after discovering  <span class="math">V_{1,16} \\neq 0</span> : there must be a forgery somewhere, even though it has not been located yet.)</p>

    <p class="text-gray-300">When there is a medium fraction of forgeries, this approach skips roots of large subtrees (since those roots are likely to fail verification and require computations of descendant nodes), but computes roots of small subtrees. For example, assume that we identified exactly 2 forgeries out of the first 16 signatures. We expect the same fraction of  <span class="math">1/8</span>  invalid signatures in the next group of 16, so we estimate that  <span class="math">V_{17,32} = 0</span>  with probability only  <span class="math">11\\%</span> , that  <span class="math">V_{17,24} = 0</span>  with probability  <span class="math">34\\%</span> , and that  <span class="math">V_{17,20} = 0</span>  with probability  <span class="math">59\\%</span> . The next step depends on scalar-multiplication costs; we might decide to skip  <span class="math">V_{17,32}</span>  and  <span class="math">V_{17,24}</span> , and proceed</p>

    <p class="text-gray-300">Faster batch forgery identification</p>

    <p class="text-gray-300">directly to <span class="math">V_{17,20}</span>. If the fraction of invalid signatures remains stable then we will check these 16 signatures as 4 batches of 4 signatures each. We then decide anew how to check the next 32 signatures.</p>

    <p class="text-gray-300">Smaller randomizers. Large randomizers <span class="math">z_{i}</span> are critical for detecting multi-forgeries, as discussed in Section 2, but this does not mean that large randomizers are required at each step of the tree. An alternative approach is to use one sequence of large randomizers at the root, and to use a second sequence of much smaller randomizers, say 20 bits each, for the subsequent levels of the tree.</p>

    <p class="text-gray-300">This approach slightly speeds up multi-scalar multiplication at non-root nodes. However, this approach also has several costs. First, the right child of the root node is no longer obtained for free. Second, the sharing described in Section 4 begins only at the children of the root node, not at the root node itself. Third, an attacker can fool the smaller randomizers with noticeable probability, on the scale of <span class="math">2^{-20}</span>, so after identifying forgeries using the smaller randomizers one must recompute the corresponding portion of the root node. If this root-node update shows that any forgeries remain then one must choose a new sequence of smaller randomizers and try the computation again on the remaining signatures.</p>

    <h2 id="sec-7" class="text-2xl font-bold">4 Low level: Trees of optional multi-scalar multiplications</h2>

    <p class="text-gray-300">This section looks more closely at the first step of the algorithm of Section 3: namely, batch verification, i.e., computing a linear combination</p>

    <p class="text-gray-300"><span class="math">V=z_{1}R_{1}+\\cdots+z_{n}R_{n}+(z_{1}h_{1})A_{1}+\\cdots+(z_{n}h_{n})A_{n}-(z_{1}S_{1}+\\cdots+z_{n}S_{n})B</span></p>

    <p class="text-gray-300">of known elliptic-curve points <span class="math">R_{1},\\ldots,R_{n},A_{1},\\ldots,A_{n},B</span>. If <span class="math">V\\neq 0</span> then the algorithm calls itself recursively and computes a smaller linear combination</p>

    <p class="text-gray-300"><span class="math">V_{1,m}=z_{1}R_{1}+\\cdots+z_{m}R_{m}+(z_{1}h_{1})A_{1}+\\cdots+(z_{m}h_{m})A_{m}-(z_{1}S_{1}+\\cdots+z_{m}S_{m})B</span></p>

    <p class="text-gray-300">with <span class="math">m=n/2</span>.</p>

    <p class="text-gray-300">The computation of <span class="math">V</span> by itself is a standard <span class="math">(2n+1)</span>-scalar-multiplication problem. The only mildly uncommon feature of this problem is that the scalars have variable size, typically <span class="math">n</span> 128-bit scalars (the <span class="math">z_{i}</span>’s) and <span class="math">n+1</span> 256-bit scalars; but typical scalar-multiplication algorithms can trivially take advantage of the shorter scalars. Similarly, the computation of <span class="math">V_{1,m}</span> by itself is a standard <span class="math">(2m+1)</span>-scalar-multiplication problem.</p>

    <p class="text-gray-300">Quite nonstandard, however, is the multi-scalar-multiplication problem that we actually face: computing <span class="math">V</span> and then <em>perhaps</em> computing <span class="math">V_{1,m}</span>. If we <em>knew</em> that we wanted to compute both <span class="math">V</span> and <span class="math">V_{1,m}</span> then the obvious approach would be two separate half-size computations, one for <span class="math">V_{1,m}</span> and one for <span class="math">V_{m+1,n}=V-V_{1,m}</span>; but we do not know this in advance. If <span class="math">V</span> turns out to be 0 then we will not need <span class="math">V_{1,m}</span> and <span class="math">V_{m+1,n}</span>, and a single full-size computation of <span class="math">V</span> will be more efficient than two separate half-size computations.</p>

    <p class="text-gray-300">The point of this section is that some — although certainly not all — state-of-the-art algorithms to compute <span class="math">V</span> can be modified at negligible cost to remember</p>

    <p class="text-gray-300">Bernstein, Doumen, Lange, Oosterwijk</p>

    <p class="text-gray-300">many intermediate results useful for computing <span class="math">V_{1,m}</span>. The same idea can easily be pushed to further levels: for example, computing <span class="math">V</span>, then optionally <span class="math">V_{1,m}</span>, then optionally <span class="math">V_{1,\\lfloor m/2\\rfloor}</span> and optionally <span class="math">V_{m+1,m+1+\\lfloor(n-m)/2\\rfloor}</span>.</p>

    <p class="text-gray-300"><strong>Overlap in the Bos–Coster approach.</strong> As an illustration of what does <em>not</em> seem to work very well in this context, consider the Bos–Coster algorithm reported in [8, Section 4]. This algorithm computes <span class="math">a_1P_1 + a_2P_2 + a_3P_3 + \\cdots</span>, where <span class="math">a_1 \\geq a_2 \\geq a_3 \\geq \\cdots</span>, by recursively computing <span class="math">(a_1 - a_2)P_1 + a_2(P_1 + P_2) + a_3P_3 + \\cdots</span>. This algorithm was used in [4] to compute <span class="math">V</span>.</p>

    <p class="text-gray-300">The first few additions performed in the Bos–Coster algorithm depend only on the largest scalars. If we permute signatures so that <span class="math">z_{1}h_{1} \\geq z_{2}h_{2} \\geq \\cdots</span>, and handle <span class="math">z_{1}S_{1} + \\cdots + z_{n}S_{n}</span> separately, then the first <span class="math">\\approx m</span> additions in the algorithm will involve only <span class="math">A_{1}, \\ldots, A_{m}</span>, and will thus be the same as the first additions involved in computing <span class="math">V_{1,m}</span>. However, this is only a slight speedup.</p>

    <p class="text-gray-300"><strong>Overlap in the Straus approach.</strong> As a better example, consider the Straus algorithm [37], often miscredited to Shamir. This algorithm computes <span class="math">a_1P_1 + a_2P_2 + \\cdots + a_nP_n</span> by recursively computing <span class="math">\\lfloor a_1 / 2^c \\rfloor P_1 + \\lfloor a_2 / 2^c \\rfloor P_2 + \\cdots + \\lfloor a_n / 2^c \\rfloor P_n</span>, doubling <span class="math">c</span> times, and then adding the precomputed quantity <span class="math">(a_1 \\bmod 2^c)P_1 + (a_2 \\bmod 2^c)P_2 + \\cdots + (a_n \\bmod 2^c)P_n</span>. Here <span class="math">2^c</span> is a radix chosen by the algorithm; for example, it is reasonable to take <span class="math">c = 5</span> for 256-bit scalars. We skip discussion of standard speedups such as signed digits.</p>

    <p class="text-gray-300">This algorithm scales poorly to large values of <span class="math">n</span> (because it involves too much precomputation, even for <span class="math">c = 1</span>), but a standard variant scales well to large values of <span class="math">n</span>: at the last step one instead adds the separate precomputed quantities <span class="math">(a_1 \\bmod 2^c)P_1</span>, <span class="math">(a_2 \\bmod 2^c)P_2</span>, etc.</p>

    <p class="text-gray-300">Evidently one can reuse these precomputed quantities for a subsequent multi-scalar multiplication involving <span class="math">P_{1}, \\ldots, P_{m}</span> with the same choice of <span class="math">c</span>. Furthermore, if the precomputed quantities are added from left to right in each step, then one of the intermediate results is exactly <span class="math">(a_{1} \\bmod 2^{c})P_{1} + \\cdots + (a_{m} \\bmod 2^{c})P_{m}</span>. This drastically reduces the cost of computing <span class="math">a_{1}P_{1} + \\cdots + a_{m}P_{m}</span> when <span class="math">m</span> is large: each step of the recursion drops from cost <span class="math">c + m</span> (<span class="math">c</span> doublings and <span class="math">m</span> additions) down to just <span class="math">c + 1</span>.</p>

    <p class="text-gray-300">The same overlap applies immediately to <span class="math">a_1P_1 + \\cdots + a_{\\lfloor m/2 \\rfloor}P_{\\lfloor m/2 \\rfloor}</span>. Even better, if we change the order to add precomputed quantities, recursively adding the <span class="math">P_1, \\ldots, P_m</span> part and the <span class="math">P_{m+1}, \\ldots, P_n</span> part, then the same overlap applies not just to left descendants but to arbitrary descendants.</p>

    <p class="text-gray-300"><strong>Overlap in the Pippenger approach.</strong> As a more advanced example, consider Pippenger’s multi-scalar-multiplication method. This method was published in [28] almost forty years ago; various special cases of the method were subsequently reinvented and published in the papers [6] and [19] and continue to be frequently miscredited to those papers. We comment that the patent accompanying [6] (U.S. patent 5299262) expired this year.</p>

    <p class="text-gray-300">Pippenger’s method is not as simple as the Bos–Coster method or the Straus method, but it is considerably faster when there are many large scalars. It is almost twice as fast in some cases, and it is within <span class="math">1 + o(1)</span> of optimal for</p>

    <p class="text-gray-300">essentially all sequences of scalars; see generally <em>[30]</em>. Of course, this does not imply that Pippenger’s method is optimal for the problem of computing <span class="math">V</span> and then perhaps <span class="math">V_{1,m}</span>, but inspecting the details shows that Pippenger’s approach does allow considerable savings in computing <span class="math">V_{1,m}</span>.</p>

    <p class="text-gray-300">The following special case of Pippenger’s algorithm has similar performance to the Bos–Coster method and is adequate to illustrate the idea. Choose a radix <span class="math">2^{c}</span> as above, and proceed as in Straus’s algorithm, but replace the last step with the following computation. Sort the points <span class="math">P_{1},P_{2},\\ldots,P_{n}</span> into <span class="math">2^{c}</span> buckets according to the values <span class="math">a_{1}\\bmod 2^{c},a_{2}\\bmod 2^{c},\\ldots,a_{n}\\bmod 2^{c}</span>. Discard bucket <span class="math">0</span> and add the points in the remaining buckets, obtaining sums <span class="math">S_{1},\\ldots,S_{2^{c}-1}</span>. Now compute</p>

    <p class="text-gray-300"><span class="math">(a_{1}\\bmod 2^{c})P_{1}+\\cdots+(a_{n}\\bmod 2^{c})P_{n}=S_{1}+2S_{2}+\\cdots+(2^{c}-1)S_{2^{c}-1}</span></p>

    <p class="text-gray-300">as the sum of the intermediate quantities <span class="math">S_{2^{c}-1}</span>, <span class="math">S_{2^{c}-1}+S_{2^{c}-2}</span>, <span class="math">\\ldots</span>, <span class="math">S_{2^{c}-1}+S_{2^{c}-2}+\\cdots+S_{1}</span>.</p>

    <p class="text-gray-300">Observe that computing <span class="math">a_{1}P_{1}+\\cdots+a_{m}P_{m}</span> in the same way, using the same value of <span class="math">c</span>, puts <span class="math">P_{1},P_{2},\\ldots,P_{m}</span> into exactly the same buckets. If for the <span class="math">a_{1}P_{1}+\\cdots+a_{n}P_{n}</span> computation we are careful to add points in each bucket from left to right then the intermediate result after <span class="math">P_{1},P_{2},\\ldots,P_{m}</span> will be exactly the sum relevant to <span class="math">a_{1}P_{1}+\\cdots+a_{m}P_{m}</span>. For typical parameters there are several points in each bucket, so this approach is several times faster than a standard computation of <span class="math">a_{1}P_{1}+\\cdots+a_{m}P_{m}</span>. As before, it is even better to change the order to add points in each bucket, recursively adding the points that come from <span class="math">P_{1},P_{2},\\ldots,P_{m}</span> and the points that come from <span class="math">P_{m+1},\\ldots,P_{n}</span>.</p>

    <p class="text-gray-300">Handling the base point. These modified versions of the Straus and Pippenger methods apply directly to</p>

    <p class="text-gray-300"><span class="math">z_{1}R_{1}+(z_{1}h_{1})A_{1}+\\cdots+z_{n}R_{n}+(z_{n}h_{n})A_{n}</span></p>

    <p class="text-gray-300">but do not apply directly to <span class="math">(z_{1}S_{1}+\\cdots+z_{n}S_{n})B</span>, the last component of <span class="math">V</span>.</p>

    <p class="text-gray-300">The simplest way to handle these multiples of <span class="math">B</span> is to compute them separately. Because <span class="math">B</span> is a fixed base point, one can afford a precomputed table of, e.g., <span class="math">B,2B,3B,\\ldots,(2^{c}-1)B</span> and <span class="math">2^{c}B,2\\cdot 2^{c}B,3\\cdot 2^{c}B,\\ldots,(2^{c}-1)\\cdot 2^{c}B</span> and so on. Computing any desired multiple of <span class="math">B</span> then takes fewer than <span class="math">1/c</span> additions for each bit of the scalar, a very small cost compared to the other computations discussed here.</p>

    <h2 id="sec-8" class="text-2xl font-bold">5 Analysis</h2>

    <p class="text-gray-300">This section analyzes the cost of identifying all of the forgeries among <span class="math">n</span> elliptic-curve signatures at a <span class="math">2^{b}</span> security level. Full-size scalars such as <span class="math">h_{i},S_{i},z_{i}h_{i}\\bmod\\ell,z_{i}S_{i}\\bmod\\ell</span> then have <span class="math">2b</span> bits as discussed in Section 1, while the randomizers <span class="math">z_{i}</span> have <span class="math">b</span> bits.</p>

    <p class="text-gray-300">Our web page http://cr.yp.to/badbatch.html includes all of the software mentioned in this section.</p>

    <p class="text-gray-300">Bernstein, Doumen, Lange, Oosterwijk</p>

    <p class="text-gray-300">Separate signature verification. Solinas’ widely used Joint Sparse Form <em>[36]</em> handles a double-scalar multiplication <span class="math">h_{i}A_{i}-S_{i}B</span> using <span class="math">2b</span> doublings and on average <span class="math">b</span> additions, for a total cost of <span class="math">3nb</span> to handle <span class="math">n</span> signatures.</p>

    <p class="text-gray-300">Straus’s method is asymptotically more efficient, handling <span class="math">n</span> signatures at cost <span class="math">(2+o(1))nb</span> as <span class="math">b\\to\\infty</span>. Straus’s method involves approximately <span class="math">2b</span> doublings; every <span class="math">c</span> doublings are followed by <span class="math">2</span> additions, and on average a fraction <span class="math">1/2^{c}</span> of the additions are skippable additions of <span class="math">0</span>. The additions rely on an initial computation of <span class="math">2A_{1},3A_{1},\\ldots,(2^{c}-1)A_{1}</span>, which costs <span class="math">2^{c}-2</span>, and a free precomputation of <span class="math">2B,3B,\\ldots,(2^{c}-1)B</span>. The total cost for <span class="math">n</span> signatures is approximately <span class="math">(2b+(1-1/2^{c})(2/c)2b+2^{c}-2)n</span>. One can balance the terms <span class="math">(1-1/2^{c})(4/c)b</span> and <span class="math">2^{c}-2</span> by taking <span class="math">c</span> close to <span class="math">2+\\lg b-\\lg\\lg b</span>; the total cost is then roughly <span class="math">(2+8/\\lg b)nb</span>.</p>

    <p class="text-gray-300">Our separate3.py software uses Straus’s method with <span class="math">c=4</span> and with two standard speedups, namely signed digits and sliding windows. This software uses, on average, fewer than <span class="math">2.8nb</span> additions for <span class="math">b=128</span>. There is a small variance: <span class="math">2.75nb</span> and <span class="math">2.82nb</span> are not unusual. We would expect more detailed optimization here, in particular using more precomputed multiples of <span class="math">B</span>, to beat <span class="math">2.7nb</span>.</p>

    <p class="text-gray-300">Batch verification. All of our batch-forgery-identification algorithms start with batch verification, computing <span class="math">V</span>. If there are no forgeries — no attackers attempting to fool the receiver or deny service — then this is the end of the computation.</p>

    <p class="text-gray-300">Straus’s algorithm computes <span class="math">V</span> with <span class="math">2b</span> doublings as above, approximately <span class="math">n(b/c)</span> additions for parts of <span class="math">z_{i}R_{i}</span>, approximately <span class="math">n(2b/c)</span> additions for parts of <span class="math">(z_{i}h_{i})A_{i}</span>, and negligible cost for <span class="math">B</span>. The additions rely on initial computations costing <span class="math">2n(2^{c}-2)</span>. The total cost is approximately <span class="math">(2/n+3/c+2(2^{c}-2)/b)nb</span>. If <span class="math">c</span> is chosen close to <span class="math">\\lg(1.5b)-\\lg\\lg b</span> then this cost is roughly <span class="math">(2/n+6/\\lg b)nb</span>.</p>

    <p class="text-gray-300">Our straus6.py software, with <span class="math">b=128</span> and <span class="math">c=5</span>, uses <span class="math">1.15nb</span> additions for <span class="math">n=16</span>; <span class="math">0.98nb</span> additions for <span class="math">n=16</span>; <span class="math">0.90nb</span> additions for <span class="math">n=32</span>; and <span class="math">0.86nb</span> additions for <span class="math">n=64</span>.</p>

    <p class="text-gray-300">We also experimented with the Bos–Coster algorithm (boscoster2.py) and did some preliminary analysis of Pippenger’s algorithm. Compared to Straus’s algorithm, we obtained better batch-verification speeds with the Bos–Coster algorithm (e.g., cost <span class="math">0.55nb</span> for <span class="math">n=64</span> and <span class="math">b=128</span>) and we expect to obtain better batch-verification speeds with Pippenger’s algorithm. Asymptotically the Bos–Coster algorithm costs <span class="math">O(nb/\\lg n)</span> and Pippenger’s algorithm costs <span class="math">O(nb/\\lg nb)</span>. However, we decided to focus on Straus’s algorithm for our experiments because Straus’s algorithm allows much better reuse of intermediate results inside batch forgery identification.</p>

    <p class="text-gray-300">Batch forgery identification. For concreteness we focus on the overlap inside Straus’s algorithm inside binary search using shared randomizers (including leaf randomizers), without early aborts. After the root node (i.e., the batch verification discussed above), reuse of intermediate results reduces each subsequent multi-scalar multiplication to approximately <span class="math">2b</span> doublings and <span class="math">4b/c</span> additions.</p>

    <p class="text-gray-300">We emphasize that, no matter how many forgeries there are, this strategy is within <span class="math">1+o(1)</span> of separate signature verification as <span class="math">b\\to\\infty</span>. At most <span class="math">n</span> tree nodes require multi-scalar multiplication, and each multi-scalar multiplication costs</p>

    <p class="text-gray-300">Faster batch forgery identification</p>

    <p class="text-gray-300">!<a href="img-3.jpeg">img-3.jpeg</a> Fig. 5.1. Observed cost  <span class="math">\\alpha nb</span>  of identifying forgeries among  <span class="math">n = 8</span>  signatures for  <span class="math">b = 128</span> . Horizontal axis is number of forgeries. Vertical axis is  <span class="math">\\alpha</span> . Each circle indicates average cost over 101 experiments; error bars indicate quartiles.</p>

    <p class="text-gray-300"><span class="math">(2 + o(1))b</span>  after  <span class="math">O(nb / \\lg b)</span>  for the root, so the total cost is at most  <span class="math">(2 + o(1))nb</span> , just like separate signature verification. If a positive constant fraction of the signatures are valid then the number of nodes required is a constant factor below  <span class="math">n</span>  and this strategy is a constant factor faster than separate signature verification; if the number of forgeries drops then this strategy becomes a logarithmic factor faster than separate signature verification.</p>

    <p class="text-gray-300">For constant  <span class="math">b</span> , such as  <span class="math">b = 128</span> , the picture is more complicated. Each computed non-root node has similar cost to a separate signature verification (in fact slightly lower cost), but the root node adds a significant extra cost, so this algorithm becomes noticeably slower than separate signature verification as the number of forgeries increases. Our straus6.py computer experiments indicate that the cutoff is around  <span class="math">n/3</span>  forgeries for  <span class="math">b = 128</span> . See Figures 5.1, 5.2, and 5.3.</p>

    <p class="text-gray-300">[1] — (no editor), 17th annual symposium on foundations of computer science, IEEE Computer Society, Long Beach, California, 1976. MR 56:1766. See [28]. [2] Mihir Bellare, Juan A. Garay, Tal Rabin, Fast batch verification for modular exponentiation and digital signatures, in Eurocrypt '98 [24] (1998), 236-250. URL: http://cseweb.ucsd.edu/~mihir/papers/batch.. Citations in this document: §1. [3] Daniel J. Bernstein, Curve25519: new Diffie-Hellman speed records, in PKC 2006 [38] (2006), 207-228. URL: http://cr.yp.to/papers.#curve25519. Citations in this document: §3. [4] Daniel J. Bernstein, Niels Duif, Tanja Lange, Peter Schwabe, Bo-Yin Yang, High-speed high-security signatures, in CHES 2011 [31] (2011). URL: http://eprint.iacr.org/2011/368. Citations in this document: §1, §1, §1, §1, §1, §1, §3, §3, §4. [5] Gilles Brassard (editor), Advances in cryptology — CRYPTO '89, 9th annual international cryptology conference, Santa Barbara, California, USA, August 20–24,</p>

    <p class="text-gray-300">Bernstein, Doumen, Lange, Oosterwijk</p>

    <p class="text-gray-300">!<a href="img-4.jpeg">img-4.jpeg</a> Fig. 5.2. Same as Figure 5.1 but for  <span class="math">n = 16</span> .</p>

    <p class="text-gray-300">1989, proceedings, Lecture Notes in Computer Science, 435, Springer, 1990. ISBN 3-540-97317-6. MR 91b:94002. See [34]. [6] Ernest F. Brickell, Daniel M. Gordon, Kevin S. McCurley, David B. Wilson, Fast exponentiation with precomputation (extended abstract), in Eurocrypt '92 [33] (1993), 200-207; see also newer version [7]. Citations in this document: §4, §4. [7] Ernest F. Brickell, Daniel M. Gordon, Kevin S. McCurley, David B. Wilson, Fast exponentiation with precomputation: algorithms and lower bounds (1995); see also older version [6]. URL: http://research.microsoft.com/~dbwilson/bgmw/. [8] Peter de Rooij, Efficient exponentiation using precomputation and vector addition chains, in Eurocrypt '94 [9] (1995), 389-399. MR 1479665. Citations in this document: §4. [9] Alfredo De Santis (editor), Advances in cryptography—EUROCRYPT '94, workshop on the theory and application of cryptographic techniques, Perugia, Italy, May 9–12, 1994, proceedings, Lecture Notes in Computer Science, 950, Springer, 1995. ISBN 3-540-60176-7. MR 98h:94001. See [8], [23]. [10] Yvo Desmedt (editor), Advances in cryptography—CRYPTO '94, 14th annual international cryptography conference, Santa Barbara, California, USA, August 21–25, 1994, proceedings, Lecture Notes in Computer Science, 839, Springer, 1994. ISBN 3-540-58333-5. See [19]. [11] Steven D. Galbraith (editor), Cryptography and coding, 11th IMA international conference, Cirencester, UK, December 18-20, 2007, proceedings, Lecture Notes in Computer Science, 4887, Springer, 2007. ISBN 978-3-540-77271-2. See [18]. [12] Hüseyin Hisil, Kenneth Koon-Ho Wong, Gary Carter, Ed Dawson, Twisted Edwards curves revisited, in Asiacrypt 2008 [27] (2008), 326-343. URL: http:// eprint.iacr.org/2008/522. Citations in this document: §1. [13] Hideki Imai, Yuliang Zheng (editors), Public key cryptography, third international workshop on practice and theory in public key cryptography, PKC 2000, Melbourne, Victoria, Australia, January 18-20, 2000, proceedings, Lecture Notes in Computer Science, 1751, Springer, 2000. ISBN 3-540-66967-1. See [25]. [14] Stanislaw Jarecki, Gene Tsudik (editors), Public key cryptography—PKC 2009, 12th international conference on practice and theory in public key cryptography, Irvine, CA, USA, March 18–20, 2009, proceedings, Lecture Notes in Computer Science, 5443, Springer, 2009. ISBN 978-3-642-00467-4. See [20].</p>

    <p class="text-gray-300">Faster batch forgery identification</p>

    <p class="text-gray-300">!<a href="img-5.jpeg">img-5.jpeg</a> Fig. 5.3. Same as Figure 5.1 but for  <span class="math">n = 32</span> .</p>

    <p class="text-gray-300">[15] Marc Joye, Atsuko Miyaji, Akira Otsuka (editors), Pairing-based cryptography — Pairing 2010—4th international conference, Yamanaka Hot Spring, Japan, December 2010, proceedings, Lecture Notes in Computer Science, 6487, Springer, 2010. ISBN 978-3-642-17454-4. See [21]. [16] Sabyasachi Karati, Abhijit Das, Dipanwita Roychowdhury, Bhargav Bellur, Debojyoti Bhattacharya, Aravind Iyer, Batch verification of ECDSA signatures, in Africacrypt 2012 [22] (2012), 1-18. Citations in this document: §2, §2, §2, §2, §2, §2, §2, §2. [17] Kaoru Kurosawa (editor), Information theoretic security, 4th international conference, ICITS 2009, Shizuoka, Japan, December 3-6, 2009, revised selected papers, Lecture Notes in Computer Science, 5973, Springer, 2010. ISBN 978-3-642-14495-0. See [39]. [18] Laurie Law, Brian J. Matt, Finding invalid signatures in pairing-based batches, in Cirencester 2007 [11] (2007), 34-53. Citations in this document: §1, §1, §3. [19] Chae Hoon Lim, Pil Joong Lee, More flexible exponentiation with precomputation, in Crypto '94 [10] (1994), 95-107. Citations in this document: §4. [20] Brian J. Matt, Identification of multiple invalid signatures in pairing-based batched signatures, in PKC 2009 [14] (2009), 337-356. Citations in this document: §1. [21] Brian J. Matt, Identification of multiple invalid pairing-based signatures in constrained batches, in Pairing 2010 [15] (2010), 78-95. Citations in this document: §1, §1. [22] Aikaterini Mitrokotsa, Serge Vaudenay (editors), Progress in cryptology — AFRICACRPT 2012, 5th international conference on cryptology in Africa, Ifrane, Morocco, July 10-12, 2012, proceedings, Lecture Notes in Computer Science, 7374, Springer, 2012. See [16]. [23] David Naccache, David M'Raihi, Serge Vaudenay, Dan Raphaeli, Can D.S.A. be improved? Complexity trade-offs with the digital signature standard, in Eurocrypt '94 [9] (1994). Citations in this document: §1, §1, §1, §1, §2. [24] Kaisa Nyberg (editor), Advances in cryptology—EUROCRYPT '98, international conference on the theory and application of cryptographic techniques, Espoo, Finland, May 31–June 4, 1998, proceedings, Lecture Notes in Computer Science, 1403, Springer, 1998. ISBN 3-540-64518-7. See [2].</p>

    <p class="text-gray-300">Bernstein, Doumen, Lange, Oosterwijk</p>

    <p class="text-gray-300">[25] Jaroslaw Pastuszak, Dariusz Michalek, Josef Pieprzyk, Jennifer Seberry, Identification of bad signatures in batches, in PKC 2000 [13] (2000), 28–45. Citations in this document: §1, §1, §1, §3.</p>

    <p class="text-gray-300">[26] Jaroslaw Pastuszak, Josef Pieprzyk, Jennifer Seberry, Codes identifying bad signature in batches, in Indocrypt 2000 [32] (2000), 143–154. Citations in this document: §1, §1.</p>

    <p class="text-gray-300">[27] Josef Pieprzyk (editor), Advances in cryptology — ASIACRYPT 2008, 14th international conference on the theory and application of cryptology and information security, Melbourne, Australia, December 7–11, 2008, Lecture Notes in Computer Science, 5350, 2008. ISBN 978-3-540-89254-0. See [12].</p>

    <p class="text-gray-300">[28] Nicholas Pippenger, On the evaluation of powers and related problems (preliminary version), in FOCS '76 [1] (1976), 258–263; newer version split into [29] and [30]. MR 58:3682. Citations in this document: §4.</p>

    <p class="text-gray-300">[29] Nicholas Pippenger, The minimum number of edges in graphs with prescribed paths, Mathematical Systems Theory 12 (1979), 325–346; see also older version [28]. ISSN 0025-5661. MR 81e:05079.</p>

    <p class="text-gray-300">[30] Nicholas Pippenger, On the evaluation of powers and monomials, SIAM Journal on Computing 9 (1980), 230–250; see also older version [28]. ISSN 0097-5397. MR 82c:10064. Citations in this document: §4.</p>

    <p class="text-gray-300">[31] Bart Preneel, Tsuyoshi Takagi (editors), Cryptographic hardware and embedded systems — CHES 2011, 13th international workshop, Nara, Japan, September 28–October 1, 2011, proceedings, Lecture Notes in Computer Science, 6917, Springer, 2011. ISBN 978-3-642-23950-2. See [4].</p>

    <p class="text-gray-300">[32] Bimal K. Roy, Eiji Okamoto (editors), Progress in cryptology — INDOCRYPT 2000, first international conference in cryptology in India, Calcutta, India, December 10–13, 2000, proceedings, Lecture Notes in Computer Science, 1977, Springer, 2000. ISBN 3-540-41452-5. See [26].</p>

    <p class="text-gray-300">[33] Rainer A. Rueppel (editor), Advances in cryptology — EUROCRYPT '92, workshop on the theory and application of cryptographic techniques, Balatonfüred, Hungary, May 24–28, 1992, proceedings, Lecture Notes in Computer Science, 658, Springer, 1993. ISBN 3-540-56413-6. MR 94e:94002. See [6].</p>

    <p class="text-gray-300">[34] Claus P. Schnorr, Efficient identification and signatures for smart cards, in Crypto '89 [5] (1990), 239–252; see also newer version [35].</p>

    <p class="text-gray-300">[35] Claus P. Schnorr, Efficient signature generation by smart cards, Journal of Cryptology 4 (1991), 161–174; see also older version [34]. URL: http://www.mi.informatik.uni-frankfurt.de/research/papers.html. Citations in this document: §1.</p>

    <p class="text-gray-300">[36] Jerome A. Solinas, Low-weight binary representations for pairs of integers CORR 2001-41 (2001). URL: http://www.cacr.math.uwaterloo.ca/techreports/2001/corr2001-41.ps. Citations in this document: §5.</p>

    <p class="text-gray-300">[37] Ernst G. Straus, Addition chains of vectors (problem 5125), American Mathematical Monthly 70 (1964), 806–808. Citations in this document: §4.</p>

    <p class="text-gray-300">[38] Moti Yung, Yevgeniy Dodis, Aggelos Kiayias, Tal Malkin (editors), Public key cryptography — 9th international conference on theory and practice in public-key cryptography, New York, NY, USA, April 24–26, 2006, proceedings, Lecture Notes in Computer Science, 3958, Springer, 2006. ISBN 978-3-540-33851-2. See [3].</p>

    <p class="text-gray-300">[39] Gregory M. Zaverucha, Douglas M. Stinson, Group testing and batch verification, in ICITS 2009 [17] (2010), 140–157. Citations in this document: §1, §1.</p>`;
---

<BaseLayout title="Faster batch forgery identification (2012/549)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2012 &middot; eprint 2012/549
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <p class="mt-4 text-xs text-gray-500">
        All content below belongs to the original authors. This page
        reproduces the paper for educational purposes. Always
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >cite the original</a>.
      </p>
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <Fragment set:html={CONTENT} />

  </article>
</BaseLayout>
