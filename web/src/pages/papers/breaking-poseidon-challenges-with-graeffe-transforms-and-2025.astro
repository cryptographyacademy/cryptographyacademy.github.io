---
import BaseLayout from '../../layouts/BaseLayout.astro';
import PaperDisclaimer from '../../components/PaperDisclaimer.astro';
import PaperHistory from '../../components/PaperHistory.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2025/950';
const CRAWLER = 'marker';
const CONVERTED_DATE = '2026-02-16';
const TITLE_HTML = 'Breaking Poseidon Challenges with Graeffe  Transforms and Complexity Analysis by FFT  Lower Bounds';
const AUTHORS_HTML = 'Ziyu Zhao, Jintai Ding';

const CONTENT = `    <section id="abstract" class="mb-10">
      <h2 class="text-2xl font-bold">Abstract</h2>
      <p class="text-gray-300">Poseidon and Poseidon2 are cryptographic hash functions designed for efficient zero-knowledge proof protocols and have been widely adopted in Ethereum applications. To encourage security research, the Ethereum Foundation announced a bounty program in November 2024 for breaking the Poseidon challenges, i.e. solving the CICO (Constrained Input, Constrained Output) problems for round-reduced Poseidon constructions. In this paper, we explain how to apply the Graeffe transform to univariate polynomial solving, enabling efficient interpolation attacks against Poseidon. We will provide an open-source code and details our approach for solving several challenges valued at $20000 in total. Compared to existing attacks, we improves 2^{13} and 2^{4.5} times in wall time and memory usage, respectively. For all challenges we solved, the cost of memory access turns out to be an essential barrier, which makes the security margin much larger than expected. We actually prove that the memory access cost for FFT grows as the 4/3-power of the input size, up to a logarithmic factor. This indicates the commonly used pseudo linear estimate may be overly conservative. This is very different from multivariate equation solving whose main bottleneck is linear algebra over finite fields. Thus, it might be preferable to choose parameters such that the best known attack is interpolation, as it presents more inherent hardness.</p>
    </section>

      <h3 id="sec-3.1" class="text-xl font-semibold mt-8"><strong>3.1 GCD-Based Methods</strong></h3>

    <p class="text-gray-300">The polynomials that appear in interpolation attacks, such as those described in Section <a href="#page-7-1">2.3,</a> usually have only a few roots in the underlying field, much like random polynomials. This allows the use of the following GCD-based root finding approach, which was used to set the Poseidon interpolation record <a href="#page-21-5">[7</a>].</p>

    <p class="text-gray-300">If <em>f</em>(<em>x</em>) is a polynomial of degree <em>d</em> over F<em>p</em>, to compute its roots, the GCDbased approach first replaces <em>f</em>(<em>x</em>) with its GCD with the Frobenius polynomial <em>x <sup>p</sup> &minus; x</em>, reducing the problem to the case where <em>f</em>(<em>x</em>) is split over F<em>p</em>, then finds the roots by the Berlekamp algorithm [<a href="#page-21-12">10]</a> or the Cantor-Zassenhaus algorithm [<a href="#page-21-13">15</a>]. Since the polynomials involved in the interpolation attack usually have only a few roots, the cost of the second step is negligible.</p>

    <p class="text-gray-300">Before the GCD computation, since <em>f</em>(<em>x</em>) usually degree much less than <em>x <sup>p</sup>&minus;x</em>, it will be helpful to compute <em>g</em>(<em>x</em>) = <em>x <sup>p</sup> &minus; x</em> mod <em>f</em>(<em>x</em>) to replace <em>x <sup>p</sup> &minus; x</em> in the computation. This step can be performed using the square-and-multiply method, which requires <em>O</em>(log <em>p</em>) modular polynomial multiplications for a total computational cost of <em>O</em>(log(<em>p</em>)M(<em>d</em>)).</p>

    <p class="text-gray-300">The computation of the GCD, although it can also be done by the Half-GCD algorithm in <em>O</em>(log(<em>n</em>)M(<em>d</em>)), turns out to be the most troublesome part of the root finding process. The key step in the classical Half-GCD algorithm <a href="#page-22-10">[31</a>], which reduces the GCD computation to half size, is presented in Algorithm <a href="#page-9-0">1.</a></p>

    <p class="text-gray-300">The greatest common divisor computation is unattractive for the following reasons:</p>

    <p class="text-gray-300">(a) As shown in Algorithm <a href="#page-9-0">1</a>, the algorithm is highly sequential. While it manages to reduce the polynomial degree, each step is entangled with the results of previous steps. This strong dependency makes the GCD computation notoriously difficult to parallelize, especially when aiming to keep the total computational cost optimal&mdash;some works <a href="#page-22-11">[34</a>] have (theoretically) achieved polylogarithmic runtime using almost quadratic (in the polynomial degree) number of arithmetic processors, which is unacceptable. Parallelizing GCD computation in practice, even with several CPU cores, is highly non-trivial&mdash;let alone achieving massive parallelism with tens of thousands of threads on modern parallel architectures like GPUs, as is common in cryptographic applications.</p>

    <h2 id="sec-misc-1" class="text-2xl font-bold"><strong>Algorithm 1:</strong> <span class="math">\\mathsf{HGCD}(f(x), g(x))</span></h2>

    <p class="text-gray-300">Input:  <span class="math">f(x), g(x) \\in \\mathbb{F}_p[x], \\deg(f) &gt; \\deg(g) \\geqslant 0.</span></p>

    <p class="text-gray-300"><strong>Output:</strong> An unimodular matrix <strong>M</strong> such that  <span class="math">\\begin{pmatrix} f \\\\ g \\end{pmatrix} = \\mathbf{M} \\begin{pmatrix} \\tilde{f} \\\\ \\tilde{g} \\end{pmatrix}</span>  where</p>

    <p class="text-gray-300"><span class="math">$\\deg(\\tilde{f}) \\geqslant \\left\\lceil \\frac{\\deg(f)}{2} \\right\\rceil &gt; \\deg(\\tilde{g}).</span>$</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li><span class="math">1 \\ d_0 \\leftarrow \\lceil \\deg(f)/2 \\rceil;</span></li>
      <li>2 if  <span class="math">\\deg(g) &lt; d_0</span>  then return  <span class="math">\\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}</span> ; 3  <span class="math">\\mathbf{M}_0 \\leftarrow \\mathsf{HGCD}((f (f \\bmod x^{d_0}))/x^{d_0}, (g (g \\bmod x^{d_0}))/x^{d_0});</span></li>
    </ul>

    <p class="text-gray-300"><span class="math">$\\mathbf{4} \\ \\begin{pmatrix} f_0 \\\\ g_0 \\end{pmatrix} \\leftarrow \\mathbf{M}_0^{-1} \\begin{pmatrix} f \\\\ g \\end{pmatrix};</span>$</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>5 if  <span class="math">deg(g_0) &lt; d_0</span>  then return  <span class="math">\\mathbf{M}_0</span> ;</li>
      <li><strong>6</strong> compute division  <span class="math">f_0 = q_0 g_0 + r_0</span>  with  <span class="math">\\deg(r_0) &lt; \\deg(g_0)</span> ;</li>
      <li>7 if  <span class="math">deg(r_0) &lt; d_0</span>  then return  <span class="math">\\mathbf{M}_0 \\begin{pmatrix} q_0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix}</span> ;</li>
      <li><strong>8</strong>  <span class="math">d_1 \\leftarrow 2d_0 \\deg(g_0);</span></li>
      <li>9  <span class="math">\\mathbf{M}_1 \\leftarrow \\mathsf{HGCD}((g_0 (g_0 \\bmod x^{d_1}))/x^{d_1}, (r_0 (r_0 \\bmod x^{d_1}))/x^{d_1});</span></li>
      <li>10 return  <span class="math">\\mathbf{M}_0 \\begin{pmatrix} q_0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix} \\mathbf{M}_1</span></li>
      <li>(b) Although the Half-GCD runs in  <span class="math">\\mathcal{O}(\\log(n)\\mathsf{M}(d))</span>  time, the constant factor is quite large. In the original paper [31], the computation cost is bounded by  <span class="math">22 \\log(n) M(d)</span> . Later works [25] reduce this constant factor somewhat by modifying the algorithm, but these modifications make the already complex algorithm even more involved and further exacerbate issue (c).</li>
      <li>(c) Last but not least, the Half-GCD algorithm is presented in recursive form and is extremely painful to implement, especially when dealing with large (possibly heterogeneous) memory or attempting to parallelize the computation. As a result, we have no interest in using it and instead choose the much simpler and cleaner Graeffe transform for the Poseidon challenges, which turns out to be even more efficient than we expected.</li>
    </ul>

    <h4 id="sec-misc-2" class="text-lg font-semibold mt-6">Root Finding via Graeffe Transform</h4>

    <p class="text-gray-300">Throughout this paper, we use the following definition of the Graeffe transform.</p>

    <p class="text-gray-300"><strong>Definition 2 (Graeffe Transform).</strong> Let  <span class="math">f(x) \\in \\mathbb{F}_p[x]</span>  be a polynomial. For any positive integer  <span class="math">\\ell</span>  coprime to p, the Graeffe transform of f(x) of order  <span class="math">\\ell</span> , denoted  <span class="math">GT_{\\ell}(f)</span> , is defined as the unique polynomial g such that</p>

    <p class="text-gray-300"><span class="math">$g(x^{\\ell}) = f(x)f(x\\omega_{\\ell}^{1})\\cdots f(x\\omega_{\\ell}^{\\ell-1}),</span>$</p>

    <p class="text-gray-300">where  <span class="math">\\omega_{\\ell}</span>  is a primitive  <span class="math">\\ell</span> -th root of unity, possibly in an extension field of  <span class="math">\\mathbb{F}_{p}</span> .</p>

    <p class="text-gray-300"><strong>Lemma 1.</strong> If  <span class="math">f(x) \\in \\mathbb{F}_p[x]</span>  is a polynomial of degree d,  <span class="math">\\ell</span>  is coprime to p, then the Graeffe transform  <span class="math">\\mathsf{GT}_\\ell(f)</span>  is also a well-defined polynomial in  <span class="math">\\mathbb{F}_p[x]</span>  of degree d.</p>

    <p class="text-gray-300">Proof. Let  <span class="math">h(x) = f(x)f(x\\omega_{\\ell}^1)\\cdots f(x\\omega_{\\ell}^{\\ell-1})</span> . By construction, h(x) is invariant under the substitution  <span class="math">x\\mapsto x\\omega_{\\ell}</span> , which multiplies the coefficient of  <span class="math">x^i</span>  by  <span class="math">\\omega_{\\ell}^i</span> . Thus, the coefficients of  <span class="math">x^i</span>  in h(x) can be nonzero only if  <span class="math">\\omega_{\\ell}^i = 1</span> , i.e., i is a multiple of  <span class="math">\\ell</span> . Therefore, the existence of g(x) is guaranteed, and the degree of g(x) is  <span class="math">\\deg(h(x))/\\ell = d</span> . It remains to show that the coefficients of g(x) lie in  <span class="math">\\mathbb{F}_p</span> . This holds because, by construction, each coefficient of h(x) is fixed under any Galois conjugation. Therefore, the coefficients must lie in the fixed field of the Galois group, which is exactly  <span class="math">\\mathbb{F}_p</span> .</p>

    <p class="text-gray-300"><strong>Lemma 2.</strong> If  <span class="math">f(x) \\in \\mathbb{F}_p[x]</span>  is a polynomial of degree d,  <span class="math">\\ell_0, \\ell_1</span>  are coprime to p, then  <span class="math">GT_{\\ell_0}(\\mathsf{GT}_{\\ell_1}(f)) = \\mathsf{GT}_{\\ell_0 \\cdot \\ell_1}(f)</span> .</p>

    <p class="text-gray-300"><em>Proof.</em> Suppose  <span class="math">\\omega</span>  is a primitive  <span class="math">\\ell_0\\ell_1</span> -th root of unity. Then  <span class="math">\\omega^{\\ell_1}</span>  is a primitive  <span class="math">\\ell_0</span> -th root of unity, and  <span class="math">\\omega^{\\ell_0}</span>  is a primitive  <span class="math">\\ell_1</span> -th root of unity. By the definition of the Graeffe transform, we have</p>

    <p class="text-gray-300"><span class="math">$\\begin{split} \\mathsf{GT}_{\\ell_0}(\\mathsf{GT}_{\\ell_1}(f))(x^{\\ell_0\\ell_1}) &amp;= \\mathsf{GT}_{\\ell_1}(f)(x^{\\ell_1})\\mathsf{GT}_{\\ell_1}(f)(x^{\\ell_1}\\omega^{\\ell_1})\\cdots\\mathsf{GT}_{\\ell_1}(f)(x^{\\ell_1}\\omega^{(\\ell_0-1)\\ell_1}) \\\\ &amp;= \\mathsf{GT}_{\\ell_1}(f)(x^{\\ell_1})\\mathsf{GT}_{\\ell_1}(f)((x\\omega)^{\\ell_1})\\cdots\\mathsf{GT}_{\\ell_1}(f)((x\\omega^{\\ell_0-1})^{\\ell_1}) \\\\ &amp;= \\prod_{j=0}^{\\ell_0-1} f(x\\omega^j)f(x\\omega^{j+\\ell_0})\\cdots f(x\\omega^{j+(\\ell_1-1)\\ell_0}) \\\\ &amp;= \\mathsf{GT}_{\\ell_0\\cdot\\ell_1}(f)(x^{\\ell_0\\ell_1}). \\end{split}</span>$</p>

    <p class="text-gray-300">which concludes the proof.</p>

    <p class="text-gray-300">Lemma 2 allows us to compute the Graeffe transform of large order  <span class="math">\\ell</span>  by successively applying Graeffe transforms of smaller orders. This can be effectively done when  <span class="math">\\ell</span>  is smooth, which is exactly the case for the challenges we solved. We now describe how we compute  <span class="math">\\mathsf{GT}_\\ell(f)</span>  in practice for prime  <span class="math">\\ell &lt; p</span> .</p>

    <p class="text-gray-300">Suppose f(x) is a polynomial of degree d over  <span class="math">\\mathbb{F}_p</span> . For  <span class="math">\\ell=2</span> , if  <span class="math">f(x)=f_0(x^2)+xf_1(x^2)</span> , then</p>

    <p class="text-gray-300"><span class="math">$f(x)f(-x) = (f_0(x^2) + xf_1(x^2))(f_0(x^2) - xf_1(x^2)) = f_0(x^2)^2 - x^2f_1(x^2)^2.</span>$</p>

    <p class="text-gray-300">Therefore, the Graeffe transform of order 2 is given by  <span class="math">\\mathsf{GT}_2(f) = f_0(x)^2 - x f_1(x)^2</span> . This can be computed using two FFTs and one invFFT with FFT group size at least d, costing only  <span class="math">\\mathsf{M}(d/2)</span> .</p>

    <p class="text-gray-300">For  <span class="math">\\ell \\geqslant 3</span> , we follow the method mentioned in [23]. If  <span class="math">P_h(x)</span>  is the product</p>

    <p class="text-gray-300"><span class="math">$P_h(x) = f(x)f(x\\omega_\\ell^1)\\cdots f(x\\omega_\\ell^{h-1}),</span>$</p>

    <p class="text-gray-300">then it suffices to compute  <span class="math">P_{\\ell}(x)</span> , which can be done by recursively compute</p>

    <p class="text-gray-300"><span class="math">$P_h(x) = \\begin{cases} f(x) &amp; \\text{if } h = 1\\\\ P_{h/2}(x)P_{h/2}(x\\omega_\\ell^{h/2}) &amp; \\text{if } h \\text{ even}\\\\ f(x)P_{(h-1)/2}(x\\omega_\\ell)P_{(h-1)/2}(x\\omega_\\ell^{(h+1)/2}) &amp; \\text{otherwise.} \\end{cases}</span>$</p>

    <p class="text-gray-300">Unlike the case of  <span class="math">\\ell=2</span> , this recursive method computes  <span class="math">\\mathsf{GT}_\\ell(f)(x^\\ell)</span>  first. As a result, it is easier to implement, but it requires roughly  <span class="math">2\\ell</span>  times more memory than the output polynomial size. However, this is not a problem in our challenge, since Graeffe transforms of order greater than 2 are only applied to small-degree polynomials in the final steps. Thus, the memory usage can be controlled, as shown in Algorithm 2.</p>

    <h4 id="sec-misc-3" class="text-lg font-semibold mt-6">Algorithm 2: Root Finding over the Goldilocks Field</h4>

    <pre><code class="language-text">Input: A polynomial f(x) \\in \\mathbb{F}_{p_{64}}[x] of degree d.
     Output: A root of f(x) in \\mathbb{F}_{p_{64}}, if one exists.
 1 \\beta \\leftarrow p_{64} - 1; \\mu \\leftarrow 1; g \\leftarrow f;
 2 while \\beta is even do
      \\beta \\leftarrow \\beta/2;
      g \\leftarrow \\mathsf{GT}_2(g) \\bmod (x^\\beta - \\mu);
 5 \\beta \\leftarrow \\beta/3; g_3 \\leftarrow \\mathsf{GT}_3(g) \\bmod (x^\\beta - \\mu);
 6 \\beta \\leftarrow \\beta/5; g_5 \\leftarrow \\mathsf{GT}_5(g_3) \\bmod (x^\\beta - \\mu);
 7 \\beta \\leftarrow \\beta/17; g_{17} \\leftarrow \\mathsf{GT}_{17}(g_5) \\bmod (x^{\\beta} - \\mu);
 8 \\beta \\leftarrow \\beta/257; g_{257} \\leftarrow \\mathsf{GT}_{257}(g_{17}) \\bmod (x^{\\beta} - \\mu);
 9 if g_{257} has no roots in \\mathbb{F}_{p_{64}} then return \\perp;
10 \\mu \\leftarrow a common root of g_{257} and x^{65537} - \\mu;
11 \\mu \\leftarrow a common root of g_{17} and x^{257} - \\mu;
12 \\mu \\leftarrow a common root of q_5 and x^{17} - \\mu;
13 \\mu \\leftarrow a common root of g_3 and x^5 - \\mu;
14 \\mu \\leftarrow a common root of g and x^3 - \\mu;
15 \\beta \\leftarrow 2^{32}; h \\leftarrow f \\mod (x^{\\beta} - \\mu);
16 return a common root of h and x^{2^{32}} - \\mu;
</code></pre>

    <p class="text-gray-300">Suppose  <span class="math">\\lambda \\in \\mathbb{F}_{p_{64}}</span>  is a root of f(x). By the definition of the Graeffe transform, in lines 2 to 8 of Algorithm 2, we compute a polynomial  <span class="math">g_{257}</span>  which shares a common root  <span class="math">\\lambda^{(p_{64}-1)/65537}</span>  with the Graeffe transform  <span class="math">\\mathsf{GT}_{(p_{64}-1)/65537}(f)</span> . This common root can be found by enumeration, since the modulo operation in lines 2 to 8 has made the polynomial degree small enough. Once  <span class="math">\\lambda^{(p_{64}-1)/65537} = \\lambda^{2^{32}.65535}</span>  is known, we successively recover  <span class="math">\\lambda^{2^{32}.255}</span> ,  <span class="math">\\lambda^{2^{32}.15}</span> ,  <span class="math">\\lambda^{2^{32}.3}</span> , and  <span class="math">\\lambda^{2^{32}}</span>  in lines 11 to 14, each corresponding to  <span class="math">\\mu</span>  in the respective step. Finally, we can recover  <span class="math">\\lambda</span>  by computing a common root of f and  <span class="math">x^{2^{32}} - \\lambda^{2^{32}}</span> , using a process similar to that in lines 5 to 14.</p>

    <p class="text-gray-300">We now briefly discuss the computational cost of Algorithm 2. As with Poseidon-64, we assume the input polynomial has degree  <span class="math">2^{32} \\ll d &lt; p_{64}</span> . All operations after line 5 of Algorithm 2 work with polynomials of degree at most  <span class="math">\\sim 2^{32}</span> , so their cost is negligible in the overall computation. In lines 2 to 4, there are roughly  <span class="math">\\log_2(p_{64}) - \\log_2(d)</span>  GT<sub>2</sub> operations applied to degree d polynomials, each costing M(d/2). The remaining GT<sub>2</sub> operations are applied to polynomials of degree at most d/2, d/4, d/8, and so on, with a total cost of roughly M(d/2).</p>

    <p class="text-gray-300">Therefore, we estimate the overall cost of Algorithm 2 to be approximately  <span class="math">(\\log_2(p_{64}) - \\log_2(d) + 1)M(d/2)</span> . The constant factor here is much smaller than that of GCD-based methods, which is part of the reason for our speedup.</p>

    <p class="text-gray-300">We conclude this section by remarking that the numbers 2, 3, 5, 17, 257 in Algorithm 2 exactly correspond to the factorization  <span class="math">p_{64}-1=2^{32}\\cdot 3\\cdot 5\\cdot 17\\cdot 257\\cdot 65537</span> . It is clear how to adapt it to other prime fields, as long as the multiplicative group size of the field is smooth enough. Furthermore, all prime numbers appearing in the Poseidon Challenge [16] have quite smooth multiplicative group sizes, e.g.</p>

    <p class="text-gray-300"><span class="math">$p_{255} - 1 = 2^{32} \\cdot 3 \\cdot 11 \\cdot 19 \\cdot 10177 \\cdot 125527 \\cdot 859267 \\cdot 906349^{2} \\cdot 2508409 \\cdot 2529403 \\cdot 52437899 \\cdot 254760293^{2}.</span>$</p>

    <section id="sec-4" class="mb-10">
      <h2 class="text-2xl font-bold">4 Implementation and Security Estimation</h2>

      <h3 id="sec-4.1" class="text-xl font-semibold mt-8">4.1 Implementation details</h3>

    <p class="text-gray-300">We have implemented our Graeffe transform-based algorithm in C++ with CUDA, with about 4k-5k lines in total. Our code uses a modular design, which decouples memory management, polynomial algorithms, and field arithmetic. Thus, it is possible to adapt the code to other 64-bit prime fields by changing the FFT group choice and field arithmetic code as long as the multiplicative group size is smooth enough, although the current implementation is constrained to the Goldilocks field. With this implementation, we have solved the 24-, 28-, and 32-bit Poseidon-64 challenges, which (using the Skip First Rounds Trick in Section 2.3) correspond to polynomial degrees  <span class="math">7^{12}</span> ,  <span class="math">7^{13}</span> , and  <span class="math">7^{15}</span> , respectively.</p>

    <p class="text-gray-300"><strong>Architecture.</strong> Our code was initially developed, tuned, and executed on a dual-socket Intel Xeon Platinum 8474C server equipped with several Nvidia RTX 4090 graphics cards. Each card is connected to the system's 2TB RAM via a PCIe 4.0 interface, providing a unidirectional bandwidth of roughly 200 GiB/s in total under real-world conditions.</p>

    <p class="text-gray-300">Although our code was developed for Nvidia's Ada architecture with compute capability 8.9, we performed almost no low-level optimization of finite field arithmetic or FFT routines specifically for GPU or CPU architectures, for reasons discussed later in this subsection. Thus, our implementation is largely hardware-agnostic and should achieve similar efficiency on most modern GPUs or CPUs&mdash;the performance is mainly determined by memory bandwidth rather than compute throughput.</p>

    <p class="text-gray-300">Also, large system RAM is not necessary for running our code. When available memory is limited, users can configure the maximum RAM usage, and our implementation will automatically swap polynomial coefficients and FFT data to and from disk as needed. All other data, aside from polynomial coefficients and FFT values, require only a few gigabytes and should fit comfortably in RAM on most systems. In this scenario, overall speed will be limited by disk I/O, but still acceptable &mdash; in our case it is about  <span class="math">2\\sim3</span>  times slower than running entirely in RAM.</p>

    <p class="text-gray-300"><strong>The Graeffe Transform.</strong> As discussed in Section 3.2, we perform the Graeffe transform of  <span class="math">f(x) = f_0(x^2) + xf_1(x^2)</span>  by computing  <span class="math">f_0(x)^2 - xf_1(x)^2</span> . This is done by first applying the FFT to both  <span class="math">f_0(x)</span>  and  <span class="math">f_1(x)</span> , then doing pointwise operations, and finally applying the invFFT to obtain the result. Thus, FFT and invFFT are the most time-consuming parts of our implementation.</p>

    <p class="text-gray-300">There are tons of papers studying various aspects of the FFT algorithm, so we only emphasize the field arithmetic and FFT group size choices in our implementation, which are not as commonly used as those in the literature.</p>

    <p class="text-gray-300">Throughout the implementation, we represented Goldilocks field elements as 64-bit unsigned integers in the range  <span class="math">[0, p_{64})</span> . The Ada architecture supports 64-bit operations poorly, as it lacks native hardware instructions and instead emulates them using multiple 32-bit operations. So we take advantage of the special structure of  <span class="math">p_{64} = 2^{64} - 2^{32} + 1</span>  to avoid costly 64-bit multiplications and modular reductions. The multiplication code is shown below, which may also be useful for implementing the Poseidon-64 hash function on similar architectures.</p>

    <pre><code class="language-text">__device__ __forceinline__ uint64_t cumul(uint64_t x, uint64_t y) {
    uint32_t x_lo = x &amp; Oxfffffffff, x_hi = x &gt;&gt; 32;
    uint52_t y_lo = y &amp; Oxfffffffff, y_hi = y &gt;&gt; 32;
    uint64_t xhyh = (uint64_t)x_hi * y_hi;
    uint64_t xlyh = (uint64_t)x_lo * y_hi;
    uint64_t xlyl = (uint64_t)x_lo * y_lo;
    uint64_t xlyl = (uint64_t)x_lo * y_lo;
    uint64_t xlyl = (uint64_t)x_lo * y_lo;
    xhyh += (xhyl &gt;&gt; 32) + (xlyh &gt;&gt; 32);
    uint32_t ph64 = xhyh &amp; Oxffffffff;
    uint64_t tt32 = (xlyh &amp; Oxffffffff) + (xlyl &gt;&gt; 32) +
</code></pre>

    <p class="text-gray-300">In the Poseidon challenges, we need to compute FFT and invFFT with group sizes of at least  <span class="math">7^{12}</span> ,  <span class="math">7^{13}</span> , and  <span class="math">7^{15}</span> , all of which are larger than  <span class="math">2^{32}</span> . This means the Goldilocks field does not support such large radix-2 fast Fourier transforms directly. Possible approaches include the Sch&ouml;nhage-Strassen algorithm [37] and the Elliptic Curve FFT [9], but we choose to use mixed-radix FFT for simplicity. Our implementation supports at most two odd radices, meaning it can handle FFT group sizes of the form  <span class="math">2^{e_2}q_1q_2</span> , where  <span class="math">q_1</span>  and  <span class="math">q_2</span>  are odd divisors of  <span class="math">|\\mathbb{F}_{p_{64}}^{\\times}|</span> . For example, for a polynomial of degree  <span class="math">7^{15}</span> , we use a group size of  <span class="math">2^{32} \\cdot 5 \\cdot 257</span> . Although using a Radix-257 FFT significantly increases the computational cost, this overhead remains well hidden below the memory bandwidth limit.</p>

    <p class="text-gray-300">Memory Issue. Surprisingly, even after we have made significant improvements to the root finding algorithm, the &quot;32-bit&quot; challenge still takes several days (see Table 3). For other well-known hard problems, such as lattice problems, multivariate polynomial system solving, or decoding [3], it is typically</p>

    <p class="text-gray-300">feasible to solve instances up to 60 or even 70 &quot;bits&quot;. This suggests that the Poseidon interpolation attack has some inherent hardness, and that previous security estimates might have been somewhat too conservative.</p>

    <p class="text-gray-300">In fact, this difficulty can largely be attributed to the cost of memory access. Even when using architectures that support 64-bit operations poorly, memory access remains the main performance bottleneck for all three challenges we solved.</p>

    <p class="text-gray-300">Due to the pseudo-linear complexity and the GPU RAM limit, the Radix-2 part of our FFT implementation executes roughly 25 field multiplications and 25 field additions (or subtractions) each time a field element (8 bytes) is transferred to the device. Assuming a <em>&sim;</em>16GB/s host-to-device bandwidth, this requires only a multiplication throughput of 50 GOPS, which is less than 10% of the architecture's capability. This ratio is even lower when the data is swapped from disk, i.e., the issue becomes increasingly severe as the polynomial degree grows.</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Table 2. Measured Throughput for Goldilocks Field Operations</th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">--</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">--------------------------------------------------------------</td>
          </tr>
        </tbody>
      </table>
    </div>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Operation</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Throughput (TOPS)</th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">float32 FMA</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">41.3</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Addition<br>Fp64</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">3.41</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Subtraction<br>Fp64</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">2.95</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Multiplication<br>Fp64</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">0.76</td>
          </tr>
        </tbody>
      </table>
    </div>

    <p class="text-gray-300"><strong>Performance.</strong> Now we compare the performance of our implementation with previous GCD-based results in <a href="#page-21-5">[7</a>]. For the GCD-based method, direct computation is infeasible even for the degree 7 <sup>12</sup> case. Therefore, we use the fitting curve from Figure 12 of <a href="#page-21-5">[7</a>] to estimate the wall time and memory usage in Table <a href="#page-15-0">3.</a> This estimation optimistically assumes that the speed will not be further slowed by accessing petabytes of memory &mdash; in reality, such memory obviously cannot be placed in fast RAM as in the original experiments used to get the fitting curve.</p>

    <p class="text-gray-300">We remark that the experiments in <a href="#page-21-5">[7</a>] were done with one Intel Xeon E7-4860 core, which may also benefit from parallelism. However, we expect this benefit to be minor. The GCD computation step, as explained in Section <a href="#page-8-1">3.1,</a> is actually highly nontrivial &mdash; if not impossible &mdash; to massively parallelize without greatly increasing the total computational cost. In contrast, the Graeffe transform-based method is algorithmically parallelism-friendly, memory efficient, and easy to implement, which fits the usual settings for large scale attacks.</p>

    <p class="text-gray-300">From the table, we can see that for degrees 7 <sup>12</sup> and 7 <sup>13</sup>, our result is about 10,000 times faster and uses only 4% to 5% of the memory, which clearly shows the advantage. For 7 <sup>15</sup>, although swapping data to disk causes some slowdown, our method is still about 6,000 times faster than the previous result.</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Degree</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Time</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Memory</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Time [7]</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Memory [7]</th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">12<br>7<br>13<br>7</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">8.56s<br>2<br>11.38s<br>2</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">0.32TB<br>1.8TB</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">21.81s<br>2<br>24.83s<br>2</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6.1 TB<br>41 TB</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">15<br>7</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">18.35s<br>2<br>&dagger;</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">90TB</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">30.88s<br>2</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">1.9 PB</td>
          </tr>
        </tbody>
      </table>
    </div>

    <p class="text-gray-300"><strong>Table 3.</strong> Comparison of Wall Time and Memory Usage</p>

      <h3 id="sec-4.2" class="text-xl font-semibold mt-8"><strong>4.2 Security Estimation</strong></h3>

    <p class="text-gray-300">Based on the results from the previous subsection, it is worthwhile to provide a security estimation for the interpolation attack. Here, we focus on the &quot;128-bit&quot; instance of Poseidon, where the prime field size is <em>p</em>255. (For 64-bit primes to achieve 128-bit security, both the input and output of the sponge permutation would need two zeros, which does not correspond to interpolation attacks).</p>

    <p class="text-gray-300">The central question is how to quantify the impact of memory access on the attack. Some literature <a href="#page-22-6">[29</a>,<a href="#page-22-7">35]</a> uses a simplified two-level hierarchical memory model (corresponding to most real-world settings) to derive lower bounds of memory access cost. But we are aiming for a lower bound that holds for any physically realizable architecture. Therefore, we avoid arbitrary or oversimplified assumptions about memory architectures and use only the physical fact that the cost of moving information is at least proportional to the distance it travels. With this, we prove in Theorem <a href="#page-16-1">1</a> (see Section <a href="#page-16-0">5</a>) that the memory access cost for executing FFT is at least proportional to the 4<em>/</em>3-power of the input size. Based on this, we also (with some boldness) model the root finding cost as scaling with the 4<em>/</em>3-power of the polynomial degree, since FFT is essential in all known attacks and it is difficult to image fast polynomial algorithms without it.</p>

    <p class="text-gray-300">Thus, the pseudo-linear arithmetic cost will ultimately become negligible. And our implementation suggests that:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>(1) the crossover point <em>dc</em>, where memory access begins to dominate, is well below 7 15 .</li>
      <li>(2) On the same machine (being conservative by not considering ASICs of similar price, which would make the number even larger), it is possible to execute about 2 <sup>59</sup> SHA-256 permutations in the same running time as a degree 7 <sup>15</sup> root finding over F<em><sup>p</sup></em><sup>257</sup> .</li>
    </ul>

    <p class="text-gray-300">For 128-bit security, the <a href="https://github.com/HorizenLabs/poseidon2" target="_blank" rel="noopener noreferrer">estimation script</a> gives round numbers <em>R<sup>F</sup></em> = 8<em>, R<sup>P</sup></em> = 56, corresponding to a polynomial degree of 5 <sup>63</sup> using the Skip First Rounds Trick (Section <a href="#page-7-1">2.3</a>). We estimate the attack cost as hard as</p>

    <p class="text-gray-300"><span class="math">$\\left(\\frac{5^{63}}{7^{15}}\\right)^{4/3} \\times 2^{59} \\approx 2^{198}</span>$</p>

    <p class="text-gray-300"><em><sup>&dagger;</sup></em> Data swapped to disk.</p>

    <p class="text-gray-300">SHA-256 permutations. So the current parameter choice looks a little bit conservative, and it may be tempting to reduce the number of rounds for better efficiency.</p>

    <p class="text-gray-300">It is crucial to remark that this estimation applies only when a single zero is required in both the input and output of the sponge permutation. Otherwise, the best known attack would involve multivariate polynomial solving, where the main bottleneck is linear algebra over finite fields and memory cost is less relevant. Therefore, it may be preferable to choose a large underlying field so that interpolation is the best known attack when parameterizing Poseidon hash functions.</p>

    </section>

    <section id="sec-5" class="mb-10">
      <h2 class="text-2xl font-bold">5 Memory Access Lower Bounds for FFT</h2>

    <p class="text-gray-300">We will prove a lower bound for the data movement cost of the fast Fourier transform in this section. Specifically, we show that any arithmetic circuit corresponding to the FFT algorithm, regardless of how it is realized in our three-dimensional world, will require information to travel a total distance roughly proportional to the 4/3-power of the input size. Here we make use of the physical fact:</p>

    <p class="text-gray-300">Fact 1. The cost of memory movement is proportional to the physical distance over which it is transferred (and, of course, the amount of data moved).</p>

    <p class="text-gray-300">Based on this, we will use the following definition to quantify the cost throughout the rest of this section.</p>

    <p class="text-gray-300"><strong>Definition 3 (Cost of Data Movement).</strong> For two positions  <span class="math">\\mathbf{x}</span>  and  <span class="math">\\mathbf{y}</span>  in  <span class="math">\\mathbb{R}^3</span> , we define the cost of moving a field element from position  <span class="math">\\mathbf{x}</span>  to position  <span class="math">\\mathbf{y}</span>  as the Euclidean distance  <span class="math">\\|\\mathbf{x} - \\mathbf{y}\\|</span> .</p>

    <p class="text-gray-300">In our proof we will also use:</p>

    <p class="text-gray-300">Fact 2. Each bit of data must be stored in a physical volume bounded below by a positive constant (i.e., information density is finite).</p>

    <p class="text-gray-300">These facts are also commonly used, for example, in the NIST hardness estimation for lattice sieving [33], where (arguably, though not formally proven) memory access is modeled as &quot;random&quot;, meaning the arithmetic unit cannot predict in advance which memory addresses will be accessed. Using Fact 2, these addresses will on average be  <span class="math">\\mathcal{O}(n^{1/3})</span>  far away if the total memory usage is  <span class="math">\\mathcal{O}(n)</span> . Fact 1 then implies a lower bound of  <span class="math">\\mathcal{O}(n^{1/3})</span>  per access.</p>

    <p class="text-gray-300">However, in the case of the fast Fourier transform, memory movement patterns are highly structured and can be far from random. Therefore, the lower bounds of NIST do not directly apply. Instead, our argument relies on the particular structure of the FFT circuit and develops techniques specifically adapted to this context. The main theorem of this section is stated as follows:</p>

    <p class="text-gray-300"><strong>Theorem 1.</strong> The arithmetic circuit of FFT over a finite field with input size  <span class="math">n = 2^{e_2}</span> , regardless of how it is realized in our three-dimensional world, will incur a memory movement cost of at least  <span class="math">\\tilde{\\mathcal{O}}(n^{4/3})</span> .</p>

    <p class="text-gray-300">To prove the theorem, we first briefly introduce the Directed Acyclic Graph (DAG) for the FFT circuit, as shown in Figure <a href="#page-17-0">3</a>.</p>

    <p class="text-gray-300">    <img src="_page_17_Figure_3.jpeg" alt="" class="my-4 max-w-full" />
</p>

    <p class="text-gray-300"><strong>Fig. 3.</strong> DAG of FFT with input size <em>n</em> = 16.</p>

    <p class="text-gray-300">Following the literature on memory analysis <a href="#page-22-6">[29</a>], the DAG consists of <em>n</em> input nodes and <em>n</em> output nodes. The remaining nodes represent intermediate values computed within the circuit. A directed edge <em>v<sup>i</sup> &rarr; v<sup>j</sup></em> indicates that the value at node <em>v<sup>j</sup></em> (linearly) depends on the value at node <em>v<sup>i</sup></em> .</p>

    <p class="text-gray-300">For simplicity, <em>&oplus;</em> is used to denote the bitwise XOR operation on integers. We then have the following lemma:</p>

    <p class="text-gray-300"><strong>Lemma 3.</strong> <em>Suppose that, when a particular realization of the FFT circuit in Theorem <a href="#page-16-1">1</a> halts, the i-th output node is stored at position</em> <strong>p</strong>(<em>i</em>) <em>&isin;</em> R 3 <em>. Then the memory movement cost during the execution is at least</em></p>

    <p class="text-gray-300"><span class="math">$\\sum_{i \\in \\{0,\\dots,n-1\\}, i &lt; i \\oplus 2^r} \\|\\mathbf{p}(i) - \\mathbf{p}(i \\oplus 2^r)\\|</span>$</p>

    <p class="text-gray-300"><em>for any r</em> = 0<em>,</em> 1<em>, . . . , e</em><sup>2</sup> <em>&minus;</em> 1<em>.</em></p>

    <p class="text-gray-300"><em>Proof.</em> We label the nodes in the DAG as <em>vi,</em>0<em>, vi,</em>1<em>, . . . , vi,e</em><sup>2</sup> for <em>i</em> = 0<em>, . . . , n &minus;</em> 1. The nodes <em>vi,</em><sup>0</sup> and <em>vi,e</em><sup>2</sup> for <em>i</em> = 0<em>, . . . , n &minus;</em> 1 correspond to the input and output nodes, respectively, while the remaining nodes represent intermediate values. From each node <em>vi,s</em> with 0 &#10877; <em>s &lt; e</em>2, there are two directed edges: one to <em>vi,s</em>+1 and one to <em>vi&oplus;</em><sup>2</sup> <em><sup>e</sup></em>2<em>&minus;</em>1<em>&minus;s,s</em>+1. The DAG for <em>n</em> = 16 is shown in Figure <a href="#page-17-0">3.</a></p>

    <p class="text-gray-300">Suppose <em>i</em><sup>0</sup> <em>&isin; {</em>0<em>, . . . , n &minus;</em> 1<em>}</em> and <em>i</em><sup>0</sup> <em>&lt; i</em><sup>0</sup> <em>&oplus;</em> 2 <em>r</em> . We construct a subgraph <em>Gi</em><sup>0</sup> = (<em>Vi</em><sup>0</sup> <em>, Ei</em><sup>0</sup> ) of the DAG as follows:</p>

    <p class="text-gray-300"><span class="math">$V_{i_0} = \\{v_{i_0 \\oplus 2^r, \\ell} : e_2 - 1 - r \\leqslant \\ell \\leqslant e_2\\} \\cup \\{v_{i_0, \\ell} : e_2 - r \\leqslant \\ell \\leqslant e_2\\},</span>$</p>

    <p class="text-gray-300"><span class="math">$E_{i_0} = \\{v_{i_0 \\oplus 2^r, e_2 - 1 - r} \\to v_{i_0, e_2 - r}\\} \\cup \\{v_{i, s} \\to v_{i, s + 1} : v_{i, s} \\in V_{i_0}, s &lt; e_2\\}.</span>$</p>

    <p class="text-gray-300">For example, when <em>n</em> = 16 and <em>r</em> = 2, the subgraph corresponding to <em>i</em><sup>0</sup> = 1 is marked as red in Figure <a href="#page-17-0">3.</a> By construction, the images for different <em>i</em><sup>0</sup> are pairwise disjoint. Therefore, the total memory movement cost is at least the sum of the costs corresponding to the subgraphs <em>Gi</em><sup>0</sup> , for all <em>i</em><sup>0</sup> <em>&isin; {</em>0<em>, . . . , n &minus;</em> 1<em>}</em> with <em>i</em><sup>0</sup> <em>&lt; i</em><sup>0</sup> <em>&oplus;</em> 2 <em>r</em> .</p>

    <p class="text-gray-300">Now, it suffices to show that the memory movement cost within <em>G<sup>i</sup></em><sup>0</sup> is at least <em>k***p**(</em>i<em>0)</em>&minus;***p<strong>(<em>i</em><sup>0</sup> <em>&oplus;</em>2 <em>r</em> )<em>k</em>. Assume that the value corresponding to node <em>vi</em>0<em>&oplus;</em><sup>2</sup> <em><sup>r</sup>,e</em>2<em>&minus;</em>1<em>&minus;r</em> is, at some point, computed at position **p</strong><sup>0</sup> <em>&isin;</em> R 3 . By the construction of the DAG, the two output nodes in <em>Gi</em><sup>0</sup> are the endpoints of two directed paths inside <em>Gi</em><sup>0</sup> , both starting from <em>vi</em>0<em>&oplus;</em><sup>2</sup> <em><sup>r</sup>,e</em>2<em>&minus;</em>1<em>&minus;r</em>. Therefore, when the algorithm halts, the information from node <em>vi</em>0<em>&oplus;</em><sup>2</sup> <em><sup>r</sup>,e</em>2<em>&minus;</em>1<em>&minus;r</em>, initially computed at <strong>p</strong>0, must be transferred to positions <strong>p</strong>(<em>i</em>0) and <strong>p</strong>(<em>i</em><sup>0</sup> <em>&oplus;</em> 2 <em>r</em> ), corresponding to nodes <em>vi</em>0<em>,e</em><sup>2</sup> and <em>vi</em>0<em>&oplus;</em><sup>2</sup> <em><sup>r</sup>,e</em><sup>2</sup> , respectively. The total movement distance is at least</p>

    <p class="text-gray-300"><span class="math">$\\|\\mathbf{p}(i_0) - \\mathbf{p}(i_0 \\oplus 2^r)\\|,</span>$</p>

    <p class="text-gray-300">by the triangle inequality in Euclidean space. <em>ut</em></p>

    <p class="text-gray-300">Now, the main theorem is a direct consequence of the following two lemmas.</p>

    <p class="text-gray-300"><strong>Lemma 4.</strong> <em>The summation of pairwise distances of the output nodes is</em></p>

    <p class="text-gray-300"><span class="math">$\\sum_{i,j\\in\\{0,\\dots,n-1\\}} \\|\\mathbf{p}(i) - \\mathbf{p}(j)\\| = \\mathcal{O}(n^{7/3}).</span>$</p>

    <p class="text-gray-300"><em>Proof.</em> This lemma is a direct consequence of Fact 2. By Fact 2, there exists a constant <em>C</em><sup>0</sup> such that there are at most <em>C</em>0<em>R</em><sup>3</sup> field elements stored within any ball of radius <em>R</em>. Thus, for each <em>i &isin; {</em>0<em>, . . . , n &minus;</em> 1<em>}</em>, we have</p>

    <p class="text-gray-300"><span class="math">$\\sum_{0 \\leqslant j \\leqslant n-1} \\|\\mathbf{p}(i) - \\mathbf{p}(j)\\| \\geqslant \\sum_{\\substack{0 \\leqslant j \\leqslant n-1, \\\\ \\|\\mathbf{p}(i) - \\mathbf{p}(j)\\| &gt; (n/2C_0)^{1/3}}} \\|\\mathbf{p}(i) - \\mathbf{p}(j)\\|</span>$</p>

    <p class="text-gray-300"><span class="math">$\\geqslant \\sum_{\\substack{0 \\leqslant j \\leqslant n-1, \\\\ \\|\\mathbf{p}(i) - \\mathbf{p}(j)\\| &gt; (n/2C_0)^{1/3}}} (2C_0)^{-1/3} \\cdot n^{1/3}</span>$</p>

    <p class="text-gray-300"><span class="math">$\\geqslant \\left(n - C_0 \\cdot \\left(\\frac{n}{2C_0}\\right)\\right) \\cdot \\left(\\frac{n}{2C_0}\\right)^{1/3}</span>$</p>

    <p class="text-gray-300"><span class="math">$= (16C_0)^{-1/3} \\cdot n^{4/3}.</span>$</p>

    <p class="text-gray-300">The proof concludes by taking the sum over all 0 &#10877; <em>i</em> &#10877; <em>n &minus;</em> 1. <em>ut</em></p>

    <p class="text-gray-300"><strong>Lemma 5.</strong> There must exist an  <span class="math">r \\in \\{0, 1, ..., e_2 - 1\\}</span>  such that</p>

    <p class="text-gray-300"><span class="math">$\\sum_{i \\in \\{0, \\dots, n-1\\}, \\ i &lt; i \\oplus 2^r} \\|\\mathbf{p}(i) - \\mathbf{p}(i \\oplus 2^r)\\| \\geqslant \\frac{1}{ne_2} \\sum_{i, j \\in \\{0, \\dots, n-1\\}} \\|\\mathbf{p}(i) - \\mathbf{p}(j)\\|.</span>$</p>

    <p class="text-gray-300"><em>Proof.</em> For simplicity, we denote by S(r) the summation</p>

    <p class="text-gray-300"><span class="math">$S(r) = \\sum_{i \\in \\{0, \\dots, n-1\\}, i &lt; i \\oplus 2^r} \\|\\mathbf{p}(i) - \\mathbf{p}(i \\oplus 2^r)\\|.</span>$</p>

    <p class="text-gray-300">Let  <span class="math">r_0</span>  be such that  <span class="math">S(r_0)</span>  is the largest among  <span class="math">S(0), \\ldots, S(e_2-1)</span> . Then we have</p>

    <p class="text-gray-300"><span class="math">$\\begin{split} \\sum_{0 \\leqslant i,j &lt; n} \\|\\mathbf{p}(i) - \\mathbf{p}(j)\\| &amp;= \\sum_{\\substack{0 \\leqslant i,j &lt; n, \\\\ r_0 &lt; \\dots &lt; r_\\ell}} \\|\\mathbf{p}(i) - \\mathbf{p}(i \\oplus j)\\| \\\\ &amp;\\leqslant \\sum_{\\substack{0 \\leqslant i,j &lt; n, \\\\ j = 2^{r_0} + \\dots + 2^{r_\\ell}, \\\\ r_0 &lt; \\dots &lt; r_\\ell}} \\left( \\|\\mathbf{p}(i) - \\mathbf{p}(i \\oplus 2^{r_0})\\| + \\dots + \\right. \\\\ &amp;\\left. \\|\\mathbf{p}(i \\oplus (2^{r_0} + \\dots + 2^{r_{\\ell-1}})) - \\mathbf{p}(i \\oplus j)\\| \\right) \\\\ &amp;= 2 \\cdot \\sum_{\\substack{0 \\leqslant j &lt; n, \\\\ j = 2^{r_0} + \\dots + 2^{r_\\ell}, \\\\ r_0 &lt; \\dots &lt; r_\\ell}} \\left( S(r_0) + \\dots + S(r_\\ell) \\right) \\\\ &amp;= n \\cdot \\left( S(0) + \\dots + S(e_2 - 1) \\right) \\\\ &amp;\\leqslant ne_2 \\cdot S(r_0) \\end{split}</span>$</p>

    <p class="text-gray-300">as desired.  <span class="math">\\Box</span></p>

    </section>

    <section id="sec-6" class="mb-10">
      <h2 class="text-2xl font-bold">6 Conclusions</h2>

    <p class="text-gray-300">So far, we have presented a Graeffe transform-based algorithm for root finding over finite fields, and how we have used it to solve several Poseidon challenges from the Ethereum Foundation. Our results show a speedup of several orders of magnitude over previous methods, while using only about 4-5% of the memory. Based on these results, we provide a security estimate for interpolation attacks with the current proposed parameters. In particular, we have proven that the FFT algorithm incurs a memory access cost proportional to the 4/3-power of the input size, up to logarithmic factors, which differs from previous analyses using hierarchical memory models.</p>

    <p class="text-gray-300">A few points are worth highlighting. First, our results apply only to the classical setting; the quantum resistance of root finding in interpolation attacks, or other potential attacks, remains unclear compared to the well-established hardness of problems used in post-quantum cryptography. Future work may focus on this if quantum resistance is a desired property.</p>

    <p class="text-gray-300">Moreover, FFTs are also used in other cryptanalytic contexts. For example, certain dual lattice attacks on LWE (see [<a href="#page-22-13">30]</a>) use FFTs to accelerate the distinguishing step. The lower bound for FFTs derived here may be relevant in those settings as well, indicating that some attack complexity estimates in the literature may be overly optimistic.</p>

    <p class="text-gray-300"><strong>Acknowledgments.</strong> We would like to thank the Ethereum Foundation for launching the Poseidon bounty program, which is the initial motivation and starting point for this research. On May 22nd, about one month after submitting our first bounty instance, we sent this report and the code to the Ethereum Foundation and later were informed about ongoing parallel work by Antonio Sanso (Ethereum Foundation Poseidon Group) and Giuseppe Vitto <a href="#page-22-14">[36]</a>, who independently studied a similar idea. While their work places more emphasis on the tangent Graeffe Transform, ours provides a complexity analysis based on FFT lower bounds. In the case of Poseidon-64, our results are roughly two orders of magnitude faster &ndash; we are not sure about the reasons yet, as we do not know the details of their implementation.</p>

    </section>

    <section id="references" class="mb-10">
      <h2 class="text-2xl font-bold"><strong>References</strong></h2>

    <ul class="space-y-2 text-gray-400 text-sm list-none">
      <li><p class="text-gray-300">1. Albrecht, M.R., Cid, C., Grassi, L., Khovratovich, D., L&uuml;ftenegger, R., Rechberger, C., Schofnegger, M.: Algebraic cryptanalysis of stark-friendly designs: Application to marvellous and mimc. In: Advances in Cryptology - ASIACRYPT 2019: 25th International Conference on the Theory and Application of Cryptology and Information Security, Kobe, Japan, December 8-12, 2019, Proceedings, Part III. pp. 371&ndash;397. Springer-Verlag, Berlin, Heidelberg (2019), <a href="https://doi.org/10.1007/978-3-030-34618-8_13" target="_blank" rel="noopener noreferrer">https:</a> <a href="https://doi.org/10.1007/978-3-030-34618-8_13" target="_blank" rel="noopener noreferrer">//doi.org/10.1007/978-3-030-34618-8\\_13</a></p></li>
      <li><p class="text-gray-300">2. Albrecht, M.R., Grassi, L., Rechberger, C., Roy, A., Tiessen, T.: Mimc: Efficient encryption and cryptographic hashing with minimal multiplicative complexity. In: Cheon, J.H., Takagi, T. (eds.) Advances in Cryptology - ASIACRYPT 2016 - 22nd International Conference on the Theory and Application of Cryptology and Information Security, Hanoi, Vietnam, December 4-8, 2016, Proceedings, Part I. Lecture Notes in Computer Science, vol. 10031, pp. 191&ndash;219 (2016), <a href="https://doi.org/10.1007/978-3-662-53887-6_7" target="_blank" rel="noopener noreferrer">https://doi.</a> <a href="https://doi.org/10.1007/978-3-662-53887-6_7" target="_blank" rel="noopener noreferrer">org/10.1007/978-3-662-53887-6\\_7</a></p></li>
      <li><p class="text-gray-300">3. Aragon, N., Lavauzelle, J., Lequesne, M.: decodingchallenge.org (2019), <a href="http://decodingchallenge.org" target="_blank" rel="noopener noreferrer">http://</a> <a href="http://decodingchallenge.org" target="_blank" rel="noopener noreferrer">decodingchallenge.org</a></p></li>
      <li><p class="text-gray-300">4. Ashur, T., Dhooghe, S.: MARVELlous: a STARK-friendly family of cryptographic primitives. Cryptology ePrint Archive, Paper 2018/1098 (2018), <a href="https://eprint.iacr.org/2018/1098" target="_blank" rel="noopener noreferrer">https://eprint.</a> <a href="https://eprint.iacr.org/2018/1098" target="_blank" rel="noopener noreferrer">iacr.org/2018/1098</a></p></li>
      <li><p class="text-gray-300">5. Ashur, T., Kindi, A., Meier, W., Szepieniec, A., Threadbare, B.: Rescue-prime optimized. Cryptology ePrint Archive, Paper 2022/1577 (2022), <a href="https://eprint.iacr.org/2022/1577" target="_blank" rel="noopener noreferrer">https://eprint.</a> <a href="https://eprint.iacr.org/2022/1577" target="_blank" rel="noopener noreferrer">iacr.org/2022/1577</a></p></li>
      <li><p class="text-gray-300">6. Bariant, A., Boeuf, A., Lemoine, A., Manterola Ayala, I., &Oslash;ygarden, M., Perrin, L., Raddum, H.: The algebraic freelunch: Efficient gr&ouml;bner basis attacks against arithmetization-oriented primitives. In: Advances in Cryptology - CRYPTO 2024: 44th Annual International Cryptology Conference, Santa Barbara, CA, USA, August 18-22, 2024, Proceedings, Part IV. pp. 139&ndash;173. Springer-Verlag, Berlin, Heidelberg (2024), <a href="https://doi.org/10.1007/978-3-031-68385-5_5" target="_blank" rel="noopener noreferrer">https://doi.org/10.1007/978-3-031-68385-5\\_5</a></p></li>
      <li><p class="text-gray-300">7. Bariant, A., Bouvier, C., Leurent, G., Perrin, L.: Algebraic attacks against some arithmetization-oriented primitives. IACR Trans. Symmetric Cryptol. <strong>2022</strong>(3), 73&ndash;101 (2022), <a href="https://doi.org/10.46586/tosc.v2022.i3.73-101" target="_blank" rel="noopener noreferrer">https://doi.org/10.46586/tosc.v2022.i3.73-101</a></p></li>
      <li><p class="text-gray-300">8. Ben-Sasson, E., Bentov, I., Horesh, Y., Riabzev, M.: Scalable zero knowledge with no trusted setup. In: Advances in Cryptology - CRYPTO 2019: 39th Annual International Cryptology Conference, Santa Barbara, CA, USA, August 18-22, 2019, Proceedings, Part III. pp. 701&ndash;732. Springer-Verlag, Berlin, Heidelberg (2019), <a href="https://doi.org/10.1007/978-3-030-26954-8_23" target="_blank" rel="noopener noreferrer">https://doi.org/10.1007/978-3-030-26954-8\\_23</a></p></li>
      <li><p class="text-gray-300">9. Ben-Sasson, E., Carmon, D., Kopparty, S., Levit, D.: Elliptic Curve Fast Fourier Transform (ECFFT) Part I: Low-degree Extension in Time O(n log n) over all Finite Fields, pp. 700&ndash;737. <a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch30" target="_blank" rel="noopener noreferrer">https://epubs.siam.org/doi/abs/10.1137/1.</a> <a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch30" target="_blank" rel="noopener noreferrer">9781611977554.ch30</a></p></li>
      <li><p class="text-gray-300">10. Berlekamp, E.R.: Factoring polynomials over large finite fields*. In: Proceedings of the Second ACM Symposium on Symbolic and Algebraic Manipulation. p. 223. SYMSAC '71, Association for Computing Machinery, New York, NY, USA (1971), <a href="https://doi.org/10.1145/800204.806290" target="_blank" rel="noopener noreferrer">https://doi.org/10.1145/800204.806290</a></p></li>
      <li><p class="text-gray-300">11. Best, G.C.: Notes on the graeffe method of root squaring. The American Mathematical Monthly <strong>56</strong>(2), 91&ndash;94 (1949), <a href="http://www.jstor.org/stable/2306166" target="_blank" rel="noopener noreferrer">http://www.jstor.org/stable/2306166</a></p></li>
      <li><p class="text-gray-300">12. Brodetsky, S., Smeal, G.: On graeffe's method for complex roots of algebraic equations. Mathematical Proceedings of the Cambridge Philosophical Society <strong>22</strong>(2), 83&ndash;87 (1924). <a href="https://doi.org/10.1017/S0305004100002802" target="_blank" rel="noopener noreferrer">https://doi.org/10.1017/S0305004100002802</a></p></li>
      <li><p class="text-gray-300">13. Buchberger, B.: Bruno buchberger's phd thesis 1965: An algorithm for finding the basis elements of the residue class ring of a zero dimensional polynomial ideal. Journal of Symbolic Computation <strong>41</strong>(3), 475&ndash;511 (2006). <a href="https://doi.org/https://doi.org/10.1016/j.jsc.2005.09.007" target="_blank" rel="noopener noreferrer">https://doi.org/https://doi.org/10.1016/j.jsc.2005.09.007</a>, logic, Mathematics and Computer Science: Interactions in honor of Bruno Buchberger (60th birthday)</p></li>
      <li><p class="text-gray-300">14. Buchmann, J.A., Ding, J., Mohamed, M.S.E., Mohamed, W.S.A.E.: Mutantxl: Solving multivariate polynomial equations for cryptanalysis. In: Symmetric Cryptography (2009)</p></li>
      <li><p class="text-gray-300">15. Cantor, D.G., Zassenhaus, H.: A new algorithm for factoring polynomials over finite fields. Mathematics of Computation <strong>36</strong>, 587&ndash;592 (1981)</p></li>
      <li><p class="text-gray-300">16. Ethereum Foundation: Poseidon cryptanalysis initiative 2024&ndash;2026. <a href="https://www.poseidon-initiative.info/" target="_blank" rel="noopener noreferrer">https://www.</a> <a href="https://www.poseidon-initiative.info/" target="_blank" rel="noopener noreferrer">poseidon-initiative.info/</a> (2024)</p></li>
      <li><p class="text-gray-300">17. Faug&egrave;re, J., Gianni, P., Lazard, D., Mora, T.: Efficient computation of zerodimensional gr&ouml;bner bases by change of ordering. Journal of Symbolic Computation <strong>16</strong>(4), 329&ndash;344 (1993). <a href="https://doi.org/https://doi.org/10.1006/jsco.1993.1051" target="_blank" rel="noopener noreferrer">https://doi.org/https://doi.org/10.1006/jsco.1993.1051</a></p></li>
      <li><p class="text-gray-300">18. Faug&eacute;re, J.C.: A new efficient algorithm for computing gr&ouml;bner bases (f4). Journal of Pure and Applied Algebra <strong>139</strong>(1), 61&ndash;88 (1999). <a href="https://doi.org/https://doi.org/10.1016/S0022-4049(99" target="_blank" rel="noopener noreferrer">https://doi.org/https://doi.org/10.1016/S0022-4049(99)00005-5</a>00005-5)</p></li>
      <li><p class="text-gray-300">19. Grassi, L., Khovratovich, D., L&uuml;ftenegger, R., Rechberger, C., Schofnegger, M., Walch, R.: Reinforced concrete: A fast hash function for verifiable computation. In: Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security. pp. 1323&ndash;1335. CCS '22, Association for Computing Machinery, New York, NY, USA (2022), <a href="https://doi.org/10.1145/3548606.3560686" target="_blank" rel="noopener noreferrer">https://doi.org/10.1145/3548606.3560686</a></p></li>
      <li><p class="text-gray-300">20. Grassi, L., Khovratovich, D., Rechberger, C., Roy, A., Schofnegger, M.: Poseidon: A new hash function for Zero-Knowledge proof systems. In: 30th USENIX Security Symposium (USENIX Security 21). pp. 519&ndash;535. USENIX Association (Aug 2021), <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/grassi" target="_blank" rel="noopener noreferrer">https://www.usenix.org/conference/usenixsecurity21/presentation/grassi</a></p></li>
      <li><p class="text-gray-300">21. Grassi, L., Khovratovich, D., Schofnegger, M.: Poseidon2: A faster version of the poseidon hash function. Cryptology ePrint Archive, Paper 2023/323 (2023), <a href="https://eprint.iacr.org/2023/323" target="_blank" rel="noopener noreferrer">https:</a> <a href="https://eprint.iacr.org/2023/323" target="_blank" rel="noopener noreferrer">//eprint.iacr.org/2023/323</a></p></li>
      <li><p class="text-gray-300">22. Grassi, L., L&uuml;ftenegger, R., Rechberger, C., Rotaru, D., Schofnegger, M.: On a generalization of substitution-permutation networks: The hades design strategy. In: Advances in Cryptology - EUROCRYPT 2020: 39th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Zagreb, Croatia, May 10&ndash;14, 2020, Proceedings, Part II. pp. 674&ndash;704. Springer-Verlag, Berlin, Heidelberg (2020), <a href="https://doi.org/10.1007/978-3-030-45724-2_23" target="_blank" rel="noopener noreferrer">https://doi.org/10.1007/978-3-030-45724-2\\_23</a></p></li>
      <li><p class="text-gray-300">23. Grenet, B., van der Hoeven, J., Lecerf, G.: Randomized root finding over finite fft-fields using tangent graeffe transforms. In: Proceedings of the 2015 ACM International Symposium on Symbolic and Algebraic Computation. pp. 197&ndash;204. ISSAC '15, Association for Computing Machinery, New York, NY, USA (2015), <a href="https://doi.org/10.1145/2755996.2756647" target="_blank" rel="noopener noreferrer">https://doi.org/10.1145/2755996.2756647</a></p></li>
      <li><p class="text-gray-300">24. Grenet, B., Hoeven, J., Lecerf, G.: Deterministic root finding over finite fields using graeffe transforms. Appl. Algebra Eng., Commun. Comput. <strong>27</strong>(3), 237&ndash;257 (Jun 2016), <a href="https://doi.org/10.1007/s00200-015-0280-5" target="_blank" rel="noopener noreferrer">https://doi.org/10.1007/s00200-015-0280-5</a></p></li>
      <li><p class="text-gray-300">25. van der Hoeven, J.: Optimizing the half-gcd algorithm. ArXiv <strong>abs/2212.12389</strong> (2022), <a href="https://arxiv.org/abs/2212.12389" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2212.12389</a></p></li>
      <li><p class="text-gray-300">26. van der Hoeven, J., Monagan, M.: Implementing the tangent graeffe root finding method. In: Bigatti, A.M., Carette, J., Davenport, J.H., Joswig, M., de Wolff, T. (eds.) Mathematical Software &ndash; ICMS 2020. pp. 482&ndash;492. Springer International Publishing, Cham (2020)</p></li>
      <li><p class="text-gray-300">27. Householder, A.S.: Dandelin, lobacevskii, or graeffe. The American Mathematical Monthly <strong>66</strong>(6), 464&ndash;466 (1959), <a href="http://www.jstor.org/stable/2310626" target="_blank" rel="noopener noreferrer">http://www.jstor.org/stable/2310626</a></p></li>
      <li><p class="text-gray-300">28. Jakobsen, T., Knudsen, L.R.: The interpolation attack on block ciphers. In: Biham, E. (ed.) Fast Software Encryption. pp. 28&ndash;40. Springer Berlin Heidelberg, Berlin, Heidelberg (1997)</p></li>
      <li><p class="text-gray-300">29. Jia-Wei, H., Kung, H.T.: I/o complexity: The red-blue pebble game. In: Proceedings of the Thirteenth Annual ACM Symposium on Theory of Computing. pp. 326&ndash;333. STOC '81, Association for Computing Machinery, New York, NY, USA (1981), <a href="https://doi.org/10.1145/800076.802486" target="_blank" rel="noopener noreferrer">https://doi.org/10.1145/800076.802486</a></p></li>
      <li><p class="text-gray-300">30. MATZOV: Report on the security of lwe: Improved dual lattice attack (Apr 2022), <a href="https://doi.org/10.5281/zenodo.6412487" target="_blank" rel="noopener noreferrer">https://doi.org/10.5281/zenodo.6412487</a></p></li>
      <li><p class="text-gray-300">31. Moenck, R.T.: Fast computation of gcds. In: Proceedings of the Fifth Annual ACM Symposium on Theory of Computing. pp. 142&ndash;151. STOC '73, Association for Computing Machinery, New York, NY, USA (1973), <a href="https://doi.org/10.1145/800125.804045" target="_blank" rel="noopener noreferrer">https://doi.org/10.1145/</a> <a href="https://doi.org/10.1145/800125.804045" target="_blank" rel="noopener noreferrer">800125.804045</a></p></li>
      <li><p class="text-gray-300">32. Mohamed, M.S.E., Cabarcas, D., Ding, J., Buchmann, J., Bulygin, S.: Mxl3: An efficient algorithm for computing gr&ouml;bner bases of zero-dimensional ideals. In: Lee, D., Hong, S. (eds.) Information, Security and Cryptology &ndash; ICISC 2009. pp. 87&ndash;100. Springer Berlin Heidelberg, Berlin, Heidelberg (2010)</p></li>
      <li><p class="text-gray-300">33. NIST: Faq on kyber512. <a href="https://csrc.nist.gov/csrc/media/Projects/post-quantum-cryptography/documents/faq/Kyber-512-FAQ.pdf" target="_blank" rel="noopener noreferrer">https://csrc.nist.gov/csrc/media/Projects/</a> <a href="https://csrc.nist.gov/csrc/media/Projects/post-quantum-cryptography/documents/faq/Kyber-512-FAQ.pdf" target="_blank" rel="noopener noreferrer">post-quantum-cryptography/documents/faq/Kyber-512-FAQ.pdf</a> (December 2023), accessed: 2025-05-09</p></li>
      <li><p class="text-gray-300">34. Pan, V.Y.: Parallel computation of polynomial gcd and some related parallel computations over abstract fields. Theoretical Computer Science <strong>162</strong>(2), 173&ndash;223 (1996). <a href="https://doi.org/https://doi.org/10.1016/0304-3975(96" target="_blank" rel="noopener noreferrer">https://doi.org/https://doi.org/10.1016/0304-3975(96)00030-8</a>00030-8)</p></li>
      <li><p class="text-gray-300">35. Ranjan, D., Savage, J., Zubair, M.: Strong i/o lower bounds for binomial and fft computation graphs. In: Fu, B., Du, D.Z. (eds.) Computing and Combinatorics. pp. 134&ndash;145. Springer Berlin Heidelberg, Berlin, Heidelberg (2011)</p></li>
      <li><p class="text-gray-300">36. Sanso, A., Vitto, G.: Attacking poseidon via graeffe-based root-finding over NTTfriendly fields. Cryptology ePrint Archive, Paper 2025/937 (2025), <a href="https://eprint.iacr.org/2025/937" target="_blank" rel="noopener noreferrer">https://eprint.</a> <a href="https://eprint.iacr.org/2025/937" target="_blank" rel="noopener noreferrer">iacr.org/2025/937</a></p></li>
      <li><p class="text-gray-300">37. Sch&ouml;nhage, A., Strassen, V.: Schnelle multiplikation gro&szlig;er zahlen. Computing <strong>7</strong>(3-4), 281&ndash;292 (1971), <a href="https://doi.org/10.1007/BF02242355" target="_blank" rel="noopener noreferrer">https://doi.org/10.1007/BF02242355</a></p></li>
      <li><p class="text-gray-300">38. Scquizzato, M., Silvestri, F.: Communication lower bounds for distributed-memory computations. CoRR <strong>abs/1307.1805</strong> (2013), <a href="http://arxiv.org/abs/1307.1805" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1307.1805</a></p></li>
      <li><p class="text-gray-300">39. Szepieniec, A., Ashur, T., Dhooghe, S.: Rescue-prime: a standard specification (SoK). Cryptology ePrint Archive, Paper 2020/1143 (2020), <a href="https://eprint.iacr.org/2020/1143" target="_blank" rel="noopener noreferrer">https://eprint.iacr.</a> <a href="https://eprint.iacr.org/2020/1143" target="_blank" rel="noopener noreferrer">org/2020/1143</a></p></li>
    </ul>

    </section>
`;
---

<BaseLayout title="Breaking Poseidon Challenges with Graeffe  Transforms and Co... (2025/950)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2025 &middot; eprint 2025/950
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <PaperDisclaimer eprintUrl={EPRINT_URL} />
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <nav id="toc" class="mb-10 p-6 rounded-lg" style="background: rgba(255,255,255,0.03); border: 1px solid rgba(255,255,255,0.06);">
      <h2 class="text-lg font-bold mb-4">Table of Contents</h2>
      <ol class="space-y-1 text-sm text-gray-300
        list-decimal list-inside">
        <li><a href="#sec-1" class="hover:text-white">Introduction</a></li>
        <li>
          <a href="#sec-2" class="hover:text-white">Preliminaries</a>
          <ol class="ml-6 mt-1 space-y-1 list-decimal
            list-inside text-gray-400">
            <li><a href="#sec-2.1" class="hover:text-white">Notations</a></li>
            <li><a href="#sec-2.2" class="hover:text-white">The Poseidon Permutation and the CICO problem</a></li>
            <li><a href="#sec-2.3" class="hover:text-white">Skip First Rounds Trick</a></li>
          </ol>
        </li>
        <li>
          <a href="#sec-3" class="hover:text-white">Root Finding for Univariate Polynomials</a>
          <ol class="ml-6 mt-1 space-y-1 list-decimal
            list-inside text-gray-400">
            <li><a href="#sec-3.1" class="hover:text-white">GCD-Based Methods</a></li>
          </ol>
        </li>
        <li>
          <a href="#sec-4" class="hover:text-white">Implementation and Security Estimation</a>
          <ol class="ml-6 mt-1 space-y-1 list-decimal
            list-inside text-gray-400">
            <li><a href="#sec-4.1" class="hover:text-white">Implementation details</a></li>
            <li><a href="#sec-4.2" class="hover:text-white">Security Estimation</a></li>
          </ol>
        </li>
        <li><a href="#sec-5" class="hover:text-white">Memory Access Lower Bounds for FFT</a></li>
        <li><a href="#sec-6" class="hover:text-white">Conclusions</a></li>
      </ol>
      <p class="text-xs text-gray-500 mt-4 mb-1 font-semibold">
        Additional
      </p>
      <ul class="space-y-1 text-sm text-gray-400
        list-disc list-inside">
        <li><a href="#references" class="hover:text-white">References</a></li>
      </ul>
    </nav>


    <Fragment set:html={CONTENT} />

    <PaperHistory slug="breaking-poseidon-challenges-with-graeffe-transforms-and-2025" />
  </article>
</BaseLayout>
