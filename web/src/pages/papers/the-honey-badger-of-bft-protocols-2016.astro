---
import BaseLayout from '../../layouts/BaseLayout.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2016/199';
const CRAWLER = 'mistral';
const CONVERTED_DATE = '2026-02-16';
const TITLE_HTML = 'The Honey Badger of BFT Protocols';
const AUTHORS_HTML = 'Andrew Miller, Yu Xia, Kyle Croman, Elaine Shi, Dawn Song';

const CONTENT = `    <p class="text-gray-300">Andrew Miller University of Illinois, Urbana-Champaign Elaine Shi Cornell University Yu Xia Tsinghua University Dawn Song University of California, Berkeley Kyle Croman Cornell University</p>

    <p class="text-gray-300">The surprising success of cryptocurrencies has led to a surge of interest in deploying large scale, highly robust, Byzantine fault tolerant (BFT) protocols for mission-critical applications, such as financial transactions. Although the conventional wisdom is to build atop a (weakly) synchronous protocol such as PBFT (or a variation thereof), such protocols rely critically on network timing assumptions, and only guarantee liveness when the network behaves as expected. We argue these protocols are ill-suited for this deployment scenario.</p>

    <p class="text-gray-300">We present an alternative, HoneyBadgerBFT, the first practical asynchronous BFT protocol, which guarantees liveness without making any timing assumptions. We base our solution on a novel atomic broadcast protocol that achieves optimal asymptotic efficiency. We present an implementation and experimental results to show our system can achieve throughput of tens of thousands of transactions per second, and scales to over a hundred nodes on a wide area network. We even conduct BFT experiments over Tor, without needing to tune any parameters. Unlike the alternatives, HoneyBadgerBFT simply does not care about the underlying network.</p>

    <p class="text-gray-300">Distributed fault tolerant protocols are promising solutions for mission-critical infrastructure, such as financial transaction databases. Traditionally, they have been deployed at relatively small scale, and typically in a single administrative domain where adversarial attacks might not be a primary concern. As a representative example, a deployment of Google's fault tolerant lock service, Chubby [14], consists of five nodes, and tolerates up to two crash faults.</p>

    <p class="text-gray-300">In recent years, a new embodiment of distributed systems called "cryptocurrencies" or "blockchains" have emerged, beginning with Bitcoin's phenomenal success [43]. Such cryptocurrency systems represent a surprising and effective breakthrough [12], and open a new chapter in our understanding of distributed systems.</p>

    <p class="text-gray-300">Cryptocurrency systems challenge our traditional belief about the deployment environment for fault tolerance protocols. Unlike the classic "5 Chubby nodes within Google" environment, cryptocurrencies have revealed and stimulated a new demand for consensus protocols over a wide area network, among a large number of nodes that are mutually distrustful, and moreover, network connections can be much more unpredictable than the classical LAN setting, or even adversarial. This new setting poses interesting new challenges, and calls upon us to rethink the design of fault tolerant protocols.</p>

    <p class="text-gray-300">Robustness is a first-class citizen. Cryptocurrencies demonstrate the demand for and viability of an unusual operating point that prioritizes robustness above all else, even at the expense of performance. In fact, Bitcoin provides terrible performance by distributed systems</p>

    <p class="text-gray-300">standards: a transaction takes on average 10 minutes to be committed, and the system as a whole achieves throughput on the order of 10 transactions per second. However, in comparison with traditional fault tolerant deployment scenarios, cryptocurrencies thrive in a highly adversarial environment, where well-motivated and malicious attacks are expected (if not commonplace). For this reason, many of Bitcoin's enthusiastic supporters refer to it as the "Honey Badger of Money" [41]. We note that the demand for robustness is often closely related to the demand for decentralization — since decentralization would typically require the participation of a large number of diverse participants in a wide-area network.</p>

    <p class="text-gray-300">Favor throughput over latency. Most existing works on scalable fault tolerance protocols [6, 49] focus on optimizing scalability in a LAN environment controlled by a single administrative domain. Since bandwidth provisioning is ample, these works often focus on reducing (cryptographic) computations and minimizing response time while under contention (i.e., requests competing for the same object).</p>

    <p class="text-gray-300">In contrast, blockchains have stirred interest in a class of financial applications where response time and contention are not the most critical factors, e.g., payment and settlement networks [1]. In fact, some financial applications intentionally introduce delays in committing transactions to allow for possible rollback/chargeback operations.</p>

    <p class="text-gray-300">Although these applications are not latency critical, banks and financial institutions have expressed interest in a high-throughput alternative of the blockchain technology, to be able to sustain high volumes of requests. For example, the Visa processes 2,000 tx/sec on average, with a peak of 59,000 tx/sec [1].</p>

    <p class="text-gray-300">Timing assumptions considered harmful. Most existing Byzantine fault tolerant (BFT) systems, even those called "robust," assume some variation of weak synchrony, where, roughly speaking, messages are guaranteed to be delivered after a certain bound <span class="math">\\Delta</span>, but <span class="math">\\Delta</span> may be time-varying or unknown to the protocol designer. We argue that protocols based on timing assumptions are unsuitable for decentralized, cryptocurrency settings, where network links can be unreliable, network speeds change rapidly, and network delays may even be adversarially induced.</p>

    <p class="text-gray-300">First, the liveness properties of weakly synchronous protocols can fail completely when the expected timing assumptions are violated (e.g., due to a malicious network adversary). To demonstrate this, we explicitly construct an adversarial "intermittently synchronous" network that violates the assumptions, such that existing weakly synchronous protocols such as PBFT [20] would grind to a halt (Section 3).</p>

    <p class="text-gray-300">Second, even when the weak synchrony assumptions are satisfied in practice, weakly synchronous protocols degrade significantly in throughput when the underlying network is unpredictable. Ideally, we would like a protocol whose throughput closely tracks the network’s performance even under rapidly changing network conditions. Unfortunately, weakly asynchronous protocols require timeout parameters that are finicky to tune, especially in cryptocurrency application settings; and when the chosen timeout values are either too long or too short, throughput can be hampered. As a concrete example, we show that even when the weak synchrony assumptions are satisfied, such protocols are slow to recover from transient network partitions (Section 3).</p>

    <p class="text-gray-300">Practical asynchronous BFT. We propose HoneyBadgerBFT, the first BFT atomic broadcast protocol to provide optimal asymptotic efficiency in the asynchronous setting. We therefore directly refute the prevailing wisdom that such protocols a re necessarily impractical.</p>

    <p class="text-gray-300">We make significant efficiency improvements on the best prior-known asynchronous atomic broadcast protocol, due to Cachin et al. <em>[15]</em>, which requires each node to transmit <span class="math">O(N^{2})</span> bits for each committed transaction, substantially limiting its throughput for all but the smallest networks. This inefficiency has two root causes. The first cause is redundant work among the parties. However, a naïve attempt to eliminate the redundancy compromises the fairness property, and allows for targeted censorship attacks. We invent a novel solution to overcome this problem by using threshold public-key encryption to prevent these attacks. The second cause is the use of a suboptimal instantiation of the Asynchronous Common Subset (ACS) subcomponent. We show how to efficiently instantiate ACS by combining existing but overlooked techniques: efficient reliable broadcast using erasure codes <em>[18]</em>, and a reduction from ACS to reliable broadcast from the multi-party computation literature <em>[9]</em>.</p>

    <p class="text-gray-300">HoneyBadgerBFT’s design is optimized for a cryptocurrency-like deployment scenario where network bandwidth is the scarce resource, but computation is relatively ample. This allows us to take advantage of cryptographic building blocks (in particular, threshold public-key encryption) that would be considered too expensive in a classical fault-tolerant database setting where the primary goal is to minimize response time even under contention.</p>

    <p class="text-gray-300">In an asynchronous network, messages are eventually delivered but no other timing assumption is made. Unlike existing weakly synchronous protocols where parameter tuning can be finicky, HoneyBadgerBFT does not care. Regardless of how network conditions fluctuate, HoneyBadgerBFT’s throughput always closely tracks the network’s available bandwidth. Imprecisely speaking, HoneyBadgerBFT eventually makes progress as long as messages eventually get delivered; moreover, it makes progress as soon as messages are delivered.</p>

    <p class="text-gray-300">We formally prove the security and liveness of our HoneyBadgerBFT protocol, and show experimentally that it provides better throughput than the classical PBFT protocol <em>[20]</em> even in the optimistic case.</p>

    <p class="text-gray-300">Implementation and large-scale experiments. We provide a full-fledged implementation of HoneyBadgerBFT, which will we release as free open source software in the near future. We demonstrate experimental results from an Amazon AWS deployment with more than 100 nodes distributed across 5 continents. To demonstrate its versatility and robustness, we also deployed HoneyBadgerBFT over the Tor anonymous relay network without changing any parameters, and present throughput and latency results.</p>

    <h3 id="sec-5" class="text-xl font-semibold mt-8">1.2 Suggested Deployment Scenarios</h3>

    <p class="text-gray-300">Among numerous conceivable applications, we highlight two likely deployment scenarios that are sought after by banks, financial institutions, and advocates for fully decentralized cryptocurrencies.</p>

    <p class="text-gray-300">Confederation cryptocurrencies. The success of decentralized cryptocurrencies such as Bitcoin has inspired banks and financial institutions to inspect their transaction processing and settlement infrastructure with a new light. “Confederation cryptocurrency” is an oft-cited vision <em>[24, 25, 47]</em>, where a conglomerate of financial institutions jointly contribute to a Byzantine agreement protocol to allow fast and robust settlement of transactions. Passions are running high that this approach will streamline today’s slow and clunky infrastructure for inter-bank settlement. As a result, several new open source projects aim to build a suitable BFT protocol for this setting, such as IBM’s Open Blockchain and the Hyperledger project <em>[40]</em>.</p>

    <p class="text-gray-300">A confederation cryptocurrency would require a BFT protocol deployed over the wide-area network, possibly involving hundreds to thousands of consensus nodes. In this setting, enrollment can easily be controlled, such that the set of consensus nodes are known a priori — often referred to as the “permissioned” blockchain. Clearly HoneyBadgerBFT is a natural candidate for use in such confederation cryptocurrencies.</p>

    <p class="text-gray-300">Applicability to permissionless blockchains. By contrast, decentralized cryptocurrencies such as Bitcoin and Ethereum opt for a “permissionless” blockchain, where enrollment is open to anyone, and nodes may join and leave dynamically and frequently. To achieve security in this setting, known consensus protocols rely on proofs-of-work to defeat Sybil attacks, and pay an enormous price in terms of throughput and latency, e.g., Bitcoin commits transactions every <span class="math">\\sim 10</span> min, and its throughput limited by 7 tx/sec even when the current block size is maximized. Several recent works have suggested the promising idea of leveraging either a slower, external blockchain such as Bitcoin or economic “proof-of-stake” assumptions involving the underlying currency itself <em>[32, 35, 37, 32]</em> to bootstrap faster BFT protocols, by selecting a random committee to perform BFT in every different epoch. These approaches promise to achieve the best of both worlds, security in an open enrollment, decentralized network, and the throughput and response time matching classical BFT protocols. Here too HoneyBadgerBFT is a natural choice since the randomly selected committee can be geographically heterogeneous.</p>

    <h2 id="sec-6" class="text-2xl font-bold">2 Background and Related Work</h2>

    <p class="text-gray-300">Our overall goal is to build a replicated state machine, where clients generate and submit transactions and a network of nodes receives and processes them. Abstracting away from application specific details (such as how to represent state and compute transitions), it suffices to build a totally globally-consistent, totally-ordered, append-only transaction log. Traditionally, such a primitive is called total order or atomic broadcast <em>[23]</em>; in Bitcoin parlance, we would call it a blockchain.</p>

    <p class="text-gray-300">Fault tolerant state machine replication protocols provide strong safety and liveness guarantees, allowing a distributed system to provide correct service in spite of network latency and the failure of some nodes. A vast body of work has studied such protocols, offering different performance tradeoffs, tolerating different forms of failures and attacks, and making varying assumptions about the underlying network. We explain below the most closely related efforts to ours.</p>

    <p class="text-gray-300">1https://github.com/amiller/HoneyBadgerBFT</p>

    <p class="text-gray-300">While Paxos [36], Raft [45], and many other well-known protocols tolerate crash faults, Byzantine fault tolerant protocols (BFT), beginning with PBFT [20], tolerate even arbitrary (e.g., maliciously) corrupted nodes. Many subsequent protocols offer improved performance, often through optimistic execution that provides excellent performance when there are no faults, clients do not contend much, and the network is well-behaved, and at least some progress otherwise [2, 5, 33, 39, 51].</p>

    <p class="text-gray-300">In general, BFT systems are evaluated in deployment scenarios where latency and CPU are the bottleneck [49], thus the most effective protocols reduce the number of rounds and minimize expensive cryptographic operations.</p>

    <p class="text-gray-300">Clement et al. [22] initiated a recent line of work [4, 6, 10, 21, 22, 50] by advocating improvement of the worst-case performance, providing service quality guarantees even when the system is under attack — even if this comes at the expense of performance in the optimistic case. However, although the "Robust BFT" protocols in this vein gracefully tolerate compromised nodes, they still rely on timing assumptions about the underlying network. Our work takes this approach further, guaranteeing good throughput even in a fully asynchronous network.</p>

    <p class="text-gray-300">Deterministic asynchronous protocols are impossible for most tasks [27]. While the vast majority of practical BFT protocols steer clear of this impossibility result by making timing assumptions, randomness (and, in particular, cryptography) provides an alternative route. Indeed we know of asynchronous BFT protocols for a variety of tasks such as binary agreement (ABA), reliable broadcast (RBC), and more [13, 15, 16].</p>

    <p class="text-gray-300">Our work is most closely related to SINTRA [17], a system implementation based on the asynchronous atomic broadcast protocol from Cachin et al. (CKPS01) [15]. This protocol consists of a reduction from atomic broadcast (ABC) to common subset agreement (ACS), as well as a reduction from ACS to multi-value validated agreement (MVBA).</p>

    <p class="text-gray-300">The key invention we contribute is a novel reduction from ABC to ACS that provides better efficiency (by an  <span class="math">O(N)</span>  factor) through batching, while using threshold encryption to preserve censorship resilience (see Section 4.4). We also obtain better efficiency by cherry-picking from the literature improved instantiations of subcomponents. In particular, we sidestep the expensive MVBA primitive by using an alternative ACS [9] along with an efficient RBC [18] as explained in Section 4.4.</p>

    <p class="text-gray-300">Table 1 summarizes the asymptotic performance of HoneyBadgerBFT with several other atomic broadcast protocols. Here "Comm. compl." denotes the expected communication complexity (i.e., total bytes transferred) per committed transaction. Since PBFT relies on weak synchrony assumptions, it may therefore fail to make progress at all in an asynchronous network. Protocols KS02 [34] and RC05 [46] are optimistic, falling back to an expensive recovery mode based on MVBA. As mentioned the protocol of Cachin et al. (CKPS01) [15] can be improved using a more efficient ACS construction [9, 18]. We also obtain another  <span class="math">O(N)</span>  improvement through our novel reduction.</p>

    <p class="text-gray-300">Finally, King and Saia [30,31] have recently developed agreement protocols with less-than-quadratic number of messages by routing communications over a sparse graph. However, extending these results to the asynchronous setting remains an open problem.</p>

    <p class="text-gray-300">Table 1: Asymptotic communication complexity (bits per transaction, expected) for atomic broadcast protocols</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Async?</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Comm. compl.</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">Optim.</td>

            <td class="px-3 py-2 border-b border-gray-700">Worst</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">PBFT</td>

            <td class="px-3 py-2 border-b border-gray-700">no</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N)</td>

            <td class="px-3 py-2 border-b border-gray-700">∞</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">KS02 [34]</td>

            <td class="px-3 py-2 border-b border-gray-700">yes</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N2)</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N3)</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">RC05 [46]</td>

            <td class="px-3 py-2 border-b border-gray-700">yes</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N)</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N3)</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">CKPS01 [15]</td>

            <td class="px-3 py-2 border-b border-gray-700">yes</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N2)</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N3)</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">CKPS01 [15]+ [9,18]</td>

            <td class="px-3 py-2 border-b border-gray-700">yes</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N2)</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N2)</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">HoneyBadgerBFT (this work)</td>

            <td class="px-3 py-2 border-b border-gray-700">yes</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N)</td>

            <td class="px-3 py-2 border-b border-gray-700">O(N)</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Almost all modern BFT protocols rely on timing assumptions (such as partial or weak synchrony) to guarantee liveness. Purely asynchronous BFT protocols have received considerably less attention in recent years. Consider the following argument, which, if it held, would justify this narrowed focus:</p>

    <p class="text-gray-300">[X] Weak synchrony assumptions are unavoidable, since in any network that violates these assumptions, even asynchronous protocols would provide unacceptable performance.</p>

    <p class="text-gray-300">In this section, we present make two counterarguments that refute the premise above. First, we illustrate the theoretical separation between the asynchronous and weakly synchronous network models. Specifically we construct an adversarial network scheduler that violates PBFT's weak synchrony assumption (and indeed causes it to fail) but under which any purely asynchronous protocol (such as HoneyBadgerBFT) makes good progress. Second, we make a practical observation: even when their assumptions are met, weakly synchronous protocols are slow to recover from a network partition once it heals, whereas asynchronous protocols make progress as soon as messages are delivered.</p>

    <p class="text-gray-300">Before proceeding we review the various standard forms of timing assumptions. In an asynchronous network, the adversary can deliver messages in any order and at any time, but nonetheless must eventually deliver every message sent between correct nodes. Nodes in an asynchronous network effectively have no use for "real time" clocks, and can only take actions based on the ordering of messages they receive.</p>

    <p class="text-gray-300">The well-known FLP [27] result rules out the possibility of deterministic asynchronous protocols for atomic broadcast and many other tasks. A deterministic protocol must therefore make some stronger timing assumptions. A convenient (but very strong) network assumption is synchrony: a  <span class="math">\\Delta</span> -synchronous network guarantees that every message sent is delivered after at most a delay of  <span class="math">\\Delta</span>  (where  <span class="math">\\Delta</span>  is a measure of real time).</p>

    <p class="text-gray-300">Weaker timing assumptions come in several forms. In the unknown-  <span class="math">\\Delta</span>  model, the protocol is unable to use the delay bound as a parameter. Alternatively, in the eventually synchronous model, the message delay bound  <span class="math">\\Delta</span>  is only guaranteed to hold after some (unknown) instant, called the "Global Stabilization Time." Collectively, these two models are referred to as partial synchrony [26]. Yet another variation is weak synchrony [26], in which the delay bound is time varying, but eventually does not grow faster than a polynomial function of time [20].</p>

    <p class="text-gray-300">In terms of feasibility, the above are equivalent — a protocol that succeeds in one setting can be systematically adapted for another. In terms of concrete performance, however, adjusting for weak synchrony means gradually increasing the timeout parameter over time</p>

    <p class="text-gray-300">(e.g., by an “exponential back-off” policy). As we show later, this results in delays when recovering from transient network partitions.</p>

    <p class="text-gray-300">Protocols typically manifest these assumptions in the form of a timeout event. For example, if parties detect that no progress has been made within a certain interval, then they take a corrective action such as electing a new leader. Asynchronous protocols do not rely on timers, and make progress whenever messages are delivered, regardless of actual clock time.</p>

    <p class="text-gray-300">Counting rounds in asynchronous networks. Although the guarantee of eventual delivery is decoupled from notions of “real time,” it is nonetheless desirable to characterize the running time of asynchronous protocols. The standard approach (e.g., as explained by Canetti and Rabin <em>[19]</em>) is for the adversary to assign each message a virtual round number, subject to the condition that every <span class="math">(r-1)</span>-message between correct nodes must be delivered before any <span class="math">(r+1)</span>-message is sent.</p>

    <h3 id="sec-11" class="text-xl font-semibold mt-8">3.2 When Weak Synchrony Fails</h3>

    <p class="text-gray-300">We now proceed to describe why weakly synchronous BFT protocols can fail (or suffer from performance degradation) when network conditions are adversarial (or unpredictable). This motivates why such protocols are unsuited for the cryptocurrency-oriented application scenarios described in Section 1.</p>

    <p class="text-gray-300">A network scheduler that thwarts PBFT. We use Practical Byzantine Fault Tolerance (PBFT) <em>[20]</em>, the classic leader-based BFT protocol, a representative example to describe how an adversarial network scheduler can cause a class of leader-based BFT protocols <em>[4, 6, 10, 22, 33, 50]</em> to grind to a halt.</p>

    <p class="text-gray-300">At any given time, the designated leader is responsible for proposing the next batch of transactions. If progress isn’t made, either because the leader is faulty or because the network has stalled, then the nodes attempt to elect a new leader. The PBFT protocol critically relies on a weakly synchronous network for liveness. We construct an adversarial scheduler that violates this assumption, and indeed prevents PBFT from making any progress at all, but for which HoneyBadgerBFT (and, in fact, any asynchronous protocol) performs well. It is unsurprising that a protocol based on timing assumptions fails when those assumptions are violated; however, demonstrating an explicit attack helps motivate our asynchronous construction.</p>

    <p class="text-gray-300">The intuition behind our scheduler is simple. First, we assume that a single node has crashed. Then, the network delays messages whenever a correct node is the leader, preventing progress and causing the next node in round-robin order to become the new leader. When the crashed node is the next up to become the leader, the scheduler immediately heals the network partition and delivers messages very rapidly among the honest nodes; however, since the leader has crashed, no progress is made here either.</p>

    <p class="text-gray-300">This attack violates the weak synchrony assumption because it must delay messages for longer and longer each cycle, since PBFT widens its timeout interval after each failed leader election. On the other hand, it provides larger and larger periods of synchrony as well. However, since these periods of synchrony occur at inconvenient times, PBFT is unable to make use of them. Looking ahead, HoneyBadgerBFT, and indeed any asynchronous protocol, would be able to make progress during these opportunistic periods of synchrony.</p>

    <p class="text-gray-300">To confirm our analysis, we implemented this malicious scheduler as a proxy that intercepted and delayed all view change messages to the new leader, and tested it against a 1200 line Python implementation of PBFT. The results and message logs we observed were consistent with the above analysis; our replicas became stuck in a loop requesting view changes that never succeeded. In the Appendix A we give a complete description of PBFT and explain how it behaves under this attack.</p>

    <p class="text-gray-300">Slow recovery from network partitions. Even if the weak synchrony assumption is eventually satisfied, protocols that rely on it may also be slow to recover from transient network partitions. Consider the following scenario, which is simply a finite prefix of the attack described above: one node is crashed, and the network is temporarily partitioned for a duration of <span class="math">2^{D}\\Delta</span>. Our scheduler heals the network partition precisely when it is the crashed node’s turn to become leader. Since the timeout interval at this point is now <span class="math">2^{D+1}\\Delta</span>, the protocol must wait for another <span class="math">2^{D+1}\\Delta</span> interval before beginning to elect a new leader, despite that the network is synchronous during this interval.</p>

    <p class="text-gray-300">The tradeoff between robustness and responsiveness. Such behaviors we observe above are not specific to PBFT, but rather are fundamentally inherent to protocols that rely on timeouts to cope with crashes. Regardless of the protocol variant, a practitioner must tune their timeout policy according to some tradeoff. At one extreme (eventual synchrony), the practitioner makes a specific estimate about the network delay <span class="math">\\Delta</span>. If the estimate is too low, then the system may make no progress at all; too high, and it does not utilize the available bandwidth. At the other extreme (weak synchrony), the practitioner avoids specifying any absolute delay, but nonetheless must choose a “gain” that affects how quickly the system tracks varying conditions. An asynchronous protocol avoids the need to tune such parameters.</p>

    <h2 id="sec-12" class="text-2xl font-bold">4 The HoneyBadgerBFT PROTOCOL</h2>

    <p class="text-gray-300">In this section we present HoneyBadgerBFT, the first asynchronous atomic broadcast protocol to achieve optimal asymptotic efficiency.</p>

    <h3 id="sec-13" class="text-xl font-semibold mt-8">4.1 Problem Definition: Atomic Broadcast</h3>

    <p class="text-gray-300">We first define our network model and the atomic broadcast problem. Our setting involves a network of <span class="math">N</span> designated nodes, with distinct well-known identities (<span class="math">\\mathcal{P}_{0}</span> through <span class="math">\\mathcal{P}_{N-1}</span>). The nodes receive transactions as input, and their goal is to reach common agreement on an ordering of these transactions. Our model particularly matches the deployment scenario of a “permissioned blockchain” where transactions can be submitted by arbitrary clients, but the nodes responsible for carrying out the protocol are fixed.</p>

    <p class="text-gray-300">The atomic broadcast primitive allows us to abstract away any application-specific details, such as how transactions are to be interpreted (to prevent replay attacks, for example, an application might define a transaction to include signatures and sequence numbers). For our purposes, transactions are simply unique strings. In practice, clients would generate transactions and send them to all of the nodes, and consider them committed after collecting signatures from a majority of nodes. To simplify our presentation, we do not explicitly model clients, but rather assume that transactions are chosen by the adversary and provided as input to the nodes. Likewise, a transaction is considered committed once it is output by a node.</p>

    <p class="text-gray-300">Our system model makes the following assumptions:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>(Purely asynchronous network) We assume each pair of nodes is connected by a reliable authenticated point-to-point channel that does not drop messages. The delivery schedule is entirely determined by the adversary, but every message sent between correct nodes must eventually be delivered. We will be interested in characterizing the running time of protocols based on the</li>

    </ul>

    <p class="text-gray-300">number of asynchronous rounds (as described in Section 2). As the network may queue messages with arbitrary delay, we also assume nodes have unbounded buffers and are able to process all the messages they receive.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>(Static Byzantine faults) The adversary is given complete control of up to <span class="math">f</span> faulty nodes, where <span class="math">f</span> is a protocol parameter. Note that <span class="math">3f+1\\leq N</span> (which our protocol achieves) is the lower bound for broadcast protocols in this setting.</li>

      <li>(Trusted setup) For ease of presentation, we assume that nodes may interact with a trusted dealer during an initial protocol-specific setup phase, which we will use to establish public keys and secret shares. Note that in a real deployment, if an actual trusted party is unavailable, then a distributed key generation protocol could be used instead (c.f., Boldyreva <em>[11]</em>). All the distributed key generation protocols we know of rely on timing assumptions; fortunately these assumptions need only to hold during setup.</li>

    </ul>

    <h6 id="sec-14" class="text-base font-medium mt-4">Definition 1</h6>

    <p class="text-gray-300">An atomic broadcast protocol must satisfy the following properties, all of which should hold with high probability (as a function <span class="math">1-\\text{negl}(\\lambda)</span> of a security parameter, <span class="math">\\lambda</span>) in an asynchronous network and in spite of an arbitrary adversary:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>(Agreement) If any correct node outputs a transaction <span class="math">\\mathsf{tx}</span>, then every correct node outputs <span class="math">\\mathsf{tx}</span>.</li>

      <li>(Total Order) If one correct node has output the sequence of transactions <span class="math">\\langle\\mathsf{tx}_{0},\\mathsf{tx}_{1},...\\mathsf{tx}_{j}\\rangle</span> and another has output <span class="math">\\langle\\mathsf{tx}^{\\prime}_{0},\\mathsf{tx}^{\\prime}_{1},...\\mathsf{tx}^{\\prime}_{j}\\rangle</span>, then <span class="math">\\mathsf{tx}_{i}=\\mathsf{tx}^{\\prime}_{i}</span> for <span class="math">i\\leq\\min(j,j^{\\prime})</span>.</li>

      <li>(Censorship Resilience) If a transaction <span class="math">\\mathsf{tx}</span> is input to <span class="math">N-f</span> correct nodes, then it is eventually output by every correct node.</li>

    </ul>

    <p class="text-gray-300">The censorship resilience property is a liveness property that prevents an adversary from blocking even a single transaction from being committed. This property has been referred to by other names, for example “fairness” by Cachin et al. <em>[15]</em>, but we prefer this more descriptive phrase.</p>

    <p class="text-gray-300">Performance metrics. We will primarily be interested in analyzing the efficiency and transaction delay of our atomic broadcast protocol.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>(Efficiency) Assume that the input buffers of each honest node are sufficiently full <span class="math">\\Omega(\\mathsf{poly}(N,\\lambda))</span>. Then efficiency is the expected communication cost for each node amortized over all committed transactions.</li>

    </ul>

    <p class="text-gray-300">Since each node must output each transaction, <span class="math">O(1)</span> efficiency (which our protocol achieves) is asymptotically optimal. The above definition of efficiency assumes the network is under load, reflecting our primary goal: to sustain high throughput while fully utilizing the network’s available bandwidth. Since we achieve good throughput by batching, our system uses more bandwidth per committed transaction during periods of low demand when transactions arrive infrequently. A stronger definition without this qualification would be appropriate if our goal was to minimize costs (e.g., for usage-based billing).</p>

    <p class="text-gray-300">In practice, network links have limited capacity, and if more transactions are submitted than the network can handle, a guarantee on confirmation time cannot hold in general. Therefore we define transaction delay below relative to the number of transactions that have been input ahead of the transaction in question. A finite transaction delay implies censorship resilience.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>(Transaction delay) Suppose an adversary passes a transaction <span class="math">\\mathsf{tx}</span> as input to <span class="math">N-f</span> correct nodes. Let <span class="math">T</span> be the “backlog”, i.e. the difference between the total number of transactions previously input to any correct node and the number of transactions that have been committed. Then transaction delay is the expected number of asynchronous rounds before <span class="math">\\mathsf{tx}</span> is output by every correct node as a function of <span class="math">T</span>.</li>

    </ul>

    <h3 id="sec-15" class="text-xl font-semibold mt-8">4.2 Overview and Intuition</h3>

    <p class="text-gray-300">In HoneyBadgerBFT, nodes receive transactions as input and store them in their (unbounded) buffers. The protocol proceeds in epochs, where after each epoch, a new batch of transactions is appended to the committed log. At the beginning of each epoch, nodes choose a subset of the transactions in their buffer (by a policy we will define shortly), and provide them as input to an instance of a randomized agreement protocol. At the end of the agreement protocol, the final set of transactions for this epoch is chosen.</p>

    <p class="text-gray-300">At this high level, our approach is similar to existing asynchronous atomic broadcast protocols, and in particular to Cachin et al. <em>[15]</em>, the basis for a large scale transaction processing system (SINTRA). Like ours, Cachin’s protocol is centered around an instance of the Asynchronous Common Subset (ACS) primitive. Roughly speaking, the ACS primitive allows each node to propose a value, and guarantees that every node outputs a common vector containing the input values of at least <span class="math">N-2f</span> correct nodes. It is trivial to build atomic broadcast from this primitive — each node simply proposes a subset of transactions from the front its queue, and outputs the union of the elements in the agreed-upon vector. However, there are two important challenges.</p>

    <p class="text-gray-300">Challenge 1: Achieving censorship resilience. The cost of ACS depends directly on size of the transaction sets proposed by each node. Since the output vector contains at least <span class="math">N-f</span> such sets, we can therefore improve the overall efficiency by ensuring that nodes propose mostly disjoint sets of transactions, thus committing more distinct transactions in one batch for the same cost. Therefore instead of simply choosing the first element(s) from its buffer (as in CKPS01 <em>[15]</em>), each node in our protocol proposes a randomly chosen sample, such that each transaction is, on average, proposed by only one node.</p>

    <p class="text-gray-300">However, implemented naïvely, this optimization would compromise censorship resilience, since the ACS primitive allows the adversary to choose which nodes’ proposals are ultimately included. The adversary could selectively censor a transaction excluding whichever node(s) propose it. We avoid this pitfall by using threshold encryption, which prevents the adversary from learning which transactions are proposed by which nodes, until after agreement is already reached. The full protocol will be described in Section 4.3.</p>

    <p class="text-gray-300">Challenge 2: Practical throughput. Although the theoretical feasibility of asynchronous ACS and atomic broadcast have been known <em>[15, 17, 9]</em>, their practical performance is not. To the best of our knowledge, the only other work that implemented ACS was by Cachin and Portiz <em>[17]</em>, who showed that they could attain a throughput of 0.4 tx/sec over a wide area network. Therefore, an interesting question is whether such protocols can attain high throughput in practice.</p>

    <p class="text-gray-300">In this paper, we show that by stitching together a carefully chosen array of sub-components, we can efficiently instantiate ACS and attain much greater throughput both asymptotically and in practice. Notably, we improve the asymptotic cost (per node) of ACS from <span class="math">O(N^{2})</span> (as in Cachin et al. <em>[15, 17]</em>) to <span class="math">O(1)</span>. Since the components we cherry-pick have not been presented together before (to our knowledge), we provide a self-contained description of the whole construction in Section 4.4.</p>

    <p class="text-gray-300">Modular protocol composition. We are now ready to present our constructions formally. Before doing so, we make a remark about the style of our presentation. We define our protocols in a modu</p>

    <p class="text-gray-300">lar style, where each protocol may run several instances of other (sub)protocols. The outer protocol can provide input to and receive output from the subprotocol. A node may begin executing a (sub)protocol even before providing it input (e.g., if it receives messages from other nodes).</p>

    <p class="text-gray-300">It is essential to isolate such (sub)protocol instances to ensure that messages pertaining to one instance cannot be replayed in another. This is achieved in practice by associating to each (sub)protocol instance a unique string (a session identifier), tagging any messages sent or received in this (sub)protocol with this identifier, and routing messages accordingly. We suppress these message tags in our protocol descriptions for ease of reading. We use brackets to distinguish between tagged instances of a subprotocol. For example,  <span class="math">\\mathsf{RBC}[i]</span>  denotes an  <span class="math">i^{th}</span>  instance of the RBC subprotocol.</p>

    <p class="text-gray-300">We implicitly assume that asynchronous communications between parties are over authenticated asynchronous channels. In reality, such channels could be instantiated using TLS sockets, for example, as we discuss in Section 5.</p>

    <p class="text-gray-300">To distinguish different message types sent between parties within a protocol, we use a label in typewriter font (e.g., VAL(m) indicates a message  <span class="math">m</span>  of type VAL).</p>

    <p class="text-gray-300">Building block: ACS. Our main building block is a primitive called asynchronous common subset (ACS). The theoretical feasibility of constructing ACS has been demonstrated in several works [9, 15]. In this section, we will present the formal definition of ACS and use it as a blackbox to construct HoneyBadgerBFT. Later in Section 4.4, we will show that by combining several constructions that were somewhat overlooked in the past, we can instantiate ACS efficiently!</p>

    <p class="text-gray-300">More formally, an ACS protocol satisfies the following properties:</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">- (Validity) If a correct node outputs a set  <span class="math">\\mathbf{v}</span> , then  $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{v}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\geq N - f<span class="math">  and  </span>\\mathbf{v}<span class="math">  contains the inputs of at least  </span>N - 2f$  correct nodes.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>(Agreement) If a correct node outputs  <span class="math">\\mathbf{v}</span> , then every node outputs  <span class="math">\\mathbf{v}</span> .</li>

      <li>(Totality) If  <span class="math">N - f</span>  correct nodes receive an input, then all correct nodes produce an output.</li>

    </ul>

    <p class="text-gray-300">Building block: threshold encryption. A threshold encryption scheme TPKE is a cryptographic primitive that allows any party to encrypt a message to a master public key, such that the network nodes must work together to decrypt it. Once  <span class="math">f + 1</span>  correct nodes compute and reveal decryption shares for a ciphertext, the plaintext can be recovered; until at least one correct node reveals its decryption share, the attacker learns nothing about the plaintext. A threshold scheme provides the following interface:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>TPKE.Setup  <span class="math">(1^{\\lambda})\\rightarrow \\mathsf{PK},\\{\\mathsf{SK}_i\\}</span>  generates a public encryption key PK, along with secret keys for each party  <span class="math">\\mathsf{SK}_i</span></li>

      <li>TPKE.Enc(PK, m)  <span class="math">\\rightarrow C</span>  encrypts a message  <span class="math">m</span></li>

      <li>TPKE.DecShare(SK <span class="math">_i</span> , C)  <span class="math">\\rightarrow \\sigma_i</span>  produces the  <span class="math">i^{th}</span>  share of the decryption (or  <span class="math">\\bot</span>  if  <span class="math">C</span>  is malformed)</li>

      <li>TPKE.Dec(PK, C, {i, σi}) → m combines a set of decryption shares {i, σi} from at least f + 1 parties obtain the plaintext m (or, if C contains invalid shares, then the invalid shares are identified).</li>

    </ul>

    <p class="text-gray-300">In our concrete instantiation, we use the threshold encryption scheme of Baek and Zheng [7]. This scheme is also robust (as required by our protocol), which means that even for an adversarially generated ciphertext  <span class="math">C</span> , at most one plaintext (besides  <span class="math">\\bot</span> ) can be recovered. Note that we assume TPKE.Dec effectively identifies invalid decryption shares among the inputs. Finally, the scheme satisfies the</p>

    <p class="text-gray-300">Let  <span class="math">B = \\Omega (\\lambda N^2\\log N)</span>  be the batch size parameter.</p>

    <p class="text-gray-300">Let PK be the public key received from TPKE.Setup (executed by a dealer), and let  <span class="math">\\mathsf{SK}_i</span>  be the secret key for  <span class="math">\\mathcal{P}_i</span> .</p>

    <p class="text-gray-300">Let  <span class="math">\\text{buf} := [ ]</span>  be a FIFO queue of input transactions.</p>

    <p class="text-gray-300">Proceed in consecutive epochs numbered  <span class="math">r</span> :</p>

    <p class="text-gray-300">// Step 1: Random selection and encryption</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>let proposed be a random selection of  <span class="math">\\lfloor B / N\\rfloor</span>  transactions from the first  <span class="math">B</span>  elements of buf</li>

      <li>encrypt  <span class="math">x \\coloneqq \\mathsf{TPKE.Enc}(\\mathsf{PK},\\mathsf{proposed})</span></li>

    </ul>

    <p class="text-gray-300">// Step 2: Agreement on ciphertexts</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>pass  <span class="math">x</span>  as input to  <span class="math">\\mathsf{ACS}[r]</span>  //see Figure 4</li>

      <li>receive  <span class="math">\\{v_{j}\\}_{j\\in S}</span> , where  <span class="math">S\\subset [1..N]</span> , from ACS[r]</li>

    </ul>

    <p class="text-gray-300">// Step 3: Decryption</p>

    <p class="text-gray-300">for each  <span class="math">j\\in S</span></p>

    <p class="text-gray-300">let  <span class="math">e_j \\coloneqq \\mathsf{TPKE.DecShare}(\\mathsf{SK}_i, v_j)</span></p>

    <p class="text-gray-300">multicast DEC  <span class="math">(r,j,i,e_j)</span></p>

    <p class="text-gray-300">wait to receive at least  <span class="math">f + 1</span>  messages of the form</p>

    <p class="text-gray-300">DEC  <span class="math">(r,j,k,e_{j,k})</span></p>

    <p class="text-gray-300">decode  <span class="math">y_{j}\\coloneqq \\mathsf{TPKE.Dec}(\\mathsf{PK},\\{(k,e_{j,k})\\})</span></p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>let  <span class="math">\\mathrm{block}_r \\coloneqq \\mathrm{sorted}(\\cup_{j \\in S} \\{y_j\\})</span> , such that  <span class="math">\\mathrm{block}_r</span>  is sorted in a canonical order (e.g., lexicographically)</li>

      <li>set buf := buf - block</li>

    </ul>

    <p class="text-gray-300">Figure 1: HoneyBadgerBFT.</p>

    <p class="text-gray-300">obvious correctness properties, as well as a threshold version of the IND-CPA game.</p>

    <p class="text-gray-300">Atomic broadcast from ACS. We now describe in more detail our atomic broadcast protocol, defined in Figure 1.</p>

    <p class="text-gray-300">As mentioned, this protocol is centered around an instance of ACS. In order to obtain scalable efficiency, we choose a batching policy. We let  <span class="math">B</span>  be a batch size, and will commit  <span class="math">\\Omega(B)</span>  transactions in each epoch. Each node proposes  <span class="math">B / N</span>  transactions from its queue. To ensure that nodes propose mostly distinct transactions, we randomly select these transactions from the first  <span class="math">B</span>  in each queue.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">As we will see in Section 4.4, our ACS instantiation has a total communication cost of  $O(N^2</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{v}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">+ \\lambda N^3\\log N)<span class="math"> , where  </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{v}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math">  bounds the size of any node&#x27;s input. We therefore choose a batch size  </span>B = \\Omega (\\lambda N^2\\log N)<span class="math">  so that the contribution from each node  </span>(B / N)$  absorbs this additive overhead.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">In order to prevent the adversary from influencing the outcome we use a threshold encryption scheme, as described below. In a nutshell, each node chooses a set of transactions, and then encrypts it. Each node then passes the encryption as input to the ACS subroutine. The output of ACS is therefore a vector of ciphertexts. The ciphertexts are decrypted once the ACS is complete. This guarantees that the set of transactions is fully determined before the adversary learns the particular contents of the proposals made by each node. This guarantees that an adversary cannot selectively prevent a transaction from being committed once it is in the front of the queue at enough correct nodes.</p>

    <p class="text-gray-300">Cachin et al. present a protocol we call CKPS01 that (implic</p>

    <p class="text-gray-300">Algorithm RBC (for party  <span class="math">\\mathcal{P}_i</span> , with sender  <span class="math">\\mathcal{P}_{\\mathrm{Sender}}</span> )</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>upon input(v) (if  <span class="math">\\mathcal{P}_i = \\mathcal{P}_{\\mathrm{Sender}}</span> ): let  <span class="math">\\{s_j\\}_{j\\in [N]}</span>  be the blocks of an  <span class="math">(N - 2f,N)</span> -erasure coding scheme applied to  <span class="math">\\nu</span>  let  <span class="math">h</span>  be a Merkle tree root computed over  <span class="math">\\{s_j\\}</span>  send VAL  <span class="math">(h,b_{j},s_{j})</span>  to each party  <span class="math">\\mathcal{P}_j</span> , where  <span class="math">b_{j}</span>  is the  <span class="math">j^{th}</span>  Merkle tree branch</li>

      <li>upon receiving VAL  <span class="math">(h,b_i,s_i)</span>  from  <span class="math">\\mathcal{P}_{\\mathrm{Sender}}</span> , multicast ECHO  <span class="math">(h,b_i,s_i)</span></li>

      <li>upon receiving ECHO  <span class="math">(h,b_j,s_j)</span>  from party  <span class="math">\\mathcal{P}_j</span> , check that  <span class="math">b_{j}</span>  is a valid Merkle branch for root  <span class="math">h</span>  and leaf  <span class="math">s_j</span> , and otherwise discard</li>

      <li>upon receiving valid ECHO  <span class="math">(h,\\cdot ,\\cdot)</span>  messages from  <span class="math">N - f</span>  distinct parties, - interpolate  <span class="math">\\{s_j^{\\prime}\\}</span>  from any  <span class="math">N - 2f</span>  leaves received - recompute Merkle root  <span class="math">h^\\prime</span>  and if  <span class="math">h^\\prime \\neq h</span>  then abort - if READY(h) has not yet been sent, multicast READY(h)</li>

      <li>upon receiving  <span class="math">f + 1</span>  matching READY(h) messages, if READY has not yet been sent, multicast READY(h)</li>

      <li>upon receiving  <span class="math">2f + 1</span>  matching READY(h) messages, wait for  <span class="math">N - 2f</span>  ECHO messages, then decode  <span class="math">\\nu</span></li>

    </ul>

    <p class="text-gray-300">Figure 2: Reliable broadcast algorithm, adapted from Bracha's broadcast [13], with erasure codes to improve efficiency [18]</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">ity) reduces ACS to multi-valued validated Byzantine agreement (MVBA) [15]. Roughly speaking, MVBA allows nodes to propose values satisfying a predicate, one of which is ultimately chosen. The reduction is simple: the validation predicate says that the output must be a vector of signed inputs from at least  <span class="math">N - f</span>  parties. Unfortunately, the MVBA primitive agreement becomes a bottleneck, because the only construction we know of incurs an overhead of  $O(N^3</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{v}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)$ .</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">We avoid this bottleneck by using an alternative instantiation of ACS that sidesteps MVBA entirely. The instantiation we use is due to Ben-Or et al. [9] and has, in our view, been somewhat overlooked. In fact, it predates CKPS01 [15], and was initially developed for a mostly unrelated purpose (as a tool for achieving efficient asynchronous multi-party computation [9]). This protocol is a reduction from ACS to reliable broadcast (RBC) and asynchronous binary Byzantine agreement (ABA). Only recently do we know of efficient constructions for these subcomponents, which we explain shortly.</p>

    <p class="text-gray-300">At a high level, the ACS protocol proceeds in two main phases. In the first phase, each node  <span class="math">\\mathcal{P}_i</span>  uses RBC to disseminate its proposed value to the other nodes, followed by ABA to decide on a bit vector that indicates which RBCs have successfully completed.</p>

    <p class="text-gray-300">We now briefly explain the RBC and ABA constructions before explaining the Ben-Or protocol in more detail.</p>

    <p class="text-gray-300">Communication-optimal reliable roadcast. An asynchronous reliable broadcast channel satisfies the following properties:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>(Agreement) If any two correct nodes deliver  <span class="math">\\nu</span>  and  <span class="math">\\nu&#x27;</span> , then  <span class="math">\\nu = \\nu&#x27;</span> .</li>

      <li>(Totality) If any correct node delivers  <span class="math">\\nu</span> , then all correct nodes deliver  <span class="math">\\nu</span></li>

      <li>(Validity) If the sender is correct and inputs  <span class="math">\\nu</span> , then all correct nodes deliver  <span class="math">\\nu</span></li>

    </ul>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">While Bracha's [13] classic reliable broadcast protocol requires  $O(N^2</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\nu</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math">  bits of total communication in order to broadcast a message of size  </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\nu</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> , Cachin and Tessaro [18] observed that erasure coding can reduce this cost to merely  </span>O(N</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\nu</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">+ \\lambda N^2\\log N)$ , even in the</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">!<a href="img-0.jpeg">img-0.jpeg</a> (a) Normal</p>

    <p class="text-gray-300">!<a href="img-1.jpeg">img-1.jpeg</a> (b) Wait for slow broadcast</p>

    <p class="text-gray-300">!<a href="img-2.jpeg">img-2.jpeg</a> (c) Broadcast fails Figure 3: (Illustrated examples of ACS executions.) Each execution of our protocol involves running  <span class="math">N</span>  concurrent instances of reliable broadcast (RBC), as well as  <span class="math">N</span>  of byzantine agreement (BA), which in turn use an expected constant number of common coins. We illustrate several possible examples of how these instances play out, from the viewpoint of Node 0. (a) In the ordinary case, Node 0 receives value  <span class="math">V_{1}</span>  (Node 1's proposed value) from the reliable broadcast at index 1. Node 0 therefore provides input "Yes" to  <span class="math">\\mathrm{BA}_1</span> , which outputs "Yes." (b)  <span class="math">\\mathrm{RBC}_2</span>  takes too long to complete, and Node 0 has already received  <span class="math">(N - f)</span>  "Yes" outputs, so it votes "No" for  <span class="math">\\mathrm{BA}_2</span> . However, other nodes have seen  <span class="math">\\mathrm{RBC}_2</span>  complete successfully, so  <span class="math">\\mathrm{BA}_2</span>  results in "Yes" and Node 0 must wait for  <span class="math">V_{2}</span> . (c)  <span class="math">\\mathrm{BA}_3</span>  concludes with "No" before  <span class="math">\\mathrm{RBC}_3</span>  completes.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">worst case. This is a significant improvement for large messages (i.e., when  $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\nu</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\gg \\lambda N \\log N<span class="math"> ), which, (looking back to Section 4.3) guides our choice of batch size. The use of erasure coding here induces at most a small constant factor of overhead, equal to  </span>\\frac{N}{N - 2f} &lt; 3$ .</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">If the sender is correct, the total running time is three (asynchronous) rounds; and in any case, at most two rounds elapse between when the first correct node outputs a value and the last outputs a value. The reliable broadcast algorithm shown in Figure 2.</p>

    <p class="text-gray-300">Binary Agreement. Binary agreement is a standard primitive that allows nodes to agree on the value of a single bit. More formally, binary agreement guarantees three properties:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>(Agreement) If any correct node outputs the bit  <span class="math">b</span> , then every correct node outputs  <span class="math">b</span> .</li>

      <li>(Termination) If all correct nodes receive input, then every correct node outputs a bit.</li>

      <li>(Validity) If any correct node outputs  <span class="math">b</span> , then at least one correct node received  <span class="math">b</span>  as input.</li>

    </ul>

    <p class="text-gray-300">The validity property implies unanimity: if all of the correct nodes receive the same input value  <span class="math">b</span> , then  <span class="math">b</span>  must be the decided value. On the other hand, if at any point two nodes receive different inputs, then the adversary may force the decision to either value even before the remaining nodes receive input.</p>

    <p class="text-gray-300">We instantiate this primitive with a protocol from Moustefaoui et al. [42], which is based on a cryptographic common coin. We defer explanation of this instantiation to the Appendix. Its expected running time is  <span class="math">O(1)</span> , and in fact completes within  <span class="math">O(k)</span>  rounds with probability  <span class="math">1 - 2^{-k}</span> . The communication complexity per node is  <span class="math">O(N\\lambda)</span> , which is due primarily to threshold cryptography used in the common coin.</p>

    <p class="text-gray-300">Agreeing on a subset of proposed values. Putting the above pieces together, we use a protocol from Ben-Or et al. [9] to agree on a set of values containing the entire proposals of at least  <span class="math">N - f</span>  nodes.</p>

    <p class="text-gray-300">At a high level, this protocol proceeds in two main phases. In the</p>

    <p class="text-gray-300">Let  <span class="math">\\{\\mathsf{RBC}_i\\}_N</span>  refer to  <span class="math">N</span>  instances of the reliable broadcast protocol, where  <span class="math">\\mathcal{P}_i</span>  is the sender of  <span class="math">\\mathsf{RBC}_i</span> . Let  <span class="math">\\{\\mathsf{BA}_i\\}_N</span>  refer to  <span class="math">N</span>  instances of the binary byzantine agreement protocol.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>upon receiving input  <span class="math">\\nu_{i}</span> , input  <span class="math">\\nu_{i}</span>  to  <span class="math">\\mathsf{RBC}_i</span>  // See Figure 2</li>

      <li>upon delivery of  <span class="math">\\nu_{j}</span>  from  <span class="math">\\mathsf{RBC}_j</span> , if input has not yet been provided to  <span class="math">\\mathsf{BA}_j</span> , then provide input 1 to  <span class="math">\\mathsf{BA}_j</span> . See Figure 11</li>

      <li>upon delivery of value 1 from at least  <span class="math">N - f</span>  instances of BA, provide input 0 to each instance of BA that has not yet been provided input.</li>

      <li>once all instances of BA have completed, let  <span class="math">C \\subset [1..N]</span>  be the indexes of each BA that delivered 1. Wait for the output  <span class="math">\\nu_{j}</span>  for each  <span class="math">\\mathsf{RBC}_j</span>  such that  <span class="math">j \\in C</span> . Finally output  <span class="math">\\cup_{j \\in C} \\nu_{j}</span> .</li>

    </ul>

    <p class="text-gray-300">Figure 4: Common Subset Agreement protocol (from Ben-Or et al. [9])</p>

    <p class="text-gray-300">first phase, each node  <span class="math">\\mathcal{P}_i</span>  uses Reliable Broadcast to disseminate its proposed value to the other nodes. In the second stage,  <span class="math">N</span>  concurrent instances of binary Byzantine agreement are used to agree on a bit vector  <span class="math">\\{b_j\\}_{j\\in [1..N]}</span> , where  <span class="math">b_{j} = 1</span>  indicates that  <span class="math">\\mathcal{P}_j</span> 's proposed value is included in the final set.</p>

    <p class="text-gray-300">Actually the simple description above conceals a subtle challenge, for which Ben-Or provide a clever solution.</p>

    <p class="text-gray-300">A naive attempt at an implementation of the above sketch would have each node to wait for the first  <span class="math">(N - f)</span>  broadcasts to complete, and then propose 1 for the binary agreement instances corresponding to those and 0 for all the others. However, correct nodes might observe the broadcasts complete in a different order. Since binary agreement only guarantees that the output is 1 if all the correct nodes unanimously propose 1, it is possible that the resulting bit vector could be empty.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">To avoid this problem, nodes abstain from proposing 0 until they are certain that the final vector will have at least  <span class="math">N - f</span>  bits set. To provide some intuition for the flow of this protocol, we narrate several possible scenarios in Figure 3. The algorithm from Ben-Or et al. [9] is given in Figure 4. The running time is  <span class="math">O(\\log N)</span>  in expectation, since it must wait for all binary agreement instances to finish. When instantiated with the reliable broadcast and binary agreement constructions described above, the total communication complexity is  $O(N^2</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\nu</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">+ \\lambda N^3\\log N)<span class="math">  assuming  </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\nu</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$  is the largest size of any node's input.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">First we observe that the agreement and total order properties follow immediately from the definition of ACS and robustness of the TPKE scheme.</p>

    <p class="text-gray-300">THEOREM 1. (Agreement and total order). The HoneyBadgerBFT protocol satisfies the agreement and total order properties, except for negligible probability.</p>

    <p class="text-gray-300">Proof. These two properties follow immediately from properties of the high-level protocols, ACS and TPKE. Each ACS instance guarantees that nodes agree on a vector of ciphertexts in each epoch (Step 2). The robustness of TPKE guarantees that each correct node decrypts these ciphertexts to consistent values (Step 3). This suffices to ensure agreement and total order.  <span class="math">\\square</span></p>

    <p class="text-gray-300">THEOREM 2. (Complexity). Assuming a batch size of  <span class="math">B = \\Omega(\\lambda N^2 \\log N)</span> , the running time for each HoneyBadgerBFT epoch</p>

    <p class="text-gray-300">is  <span class="math">O(\\log N)</span>  in expectation, and the total expected communication complexity is  <span class="math">O(B)</span> .</p>

    <p class="text-gray-300">Proof. The cost and running time of ACS is explained in Section 4.4. The  <span class="math">N</span>  instances of threshold decryption incur one additional round and an additional cost of  <span class="math">O(\\lambda N^2)</span> , which does not affect the overall asymptotic cost.  <span class="math">\\square</span></p>

    <p class="text-gray-300">The HoneyBadgerBFT protocol may commit up to  <span class="math">B</span>  transactions in a single epoch. However, the actual number may be less than this, since some correct nodes may propose overlapping transaction sets, others may respond too late, and corrupted nodes may propose an empty set. Fortunately, we prove (in the Appendix) that assuming each correct node's queue is full, then  <span class="math">B / 4</span>  serves as an lower bound for the expected number of transactions committed in an epoch.</p>

    <p class="text-gray-300">THEOREM 3. (Efficiency). Assuming each correct node's queue contains at least  <span class="math">B</span>  distinct transactions, then the expected number of transactions committed in an epoch is at least  <span class="math">\\frac{B}{4}</span> , resulting in constant efficiency.</p>

    <p class="text-gray-300">Finally, we prove (in the Appendix) that the adversary cannot significantly delay the commit of any transaction.</p>

    <p class="text-gray-300">THEOREM 4. (Censorship Resilience). Suppose an adversary passes a transaction  <span class="math">\\mathbf{tx}</span>  as input to  <span class="math">N - f</span>  correct nodes. Let  <span class="math">T</span>  be the size of the "backlog", i.e. the difference between the total number of transactions previously input to any correct node and the number of transactions that have been committed. Then  <span class="math">\\mathbf{tx}</span>  is committed within  <span class="math">O(T / B + \\lambda)</span>  epochs except with negligible probability.</p>

    <p class="text-gray-300">In this section we carry out several experiments and performance measurements using a prototype implementation of the HoneyBadgerBFT protocol. Unless otherwise noted, numbers reported in this section are by default for the optimistic case where all nodes are behaving honestly.</p>

    <p class="text-gray-300">First we demonstrate that HoneyBadgerBFT is indeed scalable by performing an experiment in a wide area network, including up to 104 nodes in five continents. Even under these conditions, HoneyBadgerBFT can reach peak throughputs of thousands of transactions per second. Furthermore, by a comparison with PBFT, a representative partially synchronous protocol, HoneyBadgerBFT performs only a small constant factor worse. Finally, we demonstrate the feasibility of running asynchronous BFT over the Tor anonymous communication layer.</p>

    <p class="text-gray-300">Implementation details. We developed a prototype implementation of HoneyBadgerBFT in Python, using the gevent library for concurrent tasks.</p>

    <p class="text-gray-300">For deterministic erasure coding, we use the zfec library [52], which implements Reed-Solomon codes. For instantiating the common coin primitive, we implement Boldyreva's pairing-based threshold signature scheme [11]. For threshold encryption of transactions, we use Baek and Zheng's scheme [7] to encrypt a 256-bit ephemeral key, followed by AES-256 in CBC mode over the actual payload. We implement these threshold cryptography schemes using the Charm [3] Python wrappers for PBC library [38]. For threshold signatures, we use the provided MNT224 curve, resulting in signatures (and signature shares) of only 65 bytes, and heuristically providing 112 bits of security. Our threshold encryption scheme requires a</p>

    <p class="text-gray-300">symmetric bilinear group: we therefore use the SS512 group, which heuristically provides 80 bits of security [44].</p>

    <p class="text-gray-300">In our EC2 experiments, we use ordinary (unauthenticated) TCP sockets. In a real deployment we would use TLS with both client and server authentication, adding insignificant overhead for long-lived sessions. Similarly, in our Tor experiment, only one endpoint of each socket is authenticated (via the "hidden service" address).</p>

    <p class="text-gray-300">Our theoretical model assumes nodes have unbounded buffers. In practice, more resources could be added dynamically to a node whenever memory consumption reaches a watermark, (e.g., whenever it is  <span class="math">75\\%</span>  full) though our prototype implementation does not yet include this feature. Failure to provision an adequate buffer would count against the failure budget  <span class="math">f</span> .</p>

    <p class="text-gray-300">We first analyze the bandwidth costs of our system. In all experiments, we assume a constant transaction size of  <span class="math">m_{\\mathrm{T}} = 250</span>  bytes each, which would admit an ECDSA signature, two public keys, as well as an application payload (i.e., approximately the size of a typical Bitcoin transaction). Our experiments use the parameter  <span class="math">N = 4f</span> , and each party proposes a batch of  <span class="math">B / N</span>  transactions. To model the worst case scenario, nodes begin with identical queues of size  <span class="math">B</span> . We record the running time as the time from the beginning of the experiment to when the  <span class="math">(N - f)</span> -th node outputs a value.</p>

    <p class="text-gray-300">Bandwidth and breakdown findings. The overall bandwidth consumed by each node consists of a fixed additive overhead as well as a transaction dependent overhead. For all parameter values we considered, the additive overhead is dominated by an  <span class="math">O(\\lambda N^2)</span>  term resulting from the threshold cryptography in the ABA phases and the decryption phase that follows. The ABA phase involves each node transmitting  <span class="math">4N^2</span>  signature shares in expectation. Only the RBC phase incurs a transaction-dependent overhead, equal to the erasure coding expansion factor  <span class="math">r = \\frac{N}{N - 2f}</span> . The RBC phase also contributes  <span class="math">N^2\\log N</span>  hashes to the overhead because of Merkle tree branches included in the ECHO messages. The total communication cost (per node) is estimated as:</p>

    <div class="my-4 text-center"><span class="math-block">m _ {\\text {a l l}} = r \\left(B m _ {\\mathrm {T}} + N m _ {\\mathrm {E}}\\right) + N ^ {2} \\left(\\left(1 + \\log N\\right) m _ {\\mathrm {H}} + m _ {\\mathrm {D}} + 4 m _ {\\mathrm {S}}\\right)</span></div>

    <p class="text-gray-300">where  <span class="math">m_{\\mathrm{E}}</span>  and  <span class="math">m_{\\mathrm{D}}</span>  are respectively the size of a ciphertext and decryption share in the TPKE scheme, and  <span class="math">m_{\\mathrm{S}}</span>  is the size of a TSIG signature share.</p>

    <p class="text-gray-300">The system's effective throughput increases as we increase the proposed batch size  <span class="math">B</span> , such that the transaction-dependent portion of the cost dominates. As Figure 5 shows, for  <span class="math">N = 128</span> , for batch sizes up to 1024 transactions, the transaction-independent bandwidth still dominates to overall cost. However, when when the batch size reaches 16384, the transaction-dependent portion begins to dominate — largely resulting from the RBC.ECHO stage where nodes transmit erasure-coded blocks.</p>

    <p class="text-gray-300">To see how practical our design is, we deployed our protocol on Amazon EC2 services and comprehensively tested its performance. We ran HoneyBagderBFT on 32, 40, 48, 56, 64, and 104 Amazon EC2 t2.medium instances uniformly distributed throughout its 8</p>

    <p class="text-gray-300">!<a href="img-3.jpeg">img-3.jpeg</a> Figure 5: Estimated communication cost in megabytes (per node) for varying batch sizes. For small batch sizes, the fixed cost grows with  <span class="math">O(N^2 \\log N)</span> . At saturation, the overhead factor approaches  <span class="math">\\frac{N}{N - 2f} &amp;lt; 3</span> .</p>

    <p class="text-gray-300">!<a href="img-4.jpeg">img-4.jpeg</a> Figure 6: Throughput (transactions committed per second) vs number of transactions proposed. Error bars indicate  <span class="math">95\\%</span>  confidence intervals.</p>

    <p class="text-gray-300">regions spanning 5 continents. In our experiments, we varied the batch size such that each node proposed 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, or 131072 transactions.</p>

    <p class="text-gray-300">Throughput. Throughput is defined as the number of transactions committed per unit of time. In our experiment, we use "confirmed transactions per second" as our measure unit if not specified otherwise. Figure 6 shows the relationship between throughput and total number of transactions proposed by all  <span class="math">N</span>  parties. The fault tolerance parameter is set to be  <span class="math">f = N / 4</span> .</p>

    <p class="text-gray-300">Findings. From Figure 6 we can see for each setting, the throughput increases as the number of proposed transactions increases. We achieve throughput exceeding 20,000 transactions per second for medium size networks of up to 40 nodes. For a large 104 node network, we attain more than 1,500 transactions per second. Given an infinite batch size, all network sizes would eventually converge to a common upper bound, limited only by available bandwidth. Although the total bandwidth consumed in the network increases (linearly) with each additional node, the additional nodes also contribute additional bandwidth capacity.</p>

    <p class="text-gray-300">Throughput, latency, and scale tradeoffs. Latency is defined as the time interval between the time the first node receives a client request and when the  <span class="math">(N - f)</span> -th node finishes the consensus pro</p>

    <p class="text-gray-300">!<a href="img-5.jpeg">img-5.jpeg</a> Figure 7: Latency vs. throughput for experiments over wide area networks. Error bars indicate  <span class="math">95\\%</span>  confidence intervals.</p>

    <p class="text-gray-300">!<a href="img-6.jpeg">img-6.jpeg</a> Figure 8: Comparison with PBFT on EC2s</p>

    <p class="text-gray-300">tocol. This is reasonable because the  <span class="math">(N - f)</span> -th node finishing the protocol implies the accomplishment of the consensus for the honest parties.</p>

    <p class="text-gray-300">Figure 7 shows the relationship between latency and throughput for different choices of  <span class="math">N</span>  and  <span class="math">f = N / 4</span> . The positive slopes indicate that our experiments have not yet fully saturated the available bandwidth, and we would attain better throughput even with larger batch sizes. Figure 7 also shows that latency increases as the number of nodes increases, largely stemming from the ABA phase of the protocol. In fact, at  <span class="math">N = 104</span> , for the range of batch sizes we tried, our system is CPU bound rather than bandwidth bound because our implementation is single threaded and must verify  <span class="math">O(N^2)</span>  threshold signatures. Regardless, our largest experiment with 104 nodes completes in under 6 minutes.</p>

    <p class="text-gray-300">Although more nodes (with equal bandwidth provisioning) could be added to the network without affecting maximum attainable throughput, the minimal bandwidth consumed to commit one batch (and therefore the latency) increases with  <span class="math">O(N^2 \\log N)</span> . This constraint implies a limit on scalability, depending on the cost of bandwidth and users' latency tolerance.</p>

    <p class="text-gray-300">Comparison with PBFT. Figure 8 shows a comparison with the PBFT protocol, a classic BFT protocol for partially synchronous networks. We use the Python implementation from Croman et al. [24], running on 8, 16, 32, and 64 nodes evenly distributed among Amazon AWS regions. Batch sizes were chosen to saturate the network's available bandwidth.</p>

    <p class="text-gray-300">Fundamentally, while PBFT and our protocol have the same</p>

    <p class="text-gray-300">asymptotic communication complexity in total, our protocol distributes this load evenly among the network links, whereas PBFT bottlenecks on the leader's available bandwidth. Thus PBFT's attainable throughput diminishes with the number of nodes, while HoneyBadgerBFT's remains roughly constant.</p>

    <p class="text-gray-300">Note that this experiment reflects only the optimistic case, with no faults or network interruptions. Even for small networks, HoneyBadgerBFT provides significantly better robustness under adversarial conditions as noted in Section 3. In particular, PBFT would achieve zero throughput against an adversarial asynchronous scheduler, whereas HoneyBadgerBFT would complete epochs at a regular rate.</p>

    <p class="text-gray-300">To demonstrate the robustness of HoneyBadgerBFT, we run the first instance (to our knowledge) of a fault tolerant consensus protocol carried out over Tor (the most successful anonymous communication network). Tor adds significant and varying latency compared to our original AWS deployment. Regardless, we show that we can run HoneyBadgerBFT without tuning any parameters. Hiding HoneyBadgerBFT nodes behind the shroud of Tor may offer even better robustness. Since it helps the nodes to conceal their IP addresses, it can help them avoid targeted network attacks and attacks involving their physical location.</p>

    <p class="text-gray-300">Brief background on Tor. The Tor network consists of approximately 6,500 relays, which are listed in a public directory service. Tor enables "hidden services," which are servers that accept connections via Tor in order to conceal their location. When a client establishes a connection to a hidden service, both the client and the server construct 3-hop circuits to a common "rendezvous point." Thus each connection to a hidden service routes data through 5 randomly chosen relays. Tor provides a means for relay nodes to advertise their capacity and utilization, and these self-reported metrics are aggregated by the Tor project. According to these metrics, the total capacity of the network is  <span class="math">\\sim 145\\mathrm{Gbps}</span> , and the current utilization is  <span class="math">\\sim 65\\mathrm{Gbps}</span> .</p>

    <p class="text-gray-300">Tor experiment setup. We design our experiment setup such that we could run all  <span class="math">N</span>  HoneyBadgerBFT nodes on a single desktop machine running the Tor daemon software, while being able to realistically reflect Tor relay paths. To do this, we configured our machine to listen on  <span class="math">N</span>  hidden services (one hidden service for each HoneyBadgerBFT node in our simulated network). Since each HoneyBadgerBFT node forms a connection to each other node, we construct a total of  <span class="math">N^2</span>  Tor circuits per experiment, beginning and ending with our machine, and passing through 5 random relays. In summary, all pairwise overlay links traverse real Tor circuits consisting of random relay nodes, designed so that the performance obtained is representative of a real HoneyBadgerBFT deployment over Tor (despite all simulated nodes running on a single host machine).</p>

    <p class="text-gray-300">Since Tor provides a critical public service for many users, it is important to ensure that research experiments conducted on the live network do not adversely impact it. We formed connections from only a single vantage point (and thus avoid receiving), and ran experiments of short duration (several minutes) and with small parameters (only 256 circuits formed in our largest experiment). In total, our experiments involved the transfer of approximately five gigabytes of data through Tor – less than a 1E-5 fraction of its daily utilization.</p>

    <p class="text-gray-300">Figure 9 shows how latency changes with throughput. In contrast to our EC2 experiment where nodes have ample bandwidth, Tor</p>

    <p class="text-gray-300">circuits are limited by the slowest link in the circuit. We attain a maximum throughput of over 800 transactions per second of Tor.</p>

    <p class="text-gray-300">In general, messages transmitted over Tor’s relay network tends to have significant and highly variable latency. For instance, during our experiment on 8 parties proposing 16384 transactions per party, a single message can be delayed for 316.18 seconds and the delay variance is over 2208 while the average delay is only 12 seconds. We stress that our protocol did not need to be tuned for such network conditions, as would a traditional eventually-synchronous protocol.</p>

    <p class="text-gray-300">!<a href="img-7.jpeg">img-7.jpeg</a> Figure 9: Latency vs throughput for experiments running HoneyBadgerBFT over Tor.</p>

    <h2 id="sec-25" class="text-2xl font-bold">6 Conclusion</h2>

    <p class="text-gray-300">We have presented HoneyBadgerBFT, the first efficient and high-throughput asynchronous BFT protocol. Through our implementation and experimental results we demonstrate that HoneyBadgerBFT can be a suitable component in incipient cryptocurrency-inspired deployments of fault tolerant transaction processing systems. More generally, we believe our work demonstrates the promise of building dependable and transaction processing systems based on asynchronous protocol.</p>

    <p class="text-gray-300">We thank Jay Lorch, Jonathan Katz, and Emin Gün Sirer for helpful suggestions, and especially Dominic Williams for several excellent discussions that inspired us to tackle this problem. This work is supported in part by NSF grants CNS-1314857, CNS-1453634, CNS-1518765, CNS-1514261, and CNS-1518899, DARPA grant N66001-15-C-4066, a Packard Fellowship, a Sloan Fellowship, two Google Faculty Research Awards, and a VMWare Research Award. This work was done in part while a subset of the authors were visiting students at UC Berkeley, and while a subset of the authors were visiting the Simons Institute for the Theory of Computing, supported by the Simons Foundation and by the DIMACS/Simons Collaboration in Cryptography through NSF grant CNS-1523467.</p>

    <h2 id="sec-26" class="text-2xl font-bold">References</h2>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[1] How a Visa transaction works. http://apps.usa.visa.com/merchants/become-a-merchant/how-a-visa-transaction-works.jsp, 2015.</li>

      <li>[2] M. Abd-El-Malek, G. R. Ganger, G. R. Goodson, M. K. Reiter, and J. J. Wylie. Fault-scalable byzantine fault-tolerant services. ACM SIGOPS Operating Systems Review, 39(5):59–74, 2005.</li>

      <li>[3] J. A. Akinyele, C. Garman, I. Miers, M. W. Pagano, M. Rushanan, M. Green, and A. D. Rubin. Charm: a framework for rapidly prototyping cryptosystems. Journal of Cryptographic Engineering, 3(2):111–128, 2013.</li>

      <li>[4] Y. Amir, B. Coan, J. Kirsch, and J. Lane. Prime: Byzantine replication under attack. Dependable and Secure Computing, IEEE Transactions on, 8(4):564–577, 2011.</li>

      <li>[5] Y. Amir, C. Danilov, D. Dolev, J. Kirsch, J. Lane, C. Nita-Rotaru, J. Olsen, and D. Zage. Steward: Scaling byzantine fault-tolerant replication to wide area networks. Dependable and Secure Computing, IEEE Transactions on, 7(1):80–93, 2010.</li>

      <li>[6] P.-L. Aublin, S. Ben Mokhtar, and V. Quéma. Rbft: Redundant byzantine fault tolerance. In Distributed Computing Systems (ICDCS), 2013 IEEE 33rd International Conference on, pages 297–306. IEEE, 2013.</li>

      <li>[7] J. Baek and Y. Zheng. Simple and efficient threshold cryptosystem from the gap diffie-hellman group. In Global Telecommunications Conference, 2003. GLOBECOM’03. IEEE, volume 3, pages 1491–1495. IEEE, 2003.</li>

      <li>[8] M. Ben-Or and R. El-Yaniv. Resilient-optimal interactive consistency in constant time. Distributed Computing, 16(4):249–262, 2003.</li>

      <li>[9] M. Ben-Or, B. Kelmer, and T. Rabin. Asynchronous secure computations with optimal resilience. In Proceedings of the thirteenth annual ACM symposium on Principles of distributed computing, pages 183–192. ACM, 1994.</li>

      <li>[10] A. Bessani, J. Sousa, and E. E. Alchieri. State machine replication for the masses with bft-smart. In Dependable Systems and Networks (DSN), 2014 44th Annual IEEE/IFIP International Conference on, pages 355–362. IEEE, 2014.</li>

      <li>[11] A. Boldyreva. Threshold signatures, multisignatures and blind signatures based on the gap-diffie-hellman-group signature scheme. In Public key cryptographyâÃŻPKC 2003, pages 31–46. Springer, 2002.</li>

      <li>[12] J. Bonneau, A. Miller, J. Clark, A. Narayanan, J. Kroll, and E. W. Felten. Research perspectives on bitcoin and second-generation digital currencies. In 2015 IEEE Symposium on Security and Privacy. IEEE, 2015.</li>

      <li>[13] G. Bracha. Asynchronous byzantine agreement protocols. Information and Computation, 75(2):130–143, 1987.</li>

      <li>[14] M. Burrows. The Chubby lock service for loosely-coupled distributed systems. In Proceedings of the 7th symposium on Operating systems design and implementation, pages 335–350. USENIX Association, 2006.</li>

      <li>[15] C. Cachin, K. Kursawe, F. Petzold, and V. Shoup. Secure and efficient asynchronous broadcast protocols. In Advances in Cryptology – Crypto 2001, pages 524–541. Springer, 2001.</li>

      <li>[16] C. Cachin, K. Kursawe, and V. Shoup. Random oracles in constantipole: Practical asynchronous byzantine agreement using cryptography. In Proceedings of the Nineteenth Annual ACM Symposium on Principles of Distributed Computing, pages 123–132. ACM, 2000.</li>

      <li>[17] C. Cachin, J. Poritz, et al. Secure intrusion-tolerant replication on the internet. In Dependable Systems and Networks, 2002. DSN 2002. Proceedings. International Conference on, pages 167–176. IEEE, 2002.</li>

      <li>[18] C. Cachin and S. Tessaro. Asynchronous verifiable information dispersal. In Reliable Distributed Systems, 2005. SRDS 2005. 24th IEEE Symposium on, pages 191–201. IEEE, 2005.</li>

      <li>[19] R. Canetti and T. Rabin. Fast asynchronous byzantine agreement with optimal resilience. In Proceedings of the twenty-fifth annual ACM symposium on Theory of computing, pages 42–51. ACM, 1993.</li>

    </ul>

    <p class="text-gray-300">[20] M. Castro, B. Liskov, et al. Practical byzantine fault tolerance. In OSDI, volume 99, pages 173–186, 1999.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[21] A. Clement, M. Kapritsos, S. Lee, Y. Wang, L. Alvisi, M. Dahlin, and T. Riche. Upright cluster services. In Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles, pages 277–290. ACM, 2009.</li>

      <li>[22] A. Clement, E. L. Wong, L. Alvisi, M. Dahlin, and M. Marchetti. Making byzantine fault tolerant systems tolerate byzantine faults. In NSDI, volume 9, pages 153–168, 2009.</li>

      <li>[23] F. Cristian, H. Aghili, R. Strong, and D. Dolev. Atomic broadcast: From simple message diffusion to Byzantine agreement. Citeseer, 1986.</li>

      <li>[24] K. Croman, C. Decker, I. Eyal, A. E. Gencer, A. Juels, A. Kosba, A. Miller, P. Saxena, E. Shi, E. G. Sirer, D. Song, and R. W. and. On scaling decentralized blockchains — a position paper. 3rd Bitcoin Research Workshop, 2015.</li>

      <li>[25] G. Danezis and S. Meiklejohn. Centrally banked cryptocurrencies. arXiv preprint arXiv:1505.06895, 2015.</li>

      <li>[26] C. Dwork, N. Lynch, and L. Stockmeyer. Consensus in the presence of partial synchrony. Journal of the ACM (JACM), 35(2):288–323, 1988.</li>

      <li>[27] M. J. Fischer, N. A. Lynch, and M. S. Paterson. Impossibility of distributed consensus with one faulty process. Journal of the ACM (JACM), 32(2):374–382, 1985.</li>

      <li>[28] A. Guillevic. Kim-barbulescu variant of the number field sieve to compute discrete logarithms in finite fields. https://ellipticnews.wordpress.com/2016/05/02/kim-barbulescu-variant-of-the-number-field-sieve-to-compute-discrete-logarithms-in-finite-fields/, May 2016.</li>

      <li>[29] T. Kim and R. Barbulescu. Extended tower number field sieve: A new complexity for medium prime case. Technical report, IACR Cryptology ePrint Archive, 2015: 1027, 2015.</li>

      <li>[30] V. King and J. Saia. From almost everywhere to everywhere: Byzantine agreement with <span class="math">O(n^{3/2})</span> bits. In Distributed Computing, pages 464–478. Springer, 2009.</li>

      <li>[31] V. King and J. Saia. Breaking the <span class="math">O(n^{2})</span> bit barrier: scalable byzantine agreement with an adaptive adversary. Journal of the ACM (JACM), 58(4):18, 2011.</li>

      <li>[32] E. Kokoris-Kogias, P. Jovanovic, N. Gailly, I. Khoffi, L. Gasser, and B. Ford. Enhancing bitcoin security and performance with strong consistency via collective signing. arXiv preprint arXiv:1602.06997, 2016.</li>

      <li>[33] R. Kotla, L. Alvisi, M. Dahlin, A. Clement, and E. Wong. Zyzyva: speculative byzantine fault tolerance. In ACM SIGOPS Operating Systems Review, volume 41, pages 45–58. ACM, 2007.</li>

      <li>[34] K. Kursawe and V. Shoup. Optimistic asynchronous atomic broadcast. In in the Proceedings of International Colloqium on Automata, Languages and Programming (ICALP05)(L Caires, GF Italiano, L. Monteiro, Eds.) LNCS 3580. Citeseer, 2001.</li>

      <li>[35] J. Kwon. TenderMint: Consensus without Mining, August 2014.</li>

      <li>[36] L. Lamport. The part-time parliament. ACM Transactions on Computer Systems (TOCS), 16(2):133–169, 1998.</li>

      <li>[37] L. Luu, V. Narayanan, K. Baweja, C. Zheng, S. Gilbert, and P. Saxena. Scp: A computationally-scalable byzantine consensus protocol for blockchains. Cryptology ePrint Archive, Report 2015/1168, 2015. http://eprint.iacr.org/.</li>

      <li>[38] B. Lynn. On the implementation of pairing-based cryptography. The Department of Computer Science and the Committee on Graduate Studies of Stanford University, 2007.</li>

      <li>[39] Y. Mao, F. P. Junqueira, and K. Marzullo. Mencius: building efficient replicated state machines for wans. In OSDI, volume 8, pages 369–384, 2008.</li>

      <li>[40] R. McMillan. Ibm bets big on bitcoin ledger. Wall Street Journal.</li>

      <li>[41] R. McMillan. How bitcoin became the honey badger of money. Wired Magazine, http://www.wired.com/2013/12/bitcoin_honey/, 2013.</li>

      <li>[42] A. Mostefaoui, H. Moumen, and M. Raynal. Signature-free asynchronous byzantine consensus with t< n/3 and o (n 2) messages. In Proceedings of the 2014 ACM symposium on Principles of distributed computing, pages 2–9. ACM, 2014.</li>

      <li>[43] S. Nakamoto. Bitcoin: A peer-to-peer electronic cash system. http://bitcon.org/bitcoin.pdf, 2008.</li>

      <li>[44] NIST. Sp 800-37. Guide for the Security Certification and Accreditation of Federal Information Systems, 2004.</li>

      <li>[45] D. Ongaro and J. Ousterhout. In search of an understandable consensus algorithm. In Proc. USENIX Annual Technical Conference, pages 305–320, 2014.</li>

      <li>[46] H. V. Ramasamy and C. Cachin. Parsimonious asynchronous byzantine-fault-tolerant atomic broadcast. In OPODIS, pages 88–102. Springer, 2005.</li>

      <li>[47] D. Schwartz, N. Youngs, and A. Britto. The Ripple Protocol Consensus Algorithm, September 2014.</li>

      <li>[48] V. Shoup. Practical threshold signatures. In EUROCRYPT, pages 207–220. Springer, 2000.</li>

      <li>[49] A. Singh, T. Das, P. Maniatis, P. Druschel, and T. Roscoe. Bft protocols under fire. In Proceedings of the 5th USENIX Symposium on Networked Systems Design and Implementation, NSDI’08, pages 189–204, Berkeley, CA, USA, 2008. USENIX Association.</li>

      <li>[50] G. S. Veronese, M. Correia, A. N. Bessani, and L. C. Lung. Spin one’s wheels? byzantine fault tolerance with a spinning primary. In Reliable Distributed Systems, 2009. SRDS’09. 28th IEEE International Symposium on, pages 135–144. IEEE, 2009.</li>

      <li>[51] G. S. Veronese, M. Correia, A. N. Bessani, and L. C. Lung. Ebawa: Efficient byzantine agreement for wide-area networks. In High-Assurance Systems Engineering (HASE), 2010 IEEE 12th International Symposium on, pages 10–19. IEEE, 2010.</li>

      <li>[52] Z. Wilcox-O’Hearn. Zfec 1.4. 0. Open source code distribution: http://pypi.python.org/pypi/zfec, 2008.</li>

    </ul>

    <h2 id="sec-27" class="text-2xl font-bold">Appendix A Attacking PBFT</h2>

    <p class="text-gray-300">PBFT. The PBFT protocol consists of two main workflows: a “fast path” that provides good performance in optimistic case (when the network is synchronous and the leader functions correctly), and a “view-change” procedure to change leaders.</p>

    <p class="text-gray-300">The fast path consists of three rounds of communication: PRE_PREPARE, PREPARE, and COMMIT. The leader of a given view is responsible for totally ordering all requests. Upon receiving a client request, the leader multicasts a PRE_PREPARE message specifying the request and a sequence number to all other replicas, who respond by multicasting a corresponding PREPARE message. Replicas multicast a COMMIT message on receipt of <span class="math">2f</span> PREPARE messages (in addition to the corresponding PRE_PREPARE message), and execute the request on receipt of <span class="math">2f+1</span> COMMIT messages (including their own).</p>

    <p class="text-gray-300">Replicas increment their view number and multicast a VIEW_CHANGE message to elect a new leader when a request takes too long to execute (i.e., longer than a timeout interval), a previously initiated view change has taken too long, or it receives  <span class="math">f + 1</span>  VIEW_CHANGE messages with a higher view number. The leader of the next view is determined by the view number modulo the number of replicas (thus, leadership is transferred in a round-robin manner). The new leader multicasts a NEW VIEW message once it receives  <span class="math">2f + 1</span>  VIEW_CHANGE messages and includes them as proof of a valid view. A replica accepts the NEW VIEW message if its number is equal to or greater than its own current view number, and resumes processing messages as normal; however messages with lower view numbers are ignored. The timeout interval is initialized to a fixed value  <span class="math">(\\Delta)</span> , but increases by a factor of 2 with each consecutive unsuccessful leader election.</p>

    <p class="text-gray-300">An intermittently synchronous network that thwarts PBFT. The scheduler does not drop or reorder any messages, but simply delays delivering messages to whichever node is the current leader. In particular, whenever the current leader is a faulty node, this means that messages among all honest nodes are delivered immediately. Shortly we provide a detailed illustration of the PBFT protocol behaves under our attack.</p>

    <p class="text-gray-300">To confirm our analysis, we implemented this malicious scheduler as a proxy that intercepted and delayed all view change messages to the new leader, and tested it against a 1200 line Python implementation of PBFT. The results and message logs we observed were consistent with the above analysis; our replicas became stuck in a loop requesting view changes that never succeeded.</p>

    <p class="text-gray-300">Since this scheduler is intermittently synchronous, any purely asynchronous protocol (including HoneyBadgerBFT) would make good progress during periods of synchrony, regardless of preceding intervals.</p>

    <p class="text-gray-300">How PBFT behaves under attack. In Figure 10, we illustrate our attack on PBFT. The scheduler does not drop or reorder any messages, but simply delays delivering messages to whichever node is the current leader. In particular, whenever the current leader is a faulty node, this means that messages among all honest nodes are delivered immediately.</p>

    <p class="text-gray-300">We abbreviate client requests as "Req," NEW VIEW messages as "N," VIEW_CHANGE messages as "V," and PRE_PREPARE messages as "PP." The subscript on a message indicates the view in which it was sent. Here,  <span class="math">\\bigcirc</span>  followed by a message indicates that this message has been broadcast to all other nodes (called replicas) by the replica specified by the column number, at the time specified by the row number multiplied by the fixed timeout interval  <span class="math">\\Delta</span> . Similarly,  <span class="math">\\bullet</span>  followed by a message indicates that this message has been delivered to the replica specified by the column number, at the time specified by the row. As multiple VIEW_CHANGE messages for a given view are sent to each individual node,  <span class="math">\\bullet V_{n}</span>  indicates the delivery of all VIEW_CHANGE messages with view number  <span class="math">n</span> . A red "X" appended to a delivered message indicates that the message is ignored because the view number does not match that replica's current view. A " <span class="math"><em></span> " indicates that a timer has been started as a result of the delivered message. " <span class="math"></em></span> " indicates that a replica's view number has incremented as a result of the delivered message(s). A red region indicates that all broadcast operations from this replica at this time will be delayed by  <span class="math">\\Delta</span> . A pink region indicates that the receipt of all messages will be delayed by  <span class="math">\\Delta</span> .</p>

    <p class="text-gray-300">In this example, the faulty replica 0 is initially the leader and withholds a PRE_PREPARE message for longer than the timeout period  <span class="math">\\Delta</span> . This triggers all nodes to increment their view counter and multicast a VIEW_CHANGE message for view number 1. The scheduler then</p>

    <p class="text-gray-300">Replicas</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">0 (faulty)</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">1</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">2</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">3</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">0Δ</td>

            <td class="px-3 py-2 border-b border-gray-700">●Req*OPP0view:0</td>

            <td class="px-3 py-2 border-b border-gray-700">●Req*view:0</td>

            <td class="px-3 py-2 border-b border-gray-700">●Req*view:0</td>

            <td class="px-3 py-2 border-b border-gray-700">●Req*view:0</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">1Δ</td>

            <td class="px-3 py-2 border-b border-gray-700">○V1●V1*view:1</td>

            <td class="px-3 py-2 border-b border-gray-700">○V1●PP0Xview:1</td>

            <td class="px-3 py-2 border-b border-gray-700">○V1●PP0X●V1*view:1</td>

            <td class="px-3 py-2 border-b border-gray-700">○V1●PP0X●V1*view:1</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">3Δ</td>

            <td class="px-3 py-2 border-b border-gray-700">○V2●N1,PP1X●V2*view:2</td>

            <td class="px-3 py-2 border-b border-gray-700">●V1○N1,PP1*●V2**○V2view:1/2</td>

            <td class="px-3 py-2 border-b border-gray-700">○V2●N1,PP1Xview:2</td>

            <td class="px-3 py-2 border-b border-gray-700">○V2●N1,PP1X●V2*view:2</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">7Δ</td>

            <td class="px-3 py-2 border-b border-gray-700">○V3●N2,PP2X●V3*view:3</td>

            <td class="px-3 py-2 border-b border-gray-700">○V3●N2,PP2X●V3*view:3</td>

            <td class="px-3 py-2 border-b border-gray-700">●V2○N2,PP2*●V3**○V3view:2/3</td>

            <td class="px-3 py-2 border-b border-gray-700">○V3●N2,PP2Xview:3</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Figure 10: An intermittently synchronous scheduler that violates PBFT's assumptions and indeed prevents it from making progress. Only the first four phases are shown - the behavior continues to repeat indefinitely. In the pink regions, messages to the leader are delayed (for longer than the timeout  <span class="math">\\Delta</span> , thus violating the eventual-synchrony assumption). However, all other messages are delivered at the ordinary rate between honest parties, hence "intermittently synchronous."</p>

    <p class="text-gray-300">delays the delivery of all VIEW_CHANGE messages to replica 1 (the leader in view 1). The view change operation for the remaining nodes times out, as they do not receive a valid NEW VIEW message from replica 1. Nodes 0,2, and 3 then increment their view counters to 2, and multicast another VIEW_CHANGE message. At this point, the VIEW_CHANGE messages for view 1 are delivered to replica 1, which responds by multicasting a NEW VIEW and a PRE_PREPARE message in view 1. These messages are then delivered and subsequently ignored by all other nodes, as they have progressed to view number 2. Replica 1 will then receive the VIEW_CHANGE messages for view 2, and increments its view counter accordingly. The scheduler then delays the delivery of all VIEW_CHANGE messages to replica 2, ensuring that the view change operation of all other nodes times out again. This process will continue until the faulty replica 0 is again elected leader, at which point the scheduler will deliver all messages at an accelerated rate while replica 0 withholds the corresponding NEW VIEW and PRE_PREPARE messages to trigger another view change and repeat this cycle. The cycle may continue indefinitely so long as the scheduler withholds VIEW_CHANGE messages from the intended non-faulty leader for longer than the</p>

    <p class="text-gray-300">(exponentially increasing) timeout interval, preventing any view changes from succeeding and stopping the protocol from making any progress, despite the fact that at time intervals where replica 0 is the leader (0<span class="math">\\Delta</span>,8<span class="math">\\Delta</span>,64<span class="math">\\Delta</span>…) all non-faulty replicas are able to communicate without any interference.</p>

    <p class="text-gray-300">Intermittently synchronous networks. To more clearly illustrate the difference between asynchronous networks, we introduce a new network performance assumption, <span class="math">\\Delta</span>-intermittently synchrony, which is strictly weaker than even weak synchrony. The idea is that a <span class="math">\\Delta</span>-intermittently synchronous network approximates a <span class="math">\\Delta</span>-synchronous network in the sense that on average it delivers messages at a rate of <span class="math">1/\\Delta</span>. However, the delivery rate may be unevenly distributed in time (e.g., “bursty”), delivering no messages at all during some time intervals and delivering messages rapidly during others.</p>

    <h6 id="sec-28" class="text-base font-medium mt-4">Definition 2</h6>

    <p class="text-gray-300">A network is <span class="math">\\Delta</span>-intermittently synchronous if for any initial time <span class="math">T_{0}</span>, and for any duration <span class="math">D</span>, there exists an interval <span class="math">[T_{0},T_{1}]</span> such that <span class="math">T_{1}-T_{0}\\geq D</span> and the number of asynchronous rounds advanced during <span class="math">[T_{0},T_{1}]</span> is at least <span class="math">(T_{1}-T_{0})/\\Delta</span>.</p>

    <p class="text-gray-300">It is clearly the case that every <span class="math">\\Delta</span>-synchronous network is also <span class="math">\\Delta</span>-intermittently synchronous, since for every interval of duration <span class="math">\\Delta</span>, messages sent prior to that interval are delivered by the end of that interval. It is also clear that any intermittently synchronous network guarantees eventual delivery (i.e., it is no weaker than the asynchronous model).</p>

    <p class="text-gray-300">Asynchronous protocols make progress whenever rounds of messages are delivered. Since an intermittently-synchronous network guarantees messages are delivered on average within <span class="math">\\Delta</span>, this means any asynchronous protocol also makes progress at an average rate of <span class="math">\\Delta</span>.</p>

    <h2 id="sec-29" class="text-2xl font-bold">Appendix B Deferred proofs</h2>

    <p class="text-gray-300">We now restate and prove the theorems originally stated in Section 4.5.</p>

    <h6 id="sec-30" class="text-base font-medium mt-4">Theorem 3</h6>

    <p class="text-gray-300">(Efficiency). Assuming each correct node’s queue contains at least <span class="math">B</span> distinct transactions, then the expected number of transactions committed in an epoch is at least <span class="math">\\frac{B}{4}</span>, resulting in constant efficiency.</p>

    <h6 id="sec-31" class="text-base font-medium mt-4">Proof 4.1.</h6>

    <p class="text-gray-300">First, we consider an experiment where the threshold-encrypted ciphertexts are replaced with encryptions of random plaintexts. In this case, the adversary does not learn any information about the proposed batch for each honest party. We will first show that in this experiment, the expected number of transactions committed in an epoch is at least <span class="math">\\frac{1}{4}B</span>.</p>

    <p class="text-gray-300">Experiment 1. Each correct node selects a random subset of <span class="math">B/N</span> distinct transactions from <span class="math">\\text{buf}\\![</span>: <span class="math">B]</span>, where <span class="math">\\text{buf}\\![</span>: <span class="math">B]</span> denotes the first <span class="math">B</span> elements in its queue. The adversary selects <span class="math">N-2f</span> correct nodes and let <span class="math">S</span> denote the union of their proposed transactions — recall that the ACS protocol guarantees that the agreed set contains at least transactions proposed by <span class="math">N-2f</span> correct nodes. Let <span class="math">\\mathbf{X}_{1}</span> denote the number of distinct transactions in <span class="math">S</span>.</p>

    <p class="text-gray-300">The contents of <span class="math">\\text{buf}\\![</span>: <span class="math">B]</span> can be adversarially chosen, and clearly, the worst case is when <span class="math">\\text{buf}\\![</span>: <span class="math">B]</span> is identical for all honest parties; since otherwise <span class="math">E[\\mathbf{X}_{1}]</span> can only be greater.</p>

    <p class="text-gray-300">We now consider a slightly different experiment where instead of choosing <span class="math">B/N</span> distinct elements from <span class="math">\\text{buf}\\![</span>: <span class="math">B]</span>; each honest party chooses a set of <span class="math">B/N</span> elements from <span class="math">\\text{buf}\\![</span>: <span class="math">B]</span> with replacement. The expected number of distinct elements in the agreed set can only be smaller in this stochastic process. Also note that we can bound <span class="math">(N-2f)(B/N)&gt;B/3</span> since <span class="math">N&gt;3f</span>. Therefore, we will bound the number of distinct items in the agreed set in Experiment 1 with the following, much simpler experiment:</p>

    <p class="text-gray-300">Experiment 2. Throw <span class="math">\\frac{B}{3}</span> balls at <span class="math">B</span> bins. Let <span class="math">\\mathbf{X}_{2}</span> denote the number of bins with at least one ball. Clearly, <span class="math">E[\\mathbf{X}_{2}]\\leq E[\\mathbf{X}_{1}]</span>.</p>

    <p class="text-gray-300">We now bound <span class="math">E[\\mathbf{X}_{2}]</span>. Since for each bin, the probability of being empty is <span class="math">1-\\frac{1}{B}^{B/3}</span>, the expected number of bins with at least one ball is <span class="math">\\mathsf{E}[\\mathbf{X}_{2}]=B(1-(1-\\frac{1}{B})^{B/3})&gt;B(1-e^{-1/3})&gt;\\frac{1}{4}B</span>.</p>

    <p class="text-gray-300">We now claim that when the ciphertexts are instantiated with real encryptions rather than random ones, no polynomial-time adversary can cause the expected number of committed transactions in an epoch to be smaller than <span class="math">\\frac{B}{3}</span>. We can prove this by contradiction. If some polynomial-time adversary <span class="math">\\mathcal{A}</span> can cause the expectation to be <span class="math">\\frac{B}{4}</span> or smaller, then we can construct a distinguisher <span class="math">\\mathcal{D}</span> that can distinguish random vs. real ciphertexts by running <span class="math">\\mathcal{A}</span> for <span class="math">\\Omega(\\lambda)</span> many epochs. If the average number of transactions across these epochs is smaller than <span class="math">\\frac{1}{4}B</span>, <span class="math">\\mathcal{D}</span> guesses that the ciphertexts are real; otherwise it guess they are random. By a standard Hoeffding bound, <span class="math">\\mathcal{D}</span> succeeds with <span class="math">1-\\exp(-\\Omega(\\lambda))</span> probability. Note that we rely only on the semantic security (i.e., IND-CPA) of the underlying threshold encryption scheme (not on a stronger definition like IND-CCA2); this is because the adversary cannot decrypt any ciphertexts in an epoch until the ACS subprotocol completes. ∎</p>

    <h6 id="sec-32" class="text-base font-medium mt-4">Theorem 4</h6>

    <p class="text-gray-300">(Censorship Resilience). Suppose an adversary passes a transaction <span class="math">\\mathsf{tx}</span> as input to <span class="math">N-f</span> correct nodes. Let <span class="math">T</span> be the size of the “backlog”, i.e. the difference between the total number of transactions previously input to any correct node and the number of transactions that have been committed. Then <span class="math">\\mathsf{tx}</span> is commited within <span class="math">O(T/B+\\lambda)</span> epochs except with negligible probability.</p>

    <p class="text-gray-300">At the beginning of each epoch, each correct node can be in one of two states: either (Type 1) <span class="math">\\mathsf{tx}</span> appears in the front of its queue (i.e., the first <span class="math">B</span> elements), or else (Type 2) it queue has more than <span class="math">B</span> elements placed in front of <span class="math">\\mathsf{tx}</span>.</p>

    <p class="text-gray-300">The main idea is that in each epoch the adversary must include the proposals of either at least <span class="math">\\lceil N/6\\rceil</span> Type 1 nodes (a Type 1 epoch), or at least <span class="math">\\lceil N/6\\rceil</span> Type 2 nodes (a Type 2 epoch). In a Type 1 epoch, <span class="math">\\mathsf{tx}</span> is committed with probability at least <span class="math">1-e^{-1/6}</span>. Clearly after <span class="math">O(\\lambda)</span> such epochs, <span class="math">\\mathsf{tx}</span> will likely have been committed. However, in a Type 2 epoch, we expect to clear at least <span class="math">B(1-e^{-1/6})</span> transactions from the initial backlog.</p>

    <p class="text-gray-300">We will therefore show that after <span class="math">O(T/B+\\lambda)</span> Type 2 epochs, with high probability all <span class="math">T</span> transactions will have been committed.</p>

    <h6 id="sec-33" class="text-base font-medium mt-4">Lemma 1</h6>

    <p class="text-gray-300">After at most <span class="math">O(T/B+\\lambda)</span> Type 2 epochs, <span class="math">T</span> transactions from the backlog will have been committed with high probability.</p>

    <p class="text-gray-300">Let <span class="math">\\epsilon&gt;0</span> be a constant, which we will use as a safety margin for our tail bound analysis. Let <span class="math">\\mathbf{X}</span> denote total number of committed transactions after <span class="math">k</span> epochs as described. Using the expectation analysis from Theorem 3, the expected value of <span class="math">\\mathbf{X}</span> is <span class="math">\\mathbb{E}[\\mathbf{X}]\\geq\\frac{kB}{3}</span>.</p>

    <p class="text-gray-300">We choose the number of epochs to wait as <span class="math">k=\\max(\\lambda,\\frac{8T}{(1-\\epsilon)B})</span>, which ensures that <span class="math">k\\geq\\lambda</span> and that <span class="math">\\mathbb{E}[\\mathbf{X}]-T\\geq\\epsilon\\mathbb{E}[\\mathbf{X}]</span>.</p>

    <p class="text-gray-300">Although the adversary may correlate its behavior from one epoch to the next, the bound on <span class="math">E[\\mathbf{X}]</span> depends only on the random choices of the parties, which are independent. Therefore using Hoeffding’s inequality, we have</p>

    <p class="text-gray-300"><span class="math">\\Pr\\left[\\mathbf{X}&lt;T\\right]</span> <span class="math">\\leq\\Pr\\left[\\mathbb{E}[\\mathbf{X}]-\\mathbf{X}&gt;\\epsilon\\mathbb{E}[\\mathbf{X}]\\right]</span> <span class="math">\\leq\\exp(-\\Omega(\\mathbb{E}[\\mathbf{X}]^{2}/kB^{2}))\\leq\\exp(-\\Omega(k)),</span></p>

    <p class="text-gray-300">giving us the desired bound. ∎</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>upon receiving input  <span class="math">b_{\\mathrm{input}}</span> , set  <span class="math">\\mathsf{est}_0 \\coloneqq b_{\\mathrm{input}}</span>  and proceed as follows in consecutive epochs, with increasing labels  <span class="math">r</span> :</li>

    </ul>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>multicast  <span class="math">\\mathrm{BVAL}_r(\\mathrm{est}_r)</span></li>

      <li>bin_values  <span class="math">r \\coloneqq \\{\\}</span></li>

      <li>upon receiving  <span class="math">\\mathrm{BVAL}_r(b)</span>  messages from  <span class="math">f + 1</span>  nodes, if  <span class="math">\\mathrm{BVAL}_r(b)</span>  has not been sent, multicast  <span class="math">\\mathrm{BVAL}_r(b)</span></li>

      <li>upon receiving  <span class="math">\\mathrm{BVAL}_r(b)</span>  messages from  <span class="math">2f + 1</span>  nodes, bin_values  <span class="math">r := \\mathrm{bin\\_values}_r \\cup \\{b\\}</span></li>

      <li>wait until bin_values  <span class="math">r \\neq \\emptyset</span> , then</li>

    </ul>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>multicast  <span class="math">\\mathrm{AUX}_r(w)</span>  where  <span class="math">w \\in \\mathrm{bin\\_values}_r</span></li>

      <li>wait until at least  <span class="math">(N - f)</span>  AUX  <span class="math">r</span>  messages have been received, such that the set of values carried by these messages, vals are a subset of bin_values  <span class="math">r</span>  (note that bin_values  <span class="math">r</span>  may continue to change as BVAL  <span class="math">r</span>  messages are received, thus this condition may be triggered upon arrival of either an AUX  <span class="math">r</span>  or a BVAL  <span class="math">r</span>  message)</li>

    </ul>

    <p class="text-gray-300"><span class="math">s\\gets \\mathrm{Coin}_r.\\mathrm{GetCoin}() / / \\mathrm{See~Figure~12}</span></p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>if vals = {b}, then</li>

    </ul>

    <p class="text-gray-300"><span class="math">\\cdot</span>  est  <span class="math">r + 1\\coloneqq b</span> if  <span class="math">(b = s\\% 2)</span>  then output  <span class="math">b</span></p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>else  <span class="math">\\operatorname{est}_{r+1} := s\\% 2</span></li>

    </ul>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>continue looping until both a value  <span class="math">b</span>  is output in some round  <span class="math">r</span> , and the value  <span class="math">\\operatorname{Coin}_{r&#x27;} = b</span>  for some round  <span class="math">r&#x27; &amp;gt; r</span></li>

    </ul>

    <p class="text-gray-300">Figure 11: Binary Byzantine Agreement from a Common Coin. Note that in the algorithm,  <span class="math">b</span>  ranges over  <span class="math">\\{0,1\\}</span> . This protocol makes use of a sequence of common coins, labeled  <span class="math">\\mathrm{Coin}_r</span> .</p>

    <p class="text-gray-300">Realizing binary agreement from a common coin. Binary agreement allows nodes to agree on the value of a single bit. More formally, binary agreement guarantees three properties:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>(Agreement) If any correct node outputs the bit  <span class="math">b</span> , then every correct node outputs  <span class="math">b</span> .</li>

      <li>(Termination) If all correct nodes receive input, then every correct node outputs a bit.</li>

      <li>(Validity) If any correct node outputs  <span class="math">b</span> , then at least one correct node received  <span class="math">b</span>  as input.</li>

    </ul>

    <p class="text-gray-300">The validity property implies unanimity: if all of the correct nodes receive the same input value  <span class="math">b</span> , then  <span class="math">b</span>  must be the decided value. On the other hand, if at any point two nodes receive different inputs, then the adversary may force the decision to either value even before the remaining nodes receive input.</p>

    <p class="text-gray-300">We instantiate this primitive with a protocol based on cryptographic common coin, which essentially act as synchronizing gadgets. The adversary only learns the value of the next coin after a majority of correct nodes have committed to a vote — if the coin matches the majority vote, then that is the decided value. The adversary can influence the majority vote each round, but only until the coin is revealed.</p>

    <p class="text-gray-300">The Byzantine agreement algorithm from Moustefaoui et al. [42] is shown in Figure 11. Its expected running time is  <span class="math">O(1)</span> , and in fact completes within  <span class="math">O(k)</span>  rounds with probability  <span class="math">1 - 2^{-k}</span> . When instantiated with the common coin defined below, the total communication complexity is  <span class="math">O(\\lambda N^2)</span> , since it uses a constant number of common coins.</p>

    <p class="text-gray-300">Realizing a common coin from a threshold signature scheme. A common coin is a protocol that satisfies the following properties:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>If  <span class="math">f + 1</span>  parties call GetCoin(), then all parties eventually receive a common value,  <span class="math">s</span> .</li>

      <li>The value  <span class="math">s</span>  is uniformly sampled in the range  <span class="math">\\{0,1\\}^{\\lambda}</span> , and cannot be influenced by the adversary.</li>

      <li>Until at least one party calls GetCoin(), no information about  <span class="math">s</span>  is revealed to the adversary.</li>

    </ul>

    <p class="text-gray-300">Following Cachin et al. [16], a common coin can be realized from a unique threshold signature scheme. An  <span class="math">(N,f)</span> -threshold signature scheme involves distributing shares of a signing key  <span class="math">\\mathsf{sk}_i</span>  to each of N parties. Given a message, a party using secret key  <span class="math">\\mathsf{sk}_i</span>  can compute a signature share on an arbitrary message  <span class="math">m</span> . Given  <span class="math">f + 1</span>  such signature shares for message  <span class="math">m</span> , anyone can combine the shares to produce a valid signature, which verifies under the public key  <span class="math">\\mathsf{pk}</span> . With fewer than  <span class="math">f + 1</span>  shares, (i.e., unless at least one honest party deliberately computes and reveals a share), the adversary learns nothing. We rely on an additional uniqueness property, which guarantees that for a given public key  <span class="math">\\mathsf{pk}</span> , there exists exactly one valid signature on each message  <span class="math">m</span> .</p>

    <p class="text-gray-300">The idea of Cachin et al. [16] is simply to use the threshold signature as a source of random bits, by signing a string that serves as the "name" of the coin. This naturally allows the protocol to be used to generate a sequence (or random-access table) of coins, and makes it convenient to use in modular subprotocols.</p>

    <p class="text-gray-300">sid is assumed to be a unique nonce that serves as "name" of this common coin</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>(Trusted Setup Phase): A trusted dealer runs  <span class="math">\\mathsf{pk},\\{sk_i\\} \\gets</span>  ThresholdSetup to generate a common public key, as well as secret key shares  <span class="math">\\{\\mathsf{sk}_i\\}</span> , one for each party (secret key  <span class="math">\\mathsf{sk}_i</span>  is distributed to party  <span class="math">\\mathcal{P}_i</span> ). Note that a single setup can be used to support a family of Coins indexed by arbitrary sid strings.</li>

      <li>on input GetCoin, multicast ThresholdSignpk(sk_i, sid)</li>

      <li>upon receiving at least  <span class="math">f + 1</span>  shares, attempt to combine them into a signature:</li>

    </ul>

    <p class="text-gray-300">sig  <span class="math">\\leftarrow</span>  ThresholdCombinepk(\\{j,sj}）</p>

    <p class="text-gray-300">if ThresholdVerifypk(sid) then deliver sig</p>

    <p class="text-gray-300">Figure 12: A common coin based on threshold signatures [48]</p>

    <p class="text-gray-300">We assume that ThresholdCombine is robust, in the sense that if it is run with a set of more than  <span class="math">f + 1</span>  signature shares, it rejects any invalid ones. In particular, if  <span class="math">2f + 1</span>  shares are provided, certainly a valid subset of  <span class="math">f + 1</span>  is among them. In practice, any incorrect shares detected this way can be used as evidence to incriminate a node.</p>

    <p class="text-gray-300">Concretely, we use an efficient threshold scheme [11] based on bilinear groups and the Gap Diffie Hellman assumption. We use TSIG to refer to this scheme. The common coin requires only one asynchronous round to complete, and the communication cost is  <span class="math">O(N\\lambda)</span>  per node.</p>`;
---

<BaseLayout title="The Honey Badger of BFT Protocols (2016/199)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2016 &middot; eprint 2016/199
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <p class="mt-4 text-xs text-gray-500">
        All content below belongs to the original authors. This page
        reproduces the paper for educational purposes. Always
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >cite the original</a>.
      </p>
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <Fragment set:html={CONTENT} />

  </article>
</BaseLayout>
