---
import BaseLayout from '../../layouts/BaseLayout.astro';
import PaperDisclaimer from '../../components/PaperDisclaimer.astro';
import PaperHistory from '../../components/PaperHistory.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2016/1118';
const CRAWLER = 'marker';
const CONVERTED_DATE = '2026-02-21';
const TITLE_HTML = '5 Searching for Optimal Implementations of Invertible Linear Layers';
const AUTHORS_HTML = 'In this section, we talk about how to get a minimum-XOR-implementation of a given linear layer  $L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)$ . In accordance with the preceding section, this question is equivalent to finding a shortest path between  $[L]_{RP}$  and  $[I_n]_{RP}$ .';

const CONTENT = `    <h2 id="sec-1" class="text-2xl font-bold"><strong>Designing Optimal Implementations of Linear Layers (Full Version)</strong></h2>

    <p class="text-gray-300">Ruoxin Zhao&lt;sup&gt;1&lt;/sup&gt;<em>,</em>&lt;sup&gt;2&lt;/sup&gt; , Baofeng Wu&lt;sup&gt;1&lt;/sup&gt; , Rui Zhang&lt;sup&gt;1&lt;/sup&gt;<em>,</em>&lt;sup&gt;2&lt;/sup&gt; and Qian Zhang&lt;sup&gt;1&lt;/sup&gt;</p>

    <blockquote class="border-l-4 border-gray-600 pl-4 my-4 text-gray-400 italic">
    <p class="text-gray-300">&lt;sup&gt;1&lt;/sup&gt; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences <a href="mailto:zhaoruoxin@iie.ac.cn">zhaoruoxin@iie.ac.cn</a></p>

    </blockquote>

    <p class="text-gray-300"><strong>Abstract.</strong> Linear layer is a fundamental primitive for many aspects of information technology. For information security, the performance of a linear layer depends on two aspects: diffusion ability and implementation cost, where the latter is usually measured by the number of XORs required to implement it. For many years, linear layers have been implemented by computing co-ordinates of the output independently. With this method, costs are determined only by the matrices representing linear layers. However, we note that the implementation cost of a given linear layer depends not only on its matrix but also on its implementation methods. So, in this paper, we focus on another implementation method: modifying input vectors to output step by step. This method uses fewer XORs than previous methods do and makes the implementation cost of every linear layer same as that of its inverse. With the new implementation method, we first clarify the measurement of implementation cost and the optimal implementation procedure of invertible linear layers. Here, &quot;optimal&quot; means using fewest XORs. Then, to find the optimal implementation procedure of a given invertible linear layer, we construct a graph-theoretical model and transfer the problem to the shortest path problem in graph theory. Next, we construct a new &quot;double-direction&quot; algorithm that uses less storage and makes the search for a shortest path more efficient in a regular graph. However, this algorithm is not practical enough for heavyweight invertible linear layers because of its high space/time complexity. So, we present another algorithm for finding efficient implementations of invertible linear layers. The advantages of this algorithm are its low complexity and high practicality. By this algorithm, we design highly efficient implementations of the linear layers of AES. To handle more general linear layers, we finally construct a practical algorithm for finding efficient implementations of singular linear layers.</p>

    <p class="text-gray-300"><strong>Keywords:</strong> Linear Layer · XOR Count · Equivalence Relation · Regular Graph · The Shortest Path</p>

    <h2 id="sec-2" class="text-2xl font-bold"><strong>1 Introduction</strong></h2>

    <p class="text-gray-300">Linear layer is a fundamental primitive for cryptography and is widely used in many aspects of computer science, electronic engineering and telecommunication. For example, it is used as diffusion layer in cryptography. Most modern block ciphers and hash functions are composed by confusion layers and diffusion layers. From the viewpoint of mathematics, confusion layers are usually nonlinear functions (S-boxes) while diffusion layers are usually linear functions. The goal of confusion layers is to increase chaos of information. And the goal of diffusion layers is to spread the chaos caused by confusion layers as much as possible. The performance of a linear layer depends on two aspects: diffusion ability and</p>

    <p class="text-gray-300">&lt;sup&gt;2&lt;/sup&gt; University of Chinese Academy of Sciences</p>

    <p class="text-gray-300">implementation efficiency. In general, these two aspects often contradict. In other words, the higher its diffusion ability is, the lower its implementation efficiency is. So, finding proper tradeoffs is a challenge for designers.</p>

    <p class="text-gray-300">Efficient hardware implementation draws more and more attention in recent years. Implementation efficiency on hardware depends on several factors including running time, area, and power consumption. In most cases, these factors restrict mutually. So, &quot;optimal implementation&quot; has different meanings under different circumstances. For example, there are many lightweight equipments such as a variety of sensors in internet of things (IoT). Consequently, area of circuits naturally becomes the primary factor when designing them.</p>

    <p class="text-gray-300">Essentially, linear layers are  <span class="math">\\mathbb{F}_q</span> -linear transformations over  <span class="math">(\\mathbb{F}_q)^n</span> . We only concern about q as powers of 2 since they are almost all the cases in computer science. According to correspondence between linear transformations and matrices, every  <span class="math">\\mathbb{F}_q</span> -linear layer can be represented by a matrix over  <span class="math">\\mathbb{F}_q</span> . And this representation is unique under a fixed basis of  <span class="math">(\\mathbb{F}_q)^n</span> . Some linear layers are just matrices over  <span class="math">\\mathbb{F}_2</span> , such as the candidates presented in [10, 13, 8]. They are very convenient for implementation. However, there are also many linear layers over the extension fields of  <span class="math">\\mathbb{F}_2</span> , such as the candidates presented in [4, 1, 9, 12]. A typical example of this type is the linear layers used in AES. They are  <span class="math">(4 \\times 4)</span>  matrices over  <span class="math">\\mathbb{F}_{2^8}</span> . Note that multiplying a fixed element in  <span class="math">\\mathbb{F}_{2^m}</span>  is actually an  <span class="math">\\mathbb{F}_2</span> -linear transformation over  <span class="math">(\\mathbb{F}_2)^m</span>  and can be represented by an  <span class="math">(m \\times m)</span>  matrix over  <span class="math">\\mathbb{F}_2</span> . So, an  <span class="math">\\mathbb{F}_{2^m}</span> -linear layer over  <span class="math">(\\mathbb{F}_{2^m})^n</span>  is essentially an  <span class="math">\\mathbb{F}_2</span> -linear transformation over  <span class="math">(\\mathbb{F}_2)^{mn}</span>  and can be represented by an  <span class="math">(mn \\times mn)</span>  matrix over  <span class="math">\\mathbb{F}_2</span> . Hence, matrices over  <span class="math">\\mathbb{F}_2</span>  are generic forms for linear layers.</p>

    <p class="text-gray-300">Because every linear layer can be represented by a matrix over  <span class="math">\\mathbb{F}_2</span>  and every element in  <span class="math">\\mathbb{F}_{2^m}</span>  is represented by a bit string in computers, the number of bit-operations required for implementing linear layers naturally becomes the measurement of implementation cost of them. In this work, we concentrate on how to implement a given linear layer with as few XORs as possible.</p>

    <h4 id="sec-3" class="text-lg font-semibold mt-6">1.1 Related Work</h4>

    <p class="text-gray-300">As we mentioned above, the implementation efficiency of a linear layer is measured by the number of XORs needed to implement it. For an invertible  <span class="math">n \\times n</span>  matrix L over  <span class="math">\\mathbb{F}_2</span> and an input column vector X, LX is usually implemented by computing the co-ordinates independently. For example, the first co-ordinate of LX is computed as the product of the first row of L and X. Consequently, the number of XORs of L is naturally considered to be the difference between the Hamming weight of L and the its order n. However, if a linear layer L' is an invertible  <span class="math">n \\times n</span>  matrix over  <span class="math">\\mathbb{F}_{2^m}</span> , counting its number of XORs is a little complicated. In [12], the authors investigated this case in details. They formally defined the XOR counts of elements in  <span class="math">\\mathbb{F}_{2^m}</span>  which is closely relevant to the computational pattern over  <span class="math">\\mathbb{F}_{2^m}</span> , or more specifically, the basis of  <span class="math">\\mathbb{F}_{2^m}</span> . Then, the XOR count of L' is the sum of XOR counts of all entries of L' plus mn(n-1). In fact, according to our statement above, L' can be alternatively represented by an  <span class="math">(mn \\times mn)</span>  matrix L'' over  <span class="math">\\mathbb{F}_2</span>  once we fix an  <span class="math">\\mathbb{F}_2</span> -basis of  <span class="math">\\mathbb{F}_{2^m}</span> . From this viewpoint, the XOR count of L' defined in [12] is exactly the number of XORs needed to implement the  <span class="math">(mn \\times mn)</span>  matrix L'', namely, the difference between the Hamming weight of L'' and its order mn. Later, some papers ([11, 9, 8]) about linear layers adopted the essentially same measurement of implementation cost as in [12]. Here we point out that all those papers have identical essence: computing the co-ordinates of outputs of a linear layer independently. Thus, the implementation cost of linear layers in those papers is only related to their matrices over  <span class="math">\\mathbb{F}_2</span>  once bases are fixed.</p>

    <p class="text-gray-300">In recent years, a new notion of implementation cost of linear layers were presented by Jean, Peyrin, and Sim ([7]). Later, Beierle, Kranz, and Leander ([1]) presented a measurement of implementation cost of multiplication with a fixed element in finite field  <span class="math">\\mathbb{F}_{2^m}</span>  and constructed a series of lightweight maximum distance separable (MDS) linear</p>

    <p class="text-gray-300">layers. In brief, their new measurement originally came from an implementation method different from previous ones. And this new one can indeed save XORs in comparison with previous methods.</p>

    <h3 id="sec-4" class="text-xl font-semibold mt-8"><strong>1.2 Our Contributions</strong></h3>

    <p class="text-gray-300">In this work, our goal is to find the optimal implementation for any given linear layer. Here, &quot;optimal&quot; means using fewest XORs. To attain the goal, we investigate the relation between implementation cost and implementation procedure. From the investigation, we present a generic measurement for implementation cost of linear layers. After that, we construct a graph-theoretical model and transfer the main problem to the shortest path problem in graph theory. Then, we construct a particular algorithm that is very proper to solve the particular shortest path problem related to invertible linear layers. However, this algorithm for finding optimal implementations of linear layers is not practical enough because of its high space/time complexity. So, we construct another practical algorithm with very low space/time complexity for finding efficient implementations of invertible linear layers, although we cannot guarantee that it necessarily gives optimal implementations. To handle more general linear layers, we finally present a highly practical algorithm for finding efficient implementations of singular linear layers.</p>

    <p class="text-gray-300">In Section 3, we firstly investigate the effect of different implementation procedures for implementation cost of invertible linear layers. Briefly, we compare two implementation strategies: computing the co-ordinates of output vectors independently and modifying the input vectors to output step by step (MIOSS). The former is straightforward and has been used for years. However, the latter requires fewer XORs. Therefore, we focus on the latter one. From the investigation, we present a reasonable measurement of implementation cost of a invertible linear layer <em>L</em>: the minimum number of additive elementary matrices in <em>L</em>'s factorization of the form <em>P</em>1<em>A</em>&lt;sup&gt;1&lt;/sup&gt; · · · <em>PsAsPs</em>+1 where every <em>P&lt;sup&gt;i&lt;/sup&gt;</em> is a permutation matrix and every <em>A&lt;sup&gt;j&lt;/sup&gt;</em> is an additive elementary matrix. Then, we give some properties that make our measurement more flexible.</p>

    <p class="text-gray-300">In Section 4, we firstly give an equivalence relation over all invertible linear layers of order <em>n</em> over F2. Based on this equivalence relation, we construct a graph whose vertices are all the equivalence classes. Then we show finding an optimal implementation of a given invertible linear layer <em>L</em> is essentially same as finding a shortest path between the vertex containing <em>L</em> and the vertex containing the identity matrix <em>In</em>.</p>

    <p class="text-gray-300">In Section 5, our main goal is to solve the shortest path problem in the graph defined in Section 4. Although there has already been &quot;single-direction&quot; methods (Dijkstra's algorithm, for example) for the shortest path problem, we abandon them and construct a &quot;double-direction&quot; algorithm. That is because the graph that we are talking about is a regular graph, and our double-direction algorithm uses less storage and makes the search for a shortest path more efficient in a regular graph. With our algorithm, we perform experiments to some linear layers and obtain good results.</p>

    <p class="text-gray-300">Although the algorithm in Section 5 can give us optimal implementations, it is not practical enough for heavyweight invertible linear layers because of its high space/time complexity. To solve this problem, we present another algorithm in Section 6. An important advantage of this algorithm is that its space/time complexity is incredibly low. On the other hand, we have to admit that it does not necessarily give us optimal implementations of linear layers. As an application, we investigate the linear layers of AES with the algorithm and get implementations much more efficient than before.</p>

    <p class="text-gray-300">In above sections, we talked about minimum-XOR-implementations and efficient implementations of invertible linear layers. However, we are confronted with singular (even not square) matrices sometimes. So, in Section 7, we pay attention to singular linear layers and construct a highly practical algorithm for finding efficient implementations of singular linear layers.</p>

    <h2 id="sec-5" class="text-2xl font-bold"><strong>2 Preliminary</strong></h2>

    <h2 id="sec-6" class="text-2xl font-bold"><strong>2.1 Notations</strong></h2>

    <p class="text-gray-300">In this paper, F<em>&lt;sup&gt;q&lt;/sup&gt;</em> or <em>GF</em>(<em>q</em>) denotes the finite field of <em>q</em> elements. M<em>m</em>×<em>n</em>(<em>R</em>) denotes the set consisting of all (<em>m</em> × <em>n</em>) matrices over a ring <em>R</em>, IM<em>n</em>×<em>n</em>(<em>R</em>) denotes the set consisting of all (<em>n</em> × <em>n</em>) invertible matrices over a ring <em>R</em>, PM<em>n</em>×<em>n</em>(<em>R</em>) denotes the set consisting of all (<em>n</em> × <em>n</em>) permutation matrices over a ring <em>R</em>, EEM<em>n</em>×<em>n</em>(<em>R</em>) denotes the set consisting of all (<em>n</em> × <em>n</em>) exchanging elementary matrices over a ring <em>R</em>, MEM<em>n</em>×<em>n</em>(<em>R</em>) denotes the set consisting of all (<em>n</em>×<em>n</em>) multiplicative elementary matrices over a ring <em>R</em>, and AEM<em>n</em>×<em>n</em>(<em>R</em>) denotes the set consisting of all (<em>n</em> × <em>n</em>) additive elementary matrices over a ring <em>R</em>. For a matrix <em>A</em>, <em>A&lt;sup&gt;T&lt;/sup&gt;</em> denotes the transpose of <em>A</em> and <em>WH</em>(<em>A</em>) denotes the Hamming weight (the number of nonzero entries) of <em>A</em>. <em>Ei,j</em> denotes the matrix whose (<em>i, j</em>)th entry is 1 and other entries are 0. <em>I&lt;sup&gt;n&lt;/sup&gt;</em> denotes the (<em>n</em> × <em>n</em>) identity matrix. For a set <em>S</em>, <em>]S</em> or |<em>S</em>| denotes the cardinality of <em>S</em>.</p>

    <h2 id="sec-7" class="text-2xl font-bold"><strong>2.2 Some Basic Facts about Linear Algebra</strong></h2>

    <p class="text-gray-300">In this subsection, we review some basic facts about linear algebra. For more details, refer to <a href="#page-20-5">[6]</a>.</p>

    <p class="text-gray-300">To begin with, let us talk about elementary operations to matrices. For every matrix <em>L</em> ∈ M<em>m</em>×<em>n</em>(<em>F</em>) where <em>F</em> is a field, there are three types of elementary row operations on it: row exchanging, row multiplication, and row addition. Row exchanging means exchanging tow rows of <em>L</em>. Row multiplication means multiplying every entries of a row by an invertible element of <em>F</em>. Row addition means adding a scalar-product of an element in <em>F</em> and a row to another row. In addition to elementary row operations, there are also three types of corresponding elementary column operations. All the elementary operations to matrices are invertible.</p>

    <p class="text-gray-300">Corresponding to elementary operations, there are three types of elementary matrices: exchanging elementary matrices, multiplicative elementary matrices, and additive elementary matrices. An (<em>n</em> × <em>n</em>) exchanging elementary matrix is the matrix obtained by exchanging two rows of <em>In</em>. An (<em>n</em> × <em>n</em>) multiplicative elementary matrix is the matrix obtained from <em>I&lt;sup&gt;n&lt;/sup&gt;</em> by substituting an invertible element in <em>F</em> for a 1 on the diagonal of <em>In</em>. An (<em>n</em> × <em>n</em>) additive elementary matrix <em>Ai,j</em> is the matrix <em>I&lt;sup&gt;n&lt;/sup&gt;</em> + <em>aEi,j</em> where <em>a</em> ∈ <em>F</em> and <em>i</em> 6= <em>j</em>. For any matrix <em>L</em> ∈ M<em>m</em>×<em>n</em>(<em>F</em>), left-multiplying <em>L</em> by an (<em>m</em> × <em>m</em>) elementary matrix causes a corresponding elementary row operation to it, while right-multiplying <em>L</em> by an (<em>n</em> × <em>n</em>) elementary matrix causes a corresponding elementary column operation to it. A useful fact is that every elementary matrix is invertible. The inverse matrix of an exchanging elementary matrix is just itself. And the inverse matrix of an additive elementary matrix <em>Ai,j</em> = <em>I&lt;sup&gt;n&lt;/sup&gt;</em> + <em>aEi,j</em> is <em>I&lt;sup&gt;n&lt;/sup&gt;</em> − <em>aEi,j</em> .</p>

    <p class="text-gray-300">Another type of matrices important for this paper is permutation matrix. An <em>n</em> × <em>n</em> permutation matrix is the matrix obtained from <em>I&lt;sup&gt;n&lt;/sup&gt;</em> by permutes the rows of it. It is trivial that a matrix <em>P</em> ∈ M<em>n</em>×<em>n</em>(<em>F</em>) is a permutation matrix if and only if it is invertible and its entries are zero except <em>n</em> entries equal to 1. For any matrix <em>L</em> ∈ M<em>m</em>×<em>n</em>(<em>F</em>), left-multiplying <em>L</em> by an (<em>m</em> × <em>m</em>) permutation matrix permutes the rows of it, while right-multiplying <em>L</em> by an (<em>n</em> × <em>n</em>) permutation matrix permutes the columns of it. In this paper, we sometimes write a permutation matrix <em>P</em> ∈ M<em>n</em>×<em>n</em>(<em>F</em>) as a column (<em>ρ</em>(1)<em>,</em> · · · <em>, ρ</em>(<em>n</em>))<em>&lt;sup&gt;T&lt;/sup&gt;</em> , where <em>ρ</em> is a permutation over the set <em>N</em> = {1<em>,</em> · · · <em>, n</em>} and <em>ρ</em>(<em>i</em>) is the column index of the nonzero entry in the <em>i</em>-th row of <em>P</em> for <em>i</em> = 1<em>,</em> · · · <em>, n</em>. For example, we write the permutation matrix</p>

    <p class="text-gray-300"><span class="math">$\\left(\\begin{array}{cccc} 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right) = \\left(\\begin{array}{c} 3 \\\\ 4 \\\\ 2 \\\\ 1 \\end{array}\\right) = \\left(\\begin{array}{c} \\rho(1) \\\\ \\rho(2) \\\\ \\rho(3) \\\\ \\rho(4) \\end{array}\\right).</span>$</p>

    <p class="text-gray-300">With this notation, if  <span class="math">P = (\\rho(1), \\dots, \\rho(n))^T</span> , then  <span class="math">P^{-1} = (\\rho^{-1}(1), \\dots, \\rho^{-1}(n))^T</span>  is also a permutation matrix, and the <em>i</em>-th row of PL is the  <span class="math">\\rho(i)</span> -th row of L.</p>

    <h2 id="sec-8" class="text-2xl font-bold">2.3 Some Basic Facts about Graph Theory</h2>

    <p class="text-gray-300">In this subsection, we review some basic facts about graph theory. For more details, refer to [2, 5].</p>

    <p class="text-gray-300">A graph G is composed of two types of objects. It has a set V of elements called vertices (or nodes) and a set of unordered pairs of vertices called edges. We denote the graph whose vertex set is V and edge set is E by G=(V,E). The cardinality of the vertex set V is called the order of the graph G. If  <span class="math">\\alpha=\\{x,y\\}</span>  is an edge of G, we say that  <span class="math">\\alpha</span>  joins x and y, x and y are adjacent, and x and  <span class="math">\\alpha</span>  are incident. The degree of a vertex x is a the number of edges that are incident with x and is denoted by  <span class="math">\\deg(x)</span> . If the degree of every vertex of the graph G is r, we say G is an r-regular graph.</p>

    <p class="text-gray-300">In a graph G = (V, E), a sequence of m edges of the form</p>

    <p class="text-gray-300"><span class="math">$\\{x_0, x_1\\}, \\{x_1, x_2\\}, \\cdots, \\{x_{m-1}, x_m\\}</span>$</p>

    <p class="text-gray-300">is called a walk of length m. We also denote it by  <span class="math">x_0 - x_1 - \\cdots - x_m</span> . The walk  <span class="math">x_0 - x_1 - \\cdots - x_m</span>  is closed (open) if  <span class="math">x_0 = x_m</span>  ( <span class="math">x_0 \\neq x_m</span> ). If a walk has distinct edges, we call it a trail. In addition, if a trail has distinct vertices, we call it a path. A closed path is called a cycle. The distance between two vertices x and y is the shortest length of walks joining them and denoted by d(x,y). It is clear that a walk joining x and y of length d(x,y) is a path. We say two vertices x and y are connected if there exists a walk joining them. And we say a graph x is connected if every pair of vertices of x is connected.</p>

    <h2 id="sec-9" class="text-2xl font-bold">2.4 Linear Layers</h2>

    <p class="text-gray-300">In general, an F-linear layer is an F-linear transformation over  <span class="math">F^n</span>  for a field F. In this paper, we merely focus on fields of characteristic 2 because they are widely used in computer science and telecommunication.</p>

    <p class="text-gray-300">Suppose L is an  <span class="math">\\mathbb{F}_q</span> -linear layer over  <span class="math">(\\mathbb{F}_q)^n</span> , where  <span class="math">q=2^m</span> . According to the correspondence between linear transformations and matrices, L can be uniquely represented by a matrix in  <span class="math">\\mathcal{M}_{n\\times n}(\\mathbb{F}_q)</span>  under a given  <span class="math">\\mathbb{F}_q</span> -basis of  <span class="math">(\\mathbb{F}_q)^n</span> . Let</p>

    <p class="text-gray-300"><span class="math">$L = \\begin{pmatrix} \\alpha_{1,1} &amp; \\alpha_{1,2} &amp; \\cdots &amp; \\alpha_{1,n} \\\\ \\alpha_{2,1} &amp; \\alpha_{2,2} &amp; \\cdots &amp; \\alpha_{2,n} \\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ \\alpha_{n,1} &amp; \\alpha_{n,2} &amp; \\cdots &amp; \\alpha_{n,n} \\end{pmatrix} \\in \\mathcal{M}_{n \\times n}(\\mathbb{F}_q).</span>$</p>

    <p class="text-gray-300">Then for every input column  <span class="math">X = (\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\cdots, \\boldsymbol{x}_n)^T \\in (\\mathbb{F}_q)^n</span> , the output is  <span class="math">Y = LX = (\\boldsymbol{y}_1, \\boldsymbol{y}_2, \\cdots, \\boldsymbol{y}_n)^T \\in (\\mathbb{F}_q)^n</span> , where  <span class="math">\\boldsymbol{y}_i = \\sum_{j=1}^n \\alpha_{i,j} \\boldsymbol{x}_j</span>  for  <span class="math">i = 1, \\cdots, n</span> . Note that being multiplied by a fixed element in  <span class="math">\\mathbb{F}_q</span>  is an  <span class="math">\\mathbb{F}_2</span> -linear transformation over  <span class="math">\\mathbb{F}_q</span>  and can be uniquely represented by a matrix in  <span class="math">\\mathcal{M}_{m \\times m}(\\mathbb{F}_2)</span>  under a given  <span class="math">\\mathbb{F}_2</span> -basis of  <span class="math">\\mathbb{F}_q</span> . Therefore, L is essentially an  <span class="math">\\mathbb{F}_2</span> -linear transformation over  <span class="math">\\mathbb{F}_2^{mn}</span>  and can be represented by a matrix</p>

    <p class="text-gray-300"><span class="math">$L = \\begin{pmatrix} L_{1,1} &amp; L_{1,2} &amp; \\cdots &amp; L_{1,n} \\\\ L_{2,1} &amp; L_{2,2} &amp; \\cdots &amp; L_{2,n} \\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ L_{n,1} &amp; L_{n,2} &amp; \\cdots &amp; L_{n,n} \\end{pmatrix} \\in \\mathcal{M}_{mn \\times mn}(\\mathbb{F}_2),</span>$</p>

    <p class="text-gray-300">where  <span class="math">L_{i,j} \\in \\mathcal{M}_{m \\times m}(\\mathbb{F}_2)</span>  for  <span class="math">i, j = 1, \\dots, n</span> . From this viewpoint, every  <span class="math">\\mathbb{F}_q</span> -linear layer is essentially an  <span class="math">\\mathbb{F}_2</span> -linear transformation and can be represented by a matrix over  <span class="math">\\mathbb{F}_2</span> . That gives us a uniform pattern for linear layers.</p>

    <h2 id="sec-10" class="text-2xl font-bold">3 Measurement of Implementation Costs of Linear Layers</h2>

    <p class="text-gray-300">In this section, we clarify how to measure the implementation costs of linear layers. As we mentioned above, considering invertible linear layers over GF(2) suffices.</p>

    <p class="text-gray-300">In general, the implementation cost of a given linear layer L over GF(2) is measured by the number of additions (XORs) required to compute the output vector LX for input X. In [12], the authors formally presented a measurement of the number of XORs (XORcount) of elements in  <span class="math">GF(2^m)</span>  as well as of whole diffusion layers. Later, some papers ([11, 9, 8]) adopted that measurement. We know that multiplication with a given element in  <span class="math">GF(2^m)</span>  can be represented by an  <span class="math">\\mathbb{F}_2</span> -linear transformation over  <span class="math">\\mathbb{F}_2^m</span> , namely, a matrix L in  <span class="math">\\mathcal{M}_{m\\times m}(\\mathbb{F}_2)</span> . From this viewpoint, their measurement XOR-count is exactly  <span class="math">W_H(L)-m</span>  since co-ordinates of output vector are computed independently. In fact, this notion has been used for years. However, the implementation cost of a given linear layer depends not only on the linear layer itself but also on ways by which we implement it. To show the effect of different implementation methods on implementation costs, we at first give Example 1.</p>

    <h4 id="sec-11" class="text-lg font-semibold mt-6">&lt;span id=&quot;page-5-0&quot;&gt;&lt;/span&gt;Example 1. Suppose</h4>

    <p class="text-gray-300"><span class="math">$L = \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\end{pmatrix} \\in \\mathcal{IM}_{4 \\times 4}(\\mathbb{F}_2)</span>$</p>

    <p class="text-gray-300">is a linear transformation over  <span class="math">\\mathbb{F}_2^4</span> . Then for every input vector  <span class="math">X = (x_1, x_2, x_3, x_4)^T \\in GF(2)^4</span> , the corresponding output is</p>

    <p class="text-gray-300"><span class="math">$Y = LX = \\begin{pmatrix} x_1 + x_2 + x_3 \\\\ x_2 \\\\ x_1 + x_2 + x_3 + x_4 \\\\ x_1 + x_2 \\end{pmatrix}.</span>$</p>

    <p class="text-gray-300">&lt;span id=&quot;page-5-1&quot;&gt;&lt;/span&gt;We may compute Y with a common method that has been used for years. That is, we compute the co-ordinates of Y independently. In this case, it requires  <span class="math">W_H(L) - 4 = 6</span>  additions (XORs) over GF(2). However, we can adopt another method to compute Y – modifying the input X to the output LX step by step as Figure 1. In Figure 1, we</p>

    <p class="text-gray-300"><span class="math">$(1) \\begin{bmatrix} x_{1} + &amp; &amp; &amp; &amp; \\\\ x_{2} &amp; &amp; &amp; \\\\ x_{3} &amp; &amp; &amp; \\\\ x_{4} \\end{bmatrix}; (2) \\begin{bmatrix} x_{1} + x_{2} &amp; &amp; &amp; \\\\ x_{2} &amp; &amp; &amp; \\\\ x_{3} + &amp; &amp; &amp; \\\\ x_{4} + &amp; &amp; &amp; \\\\ \\end{bmatrix}; (3) \\begin{bmatrix} x_{1} + x_{2} &amp; &amp; &amp; \\\\ x_{2} &amp; &amp; &amp; \\\\ x_{1} + x_{2} + x_{3} &amp; &amp; \\\\ x_{4} + &amp; &amp; &amp; \\\\ \\end{bmatrix}; (4) \\begin{bmatrix} x_{1} + x_{2} &amp; &amp; &amp; \\\\ x_{1} + x_{2} + x_{3} &amp; &amp; \\\\ x_{1} + x_{2} + x_{3} &amp; &amp; \\\\ x_{1} + x_{2} + x_{3} + x_{4} \\end{bmatrix} = LX.</span>$</p>

    <p class="text-gray-300">Figure 1: Modifying input to output step by step (MIOSS)</p>

    <p class="text-gray-300">compute LX through a series of operations on the co-ordinates of X. These operations can be easily implemented on programmable hardware (ASIC or FPGA, for instance). Step (1) adds  <span class="math">x_2</span>  to  <span class="math">x_1</span>  and remain  <span class="math">x_2, x_3, x_4</span> . So, its delay is  <span class="math">T_X</span> . Likewise, step (2) and step (3) costs  <span class="math">T_X</span>  delay, respectively. Note that step (2) requires only  <span class="math">T_X</span>  delay but not  <span class="math">2T_X</span>  delays because  <span class="math">x_1 + x_2</span>  has been already computed on preceding steps and saved in the</p>

    <p class="text-gray-300">&lt;span id=&quot;page-6-0&quot;&gt;&lt;/span&gt;    <img src="_page_6_Figure_2.jpeg" alt="" class="my-4 max-w-full" />
</p>

    <p class="text-gray-300"><strong>Figure 2:</strong> Implementing Example 1 by MIOSS in one clock cycle</p>

    <p class="text-gray-300">register. Step (4) does not cost any delay because it is merely performed by twisting wires. Although we illustrate the procedure by four steps, it can actually be implemented in one clock cycle as Figure 2. Regardless of 4 clock cycles or 1 clock cycle, the whole procedure of the second method uses 3 XORs. It is quite fewer than 6 XORs of the first method. In form of matrices, we can express the second method as  <span class="math">LX = PA_{4,3}A_{3,1}A_{1,2}X</span> , where each  <span class="math">A_{i,j} = I_4 + E_{i,j}</span>  and  <span class="math">P = (\\rho(1), \\rho(2), \\rho(3), \\rho(4))^T = (3, 2, 4, 1)^T</span> .</p>

    <p class="text-gray-300">Computing every co-ordinate independently is indeed easy to implement and has been used for years. However, it is not necessarily the optimal implementation according to Example 1. Here, &quot;optimal&quot; means &quot;using fewest XORs&quot;. To find the optimal implementations of linear layers, we must take implementation procedures into account. Let us give a lemma at first.</p>

    <p class="text-gray-300">&lt;span id=&quot;page-6-1&quot;&gt;&lt;/span&gt;<strong>Lemma 1.</strong> Suppose  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> . Then there exists a series of matrices  <span class="math">P_i \\in \\mathcal{PM}_{n \\times n}(\\mathbb{F}_2)</span>  and  <span class="math">A_j \\in \\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span>  such that  <span class="math">L = P_1 A_1 P_2 A_2 \\cdots P_s A_s P_{s+1}</span> .</p>

    <p class="text-gray-300">Proof. As we know, L can be transformed to its equivalent standard form through a series of elementary row/column operations. L's equivalent standard form is  <span class="math">I_n \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span>  because L is invertible. According to the invertibility of elementary operations,  <span class="math">I_n</span>  can be transformed to L through a series of elementary row/column operations. So, L is equal to the product of a series of elementary matrices. Note that multiplicative elementary matrix over GF(2) is just  <span class="math">I_n</span>  and can be omitted from the product. Besides, the product of some exchanging elementary matrices with order n is a permutation matrix. Thus, L is equal to the form  <span class="math">P_1A_1P_2A_2\\cdots P_sA_sP_{s+1}</span> , where  <span class="math">P_i \\in \\mathcal{PM}_{n \\times n}(\\mathbb{F}_2)</span>  for  <span class="math">i=1,\\cdots,s,</span>   <span class="math">A_j \\in \\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span>  for  <span class="math">j=1,\\cdots,s+1</span> .</p>

    <p class="text-gray-300">In accordance with Lemma 1, for every linear layer  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span>  and input vector  <span class="math">X = (x_1, \\cdots, x_n)^T \\in \\mathbb{F}_2^n</span> , the output vector is  <span class="math">LX = P_1A_1P_2A_2\\cdots P_sA_sP_{s+1}X</span>  which means we can get the output vector through a series of co-ordinate permutations or additive elementary operations. On the contrary, every modification procedure from X to LX can be expressed by the form  <span class="math">P_1A_1P_2A_2\\cdots P_sA_sP_{s+1}X</span>  as in Example 1. Note that for implementation of  <span class="math">P_1A_1P_2A_2\\cdots P_sA_sP_{s+1}X</span> , left-multiplying every  <span class="math">P_j</span>  costs 0 XOR while left-multiplying every  <span class="math">A_i</span>  costs 1 XOR. In detail, for a column vector  <span class="math">X = (x_1, \\cdots, x_n)^T</span>  in  <span class="math">\\mathbb{F}_2^n</span>  and a permutation matrix  <span class="math">P = (\\rho(1), \\cdots, \\rho(n))^T</span>  where  <span class="math">\\rho(i)</span>  denotes the column index of nonzero entry in the i-th row of P,</p>

    <p class="text-gray-300"><span class="math">$PX = (x_{\\rho(1)}, x_{\\rho(2)}, \\cdots, x_{\\rho(n)}).</span>$</p>

    <p class="text-gray-300">For a column vector  <span class="math">X = (x_1, \\dots, x_n)^T</span>  in  <span class="math">\\mathbb{F}_2^n</span>  and an additive elementary matrix  <span class="math">A_{i,j} = E_{i,j} + I_n</span>  in  <span class="math">\\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span> , left-multiplying X by  <span class="math">A_{i,j}</span>  will add the j-th row of X to its i-th row. Therefore, L's factorization  <span class="math">P_1A_1P_2A_2\\cdots P_sA_sP_{s+1}</span>  in Lemma 1 indicates the number of XORs used for its implementation. Now we can give the following definition.</p>

    <p class="text-gray-300">&lt;span id=&quot;page-6-2&quot;&gt;&lt;/span&gt;<strong>Definition 1.</strong> For a linear layer  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> , its minimum XOR-count Min-XOR-Count(L) is the minimum number of additive elementary matrices  <span class="math">A_i</span>  such that L can be</p>

    <p class="text-gray-300">written as form  <span class="math">P_1A_1P_2A_2\\cdots P_sA_sP_{s+1}</span> , where every  <span class="math">P_i\\in\\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)</span>  and every  <span class="math">A_j\\in\\mathcal{AEM}_{n\\times n}(\\mathbb{F}_2)</span> . And we call an implementation procedure  <span class="math">LX=P_1A_1P_2A_2\\cdots P_rA_rP_{r+1}X</span>  containing Min-XOR-Count(L) additive elementary matrices a minimum-XOR-implementation of L.</p>

    <p class="text-gray-300">In addition to Definition 1, we have other ways to describe the optimal implementation procedures of linear layers. To clarify those ways, we need the following lemma.</p>

    <p class="text-gray-300">&lt;span id=&quot;page-7-0&quot;&gt;&lt;/span&gt;<strong>Lemma 2.</strong> Suppose
<span class="math">$P = (\\rho(1), \\dots, \\rho(n))^T \\in \\mathcal{PM}_{n \\times n}(\\mathbb{F}_2)</span>$
,  <span class="math">A_{r,s} = I_n + E_{r,s} \\in \\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span> . Then  <span class="math">PA_{r,s} = A_{\\rho^{-1}(r),\\rho^{-1}(s)}P</span>  where  <span class="math">A_{\\rho^{-1}(r),\\rho^{-1}(s)} = I_n + E_{\\rho^{-1}(r),\\rho^{-1}(s)} \\in \\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span> .</p>

    <p class="text-gray-300">Proof. As we mentioned before, left-multiplying P permutes the rows of  <span class="math">A_{r,s}</span> . In detail, the i-th row of  <span class="math">PA_{r,s}</span>  is the  <span class="math">\\rho(i)</span> -th row of  <span class="math">A_{r,s}</span> . So, the  <span class="math">\\rho^{-1}(r)</span> -th row of  <span class="math">PA_{r,s}</span>  is the r-th row of  <span class="math">A_{r,s}</span> , and the  <span class="math">\\rho^{-1}(s)</span> -th row of  <span class="math">PA_{r,s}</span>  is the s-th row of  <span class="math">A_{r,s}</span> . Obviously,  <span class="math">PA_{r,s}</span>  will become P if we add  <span class="math">\\rho^{-1}(s)</span> -th row of  <span class="math">PA_{r,s}</span>  to  <span class="math">\\rho^{-1}(r)</span> -th row of it. Thus,  <span class="math">A_{\\rho^{-1}(r),\\rho^{-1}(s)}PA_{r,s} = P</span> . Then we get  <span class="math">PA_{r,s} = (A_{\\rho^{-1}(r),\\rho^{-1}(s)})^{-1}P = A_{\\rho^{-1}(r),\\rho^{-1}(s)}P</span>  since  <span class="math">(A_{\\rho^{-1}(r),\\rho^{-1}(s)})^{-1} = A_{\\rho^{-1}(r),\\rho^{-1}(s)}</span> .</p>

    <p class="text-gray-300">Combining Lemma 1 and Lemma 2, we get the next lemma easily. We omit its proof because it is really trivial.</p>

    <p class="text-gray-300">&lt;span id=&quot;page-7-1&quot;&gt;&lt;/span&gt;<strong>Lemma 3.</strong> Every  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span>  can be factorized to the form  <span class="math">L = \\prod_{i=1}^s B_i</span>  where every  <span class="math">B_i</span>  is either in  <span class="math">\\mathcal{PM}_{n \\times n}(\\mathbb{F}_2)</span>  or in  <span class="math">\\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span> .</p>

    <p class="text-gray-300">We call every factorization of L as the form in Lemma 3 a P-AE factorization of it. Obviously, the factorization form in Lemma 1 is a P-AE factorization. On the contrary, every P-AE factorization can be written as the form in Lemma 1 since identity matrix is also a permutation matrix, and the product of permutation matrices in  <span class="math">\\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)</span>  is a permutation matrix too. Therefore, the minimum XOR count of a given linear layer  <span class="math">L \\in \\mathcal{IM}_{n\\times n}(\\mathbb{F}_2)</span>  is exactly equal to the minimum number of additive elementary matrices in L's P-AE factorizations. In summary of this paragraph, we present the following theorem.</p>

    <p class="text-gray-300"><strong>Theorem 1.</strong> For every linear layer  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> , Min - XOR - Count(L) is equal to the minimum number of additive elementary matrices in L's P-AE factorizations.</p>

    <p class="text-gray-300">In [1], the authors measured the lowest implementation cost (it is called XOR count in [1]) of a linear layer  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span>  by the minimum number t such that L can be written as  <span class="math">L = P \\prod_{i=1}^t A_i</span>  where  <span class="math">P \\in \\mathcal{PM}_{n \\times n}(\\mathbb{F}_2)</span>  and every  <span class="math">A_i \\in \\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span> . It is easy to see that every factorization of the form  <span class="math">L = P \\prod_{i=1}^t A_i</span>  corresponds to a factorization of the form in Definition 1 according to Lemma 2. So, our definition of Min-XOR-count does not contradict theirs. We shall indicate the advantage of our definition later.</p>

    <p class="text-gray-300">Because a linear layer is often used bidirectionally (for example encryption and decryption), we also need to consider the inverse of it.</p>

    <p class="text-gray-300"><strong>Theorem 2.</strong> For every linear layer  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> ,  <span class="math">Min - XOR - Count(L^{-1})</span>  is equal to Min - XOR - Count(L). Moreover, if a minimum-XOR-implementation of L is  <span class="math">P_1A_1P_2A_2\\cdots P_rA_rP_{r+1}</span> , then  <span class="math">P_{r+1}^{-1}A_rP_r^{-1}\\cdots A_1P_1^{-1}</span>  is a minimum-XOR-implementation of  <span class="math">L^{-1}</span> .</p>

    <p class="text-gray-300"><em>Proof.</em> Suppose a factorization  <span class="math">L=Q_1B_1\\cdots Q_sB_sQ_{s+1}</span>  is a minimum-XOR-implementation of L, where each  <span class="math">Q_i\\in\\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)</span>  and each  <span class="math">B_j\\in\\mathcal{AEM}_{n\\times n}(\\mathbb{F}_2)</span> . Then we have a factorization  <span class="math">L^{-1}=Q_{s+1}^{-1}B_s^{-1}Q_s^{-1}\\cdots B_1^{-1}Q_1^{-1}</span>  of  <span class="math">L^{-1}</span> . Note that the inverse of a permutation matrix is a permutation too, and the inverse of each additive elementary matrix  <span class="math">B_j</span>  is just  <span class="math">B_j</span>  itself. Thus,</p>

    <p class="text-gray-300"><span class="math">$Min - XOR - Count(L) \\ge Min - XOR - Count(L^{-1}).</span>$</p>

    <p class="text-gray-300">Likewise, we can get</p>

    <p class="text-gray-300"><span class="math">$Min - XOR - Count(L^{-1}) \\ge Min - XOR - Count(L).</span>$</p>

    <p class="text-gray-300">So,  <span class="math">Min - XOR - Count(L^{-1})</span>  is equal to Min - XOR - Count(L). And the second part of the theorem is trivial.</p>

    <p class="text-gray-300">At the end of this section, we point out that if a linear layer L' is a power of another linear layer L, for example  <span class="math">L&#x27;=L^5</span> , then minimum-XOR-implementation of L' is better successive 5 times minimum-XOR-implementations of L. That is because successive 5 times minimum-XOR-implementations of L is merely an implementation procedure of L' and it cannot perform better than minimum-XOR-implementation of L'. Consequently, if  <span class="math">L&#x27;=L^r</span> , then  <span class="math">Min-XOR-Count(L&#x27;) \\leq rMin-XOR-Count(L)</span> .</p>

    <h2 id="sec-12" class="text-2xl font-bold">4 A Graph-Theoretical Model</h2>

    <p class="text-gray-300">After clarifying the measurement of the lowest implementation cost of linear layers, we are confronted by two problems:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>How to calculate the Min-XOR-Count of a given linear layer  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> ;</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>How to get a minimum-XOR-implementation of a given linear layer  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> .</li>
    </ol></li>
    </ul>

    <p class="text-gray-300">Obviously, the 1st problem will be trivial provided that we solve the 2nd one. In this section, we would like to handle the two problems by a graph-theoretical model.</p>

    <p class="text-gray-300">First of all, we introduce a relation over  <span class="math">\\mathcal{IM}_{n\\times n}(\\mathbb{F}_2)</span> . We say two matrices  <span class="math">B,C\\in\\mathcal{IM}_{n\\times n}(\\mathbb{F}_2)</span>  are row-permutation-equivalent (denoted by  <span class="math">B\\sim_{RP}C</span> ) if there exists a  <span class="math">Q\\in\\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)</span>  such that B=QC. For every  <span class="math">B\\in\\mathcal{IM}_{n\\times n}(\\mathbb{F}_2)</span> ,  <span class="math">B\\sim_{RP}B</span>  since  <span class="math">B=I_nB</span> . If  <span class="math">B\\sim_{RP}C</span> , then  <span class="math">C\\sim_{RP}B</span>  because  <span class="math">C=Q^{-1}B</span>  and  <span class="math">Q^{-1}\\in\\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)</span>  too. If there exists  <span class="math">Q_1,Q_2\\in\\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)</span>  such that  <span class="math">B=Q_1C</span>  and  <span class="math">C=Q_2D</span> , then  <span class="math">B=Q_1Q_2D</span>  and  <span class="math">Q_1Q_2\\in\\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)</span> . Therefore, &quot; <span class="math">\\sim_{RP}</span> &quot; is an equivalence relation over  <span class="math">\\mathcal{IM}_{n\\times n}(\\mathbb{F}_2)</span> . For every  <span class="math">B\\in\\mathcal{IM}_{n\\times n}(\\mathbb{F}_2)</span> , let  <span class="math">[B]_{RP}</span>  denote the equivalence class containing B under &quot; <span class="math">\\sim_{RP}</span> &quot;. It is easy to see that for every  <span class="math">B\\in\\mathcal{IM}_{n\\times n}(\\mathbb{F}_2)</span> ,  <span class="math">[B]_{RP}=\\{QB|Q\\in\\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)\\}</span> , and  <span class="math">Q_1B\\neq Q_2B</span>  if  <span class="math">Q_1\\neq Q_2</span> .</p>

    <p class="text-gray-300">Next, we define a graph dependent on row-permutation-equivalence over  <span class="math">\\mathcal{IM}_{n\\times n}(\\mathbb{F}_2)</span> .</p>

    <p class="text-gray-300">&lt;span id=&quot;page-8-0&quot;&gt;&lt;/span&gt;<strong>Definition 2.</strong> For a positive integer n, let G(n) = (V, E) be a graph where the vertex set consists of all the equivalence classes under row-permutation-equivalence relation over  <span class="math">\\mathcal{IM}_{n\\times n}(\\mathbb{F}_2)</span> . And two vertices  <span class="math">[B]_{RP}</span>  and  <span class="math">[C]_{RP}</span>  are adjacent if there exists  <span class="math">B&#x27; \\in [B]_{RP}</span> ,  <span class="math">C&#x27; \\in [C]_{RP}</span>  and  <span class="math">A \\in \\mathcal{AEM}_{n\\times n}(\\mathbb{F}_2)</span>  such that B' = AC'.</p>

    <p class="text-gray-300">Now let us show some useful properties of the graph in Definition 2.</p>

    <p class="text-gray-300">&lt;span id=&quot;page-8-1&quot;&gt;&lt;/span&gt;<strong>Theorem 3.</strong> Let G(n) = (V, E) be the graph described in Definition 2 and  <span class="math">[B]_{RP}</span>  be a vertex of G(n). Then G(n) is an  <span class="math">(n^2 - n)</span> -regular graph,  <span class="math">[A_1B]_{RP} \\neq [A_2B]_{RP}</span>  for distinct  <span class="math">A_1, A_2 \\in \\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span> , and  <span class="math">[AB]_{RP}</span>  runs all the vertices adjacent to  <span class="math">[B]_{RP}</span>  when A runs over  <span class="math">\\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span> .</p>

    <p class="text-gray-300"><em>Proof.</em> If  <span class="math">[A_1B]_{RP} = [A_2B]_{RP}</span> , then  <span class="math">A_1B = PA_2B</span>  for some  <span class="math">P \\in \\mathcal{PM}_{n \\times n}(\\mathbb{F}_2)</span> . Consequently, we get  <span class="math">A_1 = PA_2</span>  by right-multiplying two sides of  <span class="math">A_1B = PA_2B</span>  by  <span class="math">B^{-1}</span> . We assert that P must be  <span class="math">I_n</span> . Otherwise, some entries on the diagonal of  <span class="math">PA_2</span>  would be 0 and  <span class="math">PA_2</span>  cannot be equal to  <span class="math">A_1</span> . Then  <span class="math">A_1 = A_2</span> . So,  <span class="math">[A_1B]_{RP} \\neq [A_2B]_{RP}</span>  for distinct  <span class="math">A_1, A_2 \\in \\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span> .</p>

    <p class="text-gray-300">On the other hand, if a vertex  <span class="math">[C]_{RP}</span>  is adjacent to  <span class="math">[B]_{RP}</span> , there exists  <span class="math">B&#x27; \\in [B]_{RP}</span> ,  <span class="math">C&#x27; \\in [C]_{RP}</span>  and  <span class="math">A \\in \\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span>  such that B' = AC'. Then AB' = C' since  <span class="math">A^{-1} = A</span> .</p>

    <p class="text-gray-300">Consequently, there exists  <span class="math">P_2 \\in \\mathcal{PM}_{n \\times n}(\\mathbb{F}_2)</span>  and  <span class="math">A&#x27; \\in \\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span>  such that  <span class="math">C&#x27; = AB&#x27; = AP_1B = P_2A&#x27;B</span>  according to Lemma 2. So,  <span class="math">[C]_{RP} = [C&#x27;]_{RP} = [P_2A&#x27;B]_{RP} = [A&#x27;B]_{RP}</span> . That shows  <span class="math">[AB]_{RP}</span>  runs all the vertices adjacent to  <span class="math">[B]_{RP}</span>  when A runs over  <span class="math">\\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span> .</p>

    <p class="text-gray-300">In summary of two preceding paragraphs, the degree of every vertex of G(n) is the cardinality of  <span class="math">\\mathcal{AEM}_{n\\times n}(\\mathbb{F}_2)</span> , namely,  <span class="math">n^2-n</span> .</p>

    <p class="text-gray-300">Finally, we present a significant theorem that explains why we set up the graph-theoretical model.</p>

    <p class="text-gray-300">&lt;span id=&quot;page-9-0&quot;&gt;&lt;/span&gt;<strong>Theorem 4.</strong> Let G(n) = (V, E) be the graph described in Definition 2 and  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> . Then the minimum XOR-count of L is equal to the distance between vertices  <span class="math">[L]_{RP}</span>  and  <span class="math">[I_n]_{RP}</span>  in G(n).</p>

    <p class="text-gray-300"><em>Proof.</em> For every factorization of L with the form  <span class="math">P_1A_1P_2A_2\\cdots P_sA_sP_{s+1}</span> , where every  <span class="math">P_i \\in \\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)</span>  and every  <span class="math">A_j \\in \\mathcal{AEM}_{n\\times n}(\\mathbb{F}_2)</span> , there exists a path</p>

    <p class="text-gray-300"><span class="math">$[L]_{RP} = [P_1 A_1 P_2 A_2 \\cdots P_s A_s P_{s+1}]_{RP} - [P_2 A_2 \\cdots P_s A_s P_{s+1}]_{RP} - \\cdots - [P_s A_s P_{s+1}]_{RP} - [P_{s+1}]_{RP} = [I_n]_{RP}</span>$</p>

    <p class="text-gray-300">between  <span class="math">[L]_{RP}</span>  and  <span class="math">[I_n]_{RP}</span>  of length s.</p>

    <p class="text-gray-300">On the other hand, for every path</p>

    <p class="text-gray-300"><span class="math">$[L]_{RP} - [L_{r-1}]_{RP} - \\dots - [L_2]_{RP} - [L_1]_{RP} - [I_n]_{RP}</span>$</p>

    <p class="text-gray-300">of length r, according to Theorem 3, there must be  <span class="math">L=Q_{r-1}A_{r-1}L_{r-1}</span> ,  <span class="math">L_j=Q_{j-1}A_{j-1}L_{j-1}</span>  for  <span class="math">j=2,\\cdots,r-1</span> , and  <span class="math">L_1=Q_0A_0I_n</span>  for some  <span class="math">Q_i\\in\\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)</span>  and  <span class="math">A_i\\in\\mathcal{AEM}_{n\\times n}(\\mathbb{F}_2)</span>  for  <span class="math">i=0,1,\\cdots,r</span> . Consequently,  <span class="math">L=Q_{r-1}A_{r-1}\\cdots Q_1A_1Q_0A_0I_n</span>  which is a factorization of L with the form in Lemma 1.</p>

    <p class="text-gray-300">Therefore, the minimum XOR-count of L is equal to the minimum length of paths between  <span class="math">[L]_{RP}</span>  and  <span class="math">[I_n]_{RP}</span> , namely, their distance.</p>

    <p class="text-gray-300">From the proof of Theorem 4, we can easily see that a shortest path between  <span class="math">[L]_{RP}</span>  and  <span class="math">[I_n]_{RP}</span>  indicates a minimum-XOR-implementation of L.</p>

    <p class="text-gray-300">In this section, we talk about how to get a minimum-XOR-implementation of a given linear layer  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> . In accordance with the preceding section, this question is equivalent to finding a shortest path between  <span class="math">[L]_{RP}</span>  and  <span class="math">[I_n]_{RP}</span> .</p>

    <p class="text-gray-300">Let us sketch our strategy firstly. Suppose G(V, E) is a connected graph,  <span class="math">a, b \\in V</span> . To find a shortest path between a and b, we let  <span class="math">\\mathcal{A}_0 = \\{a\\}</span> ,  <span class="math">\\mathcal{B}_0 = \\{b\\}</span>  and check whether a = b. If a = b, we do not have to do anything. If  <span class="math">a \\neq b</span> , we construct a set  <span class="math">\\mathcal{A}_1</span>  consisting of the vertices of G that are adjacent to a. In other words,  <span class="math">\\mathcal{A}_1</span>  consists of the vertices having distance 1 from a. Then we check whether there exists  <span class="math">x \\in \\mathcal{A}_1</span>  such that x = b. If there is, we get a shortest path a - b between a and b. If there is not, we construct a set  <span class="math">\\mathcal{B}_1</span>  consisting of the vertices of G that are adjacent to b. In other words,  <span class="math">\\mathcal{B}_1</span>  consists of the vertices having distance 1 from b. Then we check whether there exists  <span class="math">x \\in \\mathcal{A}_1</span>  and  <span class="math">y \\in \\mathcal{B}_1</span>  such that x = y. If there is, we get a shortest path  <span class="math">a - a_1^* - b</span>  between a and b where  <span class="math">a_1^* \\in \\mathcal{A}_1</span> . If there is not, we proceed the procedure above: check and move forwards one step from  <span class="math">\\mathcal{A}_i</span> , then check and move forwards one step from  <span class="math">\\mathcal{B}_i</span> . Finally, we will find</p>

    <p class="text-gray-300"><span class="math">x \\in \\mathcal{A}_{k+1}</span>  and  <span class="math">y \\in \\mathcal{B}_{k+1}</span>  (or  <span class="math">y \\in \\mathcal{B}_k</span> ) for some k such that x = y. As a result, we find a shortest path</p>

    <pre><code class="language-text">a - a_1^* - \\dots - a_k^* - a_{k+1}^* - b_k^* - \\dots - b_1^* - b  (or a - a_1^* - \\dots - a_k^* - b_k^* - \\dots - b_1^* - b)
</code></pre>

    <p class="text-gray-300">between a and b, where  <span class="math">a_i^* \\in \\mathcal{A}_i</span> ,  <span class="math">b_i^* \\in \\mathcal{B}_i</span> , every pair  <span class="math">a_i^*</span> ,  <span class="math">a_{i+1}^*</span>  and every pair  <span class="math">b_j^*</span> ,  <span class="math">b_{j+1}^*</span>  are adjacent. We formally describe the above procedures in Algorithm 1 with pseudocode.</p>

    <h4 id="sec-13" class="text-lg font-semibold mt-6">&lt;span id=&quot;page-10-0&quot;&gt;&lt;/span&gt;Algorithm 1 Double-Direction Search for a Shortest Path</h4>

    <pre><code class="language-text">Require: a graph G = (V, E), a, b \\in V.
Ensure: a shortest path between a and b.
   \\mathcal{A}_0 \\leftarrow \\{a\\}, \\, \\mathcal{B}_0 \\leftarrow \\{b\\}, \\, i \\leftarrow 0, \\, j \\leftarrow 0, \\, link \\leftarrow 0;
   while link = 0 do
       if there exists a_i^* \\in \\mathcal{A}_i and b_i^* \\in \\mathcal{B}_j such that a_i^* = b_i^* then
           link \\leftarrow 1:
           continue:
       else
           i \\leftarrow i + 1;
           construct a set A_i consisting of the vertices adjacent to some vertex in A_{i-1} and not in \\bigcup_{k=0}^{i-1} A_k;
       if there exists a_i^* \\in \\mathcal{A}_i and b_i^* \\in \\mathcal{B}_j such that a_i^* = b_i^* then
           continue;
       واحو
           j \\leftarrow j + 1;
           construct a set \\mathcal{B}_j consisting of the vertices adjacent to some vertex in \\mathcal{B}_{j-1} and not in \\bigcup_{k=0}^{j-1} \\mathcal{B}_k;
       end if
   end while
   return the path a_0^* - \\cdots - a_i^* - b_{i-1}^* - \\cdots - b_0^* such that every pair a_k^*, a_{k+1} and every pair b_l^*, b_{l+1}^*
   are adjacent;
</code></pre>

    <p class="text-gray-300"><strong>Lemma 4.</strong> Suppose G = (V, E) is a graph,  <span class="math">a, b \\in V</span> . Then we get a shortest path between a and b with Algorithm 1.</p>

    <p class="text-gray-300"><em>Proof.</em> Assume there is a path  <span class="math">a-c_1-\\cdots-c_{r-1}-b</span>  shorter than the output of Algorithm 1. Then  <span class="math">c_k \\in \\mathcal{A}_k</span>  for  <span class="math">k=1,\\cdots,\\lceil\\frac{r-1}{2}\\rceil</span> , and  <span class="math">c_{r-l} \\in \\mathcal{B}_l</span>  for  <span class="math">l=1,\\cdots,\\lfloor\\frac{r-1}{2}\\rfloor</span> . Consequently, there exists  <span class="math">a_k^* \\in \\mathcal{A}_k</span> ,  <span class="math">b_l^* \\in \\mathcal{B}_l</span>  such that  <span class="math">a_k^* = b_l^*</span>  for some k &lt; i or some l &lt; j which contradicts Algorithms 1.</p>

    <p class="text-gray-300">Algorithm 1 is a generic method for any graph. Now we use it to handle our main target: a minimum-XOR-implementation of a given linear layer  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> . For this target, we present Algorithm 2 and omit the proof of its correctness because it directly comes from Algorithm 1. Here, we just give an explanation of Algorithm 2 as follows.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>We choose an element in every equivalence class to represent it. For example,  <span class="math">L_i^*</span>  represents the class  <span class="math">[L_i^*]_{RP}</span> . Hence, we determine  <span class="math">[L_i]_{RP} = [B_j]_{RP}</span>  by checking  <span class="math">L_i \\sim_{RP} B_j</span> .</li>
      <li>When we need to determine whether two invertible matrices  <span class="math">L_i</span>  and  <span class="math">B_j</span>  are row-permutation-equivalent, we check the Hamming weight of  <span class="math">L_i B_j^{-1}</span>  since  <span class="math">L_i \\sim_{RP} B_j</span>  if and only if  <span class="math">W_H(L_i B_j^{-1}) = n</span> . This method can be implemented easily when B is a product of some additive elementary matrices. More explicitly, if  <span class="math">B = A_1 \\cdots A_s</span> ,  <span class="math">B^{-1} = A_s \\cdots A_1</span> .</li>
    </ul>

    <p class="text-gray-300">For a given graph and two vertices of it, there has already been algorithms in literature for finding the shortest path of them. For example, the famous Dijkstra's algorithm ([3], Chapter 11, page 443). Certainly, we can adopt it to solve the main problem in this paper. Essentially, Dijkstra's algorithm is a single-direction search, while our algorithms are</p>

    <h4 id="sec-14" class="text-lg font-semibold mt-6">&lt;span id=&quot;page-11-0&quot;&gt;&lt;/span&gt;Algorithm 2 Finding a Minimum-XOR-Implementation of an Invertible Linear Layer</h4>

    <pre><code class="language-text">Require: a positive integer n, a matrix L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2).
Ensure: a minimum-XOR-implementation of L.
   \\mathcal{L}_0 \\leftarrow \\{L\\}, \\, \\mathcal{B}_0 \\leftarrow \\{I_n\\}, \\, i \\leftarrow 0, \\, j \\leftarrow 0, \\, link \\leftarrow 0;
   while link = 0 do
       if there exists L_i^* \\in \\mathcal{L}_i and B_j^* \\in \\mathcal{B}_j such that L_i^* \\sim_{RP} B_i^* then
           link \\leftarrow 1:
           continue;
       else
           construct a set \\mathcal{L}_i consisting of the matrices having the form A_{r,s}L_{i-1} and not row-permutation-
           equivalent to any matrix in \\bigcup_{k=0}^{i-1} \\mathcal{L}_k, where A_{r,s} runs over \\mathcal{AEM}_{n\\times n}(\\mathbb{F}_2) and L_{i-1} runs over
       end if
       if there exists L_i^* \\in \\mathcal{L}_i and B_i^* \\in \\mathcal{B}_j such that L_i^* \\sim_{RP} B_i^* then
           link \\leftarrow 1:
           continue;
       else
           j \\leftarrow j + 1;
           construct a set \\mathcal{B}_j consisting of the matrices having the form A_{r,s}B_{j-1} and not row-permutation-
           equivalent to any matrix in \\bigcup_{k=0}^{j-1} \\mathcal{B}_k, where A_{r,s} runs over \\mathcal{AEM}_{n\\times n}(\\mathbb{F}_2) and B_{j-1} runs over
           \\mathcal{B}_{i-1}
       end if
   end while
   return the path L_0^* - \\cdots - L_i^* - B_{i-1}^* - \\cdots - B_0^* such that every pair L_k^*, L_{k+1} and every pair B_l^*, B_{l+1}^*
   are adjacent;
</code></pre>

    <p class="text-gray-300">double-direction search. Let us show what will happen if we handle our main problem with Dijkstra's algorithm. We use the same notations as that in Algorithm 1. We start from vertex a to find a shortest path between a and b. By means of Dijkstra's algorithm, we need to construct a series of sets  <span class="math">A_i</span>  consisting of vertices whose distance to a is i for  <span class="math">i=1,2,\\cdots</span> . According to Theorem 3,  <span class="math">|A_1|=n^2-n</span> , and  <span class="math">|A_i|=(n^2-n)(n^2-n-1)^{i-1}</span>  for  <span class="math">i\\geq 2</span>  in the worst case. If the distance between a and b is  <span class="math">s\\geq 2</span> , the algorithm will not terminate until  <span class="math">A_s</span>  is constructed. We see the cardinality of  <span class="math">A_i</span> s increase too fast and will occupy too much space. Actually, that is the reason why we abandon single-direction strategies and adopt a double-direction strategy – moving forwards step by step from a and b alternately. With a double-direction strategy, we can save a lot of memory space and modify the search for the shortest path. For instance, suppose the distance between a and b is s=2t. If we use our method, then we need to construct  <span class="math">A_i</span> ,  <span class="math">B_i</span>  for  <span class="math">i=1,\\cdots,t</span> , where  <span class="math">|A_1|=|B_1|=n^2-n</span> ,  <span class="math">|A_2|=|B_2|=(n^2-n)(n^2-n-1),\\cdots,|A_t|=|B_t|=(n^2-n)(n^2-n-1)^{t-1}</span>  in the worst case. However, if we use a single-direction strategy, we have to construct  <span class="math">A_i</span>  for  <span class="math">i=1,\\cdots,2t</span> , where  <span class="math">|A_1|=n^2-n</span> ,  <span class="math">|A_2|=(n^2-n)(n^2-n-1),\\cdots,|A_t|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> .</p>

    <h4 id="sec-15" class="text-lg font-semibold mt-6">5.1 Experimental Results</h4>

    <p class="text-gray-300">We search the minimum XOR implementations of many linear layers and list some of them in Appendix A. The optimal implementation procedure of each one is like what we show in Example 1. Meanwhile, we list the minimum XOR count and the difference between Hamming weight and the order of the linear layers in Table 1, where the former indicates implementation cost of our strategy and the latter indicates the cost of computing co-ordinates of output independently.  <span class="math">L_1, \\dots, L_{10}</span>  in Table 1 are matrices in  <span class="math">\\mathcal{M}_{5\\times 5}(\\mathbb{F}_2)</span> , and  <span class="math">L_{11}, \\dots, L_{20}</span>  are matrices in  <span class="math">\\mathcal{M}_{6\\times 6}(\\mathbb{F}_2)</span> . We do not choose  <span class="math">(6\\times 6)</span>  matrices with large Hamming weights because of hardware limitation of the PC we use. According to the experimental results, we save approximately 55.2% XORs of implementing  <span class="math">L_1, \\dots, L_{10}</span>  in average, in comparison with the previous method (computing the co-ordinates of outputs</p>

    <p class="text-gray-300">&lt;span id=&quot;page-12-0&quot;&gt;&lt;/span&gt;</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Linear Layer</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_1</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_2</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_3</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_4</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_5</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_6</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_7</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_8</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_9</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_{10}</span></th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Min-XOR-Count</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">7</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">5</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">W_H(L_i) - n</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">14</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">14</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">14</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">13</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">13</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">14</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">13</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">13</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">13</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">13</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Linear Layer</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{11}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{12}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{13}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{14}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{15}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{16}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{17}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{18}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{19}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{20}</span></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Linear Layer Min-XOR-Count</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\begin{array}{ c c } L_{11} \\\\ \\hline 6 \\end{array}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\frac{L_{12}}{6}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\frac{L_{13}}{6}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{14}</span> <span class="math">5</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\frac{L_{15}}{6}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\frac{L_{16}}{7}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\frac{L_{17}}{6}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\frac{L_{18}}{7}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\frac{L_{19}}{6}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\frac{L_{20}}{6}</span></td>
          </tr>
        </tbody>
      </table>
    </div>

    <p class="text-gray-300">Table 1: Implementation Cost of Linear Layers</p>

    <p class="text-gray-300">independently). And the corresponding percentage for implementing  <span class="math">L_{11}, \\dots, L_{20}</span>  is 20.8%.</p>

    <p class="text-gray-300">Although we present an algorithm to search for an optimal implementation of a given linear layer, its space/time complexity skyrockets along with the increase of order and minimum XOR count of the given linear layer. For example, in the case when a linear layer L is in  <span class="math">\\mathcal{IM}_{8\\times8}(\\mathbb{F}_2)</span> , the time complexity of Algorithm 2 is  <span class="math">56^r</span> , where r=Min-XOR-Count(L). If r=15 (not very large), then the time complexity of searching for the minimum XOR implementation of L will be  <span class="math">56^{15}\\approx 2^{87}</span> .</p>

    <p class="text-gray-300">In this section, to avoid high computational complexity of searching for optimal implementations of linear layers, we switch to other efficient implementations of them. Our aim is still looking for a P-AE factorization of a given linear layer, but it is not necessarily a minimum XOR implementation of it.</p>

    <p class="text-gray-300">As we know, every matrix  <span class="math">M \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span>  can be transformed to a diagonal matrix with the same rank as M via a series of row/column exchanging and row/column addition. This diagonal matrix must be  <span class="math">I_n</span>  because M's rank is n. According to Lemma 2, M can be transformed to a permutation matrix  <span class="math">P \\in \\mathcal{PM}_{n \\times n}(\\mathbb{F}_2)</span>  via a series of row/column addition. In a form of matrix, there exists a series of additive elementary matrices  <span class="math">R_1, \\cdots, R_r</span>  and  <span class="math">C_1, \\cdots, C_s</span>  such that  <span class="math">R_r \\cdots R_1 M C_1 \\cdots C_s = P</span> . Consequently, we get a P-AE factorization  <span class="math">M = R_1 \\cdots R_r P C_s \\cdots C_1</span> . That indicates left-multiplying an input column  <span class="math">X \\in \\mathbb{F}_2^n</span>  by M with this P-AE factorization uses r+s XORs. If each row/column addition reduces the Hamming weight of the matrix by 1, the implementation cost of this MIOSS procedure will be equal to that of computing co-ordinates of output of M independently, namely,  <span class="math">W_H(M) - n</span> . Therefore, if we want an MIOSS procedure better than computing co-ordinates of output of M independently, the guideline is trying to reduce the Hamming weight of the given linear layer as much as possible by each additive row/column elementary operation in  <span class="math">R_r \\cdots R_1 M C_1 \\cdots C_s = P</span> . To attain this goal, we present Algorithm 3.</p>

    <p class="text-gray-300">In Algorithm 3, r is a variable recording the number of additive elementary operations, and RUB is assigned the difference between the Hamming weight of origin matrix M and its order n. If r exceeds RUB, it is unnecessary to let the program proceed because its output MIOSS procedure will not be better than computing co-ordinates of output of the linear layer independently. In each while loop, Algorithm 3 looks for an additive elementary operation that can reduce the Hamming weight of M most among all additive row and column elementary operations and operate M by it. If the algorithm finally displays &quot;Success&quot;, then we will get a series of additive elementary operations and a permutation matrix. In form of matrix multiplication, we will get  <span class="math">R_r \\cdots R_1 M C_1 \\cdots C_s = P</span> , where each  <span class="math">R_i</span>  and each  <span class="math">C_j</span>  are additive elementary matrices and P is a permutation matrix. Consequently, we will obtain a P-AE factorization  <span class="math">M = R_1 \\cdots R_r P C_s \\cdots C_1</span> .</p>

    <h4 id="sec-16" class="text-lg font-semibold mt-6">&lt;span id=&quot;page-13-0&quot;&gt;&lt;/span&gt;Algorithm 3 Finding an Efficient Implementation of an Invertible Linear Layer</h4>

    <pre><code class="language-text">Require: a positive integer n, a matrix M \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2);
Ensure: a series of additive elementary operations and a permutation matrix in \\mathcal{PM}_{n\\times n}(\\mathbb{F}_2);
  r \\leftarrow 0, RUB \\leftarrow W_H(M) - n;
  while W_H(M) &gt; n and r \\leq RUB do
      let \\alpha_i denote the i-th row of M and \\beta_i denote the i-th column of M for i=1,\\cdots,n, weight decrease \\leftarrow
      W_H(\\alpha_1) - W_H(\\alpha_1 + \\alpha_2), AE \\leftarrow (R, 2, 1);
      for 1 \\le i &lt; j \\le n do
          compute \\alpha_i + \\alpha_j;
          if W_H(\\alpha_i) - W_H(\\alpha_i + \\alpha_j) &gt; weight decrease then
             weightdecrease \\leftarrow W_H(\\alpha_i) - W_H(\\alpha_i + \\alpha_j), AE \\leftarrow (R, j, i);
          end if
          if W_H(\\alpha_j) - W_H(\\alpha_i + \\alpha_j) &gt; weight
decrease then
             weight decrease \\leftarrow W_H(\\alpha_i) - W_H(\\alpha_i + \\alpha_i), AE \\leftarrow (R, i, j);
          end if
      end for
      for 1 \\le i &lt; j \\le n do
          compute \\beta_i + \\beta_j;
          if W_H(\\beta_i) - W_H(\\beta_i + \\beta_j) &gt; weight decrease then
             weightdecrease \\leftarrow W_H(\\beta_i) - W_H(\\beta_i + \\beta_j), AE \\leftarrow (C, j, i);
          if W_H(\\beta_i) - W_H(\\beta_i + \\beta_i) &gt; weight decrease then
             weightdecrease \\leftarrow W_H(\\beta_j) - W_H(\\beta_i + \\beta_j), AE \\leftarrow (C, i, j);
          end if
      end for
      if AE = (R, i, j) then
          add \\alpha_i to \\alpha_i, output AE, r \\leftarrow r + 1;
         add \\beta_i to \\beta_i, output AE, r \\leftarrow r + 1;
      end if
  end while
  if W_H(M) &gt; n then
      print &quot;Fail.&quot;;
  else
      print &quot;Success via r steps.&quot;, output M;
  end if
</code></pre>

    <p class="text-gray-300">One important advantage of Algorithm 3 is that its time/space complexity is much lower than that of Algorithm 2. For a linear layer  <span class="math">M \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> , if Algorithm 3 runs k while loops, then the time complexity of it is  <span class="math">k(n^2-n)</span> . It is a piece of cake in comparison with  <span class="math">(n^2-n)^k</span>  – the time complexity of Algorithm 2 with the same number of loops. Therefore, Algorithm 3 is substantially more practical than Algorithm 2 is. Nevertheless, we have to admit that Algorithm 3 does not necessarily give us an optimal implementation of the input linear layer even though it succeeds.</p>

    <p class="text-gray-300">It is well known that implementation costs of the inverses of many linear layers are higher than that of themselves (the linear layer of AES, for example). As we mentioned before, besides using fewer XORs, another advantage of MIOSS is implementing every invertible linear layer and its inverse with the same cost. This advantage is also valid to the contents of this section. More specifically, if we get an efficient implementation  <span class="math">M = R_1 \\cdots R_r PC_s \\cdots C_1</span>  of a given linear layer  <span class="math">M \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span>  by Algorithm 3, then we immediately obtain an implementation of  <span class="math">M^{-1}</span> :  <span class="math">M^{-1} = C_1 \\cdots C_s P^{-1} R_r \\cdots R_1</span> . Obviously,  <span class="math">M^{-1}</span>  has the same implementation cost as M does. In fact, we recommend a better strategy – for a given linear layer M, one can conduct Algorithm 3 to both M and  <span class="math">M^{-1}</span> , then choose a better (with a fewer number of additive elementary matrices) one from two implementations as the final option.</p>

    <h3 id="sec-17" class="text-xl font-semibold mt-8"><strong>6.1 New Efficient Implementations of the Linear Layers of AES</strong></h3>

    <p class="text-gray-300">As an application of Algorithm <a href="#page-13-0">3,</a> we investigate the linear layers of AES. The linear layer of AES consists of two phases: ShiftRows and MixColumns. We care nothing about ShiftRows since it is just a permutation over co-ordinates of input vectors. MixColumn is the one we really concern about. The Hamming weights of the matrices of encryption MixColumns and decryption MixColumns are 184 and 472, respectively. So, if we implement them by computing co-ordinates of output independently, they will cost 152 XORs and 440 XORs, respectively. However, with Algorithm <a href="#page-13-0">3,</a> we get an efficient implementation of encryption MixColumns with 132 XORs and an efficient implementation of decryption MixColumns with 228 XORs. In other words, we can save 13<em>.</em>16% XORs of computing co-ordinates of output of MixColumns independently and 48<em>.</em>18% XORs of computing co-ordinates of output of the inverse MixColumns independently. We would like to show the design of these efficient implementations below.</p>

    <p class="text-gray-300">Let <em>MC</em> denote the matrix of encryption MixColumns of AES. Then</p>

    <p class="text-gray-300"><span class="math">$MC = \\left( \\begin{array}{cccc} M(0x02) &amp; M(Ox03) &amp; M(Ox01) &amp; M(Ox01) \\\\ M(0x01) &amp; M(0x02) &amp; M(0x03) &amp; M(0x01) \\\\ M(0x01) &amp; M(0x01) &amp; M(0x02) &amp; M(0x03) \\\\ M(0x03) &amp; M(0x01) &amp; M(0x01) &amp; M(0x02) \\end{array} \\right)</span>$</p>

    <p class="text-gray-300">is a matrix in M32×32(F2), where</p>

    <p class="text-gray-300"><span class="math">$M(0x02) = \\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0</span>$</p>

    <p class="text-gray-300">Meanwhile, the matrix of decryption MixColumns of AES is</p>

    <p class="text-gray-300"><span class="math">$MC^{-1} = \\begin{pmatrix} M(0x0e) &amp; M(0x0b) &amp; M(0x0d) &amp; M(0x09) \\\\ M(0x09) &amp; M(0x0e) &amp; M(0x0b) &amp; M(0x0d) \\\\ M(0x0d) &amp; M(0x09) &amp; M(0x0e) &amp; M(0x0b) \\\\ M(0x0b) &amp; M(0x0d) &amp; M(0x09) &amp; M(0x0e) \\end{pmatrix}</span>$</p>

    <p class="text-gray-300"><span class="math">$\\in \\mathcal{M}_{32\\times32}(\\mathbb{F}_2),</span>$</p>

    <p class="text-gray-300">where</p>

    <p class="text-gray-300"><span class="math">$M(0x0e) = \\begin{pmatrix} 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix},</span>$</p>

    <p class="text-gray-300"><span class="math">$M(0x0b) = \\begin{pmatrix} 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix},</span>$</p>

    <p class="text-gray-300"><span class="math">$M(0x0d) = \\begin{pmatrix} 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix},</span>$</p>

    <p class="text-gray-300"><span class="math">$M(0x09) = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix}.</span>$</p>

    <p class="text-gray-300">We have  <span class="math">W_H(MC) = 184</span>  and  <span class="math">W_H(MC^{-1}) = 472</span> .</p>

    <p class="text-gray-300">We conduct Algorithm 3 to MC and  <span class="math">MC^{-1}</span>  but get nothing useful. Therefore, we perform some tricks on them: divide and conquer.</p>

    <p class="text-gray-300">Firstly, let us consider MC. We divide MC to 4 blocks:</p>

    <p class="text-gray-300"><span class="math">$MC = \\left( \\begin{array}{cc} M1 &amp; M2 \\\\ M2 &amp; M1 \\end{array} \\right),</span>$</p>

    <p class="text-gray-300">where</p>

    <p class="text-gray-300"><span class="math">$M1 = \\begin{pmatrix} M(0x02) &amp; M(0x03) \\\\ M(0x01) &amp; M(0x02) \\end{pmatrix},</span>$</p>

    <p class="text-gray-300">and</p>

    <p class="text-gray-300"><span class="math">$M2 = \\left( \\begin{array}{cc} M(0x01) &amp; M(0x01) \\\\ M(0x03) &amp; M(0x01) \\end{array} \\right).</span>$</p>

    <p class="text-gray-300">We easily get  <span class="math">W_H(M1) = 49</span>  and  <span class="math">W_H(M2) = 43</span> . Meanwhile, we let the input vector  <span class="math">X = (X_1, X_2, X_3, X_4)^T</span> , where each  <span class="math">X_i \\in \\mathbb{F}_2^8</span> . Hereby, the output vector can be computed</p>

    <p class="text-gray-300">as</p>

    <p class="text-gray-300">&lt;span id=&quot;page-16-2&quot;&gt;&lt;/span&gt;
<span class="math">$MC \\cdot X = \\begin{pmatrix} M1 &amp; M2 \\\\ M2 &amp; M1 \\end{pmatrix} (X_1, X_2, X_3, X_4)^T</span>$</p>

    <p class="text-gray-300"><span class="math">$= \\begin{pmatrix} M1(X_1, X_2)^T + M2(X_3, X_4)^T \\\\ M2(X_1, X_2)^T + M1(X_3, X_4)^T \\end{pmatrix}.</span>$
(1)</p>

    <p class="text-gray-300">Then the implementation cost of MC is the sum of twice of that of M1 and twice of that of M2 plus 32. We conduct Algorithm 3 to M1 and  <span class="math">M1^{-1}</span>  and obtain an MIOSS implementation of M1 with 46 XORs. But  <span class="math">W_H(M1) = 49</span>  for which we can compute co-ordinates of  <span class="math">M1(X_1, X_2)^T</span>  independently with 49 - 16 = 33 XORs. So, such an MIOSS implementation of M1 is not a good option. However, we may use divide and conquer once again to M1. We conduct Algorithm 3 to M(0x02) and  <span class="math">M(0x02)^{-1}</span>  and obtain an MIOSS implementation of M(0x02) with 3 XORs. This MIOSS implementation of M(0x02) costs the same number of XORs as computing co-ordinates of output vector independently does. So we implement M(0x02) by computing co-ordinates of output vector independently. Next, we conduct Algorithm 3 to M(0x03) and  <span class="math">M(0x03)^{-1}</span>  and obtain an MIOSS implementation of M(0x03) with 9 XORs as follows.</p>

    <p class="text-gray-300">&lt;span id=&quot;page-16-0&quot;&gt;&lt;/span&gt;
<span class="math">$M(0x03) = A_{7,8}A_{6,7}A_{5,6}A_{4,5}A_{3,4} A_{2,3}A_{1,2}A_{8,1}P_{0x03}A_{5,1},</span>$
(2)</p>

    <p class="text-gray-300">where each  <span class="math">A_{i,j} = E_{i,j} + I_8</span>  and  <span class="math">P_{0x03} = I_8</span> . This MIOSS implementation uses less XORs than computing co-ordinates of output of M(0x03) independently does because the latter uses 11 XORs. Implementing M(0x02) by computing co-ordinates of output independently and implementing M(0x03) by the MIOSS method in Equation (2), we can implement M1 by divide and conquer with  <span class="math">3 \\times 2 + 9 + 16 = 31</span>  XORs. It is less than 33 XORs of implementing M1 by computing co-ordinates of output independently. So, we choose divide and conquer to implement M1 as follows:</p>

    <p class="text-gray-300">&lt;span id=&quot;page-16-3&quot;&gt;&lt;/span&gt;
<span class="math">$M1(X_1, X_2)^T = \\begin{pmatrix} M(0x02)X_1^T + M(0x03)X_2^T \\\\ M(0x01)X_1^T + M(0x02)X_2^T \\end{pmatrix},</span>$
(3)</p>

    <p class="text-gray-300">where left-multiplying M(0x02) is implemented by computing co-ordinates of output independently, and left-multiplying M(0x03) is implemented by MIOSS method in Equation 2. Similarly, we investigate the costs of different implementation methods of M2. We conduct Algorithm 3 to M2 and  <span class="math">M2^{-1}</span>  and obtain an MIOSS implementation of M2 with 19 XORs as follows.</p>

    <p class="text-gray-300">&lt;span id=&quot;page-16-1&quot;&gt;&lt;/span&gt;
<span class="math">$M2 = A_{9,1}A_{10,2}A_{11,3}A_{12,4}A_{13,5}A_{14,6}A_{15,7} A_{16,8}A_{1,16}A_{2,9}A_{3,10}A_{4,11}A_{7,14}A_{12,16} A_{5,12}A_{13,16}A_{6,13}A_{15,16}A_{8,15}P_{M2},</span>$</p>

    <p class="text-gray-300"><span class="math">$(4)</span>$</p>

    <p class="text-gray-300">where each  <span class="math">A_{i,j} = E_{i,j} + I_{16}</span>  and  <span class="math">P_{M2}</span>  is described in Table 2. This MIOSS implementation of M2 is better than computing co-ordinates of output of M2 independently because the latter costs 43 - 16 = 27 XORs. In addition, implementing M2 by divide and conquer costs 9 + 16 = 25 XORs which is larger than 19 too. So, we choose the MIOSS procedure in Equation (4) to implement M2. In summary of this paragraph, we choose the divide and conquer procedure in Equation (1) to implement MC, choose the divide and conquer procedure in Equation (3) to implement M1, choose the MIOSS procedure in Equation (2) to implement M(0x03), choose the MIOSS procedure in Equation (4) to implement M2, and finally obtain an efficient implementation of MC with  <span class="math">31 \\times 2 + 19 \\times 2 + 32 = 132</span>  XORs. It saves 13.16% XORs of computing co-ordinates of output of MC independently.</p>

    <p class="text-gray-300">Secondly, let us consider  <span class="math">MC^{-1}</span> . Similar to the case of MC, we divide  <span class="math">MC^{-1}</span>  to 4 blocks:</p>

    <p class="text-gray-300"><span class="math">$MC^{-1} = \\left(\\begin{array}{cc} M3 &amp; M4 \\\\ M4 &amp; M3 \\end{array}\\right),\\,</span>$</p>

    <p class="text-gray-300"><span class="math">\\rho_{M2}(i)</span>  <span class="math">\\rho_{M2}(i)</span></p>

    <p class="text-gray-300">&lt;span id=&quot;page-17-0&quot;&gt;&lt;/span&gt;<strong>Table 2:</strong> The Permutation Matrix  <span class="math">P_{M2}</span>  in the MIOSS Implementation of M2</p>

    <p class="text-gray-300">•  <span class="math">\\rho_{M2}(i)</span>  is the column index of the nonzero entry in <em>i</em>-th row of  <span class="math">P_{M2}</span> .</p>

    <p class="text-gray-300">where</p>

    <p class="text-gray-300"><span class="math">$M3 = \\begin{pmatrix} M(0x0e) &amp; M(0x0b) \\\\ M(0x09) &amp; M(0x0e) \\end{pmatrix},</span>$</p>

    <p class="text-gray-300">and</p>

    <p class="text-gray-300"><span class="math">$M4 = \\begin{pmatrix} M(0x0d) &amp; M(0x09) \\\\ M(0x0b) &amp; M(0x0d) \\end{pmatrix}.</span>$</p>

    <p class="text-gray-300">We easily get  <span class="math">W_H(M3) = 115</span>  and  <span class="math">W_H(M4) = 121</span> . Meanwhile, we let the input vector  <span class="math">Y = (Y_1, Y_2, Y_3, Y_4)^T</span> , where each  <span class="math">Y_i \\in \\mathbb{F}_2^8</span> . Hereby, the output vector can be computed as</p>

    <p class="text-gray-300">&lt;span id=&quot;page-17-2&quot;&gt;&lt;/span&gt;
<span class="math">$MC^{-1} \\cdot Y = \\begin{pmatrix} M3 &amp; M4 \\\\ M4 &amp; M3 \\end{pmatrix} (Y_1, Y_2, Y_3, Y_4)^T</span>$</p>

    <p class="text-gray-300"><span class="math">$= \\begin{pmatrix} M3(Y_1, Y_2)^T + M4(Y_3, Y_4)^T \\\\ M4(Y_1, Y_2)^T + M3(Y_3, Y_4)^T \\end{pmatrix}.</span>$
(5)</p>

    <p class="text-gray-300">Then the implementation cost of  <span class="math">MC^{-1}</span>  is the sum of twice of that of M3 and twice of that of M4 plus 32. For convenience of later investigation, let us consider the implementation costs of M(0x0e), M(0x0b), M(0x0d), and M(0x09) first. We conduct Algorithm 3 to M(0x0e) and  <span class="math">M(0x0e)^{-1}</span>  and obtain an MIOSS implementation of M(0x0e) with 15 XORs. This MIOSS implementation of M(0x0e) uses less XORs than computing co-ordinates of output of M(0x0e) independently does because the latter uses 28-8=20 XORs. Thus, we choose the MIOSS procedure to implement M(0x0e). Likewise, we choose an MIOSS procedure with 15 XORs to implement M(0x0b), choose an MIOSS procedure with 15 XORs to implement M(0x0d), and choose an MIOSS procedure with 11 XORs to implement M(0x0g). Then, we conduct Algorithm 3 to M3 and  <span class="math">M3^{-1}</span>  and obtain an MIOSS implementation of M3 with 52 XORs as follows.</p>

    <p class="text-gray-300">&lt;span id=&quot;page-17-1&quot;&gt;&lt;/span&gt;
<span class="math">$M3 = A_{3,1}A_{5,15}A_{13,7}A_{7,16}A_{4,2}A_{2,1}A_{10,2} A_{11,2}A_{11,3}A_{1,9}A_{8,1}A_{6,16}A_{12,6}A_{6,15} A_{6,7}A_{14,7}A_{1,10}A_{15,1}A_{3,6}A_{5,6}A_{13,6} A_{7,8}A_{4,7}A_{15,8}A_{4,15}A_{9,11}A_{13,10} A_{16,10}A_{14,11}A_{16,11}A_{2,1}A_{2,3}A_{2,5} A_{3,1}A_{3,14}A_{13,3}A_{9,13}A_{1,14}A_{5,1}A_{5,16} A_{12,5}A_{9,12}A_{2,9}A_{16,2}A_{1,16}P_{M3}A_{2,6} A_{12,6}A_{12,4}A_{7,1}A_{2,1}A_{11,12}A_{3,11},</span>$</p>

    <p class="text-gray-300"><span class="math">$(6)</span>$</p>

    <p class="text-gray-300">where each  <span class="math">A_{i,j} = E_{i,j} + I_{16}</span>  and  <span class="math">P_{M3}</span>  is described in Table 3. This MIOSS implementation of M3 is better than computing co-ordinates of output of M3 independently because the latter costs 115 - 16 = 99 XORs. In addition, implementing M3 by divide and conquer costs  <span class="math">15 \\times 2 + 15 + 11 + 16 = 72</span>  XORs which is larger than 52 too. So, we choose the MIOSS procedure in Equation (6) to implement M3. Next, we conduct Algorithm 3 to</p>

    <p class="text-gray-300"><em>i</em> 1 2 3 4 5 6 7 8 <em>ρM</em>3(<em>i</em>) 6 8 5 12 13 14 15 16 <em>i</em> 9 10 11 12 13 14 15 16 <em>ρM</em>3(<em>i</em>) 11 9 10 4 1 2 7 3</p>

    <p class="text-gray-300">&lt;span id=&quot;page-18-0&quot;&gt;&lt;/span&gt;<strong>Table 3:</strong> The Permutation Matrix <em>PM</em>&lt;sup&gt;3&lt;/sup&gt; in the MIOSS Implementation of <em>M</em>3</p>

    <p class="text-gray-300">• <em>ρM</em>3(<em>i</em>) is the column index of the nonzero entry in <em>i</em>-th row of <em>PM</em>3.</p>

    <p class="text-gray-300">&lt;span id=&quot;page-18-1&quot;&gt;&lt;/span&gt;<strong>Table 4:</strong> The Permutation Matrix <em>PM</em>&lt;sup&gt;4&lt;/sup&gt; in the MIOSS Implementation of <em>M</em>4</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">i</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">1</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">2</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">3</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">4</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">5</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">6</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">7</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">8</th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">ρM4(i)</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">9</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">10</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">11</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">12</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">13</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">14</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">15</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">1</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">i</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">9</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">10</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">11</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">12</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">13</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">14</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">15</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">16</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">ρM4(i)</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">8</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">3</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">4</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">5</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">7</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">16</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">2</td>
          </tr>
        </tbody>
      </table>
    </div>

    <p class="text-gray-300">• <em>ρM</em>4(<em>i</em>) is the column index of the nonzero entry in <em>i</em>-th row of <em>PM</em>4.</p>

    <p class="text-gray-300"><em>M</em>4 and <em>M</em>4 &lt;sup&gt;−&lt;/sup&gt;&lt;sup&gt;1&lt;/sup&gt; and obtain an MIOSS implementation of <em>M</em>4 with 46 XORs as follows.</p>

    <p class="text-gray-300">&lt;span id=&quot;page-18-2&quot;&gt;&lt;/span&gt;
<span class="math">$M4 = A_{10,2}A_{5,9}A_{13,1}A_{14,10}A_{11,3}A_{13,5}A_{3,12} A_{12,4}A_{5,14}A_{14,6}A_{2,11}A_{11,5}A_{5,7}A_{5,12} A_{5,15}A_{11,6}A_{12,6}A_{6,13}A_{6,15}A_{15,9}A_{9,8} A_{8,16}P_{M4}A_{12,6}A_{11,6}A_{6,15}A_{15,2}A_{2,12} A_{2,11}A_{2,1}A_{8,9}A_{7,9}A_{16,15}A_{15,7}A_{7,8} A_{16,3}A_{11,3}A_{9,3}A_{2,8}A_{3,12}A_{13,1}A_{1,10} A_{12,4}A_{8,16}A_{10,2}A_{9,1}A_{5,8},</span>$</p>

    <p class="text-gray-300"><span class="math">$(7)</span>$</p>

    <p class="text-gray-300">where each <em>Ai,j</em> = <em>Ei,j</em> +<em>I</em>&lt;sup&gt;16&lt;/sup&gt; and <em>PM</em>&lt;sup&gt;4&lt;/sup&gt; is described in Table <a href="#page-18-1">4.</a> This MIOSS implementation of <em>M</em>4 is better than computing co-ordinates of output of <em>M</em>4 independently because the latter costs 121 − 16 = 105 XORs. In addition, implementing <em>M</em>4 by divide and conquer costs 15 × 2 + 15 + 11 + 16 = 72 XORs which is larger than 46 too. So, we choose the MIOSS procedure in Equation <a href="#page-18-2">(7)</a> to implement <em>M</em>4. In summary of this paragraph, we choose the divide and conquer procedure in Equation <a href="#page-17-2">(5)</a> to implement <em>MC</em>&lt;sup&gt;−&lt;/sup&gt;&lt;sup&gt;1&lt;/sup&gt; , choose the MIOSS procedure in Equation <a href="#page-17-1">(6)</a> to implement <em>M</em>3, choose the MIOSS procedure in Equation <a href="#page-18-2">(7)</a> to implement <em>M</em>4, and finally obtain an efficient implementation of <em>MC</em>&lt;sup&gt;−&lt;/sup&gt;&lt;sup&gt;1&lt;/sup&gt; with 52 × 2 + 46 × 2 + 32 = 228 XORs. It saves 48<em>.</em>18% XORs of computing co-ordinates of output of <em>MC</em>&lt;sup&gt;−&lt;/sup&gt;&lt;sup&gt;1&lt;/sup&gt; independently.</p>

    <h2 id="sec-18" class="text-2xl font-bold"><strong>7 Efficient Implementations of Singular Linear Layers</strong></h2>

    <p class="text-gray-300">In above sections, we talked about minimum-XOR-implementations and efficient implementations of invertible linear layers. However, we are confronted with singular (even not square) matrices sometimes. So, let us pay attention to singular linear layers in this section. Actually, we can find efficient implementations of singular linear layers with a method similar to that we used to find efficient implementations of invertible linear layers.</p>

    <p class="text-gray-300">As we know, every matrix <em>M</em> ∈ M<em>m</em>×<em>n</em>(F2) can be transformed to a rectangular diagonal matrix with the same rank as <em>M</em> via a series of row/column exchanging and row/column addition. <em>rank</em>(<em>M</em>) entries on the diagonal of this rectangular diagonal matrix are 1, and other entries are all 0. According to Lemma <a href="#page-7-0">2,</a> <em>M</em> can be transformed to a matrix <em>B</em> ∈ M<em>m</em>×<em>n</em>(F2) via a series of row/column addition, where all but <em>rank</em>(<em>M</em>) entries of <em>B</em> are 0, each row of <em>B</em> contains one 1 and so does each column of it. In a form of matrix, there exists a series of additive elementary matrices  <span class="math">R_1, \\dots, R_r</span>  in  <span class="math">\\mathcal{AEM}_{m \\times m}(\\mathbb{F}_2)</span>  and a series of  <span class="math">C_1, \\dots, C_s</span>  in  <span class="math">\\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span>  such that  <span class="math">R_r \\dots R_1 M C_1 \\dots C_s = B</span> . Consequently, we get a factorization  <span class="math">M = R_1 \\dots R_r B C_s \\dots C_1</span> . Note that left-multiplying a column  <span class="math">X \\in \\mathbb{F}_2^n</span>  by B results in a column in  <span class="math">\\mathbb{F}_2^m</span>  by using no XOR. For example,</p>

    <p class="text-gray-300"><span class="math">$\\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0</span>$</p>

    <p class="text-gray-300">That indicates left-multiplying an input column  <span class="math">X \\in \\mathbb{F}_2^n</span>  by M with this factorization uses r+s XORs. If each row/column addition reduces the Hamming weight of the matrix by 1, the implementation cost of this MIOSS procedure will be  <span class="math">W_H(M) - rank(M)</span> . It is probably greater than the implementation cost  <span class="math">W_H(M) - m</span>  of computing co-ordinates of output of M independently. Therefore, if we want an MIOSS procedure better than computing co-ordinates of output of M independently, the guideline is trying to reduce the Hamming weight of the given linear layer as much as possible by an additive row/column elementary operation on each step. To attain this goal, we present Algorithm 4.</p>

    <h4 id="sec-19" class="text-lg font-semibold mt-6">&lt;span id=&quot;page-19-0&quot;&gt;&lt;/span&gt;Algorithm 4 Finding an Efficient Implementation of a Linear Layer</h4>

    <pre><code class="language-text">Require: a matrix M \\in \\mathcal{M}_{m \\times n}(\\mathbb{F}_2), rank(M);
Ensure: a series of additive elementary operations and a matrix in \\mathcal{M}_{m \\times n}(\\mathbb{F}_2);
  r \\leftarrow 0, RUB \\leftarrow W_H(M) - rank(M);
  while W_H(M) &gt; rank(M) and r \\leq RUB do
      let \\alpha_i denote the i-th row of M for i=1,\\cdots,m and \\beta_j denote the j-th column of M for j=1,\\cdots,n,
      weightdecrease \\leftarrow W_H(\\alpha_1) - W_H(\\alpha_1 + \\alpha_2), AE \\leftarrow (R, 2, 1);
      for 1 \\le i &lt; j \\le m do
         compute \\alpha_i + \\alpha_j;
         if W_H(\\alpha_i) - W_H(\\alpha_i + \\alpha_j) &gt; weight decrease then
             weightdecrease \\leftarrow W_H(\\alpha_i) - W_H(\\alpha_i + \\alpha_j), AE \\leftarrow (R, j, i);
         end if
         if W_H(\\alpha_j) - W_H(\\alpha_i + \\alpha_j) &gt; weightdecrease then
             weight decrease \\leftarrow W_H(\\alpha_i) - W_H(\\alpha_i + \\alpha_i), AE \\leftarrow (R, i, j);
         end if
      end for
      for 1 \\le i &lt; j \\le n do
          compute \\beta_i + \\beta_j;
          if W_H(\\beta_i) - W_H(\\beta_i + \\beta_i) &gt; weight decrease then
             weight decrease \\leftarrow W_H(\\beta_i) - W_H(\\beta_i + \\beta_j), AE \\leftarrow (C, j, i);
          end if
         if W_H(\\beta_i) - W_H(\\beta_i + \\beta_i) &gt; weight decrease then
             weight decrease \\leftarrow W_H(\\beta_j) - W_H(\\beta_i + \\beta_j), AE \\leftarrow (C, i, j);
          end if
      end for
      if AE = (R, i, j) then
         add \\alpha_i to \\alpha_j, output AE, r \\leftarrow r + 1;
      end if
      if AE = (C, i, j) then
         add \\beta_i to \\beta_i, output AE, r \\leftarrow r + 1;
      end if
  end while
  if W_H(M) &gt; rank(M) then
     print &quot;Fail.&quot;;
   else
     print &quot;Success via r steps.&quot;, output M;
  end if
</code></pre>

    <h2 id="sec-20" class="text-2xl font-bold"><strong>8 Conclusion</strong></h2>

    <p class="text-gray-300">In this paper, we first investigate the effect of two implementation methods on implementation costs of linear layers: computing the co-ordinates of outputs independently and modifying input vectors to outputs step by step. We focus on the latter because it preforms better than the former. Then, we clarify the measurement of implementation cost and optimal implementation procedures of linear layers. Next, to find an optimal implementation procedure of a given linear layer, we construct a graph-theoretical model and transfer the problem to the shortest path problem in graph theory. Then, we adopt a &quot;double-direction&quot; algorithm that uses less storage space and makes the search for a shortest path more efficient in our regular graph. After that, we construct another algorithm for finding efficient implementations of linear layers. The advantages of this algorithm are its low complexity and high practicality. We conduct it to the linear layers of AES and obtain highly efficient implementations of them. To handle more general linear layers, we finally present a practical algorithm for finding efficient implementations of singular linear layers. We wish our work would be beneficial to the design of implementation of linear layers.</p>

    <p class="text-gray-300"><strong>Acknowledgements.</strong> Ruoxin Zhao would like to thank Dr. Meicheng Liu and Dr. Yongqiang Li for sincere discussion and constructive suggestion.</p>

    <h2 id="sec-21" class="text-2xl font-bold"><strong>References</strong></h2>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li><p class="text-gray-300">&lt;span id=&quot;page-20-2&quot;&gt;&lt;/span&gt;[1] Christof Beierle, Thorsten Kranz, and Gregor Leander. Lightweight multiplication in <em>GF</em>(2<em>&lt;sup&gt;n&lt;/sup&gt;</em>) with applications to MDS matrices. In <em>Advances in Cryptology - CRYPTO 2016 - 36th Annual International Cryptology Conference, Santa Barbara, CA, USA, August 14-18, 2016, Proceedings, Part I</em>, pages 625–653, 2016.</p></li>
      <li><p class="text-gray-300">&lt;span id=&quot;page-20-6&quot;&gt;&lt;/span&gt;[2] Béla Bollobás. <em>Modern Graph Theory</em>, volume 184 of <em>Graduate Texts in Mathematics</em>. Springer Science+Business Media, New York City, USA, 1998.</p></li>
      <li><p class="text-gray-300">&lt;span id=&quot;page-20-8&quot;&gt;&lt;/span&gt;[3] Richard A. Brualdi. <em>Introductory Combinatorics</em>. Pearson Education, New York City, USA, 5th edition, 2009.</p></li>
      <li><p class="text-gray-300">&lt;span id=&quot;page-20-1&quot;&gt;&lt;/span&gt;[4] Joan Daemen and Vincent Rijmen. <em>The Design of Rijndael: AES - The Advanced Encryption Standard</em>. Information Security and Cryptography. Springer, 2002.</p></li>
      <li><p class="text-gray-300">&lt;span id=&quot;page-20-7&quot;&gt;&lt;/span&gt;[5] John M. Harris, Jeffry L. Hirst, and Michael J. Mossinghoff. <em>Combinatorics and Graph Theory</em>. Undergraduate Texts in Mathematics. Springer Science+Business Media, New York City, USA, 2nd edition, 2008.</p></li>
      <li><p class="text-gray-300">&lt;span id=&quot;page-20-5&quot;&gt;&lt;/span&gt;[6] Kenneth Hoffman. <em>Linear Algebra</em>. Prentice-Hall, Englewood Cliffs, USA, 2nd edition, 1971.</p></li>
      <li><p class="text-gray-300">&lt;span id=&quot;page-20-4&quot;&gt;&lt;/span&gt;[7] Jérémy Jean, Thomas Peyrin, and Siang Meng Sim. Minimal implementations of linear and non-linear lightweight building blocks. Personal communication, 2015.</p></li>
      <li><p class="text-gray-300">&lt;span id=&quot;page-20-0&quot;&gt;&lt;/span&gt;[8] Yongqiang Li and Mingsheng Wang. On the construction of lightweight circulant involutory MDS matrices. In <em>Fast Software Encryption - 23rd International Conference, FSE 2016, Bochum, Germany, March 20-23, 2016, Revised Selected Papers</em>, pages 121–139, 2016.</p></li>
      <li><p class="text-gray-300">&lt;span id=&quot;page-20-3&quot;&gt;&lt;/span&gt;[9] Meicheng Liu and Siang Meng Sim. Lightweight MDS generalized circulant matrices. In <em>Fast Software Encryption - 23rd International Conference, FSE 2016, Bochum, Germany, March 20-23, 2016, Revised Selected Papers</em>, pages 101–120, 2016.</p></li>
      <li><p class="text-gray-300">&lt;span id=&quot;page-21-0&quot;&gt;&lt;/span&gt;[10] Mahdi Sajadieh, Mohammad Dakhilalian, Hamid Mala, and Pouyan Sepehrdad. Recursive diffusion layers for block ciphers and hash functions. In <em>Fast Software Encryption - 19th International Workshop, FSE 2012, Washington, DC, USA, March 19-21, 2012. Revised Selected Papers</em>, pages 385–401, 2012.</p></li>
      <li><p class="text-gray-300">&lt;span id=&quot;page-21-3&quot;&gt;&lt;/span&gt;[11] Sumanta Sarkar and Siang Meng Sim. A deeper understanding of the XOR count distribution in the context of lightweight cryptography. In <em>Progress in Cryptology - AFRICACRYPT 2016 - 8th International Conference on Cryptology in Africa, Fes, Morocco, April 13-15, 2016, Proceedings</em>, pages 167–182, 2016.</p></li>
      <li><p class="text-gray-300">&lt;span id=&quot;page-21-2&quot;&gt;&lt;/span&gt;[12] Siang Meng Sim, Khoongming Khoo, Frédérique E. Oggier, and Thomas Peyrin. Lightweight MDS involution matrices. In <em>Fast Software Encryption - 22nd International Workshop, FSE 2015, Istanbul, Turkey, March 8-11, 2015, Revised Selected Papers</em>, pages 471–493, 2015.</p></li>
      <li><p class="text-gray-300">&lt;span id=&quot;page-21-1&quot;&gt;&lt;/span&gt;[13] Shengbao Wu, Mingsheng Wang, and Wenling Wu. Recursive diffusion layers for (lightweight) block ciphers and hash functions. In <em>Selected Areas in Cryptography, 19th International Conference, SAC 2012, Windsor, ON, Canada, August 15-16, 2012, Revised Selected Papers</em>, pages 355–371, 2012.</p></li>
    </ul>

    <h2 id="sec-22" class="text-2xl font-bold">&lt;span id=&quot;page-21-4&quot;&gt;&lt;/span&gt;<strong>A Experimental Results</strong></h2>

    <p class="text-gray-300">In this appendix, every <em>Ai,j</em> denotes <em>I&lt;sup&gt;n&lt;/sup&gt;</em> + <em>Ei,j</em> with a proper order <em>n</em>.</p>

    <p class="text-gray-300"><span class="math">$L_1 = \\left(\\begin{array}{ccccc} 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\end{array}\\right) = A_{1,4}A_{4,2}A_{3,4} \\left(\\begin{array}{cccccc} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right) A_{4,1}A_{1,5}A_{5,2}.</span>$</p>

    <p class="text-gray-300"><span class="math">$L_2 = \\left(\\begin{array}{ccccc} 1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\end{array}\\right) = A_{4,1} A_{1,2} A_{5,1} A_{2,3} \\left(\\begin{array}{cccccc} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{array}\\right) A_{2,5} A_{5,3} A_{3,5}.</span>$</p>

    <p class="text-gray-300"><span class="math">$L_{3} = \\begin{pmatrix} 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\end{pmatrix} = A_{3,5}A_{5,3}A_{2,5} \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\end{pmatrix} A_{4,1}A_{1,5}A_{5,2}.</span>$</p>

    <p class="text-gray-300"><span class="math">$L_4 = \\begin{pmatrix} 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\end{pmatrix} = A_{4,2} A_{2,5} A_{5,3} \\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix} A_{4,2} A_{2,1}.</span>$</p>

    <p class="text-gray-300"><span class="math">$L_{5} = \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\end{pmatrix} = A_{2,4}A_{4,5}A_{5,1} \\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} A_{2,3}A_{3,4}A_{4,1}.</span>$</p>

    <p class="text-gray-300"><span class="math">$L_{6} = \\begin{pmatrix} 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1</span>$</p>

    <p class="text-gray-300"><span class="math">$L_{15} = \\begin{pmatrix} 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix} = A_{1,3}A_{2,1}A_{1,6} \\begin{pmatrix} 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\end{pmatrix} = A_{1,4}A_{2,1}A_{1,6}A_{3,1} \\begin{pmatrix} 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0</span>$</p>

    <p class="text-gray-300">0 0 0 1 0 0</p>

    <p class="text-gray-300">0 0 1 1 0 0</p>

`;
---

<BaseLayout title="5 Searching for Optimal Implementations of Invertible Linear... (2016/1118)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2016 &middot; eprint 2016/1118
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <PaperDisclaimer eprintUrl={EPRINT_URL} />
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <Fragment set:html={CONTENT} />

    <PaperHistory slug="5-searching-for-optimal-implementations-of-invertible-2016" />
  </article>
</BaseLayout>
