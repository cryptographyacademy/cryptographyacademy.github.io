---
import BaseLayout from '../../layouts/BaseLayout.astro';
import PaperDisclaimer from '../../components/PaperDisclaimer.astro';
import PaperHistory from '../../components/PaperHistory.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2016/1118';
const CRAWLER = 'marker';
const CONVERTED_DATE = '2026-02-21';
const TITLE_HTML = 'Designing Optimal Implementations of Linear Layers (Full Version)';
const AUTHORS_HTML = 'Ruoxin Zhao, Baofeng Wu, Rui Zhang, Qian Zhang';

const CONTENT = `    <section id="abstract" class="mb-10">
      <h2 class="text-2xl font-bold">Abstract</h2>
      <p class="text-gray-300">Linear layer is a fundamental primitive for computer science, electronic engineering, and telecommunication. The performance of a linear layer depends on two aspects: diffusion ability and implementation cost, where the latter is usually measured by the number of XORs needed to implement it. For many years, linear layers have been implemented by computing co-ordinates of the output independently. With this method, costs are determined only by the matrices representing linear layers. However, we note that the implementation cost of a given linear layer depends not only on its matrix but also on the ways with which we implement it. So, in this paper, we focus on another implementation method: modifying input vectors to output step by step (MIOSS). This method uses fewer XORs than previous methods do and needs no extra temporary register. Besides, this method makes the implementation cost of a linear layer same as that of its inverse. With the new implementation method, we first clarify the measurement of implementation cost and the optimal implementation procedure of linear layers. Here, \`\`optimal&#x27;&#x27; means using fewest XORs. Then, to find the optimal implementation procedure of a given linear layer, we construct a graph-theoretical model and transfer the problem to the shortest path problem in graph theory. Although there has been several algorithms for the shortest path problem, they do not perform best for the graph that we construct in this paper because of its regularity. Therefore, we adopt a new \`\`double-direction&#x27;&#x27; algorithm that uses less storage and makes the search for a shortest path more efficient in a regular graph. Unfortunately, this algorithm is not practical for large size linear layers because of its high space/time complexity. So, we finally construct another algorithm for finding efficient implementations of linear layers. An important advantage of this last algorithm is its extremely low complexity. We conduct it to the linear layer of AES and get very efficient implementations.</p>
      <p class="text-gray-300"><strong>Keywords:</strong> Linear Layer &middot; XOR Count &middot; Equivalence Relation &middot; Regular Graph &middot; The Shortest Path</p>
    </section>

    <section id="sec-3" class="mb-10">
      <h2 class="text-2xl font-bold">3 Measurement of Implementation Costs of Linear Layers</h2>

    <p class="text-gray-300">In this section, we clarify how to measure the implementation costs of linear layers. As we mentioned above, considering invertible linear layers over GF(2) suffices.</p>

    <p class="text-gray-300">In general, the implementation cost of a given linear layer L over GF(2) is measured by the number of additions (XORs) required to compute the output vector LX for input X. In [12], the authors formally presented a measurement of the number of XORs (XORcount) of elements in  <span class="math">GF(2^m)</span>  as well as of whole diffusion layers. Later, some papers ([11, 9, 8]) adopted that measurement. We know that multiplication with a given element in  <span class="math">GF(2^m)</span>  can be represented by an  <span class="math">\\mathbb{F}_2</span> -linear transformation over  <span class="math">\\mathbb{F}_2^m</span> , namely, a matrix L in  <span class="math">\\mathcal{M}_{m\\times m}(\\mathbb{F}_2)</span> . From this viewpoint, their measurement XOR-count is exactly  <span class="math">W_H(L)-m</span>  since co-ordinates of output vector are computed independently. In fact, this notion has been used for years. However, the implementation cost of a given linear layer depends not only on the linear layer itself but also on ways by which we implement it. To show the effect of different implementation methods on implementation costs, we at first give Example 1.</p>

    <h4 id="sec-misc-1" class="text-lg font-semibold mt-6">Example 1. Suppose</h4>

    <p class="text-gray-300"><span class="math">$L = \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\end{pmatrix} \\in \\mathcal{IM}_{4 \\times 4}(\\mathbb{F}_2)</span>$</p>

    <p class="text-gray-300">is a linear transformation over  <span class="math">\\mathbb{F}_2^4</span> . Then for every input vector  <span class="math">X = (x_1, x_2, x_3, x_4)^T \\in GF(2)^4</span> , the corresponding output is</p>

    <p class="text-gray-300"><span class="math">$Y = LX = \\begin{pmatrix} x_1 + x_2 + x_3 \\\\ x_2 \\\\ x_1 + x_2 + x_3 + x_4 \\\\ x_1 + x_2 \\end{pmatrix}.</span>$</p>

    <p class="text-gray-300">We may compute Y with a common method that has been used for years. That is, we compute the co-ordinates of Y independently. In this case, it requires  <span class="math">W_H(L) - 4 = 6</span>  additions (XORs) over GF(2). However, we can adopt another method to compute Y &ndash; modifying the input X to the output LX step by step as Figure 1. In Figure 1, we</p>

    <p class="text-gray-300"><span class="math">$(1) \\begin{bmatrix} x_{1} + &amp; &amp; &amp; &amp; \\\\ x_{2} &amp; &amp; &amp; \\\\ x_{3} &amp; &amp; &amp; \\\\ x_{4} \\end{bmatrix}; (2) \\begin{bmatrix} x_{1} + x_{2} &amp; &amp; &amp; \\\\ x_{2} &amp; &amp; &amp; \\\\ x_{3} + &amp; &amp; &amp; \\\\ x_{4} + &amp; &amp; &amp; \\\\ \\end{bmatrix}; (3) \\begin{bmatrix} x_{1} + x_{2} &amp; &amp; &amp; \\\\ x_{2} &amp; &amp; &amp; \\\\ x_{1} + x_{2} + x_{3} &amp; &amp; \\\\ x_{4} + &amp; &amp; &amp; \\\\ \\end{bmatrix}; (4) \\begin{bmatrix} x_{1} + x_{2} &amp; &amp; &amp; \\\\ x_{1} + x_{2} + x_{3} &amp; &amp; \\\\ x_{1} + x_{2} + x_{3} &amp; &amp; \\\\ x_{1} + x_{2} + x_{3} + x_{4} \\end{bmatrix} = LX.</span>$</p>

    <p class="text-gray-300">Figure 1: Modifying input to output step by step (MIOSS)</p>

    <p class="text-gray-300">compute LX through a series of operations on the co-ordinates of X. These operations can be easily implemented on programmable hardware (ASIC or FPGA, for instance). Step (1) adds  <span class="math">x_2</span>  to  <span class="math">x_1</span>  and remain  <span class="math">x_2, x_3, x_4</span> . So, its delay is  <span class="math">T_X</span> . Likewise, step (2) and step (3) costs  <span class="math">T_X</span>  delay, respectively. Note that step (2) requires only  <span class="math">T_X</span>  delay but not  <span class="math">2T_X</span>  delays because  <span class="math">x_1 + x_2</span>  has been already computed on preceding steps and saved in the</p>

    <p class="text-gray-300">    <img src="_page_6_Figure_2.jpeg" alt="" class="my-4 max-w-full" />
</p>

    <p class="text-gray-300"><strong>Figure 2:</strong> Implementing Example 1 by MIOSS in one clock cycle</p>

    <p class="text-gray-300">register. Step (4) does not cost any delay because it is merely performed by twisting wires. Although we illustrate the procedure by four steps, it can actually be implemented in one clock cycle as Figure 2. Regardless of 4 clock cycles or 1 clock cycle, the whole procedure of the second method uses 3 XORs. It is quite fewer than 6 XORs of the first method. In form of matrices, we can express the second method as  <span class="math">LX = PA_{4,3}A_{3,1}A_{1,2}X</span> , where each  <span class="math">A_{i,j} = I_4 + E_{i,j}</span>  and  <span class="math">P = (\\rho(1), \\rho(2), \\rho(3), \\rho(4))^T = (3, 2, 4, 1)^T</span> .</p>

    <p class="text-gray-300">Computing every co-ordinate independently is indeed easy to implement and has been used for years. However, it is not necessarily the optimal implementation according to Example 1. Here, &quot;optimal&quot; means &quot;using fewest XORs&quot;. To find the optimal implementations of linear layers, we must take implementation procedures into account. Let us give a lemma at first.</p>

    <p class="text-gray-300"><strong>Lemma 1.</strong> Suppose  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> . Then there exists a series of matrices  <span class="math">P_i \\in \\mathcal{PM}_{n \\times n}(\\mathbb{F}_2)</span>  and  <span class="math">A_j \\in \\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span>  such that  <span class="math">L = P_1 A_1 P_2 A_2 \\cdots P_s A_s P_{s+1}</span> .</p>

    <p class="text-gray-300">Proof. As we know, L can be transformed to its equivalent standard form through a series of elementary row/column operations. L's equivalent standard form is  <span class="math">I_n \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span>  because L is invertible. According to the invertibility of elementary operations,  <span class="math">I_n</span>  can be transformed to L through a series of elementary row/column operations. So, L is equal to the product of a series of elementary matrices. Note that multiplicative elementary matrix over GF(2) is just  <span class="math">I_n</span>  and can be omitted from the product. Besides, the product of some exchanging elementary matrices with order n is a permutation matrix. Thus, L is equal to the form  <span class="math">P_1A_1P_2A_2\\cdots P_sA_sP_{s+1}</span> , where  <span class="math">P_i \\in \\mathcal{PM}_{n \\times n}(\\mathbb{F}_2)</span>  for  <span class="math">i=1,\\cdots,s,</span>   <span class="math">A_j \\in \\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span>  for  <span class="math">j=1,\\cdots,s+1</span> .</p>

    <p class="text-gray-300">In accordance with Lemma 1, for every linear layer  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span>  and input vector  <span class="math">X = (x_1, \\cdots, x_n)^T \\in \\mathbb{F}_2^n</span> , the output vector is  <span class="math">LX = P_1A_1P_2A_2\\cdots P_sA_sP_{s+1}X</span>  which means we can get the output vector through a series of co-ordinate permutations or additive elementary operations. On the contrary, every modification procedure from X to LX can be expressed by the form  <span class="math">P_1A_1P_2A_2\\cdots P_sA_sP_{s+1}X</span>  as in Example 1. Note that for implementation of  <span class="math">P_1A_1P_2A_2\\cdots P_sA_sP_{s+1}X</span> , left-multiplying every  <span class="math">P_j</span>  costs 0 XOR while left-multiplying every  <span class="math">A_i</span>  costs 1 XOR. In detail, for a column vector  <span class="math">X = (x_1, \\cdots, x_n)^T</span>  in  <span class="math">\\mathbb{F}_2^n</span>  and a permutation matrix  <span class="math">P = (\\rho(1), \\cdots, \\rho(n))^T</span>  where  <span class="math">\\rho(i)</span>  denotes the column index of nonzero entry in the i-th row of P,</p>

    <p class="text-gray-300"><span class="math">$PX = (x_{\\rho(1)}, x_{\\rho(2)}, \\cdots, x_{\\rho(n)}).</span>$</p>

    <p class="text-gray-300">For a column vector  <span class="math">X = (x_1, \\dots, x_n)^T</span>  in  <span class="math">\\mathbb{F}_2^n</span>  and an additive elementary matrix  <span class="math">A_{i,j} = E_{i,j} + I_n</span>  in  <span class="math">\\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span> , left-multiplying X by  <span class="math">A_{i,j}</span>  will add the j-th row of X to its i-th row. Therefore, L's factorization  <span class="math">P_1A_1P_2A_2\\cdots P_sA_sP_{s+1}</span>  in Lemma 1 indicates the number of XORs used for its implementation. Now we can give the following definition.</p>

    <p class="text-gray-300"><strong>Definition 1.</strong> For a linear layer  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> , its minimum XOR-count Min-XOR-Count(L) is the minimum number of additive elementary matrices  <span class="math">A_i</span>  such that L can be</p>

    <p class="text-gray-300">written as form  <span class="math">P_1A_1P_2A_2\\cdots P_sA_sP_{s+1}</span> , where every  <span class="math">P_i\\in\\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)</span>  and every  <span class="math">A_j\\in\\mathcal{AEM}_{n\\times n}(\\mathbb{F}_2)</span> . And we call an implementation procedure  <span class="math">LX=P_1A_1P_2A_2\\cdots P_rA_rP_{r+1}X</span>  containing Min-XOR-Count(L) additive elementary matrices a minimum-XOR-implementation of L.</p>

    <p class="text-gray-300">In addition to Definition 1, we have other ways to describe the optimal implementation procedures of linear layers. To clarify those ways, we need the following lemma.</p>

    <p class="text-gray-300"><strong>Lemma 2.</strong> Suppose
<span class="math">$P = (\\rho(1), \\dots, \\rho(n))^T \\in \\mathcal{PM}_{n \\times n}(\\mathbb{F}_2)</span>$
,  <span class="math">A_{r,s} = I_n + E_{r,s} \\in \\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span> . Then  <span class="math">PA_{r,s} = A_{\\rho^{-1}(r),\\rho^{-1}(s)}P</span>  where  <span class="math">A_{\\rho^{-1}(r),\\rho^{-1}(s)} = I_n + E_{\\rho^{-1}(r),\\rho^{-1}(s)} \\in \\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span> .</p>

    <p class="text-gray-300">Proof. As we mentioned before, left-multiplying P permutes the rows of  <span class="math">A_{r,s}</span> . In detail, the i-th row of  <span class="math">PA_{r,s}</span>  is the  <span class="math">\\rho(i)</span> -th row of  <span class="math">A_{r,s}</span> . So, the  <span class="math">\\rho^{-1}(r)</span> -th row of  <span class="math">PA_{r,s}</span>  is the r-th row of  <span class="math">A_{r,s}</span> , and the  <span class="math">\\rho^{-1}(s)</span> -th row of  <span class="math">PA_{r,s}</span>  is the s-th row of  <span class="math">A_{r,s}</span> . Obviously,  <span class="math">PA_{r,s}</span>  will become P if we add  <span class="math">\\rho^{-1}(s)</span> -th row of  <span class="math">PA_{r,s}</span>  to  <span class="math">\\rho^{-1}(r)</span> -th row of it. Thus,  <span class="math">A_{\\rho^{-1}(r),\\rho^{-1}(s)}PA_{r,s} = P</span> . Then we get  <span class="math">PA_{r,s} = (A_{\\rho^{-1}(r),\\rho^{-1}(s)})^{-1}P = A_{\\rho^{-1}(r),\\rho^{-1}(s)}P</span>  since  <span class="math">(A_{\\rho^{-1}(r),\\rho^{-1}(s)})^{-1} = A_{\\rho^{-1}(r),\\rho^{-1}(s)}</span> .</p>

    <p class="text-gray-300">Combining Lemma 1 and Lemma 2, we get the next lemma easily. We omit its proof because it is really trivial.</p>

    <p class="text-gray-300"><strong>Lemma 3.</strong> Every  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span>  can be factorized to the form  <span class="math">L = \\prod_{i=1}^s B_i</span>  where every  <span class="math">B_i</span>  is either in  <span class="math">\\mathcal{PM}_{n \\times n}(\\mathbb{F}_2)</span>  or in  <span class="math">\\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span> .</p>

    <p class="text-gray-300">We call every factorization of L as the form in Lemma 3 a P-AE factorization of it. Obviously, the factorization form in Lemma 1 is a P-AE factorization. On the contrary, every P-AE factorization can be written as the form in Lemma 1 since identity matrix is also a permutation matrix, and the product of permutation matrices in  <span class="math">\\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)</span>  is a permutation matrix too. Therefore, the minimum XOR count of a given linear layer  <span class="math">L \\in \\mathcal{IM}_{n\\times n}(\\mathbb{F}_2)</span>  is exactly equal to the minimum number of additive elementary matrices in L's P-AE factorizations. In summary of this paragraph, we present the following theorem.</p>

    <p class="text-gray-300"><strong>Theorem 1.</strong> For every linear layer  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> , Min - XOR - Count(L) is equal to the minimum number of additive elementary matrices in L's P-AE factorizations.</p>

    <p class="text-gray-300">In [1], the authors measured the lowest implementation cost (it is called XOR count in [1]) of a linear layer  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span>  by the minimum number t such that L can be written as  <span class="math">L = P \\prod_{i=1}^t A_i</span>  where  <span class="math">P \\in \\mathcal{PM}_{n \\times n}(\\mathbb{F}_2)</span>  and every  <span class="math">A_i \\in \\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span> . It is easy to see that every factorization of the form  <span class="math">L = P \\prod_{i=1}^t A_i</span>  corresponds to a factorization of the form in Definition 1 according to Lemma 2. So, our definition of Min-XOR-count does not contradict theirs. We shall indicate the advantage of our definition later.</p>

    <p class="text-gray-300">Because a linear layer is often used bidirectionally (for example encryption and decryption), we also need to consider the inverse of it.</p>

    <p class="text-gray-300"><strong>Theorem 2.</strong> For every linear layer  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> ,  <span class="math">Min - XOR - Count(L^{-1})</span>  is equal to Min - XOR - Count(L). Moreover, if a minimum-XOR-implementation of L is  <span class="math">P_1A_1P_2A_2\\cdots P_rA_rP_{r+1}</span> , then  <span class="math">P_{r+1}^{-1}A_rP_r^{-1}\\cdots A_1P_1^{-1}</span>  is a minimum-XOR-implementation of  <span class="math">L^{-1}</span> .</p>

    <p class="text-gray-300"><em>Proof.</em> Suppose a factorization  <span class="math">L=Q_1B_1\\cdots Q_sB_sQ_{s+1}</span>  is a minimum-XOR-implementation of L, where each  <span class="math">Q_i\\in\\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)</span>  and each  <span class="math">B_j\\in\\mathcal{AEM}_{n\\times n}(\\mathbb{F}_2)</span> . Then we have a factorization  <span class="math">L^{-1}=Q_{s+1}^{-1}B_s^{-1}Q_s^{-1}\\cdots B_1^{-1}Q_1^{-1}</span>  of  <span class="math">L^{-1}</span> . Note that the inverse of a permutation matrix is a permutation too, and the inverse of each additive elementary matrix  <span class="math">B_j</span>  is just  <span class="math">B_j</span>  itself. Thus,</p>

    <p class="text-gray-300"><span class="math">$Min - XOR - Count(L) \\ge Min - XOR - Count(L^{-1}).</span>$</p>

    <p class="text-gray-300">Likewise, we can get</p>

    <p class="text-gray-300"><span class="math">$Min - XOR - Count(L^{-1}) \\ge Min - XOR - Count(L).</span>$</p>

    <p class="text-gray-300">So,  <span class="math">Min - XOR - Count(L^{-1})</span>  is equal to Min - XOR - Count(L). And the second part of the theorem is trivial.</p>

    <p class="text-gray-300">At the end of this section, we point out that if a linear layer L' is a power of another linear layer L, for example  <span class="math">L&#x27;=L^5</span> , then minimum-XOR-implementation of L' is better successive 5 times minimum-XOR-implementations of L. That is because successive 5 times minimum-XOR-implementations of L is merely an implementation procedure of L' and it cannot perform better than minimum-XOR-implementation of L'. Consequently, if  <span class="math">L&#x27;=L^r</span> , then  <span class="math">Min-XOR-Count(L&#x27;) \\leq rMin-XOR-Count(L)</span> .</p>

    </section>

    <section id="sec-4" class="mb-10">
      <h2 class="text-2xl font-bold">4 A Graph-Theoretical Model</h2>

    <p class="text-gray-300">After clarifying the measurement of the lowest implementation cost of linear layers, we are confronted by two problems:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>How to calculate the Min-XOR-Count of a given linear layer  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> ;</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>How to get a minimum-XOR-implementation of a given linear layer  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> .</li>
    </ol></li>
    </ul>

    <p class="text-gray-300">Obviously, the 1st problem will be trivial provided that we solve the 2nd one. In this section, we would like to handle the two problems by a graph-theoretical model.</p>

    <p class="text-gray-300">First of all, we introduce a relation over  <span class="math">\\mathcal{IM}_{n\\times n}(\\mathbb{F}_2)</span> . We say two matrices  <span class="math">B,C\\in\\mathcal{IM}_{n\\times n}(\\mathbb{F}_2)</span>  are row-permutation-equivalent (denoted by  <span class="math">B\\sim_{RP}C</span> ) if there exists a  <span class="math">Q\\in\\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)</span>  such that B=QC. For every  <span class="math">B\\in\\mathcal{IM}_{n\\times n}(\\mathbb{F}_2)</span> ,  <span class="math">B\\sim_{RP}B</span>  since  <span class="math">B=I_nB</span> . If  <span class="math">B\\sim_{RP}C</span> , then  <span class="math">C\\sim_{RP}B</span>  because  <span class="math">C=Q^{-1}B</span>  and  <span class="math">Q^{-1}\\in\\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)</span>  too. If there exists  <span class="math">Q_1,Q_2\\in\\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)</span>  such that  <span class="math">B=Q_1C</span>  and  <span class="math">C=Q_2D</span> , then  <span class="math">B=Q_1Q_2D</span>  and  <span class="math">Q_1Q_2\\in\\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)</span> . Therefore, &quot; <span class="math">\\sim_{RP}</span> &quot; is an equivalence relation over  <span class="math">\\mathcal{IM}_{n\\times n}(\\mathbb{F}_2)</span> . For every  <span class="math">B\\in\\mathcal{IM}_{n\\times n}(\\mathbb{F}_2)</span> , let  <span class="math">[B]_{RP}</span>  denote the equivalence class containing B under &quot; <span class="math">\\sim_{RP}</span> &quot;. It is easy to see that for every  <span class="math">B\\in\\mathcal{IM}_{n\\times n}(\\mathbb{F}_2)</span> ,  <span class="math">[B]_{RP}=\\{QB|Q\\in\\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)\\}</span> , and  <span class="math">Q_1B\\neq Q_2B</span>  if  <span class="math">Q_1\\neq Q_2</span> .</p>

    <p class="text-gray-300">Next, we define a graph dependent on row-permutation-equivalence over  <span class="math">\\mathcal{IM}_{n\\times n}(\\mathbb{F}_2)</span> .</p>

    <p class="text-gray-300"><strong>Definition 2.</strong> For a positive integer n, let G(n) = (V, E) be a graph where the vertex set consists of all the equivalence classes under row-permutation-equivalence relation over  <span class="math">\\mathcal{IM}_{n\\times n}(\\mathbb{F}_2)</span> . And two vertices  <span class="math">[B]_{RP}</span>  and  <span class="math">[C]_{RP}</span>  are adjacent if there exists  <span class="math">B&#x27; \\in [B]_{RP}</span> ,  <span class="math">C&#x27; \\in [C]_{RP}</span>  and  <span class="math">A \\in \\mathcal{AEM}_{n\\times n}(\\mathbb{F}_2)</span>  such that B' = AC'.</p>

    <p class="text-gray-300">Now let us show some useful properties of the graph in Definition 2.</p>

    <p class="text-gray-300"><strong>Theorem 3.</strong> Let G(n) = (V, E) be the graph described in Definition 2 and  <span class="math">[B]_{RP}</span>  be a vertex of G(n). Then G(n) is an  <span class="math">(n^2 - n)</span> -regular graph,  <span class="math">[A_1B]_{RP} \\neq [A_2B]_{RP}</span>  for distinct  <span class="math">A_1, A_2 \\in \\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span> , and  <span class="math">[AB]_{RP}</span>  runs all the vertices adjacent to  <span class="math">[B]_{RP}</span>  when A runs over  <span class="math">\\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span> .</p>

    <p class="text-gray-300"><em>Proof.</em> If  <span class="math">[A_1B]_{RP} = [A_2B]_{RP}</span> , then  <span class="math">A_1B = PA_2B</span>  for some  <span class="math">P \\in \\mathcal{PM}_{n \\times n}(\\mathbb{F}_2)</span> . Consequently, we get  <span class="math">A_1 = PA_2</span>  by right-multiplying two sides of  <span class="math">A_1B = PA_2B</span>  by  <span class="math">B^{-1}</span> . We assert that P must be  <span class="math">I_n</span> . Otherwise, some entries on the diagonal of  <span class="math">PA_2</span>  would be 0 and  <span class="math">PA_2</span>  cannot be equal to  <span class="math">A_1</span> . Then  <span class="math">A_1 = A_2</span> . So,  <span class="math">[A_1B]_{RP} \\neq [A_2B]_{RP}</span>  for distinct  <span class="math">A_1, A_2 \\in \\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span> .</p>

    <p class="text-gray-300">On the other hand, if a vertex  <span class="math">[C]_{RP}</span>  is adjacent to  <span class="math">[B]_{RP}</span> , there exists  <span class="math">B&#x27; \\in [B]_{RP}</span> ,  <span class="math">C&#x27; \\in [C]_{RP}</span>  and  <span class="math">A \\in \\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span>  such that B' = AC'. Then AB' = C' since  <span class="math">A^{-1} = A</span> .</p>

    <p class="text-gray-300">Consequently, there exists  <span class="math">P_2 \\in \\mathcal{PM}_{n \\times n}(\\mathbb{F}_2)</span>  and  <span class="math">A&#x27; \\in \\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span>  such that  <span class="math">C&#x27; = AB&#x27; = AP_1B = P_2A&#x27;B</span>  according to Lemma 2. So,  <span class="math">[C]_{RP} = [C&#x27;]_{RP} = [P_2A&#x27;B]_{RP} = [A&#x27;B]_{RP}</span> . That shows  <span class="math">[AB]_{RP}</span>  runs all the vertices adjacent to  <span class="math">[B]_{RP}</span>  when A runs over  <span class="math">\\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span> .</p>

    <p class="text-gray-300">In summary of two preceding paragraphs, the degree of every vertex of G(n) is the cardinality of  <span class="math">\\mathcal{AEM}_{n\\times n}(\\mathbb{F}_2)</span> , namely,  <span class="math">n^2-n</span> .</p>

    <p class="text-gray-300">Finally, we present a significant theorem that explains why we set up the graph-theoretical model.</p>

    <p class="text-gray-300"><strong>Theorem 4.</strong> Let G(n) = (V, E) be the graph described in Definition 2 and  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> . Then the minimum XOR-count of L is equal to the distance between vertices  <span class="math">[L]_{RP}</span>  and  <span class="math">[I_n]_{RP}</span>  in G(n).</p>

    <p class="text-gray-300"><em>Proof.</em> For every factorization of L with the form  <span class="math">P_1A_1P_2A_2\\cdots P_sA_sP_{s+1}</span> , where every  <span class="math">P_i \\in \\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)</span>  and every  <span class="math">A_j \\in \\mathcal{AEM}_{n\\times n}(\\mathbb{F}_2)</span> , there exists a path</p>

    <p class="text-gray-300"><span class="math">$[L]_{RP} = [P_1 A_1 P_2 A_2 \\cdots P_s A_s P_{s+1}]_{RP} - [P_2 A_2 \\cdots P_s A_s P_{s+1}]_{RP} - \\cdots - [P_s A_s P_{s+1}]_{RP} - [P_{s+1}]_{RP} = [I_n]_{RP}</span>$</p>

    <p class="text-gray-300">between  <span class="math">[L]_{RP}</span>  and  <span class="math">[I_n]_{RP}</span>  of length s.</p>

    <p class="text-gray-300">On the other hand, for every path</p>

    <p class="text-gray-300"><span class="math">$[L]_{RP} - [L_{r-1}]_{RP} - \\dots - [L_2]_{RP} - [L_1]_{RP} - [I_n]_{RP}</span>$</p>

    <p class="text-gray-300">of length r, according to Theorem 3, there must be  <span class="math">L=Q_{r-1}A_{r-1}L_{r-1}</span> ,  <span class="math">L_j=Q_{j-1}A_{j-1}L_{j-1}</span>  for  <span class="math">j=2,\\cdots,r-1</span> , and  <span class="math">L_1=Q_0A_0I_n</span>  for some  <span class="math">Q_i\\in\\mathcal{PM}_{n\\times n}(\\mathbb{F}_2)</span>  and  <span class="math">A_i\\in\\mathcal{AEM}_{n\\times n}(\\mathbb{F}_2)</span>  for  <span class="math">i=0,1,\\cdots,r</span> . Consequently,  <span class="math">L=Q_{r-1}A_{r-1}\\cdots Q_1A_1Q_0A_0I_n</span>  which is a factorization of L with the form in Lemma 1.</p>

    <p class="text-gray-300">Therefore, the minimum XOR-count of L is equal to the minimum length of paths between  <span class="math">[L]_{RP}</span>  and  <span class="math">[I_n]_{RP}</span> , namely, their distance.</p>

    <p class="text-gray-300">From the proof of Theorem 4, we can easily see that a shortest path between  <span class="math">[L]_{RP}</span>  and  <span class="math">[I_n]_{RP}</span>  indicates a minimum-XOR-implementation of L.</p>

    <p class="text-gray-300">In this section, we talk about how to get a minimum-XOR-implementation of a given linear layer  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> . In accordance with the preceding section, this question is equivalent to finding a shortest path between  <span class="math">[L]_{RP}</span>  and  <span class="math">[I_n]_{RP}</span> .</p>

    <p class="text-gray-300">Let us sketch our strategy firstly. Suppose G(V, E) is a connected graph,  <span class="math">a, b \\in V</span> . To find a shortest path between a and b, we let  <span class="math">\\mathcal{A}_0 = \\{a\\}</span> ,  <span class="math">\\mathcal{B}_0 = \\{b\\}</span>  and check whether a = b. If a = b, we do not have to do anything. If  <span class="math">a \\neq b</span> , we construct a set  <span class="math">\\mathcal{A}_1</span>  consisting of the vertices of G that are adjacent to a. In other words,  <span class="math">\\mathcal{A}_1</span>  consists of the vertices having distance 1 from a. Then we check whether there exists  <span class="math">x \\in \\mathcal{A}_1</span>  such that x = b. If there is, we get a shortest path a - b between a and b. If there is not, we construct a set  <span class="math">\\mathcal{B}_1</span>  consisting of the vertices of G that are adjacent to b. In other words,  <span class="math">\\mathcal{B}_1</span>  consists of the vertices having distance 1 from b. Then we check whether there exists  <span class="math">x \\in \\mathcal{A}_1</span>  and  <span class="math">y \\in \\mathcal{B}_1</span>  such that x = y. If there is, we get a shortest path  <span class="math">a - a_1^* - b</span>  between a and b where  <span class="math">a_1^* \\in \\mathcal{A}_1</span> . If there is not, we proceed the procedure above: check and move forwards one step from  <span class="math">\\mathcal{A}_i</span> , then check and move forwards one step from  <span class="math">\\mathcal{B}_i</span> . Finally, we will find</p>

    <p class="text-gray-300"><span class="math">x \\in \\mathcal{A}_{k+1}</span>  and  <span class="math">y \\in \\mathcal{B}_{k+1}</span>  (or  <span class="math">y \\in \\mathcal{B}_k</span> ) for some k such that x = y. As a result, we find a shortest path</p>

    <pre><code class="language-text">a - a_1^* - \\dots - a_k^* - a_{k+1}^* - b_k^* - \\dots - b_1^* - b  (or a - a_1^* - \\dots - a_k^* - b_k^* - \\dots - b_1^* - b)
</code></pre>

    <p class="text-gray-300">between a and b, where  <span class="math">a_i^* \\in \\mathcal{A}_i</span> ,  <span class="math">b_i^* \\in \\mathcal{B}_i</span> , every pair  <span class="math">a_i^*</span> ,  <span class="math">a_{i+1}^*</span>  and every pair  <span class="math">b_j^*</span> ,  <span class="math">b_{j+1}^*</span>  are adjacent. We formally describe the above procedures in Algorithm 1 with pseudocode.</p>

    <h4 id="sec-misc-2" class="text-lg font-semibold mt-6">Algorithm 1 Double-Direction Search for a Shortest Path</h4>

    <pre><code class="language-text">Require: a graph G = (V, E), a, b \\in V.
Ensure: a shortest path between a and b.
   \\mathcal{A}_0 \\leftarrow \\{a\\}, \\, \\mathcal{B}_0 \\leftarrow \\{b\\}, \\, i \\leftarrow 0, \\, j \\leftarrow 0, \\, link \\leftarrow 0;
   while link = 0 do
       if there exists a_i^* \\in \\mathcal{A}_i and b_i^* \\in \\mathcal{B}_j such that a_i^* = b_i^* then
           link \\leftarrow 1:
           continue:
       else
           i \\leftarrow i + 1;
           construct a set A_i consisting of the vertices adjacent to some vertex in A_{i-1} and not in \\bigcup_{k=0}^{i-1} A_k;
       if there exists a_i^* \\in \\mathcal{A}_i and b_i^* \\in \\mathcal{B}_j such that a_i^* = b_i^* then
           continue;
       &#1608;&#1575;&#1581;&#1608;
           j \\leftarrow j + 1;
           construct a set \\mathcal{B}_j consisting of the vertices adjacent to some vertex in \\mathcal{B}_{j-1} and not in \\bigcup_{k=0}^{j-1} \\mathcal{B}_k;
       end if
   end while
   return the path a_0^* - \\cdots - a_i^* - b_{i-1}^* - \\cdots - b_0^* such that every pair a_k^*, a_{k+1} and every pair b_l^*, b_{l+1}^*
   are adjacent;
</code></pre>

    <p class="text-gray-300"><strong>Lemma 4.</strong> Suppose G = (V, E) is a graph,  <span class="math">a, b \\in V</span> . Then we get a shortest path between a and b with Algorithm 1.</p>

    <p class="text-gray-300"><em>Proof.</em> Assume there is a path  <span class="math">a-c_1-\\cdots-c_{r-1}-b</span>  shorter than the output of Algorithm 1. Then  <span class="math">c_k \\in \\mathcal{A}_k</span>  for  <span class="math">k=1,\\cdots,\\lceil\\frac{r-1}{2}\\rceil</span> , and  <span class="math">c_{r-l} \\in \\mathcal{B}_l</span>  for  <span class="math">l=1,\\cdots,\\lfloor\\frac{r-1}{2}\\rfloor</span> . Consequently, there exists  <span class="math">a_k^* \\in \\mathcal{A}_k</span> ,  <span class="math">b_l^* \\in \\mathcal{B}_l</span>  such that  <span class="math">a_k^* = b_l^*</span>  for some k &lt; i or some l &lt; j which contradicts Algorithms 1.</p>

    <p class="text-gray-300">Algorithm 1 is a generic method for any graph. Now we use it to handle our main target: a minimum-XOR-implementation of a given linear layer  <span class="math">L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> . For this target, we present Algorithm 2 and omit the proof of its correctness because it directly comes from Algorithm 1. Here, we just give an explanation of Algorithm 2 as follows.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>We choose an element in every equivalence class to represent it. For example,  <span class="math">L_i^*</span>  represents the class  <span class="math">[L_i^*]_{RP}</span> . Hence, we determine  <span class="math">[L_i]_{RP} = [B_j]_{RP}</span>  by checking  <span class="math">L_i \\sim_{RP} B_j</span> .</li>
      <li>When we need to determine whether two invertible matrices  <span class="math">L_i</span>  and  <span class="math">B_j</span>  are row-permutation-equivalent, we check the Hamming weight of  <span class="math">L_i B_j^{-1}</span>  since  <span class="math">L_i \\sim_{RP} B_j</span>  if and only if  <span class="math">W_H(L_i B_j^{-1}) = n</span> . This method can be implemented easily when B is a product of some additive elementary matrices. More explicitly, if  <span class="math">B = A_1 \\cdots A_s</span> ,  <span class="math">B^{-1} = A_s \\cdots A_1</span> .</li>
    </ul>

    <p class="text-gray-300">For a given graph and two vertices of it, there has already been algorithms in literature for finding the shortest path of them. For example, the famous Dijkstra's algorithm ([3], Chapter 11, page 443). Certainly, we can adopt it to solve the main problem in this paper. Essentially, Dijkstra's algorithm is a single-direction search, while our algorithms are</p>

    <h4 id="sec-misc-3" class="text-lg font-semibold mt-6">Algorithm 2 Finding a Minimum-XOR-Implementation of an Invertible Linear Layer</h4>

    <pre><code class="language-text">Require: a positive integer n, a matrix L \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2).
Ensure: a minimum-XOR-implementation of L.
   \\mathcal{L}_0 \\leftarrow \\{L\\}, \\, \\mathcal{B}_0 \\leftarrow \\{I_n\\}, \\, i \\leftarrow 0, \\, j \\leftarrow 0, \\, link \\leftarrow 0;
   while link = 0 do
       if there exists L_i^* \\in \\mathcal{L}_i and B_j^* \\in \\mathcal{B}_j such that L_i^* \\sim_{RP} B_i^* then
           link \\leftarrow 1:
           continue;
       else
           construct a set \\mathcal{L}_i consisting of the matrices having the form A_{r,s}L_{i-1} and not row-permutation-
           equivalent to any matrix in \\bigcup_{k=0}^{i-1} \\mathcal{L}_k, where A_{r,s} runs over \\mathcal{AEM}_{n\\times n}(\\mathbb{F}_2) and L_{i-1} runs over
       end if
       if there exists L_i^* \\in \\mathcal{L}_i and B_i^* \\in \\mathcal{B}_j such that L_i^* \\sim_{RP} B_i^* then
           link \\leftarrow 1:
           continue;
       else
           j \\leftarrow j + 1;
           construct a set \\mathcal{B}_j consisting of the matrices having the form A_{r,s}B_{j-1} and not row-permutation-
           equivalent to any matrix in \\bigcup_{k=0}^{j-1} \\mathcal{B}_k, where A_{r,s} runs over \\mathcal{AEM}_{n\\times n}(\\mathbb{F}_2) and B_{j-1} runs over
           \\mathcal{B}_{i-1}
       end if
   end while
   return the path L_0^* - \\cdots - L_i^* - B_{i-1}^* - \\cdots - B_0^* such that every pair L_k^*, L_{k+1} and every pair B_l^*, B_{l+1}^*
   are adjacent;
</code></pre>

    <p class="text-gray-300">double-direction search. Let us show what will happen if we handle our main problem with Dijkstra's algorithm. We use the same notations as that in Algorithm 1. We start from vertex a to find a shortest path between a and b. By means of Dijkstra's algorithm, we need to construct a series of sets  <span class="math">A_i</span>  consisting of vertices whose distance to a is i for  <span class="math">i=1,2,\\cdots</span> . According to Theorem 3,  <span class="math">|A_1|=n^2-n</span> , and  <span class="math">|A_i|=(n^2-n)(n^2-n-1)^{i-1}</span>  for  <span class="math">i\\geq 2</span>  in the worst case. If the distance between a and b is  <span class="math">s\\geq 2</span> , the algorithm will not terminate until  <span class="math">A_s</span>  is constructed. We see the cardinality of  <span class="math">A_i</span> s increase too fast and will occupy too much space. Actually, that is the reason why we abandon single-direction strategies and adopt a double-direction strategy &ndash; moving forwards step by step from a and b alternately. With a double-direction strategy, we can save a lot of memory space and modify the search for the shortest path. For instance, suppose the distance between a and b is s=2t. If we use our method, then we need to construct  <span class="math">A_i</span> ,  <span class="math">B_i</span>  for  <span class="math">i=1,\\cdots,t</span> , where  <span class="math">|A_1|=|B_1|=n^2-n</span> ,  <span class="math">|A_2|=|B_2|=(n^2-n)(n^2-n-1),\\cdots,|A_t|=|B_t|=(n^2-n)(n^2-n-1)^{t-1}</span>  in the worst case. However, if we use a single-direction strategy, we have to construct  <span class="math">A_i</span>  for  <span class="math">i=1,\\cdots,2t</span> , where  <span class="math">|A_1|=n^2-n</span> ,  <span class="math">|A_2|=(n^2-n)(n^2-n-1),\\cdots,|A_t|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> ,  <span class="math">|A_{t+1}|=(n^2-n)(n^2-n-1)^{t-1}</span> .</p>

      <h3 id="sec-5.1" class="text-xl font-semibold mt-8">5.1 Experimental Results</h3>

    <p class="text-gray-300">We search the minimum XOR implementations of many linear layers and list some of them in Appendix A. The optimal implementation procedure of each one is like what we show in Example 1. Meanwhile, we list the minimum XOR count and the difference between Hamming weight and the order of the linear layers in Table 1, where the former indicates implementation cost of our strategy and the latter indicates the cost of computing co-ordinates of output independently.  <span class="math">L_1, \\dots, L_{10}</span>  in Table 1 are matrices in  <span class="math">\\mathcal{M}_{5\\times 5}(\\mathbb{F}_2)</span> , and  <span class="math">L_{11}, \\dots, L_{20}</span>  are matrices in  <span class="math">\\mathcal{M}_{6\\times 6}(\\mathbb{F}_2)</span> . We do not choose  <span class="math">(6\\times 6)</span>  matrices with large Hamming weights because of hardware limitation of the PC we use. According to the experimental results, we save approximately 55.2% XORs of implementing  <span class="math">L_1, \\dots, L_{10}</span>  in average, in comparison with the previous method (computing the co-ordinates of outputs</p>

    <p class="text-gray-300"></p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Linear Layer</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_1</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_2</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_3</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_4</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_5</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_6</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_7</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_8</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_9</span></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"><span class="math">L_{10}</span></th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Min-XOR-Count</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">7</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">5</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">W_H(L_i) - n</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">14</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">14</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">14</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">13</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">13</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">14</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">13</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">13</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">13</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">13</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Linear Layer</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{11}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{12}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{13}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{14}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{15}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{16}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{17}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{18}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{19}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{20}</span></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Linear Layer Min-XOR-Count</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\begin{array}{ c c } L_{11} \\\\ \\hline 6 \\end{array}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\frac{L_{12}}{6}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\frac{L_{13}}{6}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">L_{14}</span> <span class="math">5</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\frac{L_{15}}{6}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\frac{L_{16}}{7}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\frac{L_{17}}{6}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\frac{L_{18}}{7}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\frac{L_{19}}{6}</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">\\frac{L_{20}}{6}</span></td>
          </tr>
        </tbody>
      </table>
    </div>

    <p class="text-gray-300">Table 1: Implementation Cost of Linear Layers</p>

    <p class="text-gray-300">independently). And the corresponding percentage for implementing  <span class="math">L_{11}, \\dots, L_{20}</span>  is 20.8%.</p>

    <p class="text-gray-300">Although we present an algorithm to search for an optimal implementation of a given linear layer, its space/time complexity skyrockets along with the increase of order and minimum XOR count of the given linear layer. For example, in the case when a linear layer L is in  <span class="math">\\mathcal{IM}_{8\\times8}(\\mathbb{F}_2)</span> , the time complexity of Algorithm 2 is  <span class="math">56^r</span> , where r=Min-XOR-Count(L). If r=15 (not very large), then the time complexity of searching for the minimum XOR implementation of L will be  <span class="math">56^{15}\\approx 2^{87}</span> .</p>

    <p class="text-gray-300">In this section, to avoid high computational complexity of searching for optimal implementations of linear layers, we switch to other efficient implementations of them. Our aim is still looking for a P-AE factorization of a given linear layer, but it is not necessarily a minimum XOR implementation of it.</p>

    <p class="text-gray-300">As we know, every matrix  <span class="math">M \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span>  can be transformed to a diagonal matrix with the same rank as M via a series of row/column exchanging and row/column addition. This diagonal matrix must be  <span class="math">I_n</span>  because M's rank is n. According to Lemma 2, M can be transformed to a permutation matrix  <span class="math">P \\in \\mathcal{PM}_{n \\times n}(\\mathbb{F}_2)</span>  via a series of row/column addition. In a form of matrix, there exists a series of additive elementary matrices  <span class="math">R_1, \\cdots, R_r</span>  and  <span class="math">C_1, \\cdots, C_s</span>  such that  <span class="math">R_r \\cdots R_1 M C_1 \\cdots C_s = P</span> . Consequently, we get a P-AE factorization  <span class="math">M = R_1 \\cdots R_r P C_s \\cdots C_1</span> . That indicates left-multiplying an input column  <span class="math">X \\in \\mathbb{F}_2^n</span>  by M with this P-AE factorization uses r+s XORs. If each row/column addition reduces the Hamming weight of the matrix by 1, the implementation cost of this MIOSS procedure will be equal to that of computing co-ordinates of output of M independently, namely,  <span class="math">W_H(M) - n</span> . Therefore, if we want an MIOSS procedure better than computing co-ordinates of output of M independently, the guideline is trying to reduce the Hamming weight of the given linear layer as much as possible by each additive row/column elementary operation in  <span class="math">R_r \\cdots R_1 M C_1 \\cdots C_s = P</span> . To attain this goal, we present Algorithm 3.</p>

    <p class="text-gray-300">In Algorithm 3, r is a variable recording the number of additive elementary operations, and RUB is assigned the difference between the Hamming weight of origin matrix M and its order n. If r exceeds RUB, it is unnecessary to let the program proceed because its output MIOSS procedure will not be better than computing co-ordinates of output of the linear layer independently. In each while loop, Algorithm 3 looks for an additive elementary operation that can reduce the Hamming weight of M most among all additive row and column elementary operations and operate M by it. If the algorithm finally displays &quot;Success&quot;, then we will get a series of additive elementary operations and a permutation matrix. In form of matrix multiplication, we will get  <span class="math">R_r \\cdots R_1 M C_1 \\cdots C_s = P</span> , where each  <span class="math">R_i</span>  and each  <span class="math">C_j</span>  are additive elementary matrices and P is a permutation matrix. Consequently, we will obtain a P-AE factorization  <span class="math">M = R_1 \\cdots R_r P C_s \\cdots C_1</span> .</p>

    <h4 id="sec-misc-4" class="text-lg font-semibold mt-6">Algorithm 3 Finding an Efficient Implementation of an Invertible Linear Layer</h4>

    <pre><code class="language-text">Require: a positive integer n, a matrix M \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2);
Ensure: a series of additive elementary operations and a permutation matrix in \\mathcal{PM}_{n\\times n}(\\mathbb{F}_2);
  r \\leftarrow 0, RUB \\leftarrow W_H(M) - n;
  while W_H(M) &gt; n and r \\leq RUB do
      let \\alpha_i denote the i-th row of M and \\beta_i denote the i-th column of M for i=1,\\cdots,n, weight decrease \\leftarrow
      W_H(\\alpha_1) - W_H(\\alpha_1 + \\alpha_2), AE \\leftarrow (R, 2, 1);
      for 1 \\le i &lt; j \\le n do
          compute \\alpha_i + \\alpha_j;
          if W_H(\\alpha_i) - W_H(\\alpha_i + \\alpha_j) &gt; weight decrease then
             weightdecrease \\leftarrow W_H(\\alpha_i) - W_H(\\alpha_i + \\alpha_j), AE \\leftarrow (R, j, i);
          end if
          if W_H(\\alpha_j) - W_H(\\alpha_i + \\alpha_j) &gt; weight
decrease then
             weight decrease \\leftarrow W_H(\\alpha_i) - W_H(\\alpha_i + \\alpha_i), AE \\leftarrow (R, i, j);
          end if
      end for
      for 1 \\le i &lt; j \\le n do
          compute \\beta_i + \\beta_j;
          if W_H(\\beta_i) - W_H(\\beta_i + \\beta_j) &gt; weight decrease then
             weightdecrease \\leftarrow W_H(\\beta_i) - W_H(\\beta_i + \\beta_j), AE \\leftarrow (C, j, i);
          if W_H(\\beta_i) - W_H(\\beta_i + \\beta_i) &gt; weight decrease then
             weightdecrease \\leftarrow W_H(\\beta_j) - W_H(\\beta_i + \\beta_j), AE \\leftarrow (C, i, j);
          end if
      end for
      if AE = (R, i, j) then
          add \\alpha_i to \\alpha_i, output AE, r \\leftarrow r + 1;
         add \\beta_i to \\beta_i, output AE, r \\leftarrow r + 1;
      end if
  end while
  if W_H(M) &gt; n then
      print &quot;Fail.&quot;;
  else
      print &quot;Success via r steps.&quot;, output M;
  end if
</code></pre>

    <p class="text-gray-300">One important advantage of Algorithm 3 is that its time/space complexity is much lower than that of Algorithm 2. For a linear layer  <span class="math">M \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span> , if Algorithm 3 runs k while loops, then the time complexity of it is  <span class="math">k(n^2-n)</span> . It is a piece of cake in comparison with  <span class="math">(n^2-n)^k</span>  &ndash; the time complexity of Algorithm 2 with the same number of loops. Therefore, Algorithm 3 is substantially more practical than Algorithm 2 is. Nevertheless, we have to admit that Algorithm 3 does not necessarily give us an optimal implementation of the input linear layer even though it succeeds.</p>

    <p class="text-gray-300">It is well known that implementation costs of the inverses of many linear layers are higher than that of themselves (the linear layer of AES, for example). As we mentioned before, besides using fewer XORs, another advantage of MIOSS is implementing every invertible linear layer and its inverse with the same cost. This advantage is also valid to the contents of this section. More specifically, if we get an efficient implementation  <span class="math">M = R_1 \\cdots R_r PC_s \\cdots C_1</span>  of a given linear layer  <span class="math">M \\in \\mathcal{IM}_{n \\times n}(\\mathbb{F}_2)</span>  by Algorithm 3, then we immediately obtain an implementation of  <span class="math">M^{-1}</span> :  <span class="math">M^{-1} = C_1 \\cdots C_s P^{-1} R_r \\cdots R_1</span> . Obviously,  <span class="math">M^{-1}</span>  has the same implementation cost as M does. In fact, we recommend a better strategy &ndash; for a given linear layer M, one can conduct Algorithm 3 to both M and  <span class="math">M^{-1}</span> , then choose a better (with a fewer number of additive elementary matrices) one from two implementations as the final option.</p>

      <h3 id="sec-6.1" class="text-xl font-semibold mt-8"><strong>6.1 New Efficient Implementations of the Linear Layers of AES</strong></h3>

    <p class="text-gray-300">As an application of Algorithm <a href="#page-13-0">3,</a> we investigate the linear layers of AES. The linear layer of AES consists of two phases: ShiftRows and MixColumns. We care nothing about ShiftRows since it is just a permutation over co-ordinates of input vectors. MixColumn is the one we really concern about. The Hamming weights of the matrices of encryption MixColumns and decryption MixColumns are 184 and 472, respectively. So, if we implement them by computing co-ordinates of output independently, they will cost 152 XORs and 440 XORs, respectively. However, with Algorithm <a href="#page-13-0">3,</a> we get an efficient implementation of encryption MixColumns with 132 XORs and an efficient implementation of decryption MixColumns with 228 XORs. In other words, we can save 13<em>.</em>16% XORs of computing co-ordinates of output of MixColumns independently and 48<em>.</em>18% XORs of computing co-ordinates of output of the inverse MixColumns independently. We would like to show the design of these efficient implementations below.</p>

    <p class="text-gray-300">Let <em>MC</em> denote the matrix of encryption MixColumns of AES. Then</p>

    <p class="text-gray-300"><span class="math">$MC = \\left( \\begin{array}{cccc} M(0x02) &amp; M(Ox03) &amp; M(Ox01) &amp; M(Ox01) \\\\ M(0x01) &amp; M(0x02) &amp; M(0x03) &amp; M(0x01) \\\\ M(0x01) &amp; M(0x01) &amp; M(0x02) &amp; M(0x03) \\\\ M(0x03) &amp; M(0x01) &amp; M(0x01) &amp; M(0x02) \\end{array} \\right)</span>$</p>

    <p class="text-gray-300">is a matrix in M32&times;32(F2), where</p>

    <p class="text-gray-300"><span class="math">$M(0x02) = \\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0</span>$</p>

    <p class="text-gray-300">Meanwhile, the matrix of decryption MixColumns of AES is</p>

    <p class="text-gray-300"><span class="math">$MC^{-1} = \\begin{pmatrix} M(0x0e) &amp; M(0x0b) &amp; M(0x0d) &amp; M(0x09) \\\\ M(0x09) &amp; M(0x0e) &amp; M(0x0b) &amp; M(0x0d) \\\\ M(0x0d) &amp; M(0x09) &amp; M(0x0e) &amp; M(0x0b) \\\\ M(0x0b) &amp; M(0x0d) &amp; M(0x09) &amp; M(0x0e) \\end{pmatrix}</span>$</p>

    <p class="text-gray-300"><span class="math">$\\in \\mathcal{M}_{32\\times32}(\\mathbb{F}_2),</span>$</p>

    <p class="text-gray-300">where</p>

    <p class="text-gray-300"><span class="math">$M(0x0e) = \\begin{pmatrix} 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix},</span>$</p>

    <p class="text-gray-300"><span class="math">$M(0x0b) = \\begin{pmatrix} 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix},</span>$</p>

    <p class="text-gray-300"><span class="math">$M(0x0d) = \\begin{pmatrix} 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix},</span>$</p>

    <p class="text-gray-300"><span class="math">$M(0x09) = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix}.</span>$</p>

    <p class="text-gray-300">We have  <span class="math">W_H(MC) = 184</span>  and  <span class="math">W_H(MC^{-1}) = 472</span> .</p>

    <p class="text-gray-300">We conduct Algorithm 3 to MC and  <span class="math">MC^{-1}</span>  but get nothing useful. Therefore, we perform some tricks on them: divide and conquer.</p>

    <p class="text-gray-300">Firstly, let us consider MC. We divide MC to 4 blocks:</p>

    <p class="text-gray-300"><span class="math">$MC = \\left( \\begin{array}{cc} M1 &amp; M2 \\\\ M2 &amp; M1 \\end{array} \\right),</span>$</p>

    <p class="text-gray-300">where</p>

    <p class="text-gray-300"><span class="math">$M1 = \\begin{pmatrix} M(0x02) &amp; M(0x03) \\\\ M(0x01) &amp; M(0x02) \\end{pmatrix},</span>$</p>

    <p class="text-gray-300">and</p>

    <p class="text-gray-300"><span class="math">$M2 = \\left( \\begin{array}{cc} M(0x01) &amp; M(0x01) \\\\ M(0x03) &amp; M(0x01) \\end{array} \\right).</span>$</p>

    <p class="text-gray-300">We easily get  <span class="math">W_H(M1) = 49</span>  and  <span class="math">W_H(M2) = 43</span> . Meanwhile, we let the input vector  <span class="math">X = (X_1, X_2, X_3, X_4)^T</span> , where each  <span class="math">X_i \\in \\mathbb{F}_2^8</span> . Hereby, the output vector can be computed</p>

    <p class="text-gray-300">as</p>

    <p class="text-gray-300">
<span class="math">$MC \\cdot X = \\begin{pmatrix} M1 &amp; M2 \\\\ M2 &amp; M1 \\end{pmatrix} (X_1, X_2, X_3, X_4)^T</span>$</p>

    <p class="text-gray-300"><span class="math">$= \\begin{pmatrix} M1(X_1, X_2)^T + M2(X_3, X_4)^T \\\\ M2(X_1, X_2)^T + M1(X_3, X_4)^T \\end{pmatrix}.</span>$
(1)</p>

    <p class="text-gray-300">Then the implementation cost of MC is the sum of twice of that of M1 and twice of that of M2 plus 32. We conduct Algorithm 3 to M1 and  <span class="math">M1^{-1}</span>  and obtain an MIOSS implementation of M1 with 46 XORs. But  <span class="math">W_H(M1) = 49</span>  for which we can compute co-ordinates of  <span class="math">M1(X_1, X_2)^T</span>  independently with 49 - 16 = 33 XORs. So, such an MIOSS implementation of M1 is not a good option. However, we may use divide and conquer once again to M1. We conduct Algorithm 3 to M(0x02) and  <span class="math">M(0x02)^{-1}</span>  and obtain an MIOSS implementation of M(0x02) with 3 XORs. This MIOSS implementation of M(0x02) costs the same number of XORs as computing co-ordinates of output vector independently does. So we implement M(0x02) by computing co-ordinates of output vector independently. Next, we conduct Algorithm 3 to M(0x03) and  <span class="math">M(0x03)^{-1}</span>  and obtain an MIOSS implementation of M(0x03) with 9 XORs as follows.</p>

    <p class="text-gray-300">
<span class="math">$M(0x03) = A_{7,8}A_{6,7}A_{5,6}A_{4,5}A_{3,4} A_{2,3}A_{1,2}A_{8,1}P_{0x03}A_{5,1},</span>$
(2)</p>

    <p class="text-gray-300">where each  <span class="math">A_{i,j} = E_{i,j} + I_8</span>  and  <span class="math">P_{0x03} = I_8</span> . This MIOSS implementation uses less XORs than computing co-ordinates of output of M(0x03) independently does because the latter uses 11 XORs. Implementing M(0x02) by computing co-ordinates of output independently and implementing M(0x03) by the MIOSS method in Equation (2), we can implement M1 by divide and conquer with  <span class="math">3 \\times 2 + 9 + 16 = 31</span>  XORs. It is less than 33 XORs of implementing M1 by computing co-ordinates of output independently. So, we choose divide and conquer to implement M1 as follows:</p>

    <p class="text-gray-300">
<span class="math">$M1(X_1, X_2)^T = \\begin{pmatrix} M(0x02)X_1^T + M(0x03)X_2^T \\\\ M(0x01)X_1^T + M(0x02)X_2^T \\end{pmatrix},</span>$
(3)</p>

    <p class="text-gray-300">where left-multiplying M(0x02) is implemented by computing co-ordinates of output independently, and left-multiplying M(0x03) is implemented by MIOSS method in Equation 2. Similarly, we investigate the costs of different implementation methods of M2. We conduct Algorithm 3 to M2 and  <span class="math">M2^{-1}</span>  and obtain an MIOSS implementation of M2 with 19 XORs as follows.</p>

    <p class="text-gray-300">
<span class="math">$M2 = A_{9,1}A_{10,2}A_{11,3}A_{12,4}A_{13,5}A_{14,6}A_{15,7} A_{16,8}A_{1,16}A_{2,9}A_{3,10}A_{4,11}A_{7,14}A_{12,16} A_{5,12}A_{13,16}A_{6,13}A_{15,16}A_{8,15}P_{M2},</span>$</p>

    <p class="text-gray-300"><span class="math">$(4)</span>$</p>

    <p class="text-gray-300">where each  <span class="math">A_{i,j} = E_{i,j} + I_{16}</span>  and  <span class="math">P_{M2}</span>  is described in Table 2. This MIOSS implementation of M2 is better than computing co-ordinates of output of M2 independently because the latter costs 43 - 16 = 27 XORs. In addition, implementing M2 by divide and conquer costs 9 + 16 = 25 XORs which is larger than 19 too. So, we choose the MIOSS procedure in Equation (4) to implement M2. In summary of this paragraph, we choose the divide and conquer procedure in Equation (1) to implement MC, choose the divide and conquer procedure in Equation (3) to implement M1, choose the MIOSS procedure in Equation (2) to implement M(0x03), choose the MIOSS procedure in Equation (4) to implement M2, and finally obtain an efficient implementation of MC with  <span class="math">31 \\times 2 + 19 \\times 2 + 32 = 132</span>  XORs. It saves 13.16% XORs of computing co-ordinates of output of MC independently.</p>

    <p class="text-gray-300">Secondly, let us consider  <span class="math">MC^{-1}</span> . Similar to the case of MC, we divide  <span class="math">MC^{-1}</span>  to 4 blocks:</p>

    <p class="text-gray-300"><span class="math">$MC^{-1} = \\left(\\begin{array}{cc} M3 &amp; M4 \\\\ M4 &amp; M3 \\end{array}\\right),\\,</span>$</p>

    <p class="text-gray-300"><span class="math">\\rho_{M2}(i)</span>  <span class="math">\\rho_{M2}(i)</span></p>

    <p class="text-gray-300"><strong>Table 2:</strong> The Permutation Matrix  <span class="math">P_{M2}</span>  in the MIOSS Implementation of M2</p>

    <p class="text-gray-300">&bull;  <span class="math">\\rho_{M2}(i)</span>  is the column index of the nonzero entry in <em>i</em>-th row of  <span class="math">P_{M2}</span> .</p>

    <p class="text-gray-300">where</p>

    <p class="text-gray-300"><span class="math">$M3 = \\begin{pmatrix} M(0x0e) &amp; M(0x0b) \\\\ M(0x09) &amp; M(0x0e) \\end{pmatrix},</span>$</p>

    <p class="text-gray-300">and</p>

    <p class="text-gray-300"><span class="math">$M4 = \\begin{pmatrix} M(0x0d) &amp; M(0x09) \\\\ M(0x0b) &amp; M(0x0d) \\end{pmatrix}.</span>$</p>

    <p class="text-gray-300">We easily get  <span class="math">W_H(M3) = 115</span>  and  <span class="math">W_H(M4) = 121</span> . Meanwhile, we let the input vector  <span class="math">Y = (Y_1, Y_2, Y_3, Y_4)^T</span> , where each  <span class="math">Y_i \\in \\mathbb{F}_2^8</span> . Hereby, the output vector can be computed as</p>

    <p class="text-gray-300">
<span class="math">$MC^{-1} \\cdot Y = \\begin{pmatrix} M3 &amp; M4 \\\\ M4 &amp; M3 \\end{pmatrix} (Y_1, Y_2, Y_3, Y_4)^T</span>$</p>

    <p class="text-gray-300"><span class="math">$= \\begin{pmatrix} M3(Y_1, Y_2)^T + M4(Y_3, Y_4)^T \\\\ M4(Y_1, Y_2)^T + M3(Y_3, Y_4)^T \\end{pmatrix}.</span>$
(5)</p>

    <p class="text-gray-300">Then the implementation cost of  <span class="math">MC^{-1}</span>  is the sum of twice of that of M3 and twice of that of M4 plus 32. For convenience of later investigation, let us consider the implementation costs of M(0x0e), M(0x0b), M(0x0d), and M(0x09) first. We conduct Algorithm 3 to M(0x0e) and  <span class="math">M(0x0e)^{-1}</span>  and obtain an MIOSS implementation of M(0x0e) with 15 XORs. This MIOSS implementation of M(0x0e) uses less XORs than computing co-ordinates of output of M(0x0e) independently does because the latter uses 28-8=20 XORs. Thus, we choose the MIOSS procedure to implement M(0x0e). Likewise, we choose an MIOSS procedure with 15 XORs to implement M(0x0b), choose an MIOSS procedure with 15 XORs to implement M(0x0d), and choose an MIOSS procedure with 11 XORs to implement M(0x0g). Then, we conduct Algorithm 3 to M3 and  <span class="math">M3^{-1}</span>  and obtain an MIOSS implementation of M3 with 52 XORs as follows.</p>

    <p class="text-gray-300">
<span class="math">$M3 = A_{3,1}A_{5,15}A_{13,7}A_{7,16}A_{4,2}A_{2,1}A_{10,2} A_{11,2}A_{11,3}A_{1,9}A_{8,1}A_{6,16}A_{12,6}A_{6,15} A_{6,7}A_{14,7}A_{1,10}A_{15,1}A_{3,6}A_{5,6}A_{13,6} A_{7,8}A_{4,7}A_{15,8}A_{4,15}A_{9,11}A_{13,10} A_{16,10}A_{14,11}A_{16,11}A_{2,1}A_{2,3}A_{2,5} A_{3,1}A_{3,14}A_{13,3}A_{9,13}A_{1,14}A_{5,1}A_{5,16} A_{12,5}A_{9,12}A_{2,9}A_{16,2}A_{1,16}P_{M3}A_{2,6} A_{12,6}A_{12,4}A_{7,1}A_{2,1}A_{11,12}A_{3,11},</span>$</p>

    <p class="text-gray-300"><span class="math">$(6)</span>$</p>

    <p class="text-gray-300">where each  <span class="math">A_{i,j} = E_{i,j} + I_{16}</span>  and  <span class="math">P_{M3}</span>  is described in Table 3. This MIOSS implementation of M3 is better than computing co-ordinates of output of M3 independently because the latter costs 115 - 16 = 99 XORs. In addition, implementing M3 by divide and conquer costs  <span class="math">15 \\times 2 + 15 + 11 + 16 = 72</span>  XORs which is larger than 52 too. So, we choose the MIOSS procedure in Equation (6) to implement M3. Next, we conduct Algorithm 3 to</p>

    <p class="text-gray-300"><em>i</em> 1 2 3 4 5 6 7 8 <em>&rho;M</em>3(<em>i</em>) 6 8 5 12 13 14 15 16 <em>i</em> 9 10 11 12 13 14 15 16 <em>&rho;M</em>3(<em>i</em>) 11 9 10 4 1 2 7 3</p>

    <p class="text-gray-300"><strong>Table 3:</strong> The Permutation Matrix <em>PM</em><sup>3</sup> in the MIOSS Implementation of <em>M</em>3</p>

    <p class="text-gray-300">&bull; <em>&rho;M</em>3(<em>i</em>) is the column index of the nonzero entry in <em>i</em>-th row of <em>PM</em>3.</p>

    <p class="text-gray-300"><strong>Table 4:</strong> The Permutation Matrix <em>PM</em><sup>4</sup> in the MIOSS Implementation of <em>M</em>4</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">i</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">1</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">2</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">3</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">4</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">5</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">6</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">7</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">8</th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">&rho;M4(i)</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">9</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">10</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">11</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">12</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">13</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">14</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">15</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">1</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">i</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">9</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">10</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">11</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">12</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">13</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">14</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">15</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">16</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">&rho;M4(i)</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">8</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">3</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">4</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">5</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">6</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">7</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">16</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">2</td>
          </tr>
        </tbody>
      </table>
    </div>

    <p class="text-gray-300">&bull; <em>&rho;M</em>4(<em>i</em>) is the column index of the nonzero entry in <em>i</em>-th row of <em>PM</em>4.</p>

    <p class="text-gray-300"><em>M</em>4 and <em>M</em>4 <sup>&minus;</sup><sup>1</sup> and obtain an MIOSS implementation of <em>M</em>4 with 46 XORs as follows.</p>

    <p class="text-gray-300">
<span class="math">$M4 = A_{10,2}A_{5,9}A_{13,1}A_{14,10}A_{11,3}A_{13,5}A_{3,12} A_{12,4}A_{5,14}A_{14,6}A_{2,11}A_{11,5}A_{5,7}A_{5,12} A_{5,15}A_{11,6}A_{12,6}A_{6,13}A_{6,15}A_{15,9}A_{9,8} A_{8,16}P_{M4}A_{12,6}A_{11,6}A_{6,15}A_{15,2}A_{2,12} A_{2,11}A_{2,1}A_{8,9}A_{7,9}A_{16,15}A_{15,7}A_{7,8} A_{16,3}A_{11,3}A_{9,3}A_{2,8}A_{3,12}A_{13,1}A_{1,10} A_{12,4}A_{8,16}A_{10,2}A_{9,1}A_{5,8},</span>$</p>

    <p class="text-gray-300"><span class="math">$(7)</span>$</p>

    <p class="text-gray-300">where each <em>Ai,j</em> = <em>Ei,j</em> +<em>I</em><sup>16</sup> and <em>PM</em><sup>4</sup> is described in Table <a href="#page-18-1">4.</a> This MIOSS implementation of <em>M</em>4 is better than computing co-ordinates of output of <em>M</em>4 independently because the latter costs 121 &minus; 16 = 105 XORs. In addition, implementing <em>M</em>4 by divide and conquer costs 15 &times; 2 + 15 + 11 + 16 = 72 XORs which is larger than 46 too. So, we choose the MIOSS procedure in Equation <a href="#page-18-2">(7)</a> to implement <em>M</em>4. In summary of this paragraph, we choose the divide and conquer procedure in Equation <a href="#page-17-2">(5)</a> to implement <em>MC</em><sup>&minus;</sup><sup>1</sup> , choose the MIOSS procedure in Equation <a href="#page-17-1">(6)</a> to implement <em>M</em>3, choose the MIOSS procedure in Equation <a href="#page-18-2">(7)</a> to implement <em>M</em>4, and finally obtain an efficient implementation of <em>MC</em><sup>&minus;</sup><sup>1</sup> with 52 &times; 2 + 46 &times; 2 + 32 = 228 XORs. It saves 48<em>.</em>18% XORs of computing co-ordinates of output of <em>MC</em><sup>&minus;</sup><sup>1</sup> independently.</p>

    </section>

    <section id="sec-7" class="mb-10">
      <h2 class="text-2xl font-bold"><strong>7 Efficient Implementations of Singular Linear Layers</strong></h2>

    <p class="text-gray-300">In above sections, we talked about minimum-XOR-implementations and efficient implementations of invertible linear layers. However, we are confronted with singular (even not square) matrices sometimes. So, let us pay attention to singular linear layers in this section. Actually, we can find efficient implementations of singular linear layers with a method similar to that we used to find efficient implementations of invertible linear layers.</p>

    <p class="text-gray-300">As we know, every matrix <em>M</em> &isin; M<em>m</em>&times;<em>n</em>(F2) can be transformed to a rectangular diagonal matrix with the same rank as <em>M</em> via a series of row/column exchanging and row/column addition. <em>rank</em>(<em>M</em>) entries on the diagonal of this rectangular diagonal matrix are 1, and other entries are all 0. According to Lemma <a href="#page-7-0">2,</a> <em>M</em> can be transformed to a matrix <em>B</em> &isin; M<em>m</em>&times;<em>n</em>(F2) via a series of row/column addition, where all but <em>rank</em>(<em>M</em>) entries of <em>B</em> are 0, each row of <em>B</em> contains one 1 and so does each column of it. In a form of matrix, there exists a series of additive elementary matrices  <span class="math">R_1, \\dots, R_r</span>  in  <span class="math">\\mathcal{AEM}_{m \\times m}(\\mathbb{F}_2)</span>  and a series of  <span class="math">C_1, \\dots, C_s</span>  in  <span class="math">\\mathcal{AEM}_{n \\times n}(\\mathbb{F}_2)</span>  such that  <span class="math">R_r \\dots R_1 M C_1 \\dots C_s = B</span> . Consequently, we get a factorization  <span class="math">M = R_1 \\dots R_r B C_s \\dots C_1</span> . Note that left-multiplying a column  <span class="math">X \\in \\mathbb{F}_2^n</span>  by B results in a column in  <span class="math">\\mathbb{F}_2^m</span>  by using no XOR. For example,</p>

    <p class="text-gray-300"><span class="math">$\\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0</span>$</p>

    <p class="text-gray-300">That indicates left-multiplying an input column  <span class="math">X \\in \\mathbb{F}_2^n</span>  by M with this factorization uses r+s XORs. If each row/column addition reduces the Hamming weight of the matrix by 1, the implementation cost of this MIOSS procedure will be  <span class="math">W_H(M) - rank(M)</span> . It is probably greater than the implementation cost  <span class="math">W_H(M) - m</span>  of computing co-ordinates of output of M independently. Therefore, if we want an MIOSS procedure better than computing co-ordinates of output of M independently, the guideline is trying to reduce the Hamming weight of the given linear layer as much as possible by an additive row/column elementary operation on each step. To attain this goal, we present Algorithm 4.</p>

    <h4 id="sec-misc-5" class="text-lg font-semibold mt-6">Algorithm 4 Finding an Efficient Implementation of a Linear Layer</h4>

    <pre><code class="language-text">Require: a matrix M \\in \\mathcal{M}_{m \\times n}(\\mathbb{F}_2), rank(M);
Ensure: a series of additive elementary operations and a matrix in \\mathcal{M}_{m \\times n}(\\mathbb{F}_2);
  r \\leftarrow 0, RUB \\leftarrow W_H(M) - rank(M);
  while W_H(M) &gt; rank(M) and r \\leq RUB do
      let \\alpha_i denote the i-th row of M for i=1,\\cdots,m and \\beta_j denote the j-th column of M for j=1,\\cdots,n,
      weightdecrease \\leftarrow W_H(\\alpha_1) - W_H(\\alpha_1 + \\alpha_2), AE \\leftarrow (R, 2, 1);
      for 1 \\le i &lt; j \\le m do
         compute \\alpha_i + \\alpha_j;
         if W_H(\\alpha_i) - W_H(\\alpha_i + \\alpha_j) &gt; weight decrease then
             weightdecrease \\leftarrow W_H(\\alpha_i) - W_H(\\alpha_i + \\alpha_j), AE \\leftarrow (R, j, i);
         end if
         if W_H(\\alpha_j) - W_H(\\alpha_i + \\alpha_j) &gt; weightdecrease then
             weight decrease \\leftarrow W_H(\\alpha_i) - W_H(\\alpha_i + \\alpha_i), AE \\leftarrow (R, i, j);
         end if
      end for
      for 1 \\le i &lt; j \\le n do
          compute \\beta_i + \\beta_j;
          if W_H(\\beta_i) - W_H(\\beta_i + \\beta_i) &gt; weight decrease then
             weight decrease \\leftarrow W_H(\\beta_i) - W_H(\\beta_i + \\beta_j), AE \\leftarrow (C, j, i);
          end if
         if W_H(\\beta_i) - W_H(\\beta_i + \\beta_i) &gt; weight decrease then
             weight decrease \\leftarrow W_H(\\beta_j) - W_H(\\beta_i + \\beta_j), AE \\leftarrow (C, i, j);
          end if
      end for
      if AE = (R, i, j) then
         add \\alpha_i to \\alpha_j, output AE, r \\leftarrow r + 1;
      end if
      if AE = (C, i, j) then
         add \\beta_i to \\beta_i, output AE, r \\leftarrow r + 1;
      end if
  end while
  if W_H(M) &gt; rank(M) then
     print &quot;Fail.&quot;;
   else
     print &quot;Success via r steps.&quot;, output M;
  end if
</code></pre>

    </section>

    <section id="sec-8" class="mb-10">
      <h2 class="text-2xl font-bold"><strong>8 Conclusion</strong></h2>

    <p class="text-gray-300">In this paper, we first investigate the effect of two implementation methods on implementation costs of linear layers: computing the co-ordinates of outputs independently and modifying input vectors to outputs step by step. We focus on the latter because it preforms better than the former. Then, we clarify the measurement of implementation cost and optimal implementation procedures of linear layers. Next, to find an optimal implementation procedure of a given linear layer, we construct a graph-theoretical model and transfer the problem to the shortest path problem in graph theory. Then, we adopt a &quot;double-direction&quot; algorithm that uses less storage space and makes the search for a shortest path more efficient in our regular graph. After that, we construct another algorithm for finding efficient implementations of linear layers. The advantages of this algorithm are its low complexity and high practicality. We conduct it to the linear layers of AES and obtain highly efficient implementations of them. To handle more general linear layers, we finally present a practical algorithm for finding efficient implementations of singular linear layers. We wish our work would be beneficial to the design of implementation of linear layers.</p>

    <p class="text-gray-300"><strong>Acknowledgements.</strong> Ruoxin Zhao would like to thank Dr. Meicheng Liu and Dr. Yongqiang Li for sincere discussion and constructive suggestion.</p>

    </section>

    <section id="references" class="mb-10">
      <h2 class="text-2xl font-bold"><strong>References</strong></h2>

    <ul class="space-y-2 text-gray-400 text-sm list-none">
      <li><p class="text-gray-300">[1] Christof Beierle, Thorsten Kranz, and Gregor Leander. Lightweight multiplication in <em>GF</em>(2<em><sup>n</sup></em>) with applications to MDS matrices. In <em>Advances in Cryptology - CRYPTO 2016 - 36th Annual International Cryptology Conference, Santa Barbara, CA, USA, August 14-18, 2016, Proceedings, Part I</em>, pages 625&ndash;653, 2016.</p></li>
      <li><p class="text-gray-300">[2] B&eacute;la Bollob&aacute;s. <em>Modern Graph Theory</em>, volume 184 of <em>Graduate Texts in Mathematics</em>. Springer Science+Business Media, New York City, USA, 1998.</p></li>
      <li><p class="text-gray-300">[3] Richard A. Brualdi. <em>Introductory Combinatorics</em>. Pearson Education, New York City, USA, 5th edition, 2009.</p></li>
      <li><p class="text-gray-300">[4] Joan Daemen and Vincent Rijmen. <em>The Design of Rijndael: AES - The Advanced Encryption Standard</em>. Information Security and Cryptography. Springer, 2002.</p></li>
      <li><p class="text-gray-300">[5] John M. Harris, Jeffry L. Hirst, and Michael J. Mossinghoff. <em>Combinatorics and Graph Theory</em>. Undergraduate Texts in Mathematics. Springer Science+Business Media, New York City, USA, 2nd edition, 2008.</p></li>
      <li><p class="text-gray-300">[6] Kenneth Hoffman. <em>Linear Algebra</em>. Prentice-Hall, Englewood Cliffs, USA, 2nd edition, 1971.</p></li>
      <li><p class="text-gray-300">[7] J&eacute;r&eacute;my Jean, Thomas Peyrin, and Siang Meng Sim. Minimal implementations of linear and non-linear lightweight building blocks. Personal communication, 2015.</p></li>
      <li><p class="text-gray-300">[8] Yongqiang Li and Mingsheng Wang. On the construction of lightweight circulant involutory MDS matrices. In <em>Fast Software Encryption - 23rd International Conference, FSE 2016, Bochum, Germany, March 20-23, 2016, Revised Selected Papers</em>, pages 121&ndash;139, 2016.</p></li>
      <li><p class="text-gray-300">[9] Meicheng Liu and Siang Meng Sim. Lightweight MDS generalized circulant matrices. In <em>Fast Software Encryption - 23rd International Conference, FSE 2016, Bochum, Germany, March 20-23, 2016, Revised Selected Papers</em>, pages 101&ndash;120, 2016.</p></li>
      <li><p class="text-gray-300">[10] Mahdi Sajadieh, Mohammad Dakhilalian, Hamid Mala, and Pouyan Sepehrdad. Recursive diffusion layers for block ciphers and hash functions. In <em>Fast Software Encryption - 19th International Workshop, FSE 2012, Washington, DC, USA, March 19-21, 2012. Revised Selected Papers</em>, pages 385&ndash;401, 2012.</p></li>
      <li><p class="text-gray-300">[11] Sumanta Sarkar and Siang Meng Sim. A deeper understanding of the XOR count distribution in the context of lightweight cryptography. In <em>Progress in Cryptology - AFRICACRYPT 2016 - 8th International Conference on Cryptology in Africa, Fes, Morocco, April 13-15, 2016, Proceedings</em>, pages 167&ndash;182, 2016.</p></li>
      <li><p class="text-gray-300">[12] Siang Meng Sim, Khoongming Khoo, Fr&eacute;d&eacute;rique E. Oggier, and Thomas Peyrin. Lightweight MDS involution matrices. In <em>Fast Software Encryption - 22nd International Workshop, FSE 2015, Istanbul, Turkey, March 8-11, 2015, Revised Selected Papers</em>, pages 471&ndash;493, 2015.</p></li>
      <li><p class="text-gray-300">[13] Shengbao Wu, Mingsheng Wang, and Wenling Wu. Recursive diffusion layers for (lightweight) block ciphers and hash functions. In <em>Selected Areas in Cryptography, 19th International Conference, SAC 2012, Windsor, ON, Canada, August 15-16, 2012, Revised Selected Papers</em>, pages 355&ndash;371, 2012.</p></li>
    </ul>

    </section>

    <section id="app-a" class="mb-10">
      <h2 class="text-2xl font-bold"><strong>A Experimental Results</strong></h2>

    <p class="text-gray-300">In this appendix, every <em>Ai,j</em> denotes <em>I<sup>n</sup></em> + <em>Ei,j</em> with a proper order <em>n</em>.</p>

    <p class="text-gray-300"><span class="math">$L_1 = \\left(\\begin{array}{ccccc} 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\end{array}\\right) = A_{1,4}A_{4,2}A_{3,4} \\left(\\begin{array}{cccccc} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right) A_{4,1}A_{1,5}A_{5,2}.</span>$</p>

    <p class="text-gray-300"><span class="math">$L_2 = \\left(\\begin{array}{ccccc} 1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\end{array}\\right) = A_{4,1} A_{1,2} A_{5,1} A_{2,3} \\left(\\begin{array}{cccccc} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{array}\\right) A_{2,5} A_{5,3} A_{3,5}.</span>$</p>

    <p class="text-gray-300"><span class="math">$L_{3} = \\begin{pmatrix} 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\end{pmatrix} = A_{3,5}A_{5,3}A_{2,5} \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\end{pmatrix} A_{4,1}A_{1,5}A_{5,2}.</span>$</p>

    <p class="text-gray-300"><span class="math">$L_4 = \\begin{pmatrix} 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\end{pmatrix} = A_{4,2} A_{2,5} A_{5,3} \\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix} A_{4,2} A_{2,1}.</span>$</p>

    <p class="text-gray-300"><span class="math">$L_{5} = \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\end{pmatrix} = A_{2,4}A_{4,5}A_{5,1} \\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} A_{2,3}A_{3,4}A_{4,1}.</span>$</p>

    <p class="text-gray-300"><span class="math">$L_{6} = \\begin{pmatrix} 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1</span>$</p>

    <p class="text-gray-300"><span class="math">$L_{15} = \\begin{pmatrix} 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix} = A_{1,3}A_{2,1}A_{1,6} \\begin{pmatrix} 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\end{pmatrix} = A_{1,4}A_{2,1}A_{1,6}A_{3,1} \\begin{pmatrix} 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0</span>$</p>

    <p class="text-gray-300">0 0 0 1 0 0</p>

    <p class="text-gray-300">0 0 1 1 0 0</p>

    </section>
`;
---

<BaseLayout title="Designing Optimal Implementations of Linear Layers (Full Ver... (2016/1118)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2016 &middot; eprint 2016/1118
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <PaperDisclaimer eprintUrl={EPRINT_URL} />
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <nav id="toc" class="mb-10 p-6 rounded-lg" style="background: rgba(255,255,255,0.03); border: 1px solid rgba(255,255,255,0.06);">
      <h2 class="text-lg font-bold mb-4">Table of Contents</h2>
      <ol class="space-y-1 text-sm text-gray-300
        list-decimal list-inside">
        <li>
          <a href="#sec-1" class="hover:text-white">Introduction</a>
          <ol class="ml-6 mt-1 space-y-1 list-decimal
            list-inside text-gray-400">
            <li><a href="#sec-1.1" class="hover:text-white">Related Work</a></li>
            <li><a href="#sec-1.2" class="hover:text-white">Our Contributions</a></li>
          </ol>
        </li>
        <li>
          <a href="#sec-2" class="hover:text-white">Preliminary</a>
          <ol class="ml-6 mt-1 space-y-1 list-decimal
            list-inside text-gray-400">
            <li><a href="#sec-2.1" class="hover:text-white">Notations</a></li>
            <li><a href="#sec-2.2" class="hover:text-white">Some Basic Facts about Linear Algebra</a></li>
            <li><a href="#sec-2.3" class="hover:text-white">Some Basic Facts about Graph Theory</a></li>
            <li><a href="#sec-2.4" class="hover:text-white">Linear Layers</a></li>
          </ol>
        </li>
        <li><a href="#sec-3" class="hover:text-white">Measurement of Implementation Costs of Linear Layers</a></li>
        <li><a href="#sec-4" class="hover:text-white">A Graph-Theoretical Model</a></li>
        <li>
          <a href="#sec-5" class="hover:text-white">Searching for Optimal Implementations of Invertible Linear Layers</a>
          <ol class="ml-6 mt-1 space-y-1 list-decimal
            list-inside text-gray-400">
            <li><a href="#sec-5.1" class="hover:text-white">Experimental Results</a></li>
          </ol>
        </li>
        <li>
          <a href="#sec-6" class="hover:text-white">A More Practical Strategy for Efficient Implementations of Invertible Linear Layers</a>
          <ol class="ml-6 mt-1 space-y-1 list-decimal
            list-inside text-gray-400">
            <li><a href="#sec-6.1" class="hover:text-white">New Efficient Implementations of the Linear Layers of AES</a></li>
          </ol>
        </li>
        <li><a href="#sec-7" class="hover:text-white">Efficient Implementations of Singular Linear Layers</a></li>
        <li><a href="#sec-8" class="hover:text-white">Conclusion</a></li>
      </ol>
      <p class="text-xs text-gray-500 mt-4 mb-1 font-semibold">
        Appendices
      </p>
      <ol class="space-y-1 text-sm text-gray-400
        list-[upper-alpha] list-inside">
        <li><a href="#app-a" class="hover:text-white">Experimental Results</a></li>
      </ol>
      <p class="text-xs text-gray-500 mt-4 mb-1 font-semibold">
        Additional
      </p>
      <ul class="space-y-1 text-sm text-gray-400
        list-disc list-inside">
        <li><a href="#references" class="hover:text-white">References</a></li>
      </ul>
    </nav>


    <Fragment set:html={CONTENT} />

    <PaperHistory slug="5-searching-for-optimal-implementations-of-invertible-2016" />
  </article>
</BaseLayout>
