---
import BaseLayout from '../../layouts/BaseLayout.astro';
import PaperDisclaimer from '../../components/PaperDisclaimer.astro';
import PaperHistory from '../../components/PaperHistory.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2009/547';
const CRAWLER = 'marker';
const CONVERTED_DATE = '2026-02-16';
const TITLE_HTML = 'Non-Interactive Verifiable Computing: Outsourcing Computation to Untrusted Workers';
const AUTHORS_HTML = 'Rosario Gennaro, Craig Gentry, Bryan Parno';

const CONTENT = `    <section id="abstract" class="mb-10">
      <h2 class="text-2xl font-bold">Abstract</h2>
      <p class="text-gray-300">Verifiable Computation enables a computationally weak client to &quot;outsource&quot; the computation of a function F on various inputs x_1,...,x_k to one or more workers.  The workers return the result of the function evaluation, e.g., y_i=F(x_i), as well as a proof that the computation of F was carried out correctly on the given value x_i.  The verification of the proof should require substantially less computational effort than computing F(x_i) from scratch.

We present a protocol that allows the worker to return a computationally-sound, non-interactive proof that can be verified in O(m) time, where m is the bit-length of the output of F. The protocol requires a one-time pre-processing stage by the client which takes O(|C|) time, where C is the smallest Boolean circuit computing F.  Our scheme also provides input and output privacy for the client, meaning that the workers do not learn any information about the x_i or y_i values.</p>
      <p class="text-gray-300"><strong>Keywords:</strong> Verifiable &middot; Integrity &middot; Outsourcing &middot; Protocols &middot; Public-key Cryptography</p>
    </section>

    <p class="text-gray-300">Our definition of security (Definition 3) assumes that the adversary does not see the output of the <strong>Verify</strong> procedure run by the client on the value  <span class="math">\\sigma</span>  returned by the adversary. Theorem 1 is proven under the same assumption. In practice this means that our protocol  <span class="math">\\mathcal{VC}</span>  is secure if the client keeps the result of the computation private.</p>

    <p class="text-gray-300">In practice, there might be circumstances where this is not feasible, as the behavior of the client will change depending on the result of the evaluation (e.g., the client might refuse to pay the worker). Intuitively, and we prove this formally below, seeing the result of <strong>Verify</strong> on proofs the adversary correctly <strong>Computes</strong> using the output of <strong>PubProbGen</strong> does not help the adversary (since it already knows the result based on the inputs it supplied to <strong>PubProbGen</strong>). But what if the worker returns a malformed response &ndash; i.e., something for which <strong>Verify</strong> outputs  <span class="math">\\bot</span> . How does the client respond, if at all? One option is for the client to ask the worker to perform the computation again. But this repeated request informs the worker that its response was malformed, which is an additional bit of information that a cheating worker might exploit in its effort to generate forgeries. Is our scheme secure in this setting? In this section, we prove that our scheme remains secure as long as the client terminates after detecting a malformed response. We also consider the interesting question of whether our scheme is secure if the client terminates only after detecting k &gt; 1 malformed responses, but we are unable to provide a proof of security in this setting.</p>

    <p class="text-gray-300">Note that there is a real attack on the scheme in this setting if the client does not terminate. Specifically, for concreteness, suppose that each ciphertext output by  <span class="math">\\mathbf{Encrypt}_{\\mathcal{E}}</span>  encrypts a single bit of a label for an input wire of the garbled circuit, and that the adversary wants to determine the first bit  <span class="math">w_{11}^{b_1}</span>  of the first label (where that label stands in for unknown input  <span class="math">b_1 \\in \\{0,1\\}</span> ). To do this, the adversary runs  <span class="math">\\mathbf{Compute}</span>  as before, obtaining ciphertexts that encrypt the bits  <span class="math">\\overline{w_i}</span>  of a label for the output wire. Using the homomorphism of the encryption scheme  <span class="math">\\mathcal{E}</span> , it XORs  <span class="math">w_{11}^{b_1}</span>  with the first bit of  <span class="math">\\overline{w_i}</span>  to obtain  <span class="math">\\overline{w_i}</span> , and it sends (the encryption of)  <span class="math">\\overline{w_i}</span>  as its response. If  <span class="math">\\mathbf{Verify}</span>  outputs  <span class="math">\\bot</span> , then  <span class="math">w_{11}^{b_1}</span>  must have been a 1; otherwise, it is a 0 with overwhelming probability. The adversary can thereby learn the labels of the garbled circuit one bit at a time &ndash; in particular, it can similarly learn the labels of the output wire, and thereafter generate a verifiable response without actually performing the computation.</p>

    <p class="text-gray-300">Intuitively, one might think that if the client terminates after detecting k malformed responses, then the adversary should only be able to obtain about k bits of information about the garbled circuit before the client terminates (using standard entropy arguments), and therefore it should still be hard for the adversary to output the entire &quot;wrong&quot; label for the output wire as long as  <span class="math">\\lambda</span>  is sufficiently larger than k. However, we are unable to make this argument go through. In particular, the difficulty is with the hybrid argument in the proof of Theorem 1, where we gradually transition to an experiment in which the simulator is encrypting the same Yao input labels in every round. This experiment must be indistinguishable from the real world experiment, which permits different inputs in different rounds. When we don't give the adversary information about whether or not its response was well-formed or not, the hybrid argument is straightforward &ndash; it simply depends on the semantic security of the FHE scheme.</p>

    <p class="text-gray-300">However, if we do give the adversary that information, then the adversary can easily distinguish rounds with the same input from rounds with random inputs. To do so, it chooses some &quot;random&quot; predicate P over the input labels, such that  <span class="math">P(w_{b_1}^1, w_{b_2}^2, \\ldots) = P(w_{b_1&#x27;}^1, w_{b_2&#x27;}^2, \\ldots)</span>  with probability 1/2 if  <span class="math">(b_1, b_2, \\ldots) \\neq (b_1&#x27;, b_2&#x27;, \\ldots)</span> . Given the encryptions of  <span class="math">w_{b_1}^1, w_{b_2}^2, \\ldots</span> , the adversary runs <strong>Compute</strong> as in the scheme, obtaining ciphertexts that encrypt the bits  <span class="math">\\overline{w_i}</span>  of a label for the output wire, XORs (using the homomorphism)  <span class="math">P(w_{b_1}^1, w_{b_2}^2, \\ldots)</span>  with the first bit of  <span class="math">\\overline{w_i}</span> , and sends (an encryption of) the result  <span class="math">\\overline{w_i&#x27;}</span>  as its response. If the client is making the same query in every round &ndash; i.e., the Yao input labels are the same every time &ndash; then, the predicate always outputs the same bit, and thus the adversary gets the same response (well-formed or malformed) in every round. Otherwise, the responses will tend to vary.</p>

    <p class="text-gray-300">One could try to make the adversary's distinguishing attack more difficult by (for example) trying to hide which ciphertexts encrypt the bits of which labels &ndash; i.e., via some form of obfuscation. However, the adversary may define its predicate in such a way that it &quot;analyzes&quot; this obfuscated circuit, determines whether two ostensibly different inputs in fact represent the same set of Yao input labels, and outputs the same bit if they do. (It performs this analysis on the encrypted inputs, using the homomorphism.) We do not know of any way to prevent this distinguishing attack, and suspect that preventing it may be rather difficult in light of Barak et al.'s result that there is no general obfuscator [7].</p>

    <p class="text-gray-300">Security with Verification Access. We say that a verifiable computation scheme is secure with verification access if the adversary is allowed to see the result of <strong>Verify</strong> over the queries  <span class="math">x_i</span>  he has made to the <strong>ProbGen</strong> oracle in  <span class="math">\\mathbf{Exp}_{i}^{Verif}</span>  (see Definition 3).</p>

    <p class="text-gray-300">Let  <span class="math">\\mathcal{VC}^{\\dagger}</span>  be like  <span class="math">\\mathcal{VC}</span> , except that the client terminates if it receives a malformed response from the worker. Below, we show that  <span class="math">\\mathcal{VC}^{\\dagger}</span>  is secure with verification access. In other words, it is secure to provide the worker with verification access (indicating whether a response was well-formed or not), until the worker gives a malformed response. Let  <span class="math">\\mathbf{Exp}_A^{Verif^{\\dagger}} \\left[ \\mathcal{VC}^{\\dagger}, F, \\lambda \\right]</span>  denote the experiment described in Section 3.1, with the obvious modifications.</p>

    <p class="text-gray-300"><strong>Theorem 4</strong> If VC is a secure outsourceable verifiable computation scheme, then  <span class="math">VC^{\\dagger}</span>  is a secure outsourceable verifiable computation scheme with verification access. If VC is private, then so is  <span class="math">VC^{\\dagger}</span> .</p>

    <p class="text-gray-300"><strong>Proof of Theorem 4:</strong> Consider two games between a challenger and an adversary A. In the real world game for  <span class="math">\\mathcal{VC}^{\\dagger}</span> , Game 0, the interactions between the challenger and A are exactly like those between the client and a worker in the real world &ndash; in particular, if A's response was well-formed, the challenger tells A so, but the challenger immediately aborts if A's response is malformed. Game 1 is identical to Game 0, except that when A queries <strong>Verify</strong>, the challenger always answers with the correct y, whether A's response was well-formed or not, and the challenger never aborts. Let  <span class="math">\\varepsilon_i</span>  be A's success probability in Game i.</p>

    <p class="text-gray-300">First, we show that if VC is secure, then  <span class="math">\\varepsilon_1</span>  must be negligible. The intuition is simple: since the challenger always responds with the correct y, there is actually no information in these responses, since A could have computed y on its own. More formally, there is an algorithm B that breaks VC with probability  <span class="math">\\varepsilon_1</span>  by using A as a sub-routine. B simply forwards communications between the challenger (now a challenger</p>

    <p class="text-gray-300">for the VC game) and A, except that B tells A the correct y w.r.t. all of A's responses. B forwards A's forgery along to the challenger.</p>

    <p class="text-gray-300">Now, we show that  <span class="math">\\varepsilon_0 \\leq \\varepsilon_1</span> , from which the result follows. Let  <span class="math">E_{mal}</span>  be the event that A makes a malformed response, and let  <span class="math">E_f</span>  be the event that A successfully outputs a forgery &ndash; i.e., where  <span class="math">\\mathbf{Exp}_A^{Verif^{\\dagger}}[\\mathcal{VC}^{\\dagger}, F, \\lambda]</span>  outputs '1'. A's success probability, in either Game 0 or Game 1, is:</p>

    <p class="text-gray-300"><span class="math">$Prob[E_f] = Prob[E_f|E_{mal}] \\cdot Prob[E_{mal}] + Prob[E_f|\\neg E_{mal}] \\cdot Prob[\\neg E_{mal}]</span>$
(7)</p>

    <p class="text-gray-300">If A does not make a malformed response, then Games 0 and 1 are indistinguishable to A; therefore, the second term above has the same value in Games 0 and 1. In Game 0,  <span class="math">Prob[E_f|E_{mal}] = 0</span> , since the challenger aborts. Therefore,  <span class="math">\\varepsilon_0 \\leq \\varepsilon_1</span> .</p>

    <p class="text-gray-300">In practice Theorem 4 implies that every time a malformed response is received, the client must regarble the circuit (or, as we said above, make sure that the results of the verification procedure remain secret). Therefore the amortized efficiency of the client holds only if we assume that malformed responses do not happen very frequently.</p>

    <p class="text-gray-300">In some settings, it is not necessary to inform the worker that its response is malformed, at least not immediately. For example, in the Folding@Home application [2], suppose the client generates a new garbled circuit each morning for its many workers. At the end of the day, the client stops accepting computations using this garbled circuit, and it (optionally) gives the workers information about the well-formedness of their responses. (Indeed, the client may reveal all of its secrets for that day.) In this setting, our previous security proof clearly holds even if there are arbitrarily many malformed responses.</p>

    <section id="sec-6" class="mb-10">
      <h2 class="text-2xl font-bold"><strong>6</strong> Conclusions and Future Directions</h2>

    <p class="text-gray-300">In this work, we introduced the notion of Verifiable Computation as a natural formulation for the increasingly common phenomenon of outsourcing computational tasks to untrusted workers. We describe a scheme that combines Yao's Garbled Circuits with a fully-homomorphic encryption scheme to provide extremely efficient outsourcing, even in the presence of an adaptive adversary. As an additional benefit, our scheme maintains the privacy of the client's inputs and outputs.</p>

    <p class="text-gray-300">Our work leaves open several interesting problems. It would be desirable to devise a verifiable computation scheme that used a more efficient primitive than fully-homomorphic encryption. Similarly, it seems plausible that a verifiable scheme might sacrifice input privacy to increase its efficiency. While our scheme is resilient against a single malformed response from the worker, ideally we would like a scheme that tolerates k &gt; 1 malformed responses. Finally, it would be interesting to enhance a verifiable computation scheme to include a non-repudiation property, so that a client who receives a malformed response from a worker can demonstrate the worker's misbehavior to a third party.</p>

    </section>

    <section id="references" class="mb-10">
      <h2 class="text-2xl font-bold">References</h2>

    <ul class="space-y-2 text-gray-400 text-sm list-none">
      <li><p class="text-gray-300">[1] Amazon Elastic Compute Cloud. Online at http://aws.amazon.com/ec2.</p></li>
      <li><p class="text-gray-300">[2] The Folding@home project. Stanford University, http://www.stanford.edu/group/pandegroup/cosm/.</p></li>
      <li><p class="text-gray-300">[3] Sun Utility Computing. Online at http://www.sun.com/service/sungrid/index.jsp.</p></li>
      <li><p class="text-gray-300">[4] The Great Internet Mersenne Prime Search. http://www.mersenne.org/prime.htm.</p></li>
      <li><p class="text-gray-300">[5] D. P. Anderson, J. Cobb, E. Korpela, M. Lebofsky, and D. Werthimer. SETI@Home: An experiment in public-resource computing. <em>Communications of the ACM</em>, 45(11):56&ndash;61, 2002.</p></li>
      <li><p class="text-gray-300">[6] L. Babai. Trading group theory for randomness. In <em>Proceedings of the ACM Symposium on Theory of Computing (STOC)</em>, pages 421&ndash;429, New York, NY, USA, 1985. ACM.</p></li>
      <li><p class="text-gray-300">[7] B. Barak, O. Goldreich, R. Impagliazzo, S. Rudich, A. Sahay, S. Vadhan, and K. Yang. On the (im)possibility of obfuscating programs. In <em>Proceedings of CRYPTO</em>, pages 1&ndash;18, 2001.</p></li>
      <li><p class="text-gray-300">[8] B. Barak, I. Haitner, D. Hofheinz, and Y. Ishai. Bounded key-dependent message security. 2009.</p></li>
      <li><p class="text-gray-300">[9] M. Belenkiy, M. Chase, C. C. Erway, J. Jannotti, A. Kupc&cedil; &uml; u, and A. Lysyanskaya. Incentivizing out- &uml; sourced computation. In <em>Proceedings of the Workshop on Economics of Networked Systems (NetEcon)</em>, pages 85&ndash;90, New York, NY, USA, 2008. ACM.</p></li>
      <li><p class="text-gray-300">[10] D. Chaum and T. Pedersen. Wallet databases with observers. In <em>Proceedings of CRYPTO</em>, 1992.</p></li>
      <li><p class="text-gray-300">[11] C. Gentry. <em>A fully homomorphic encryption scheme</em>. PhD thesis, Stanford University, 2009.</p></li>
      <li><p class="text-gray-300">[12] C. Gentry. Fully homomorphic encryption using ideal lattices. In <em>Proceedings of the ACM Symposium on the Theory of Computing (STOC)</em>, 2009.</p></li>
      <li><p class="text-gray-300">[13] S. Goldwasser, Y. T. Kalai, and G. N. Rothblum. Delegating computation: interactive proofs for muggles. In <em>Proceedings of the ACM Symposium on the Theory of Computing (STOC)</em>, 2008.</p></li>
      <li><p class="text-gray-300">[14] S. Goldwasser, S. Micali, and C. Rackoff. The knowledge complexity of interactive proof-systems. <em>SIAM Journal on Computing</em>, 18(1):186&ndash;208, 1989.</p></li>
      <li><p class="text-gray-300">[15] P. Golle and I. Mironov. Uncheatable distributed computations. In <em>Proceedings of the RSA Conference</em>, 2001.</p></li>
      <li><p class="text-gray-300">[16] S. Hohenberger and A. Lysyanskaya. How to securely outsource cryptographic computations. In <em>Proceedings of TCC</em>, 2005.</p></li>
      <li><p class="text-gray-300">[17] Y. T. Kalai and R. Raz. Probabilistically checkable arguments. In <em>Proceedings of CRYPTO</em>, 2009.</p></li>
      <li><p class="text-gray-300">[18] J. Kilian. A note on efficient zero-knowledge proofs and arguments (extended abstract). In <em>Proceedings of the ACM Symposium on Theory of computing (STOC)</em>, pages 723&ndash;732, New York, NY, USA, 1992. ACM.</p></li>
      <li><p class="text-gray-300">[19] J. Kilian. Improved efficient arguments (preliminary version). In <em>Proceedings of the International Cryptology Conference on Advances in Cryptology</em>, pages 311&ndash;324, London, UK, 1995. Springer-Verlag.</p></li>
      <li><p class="text-gray-300">[20] Y. Lindell and B. Pinkas. A proof of Yao's protocol for secure two-party computation. <em>Journal of Cryptology</em>, 22(2):161&ndash;188, 2009.</p></li>
      <li><p class="text-gray-300">[21] S. Micali. CS proofs (extended abstract). In <em>Proceedings of the IEEE Symposium on Foundations of Computer Science</em>, 1994.</p></li>
      <li><p class="text-gray-300">[22] D. Molnar. The SETI@Home problem. <em>ACM Crossroads</em>, 7.1, 2000.</p></li>
      <li><p class="text-gray-300">[23] F. Monrose, P. Wyckoff, and A. Rubin. Distributed execution with remote audit. In <em>Proceedings of ISOC Network and Distributed System Security Symposium (NDSS)</em>, Feb. 1999.</p></li>
      <li><p class="text-gray-300">[24] G. Rothblum. <em>Delegating Computation Reliably: Paradigms and Constructions</em>. PhD thesis, Massachusetts Institute of Technology, 2009.</p></li>
      <li><p class="text-gray-300">[25] A. Seshadri, M. Luk, E. Shi, A. Perrig, L. VanDoorn, and P. Khosla. Pioneer: Verifying integrity and guaranteeing execution of code on legacy platforms. In <em>Proceedings of the Symposium on Operating Systems Principals</em>, 2005.</p></li>
      <li><p class="text-gray-300">[26] S. Smith and S. Weingart. Building a high-performance, programmable secure coprocessor. <em>Computer Networks (Special Issue on Computer Network Security)</em>, 31:831&ndash;960, 1999.</p></li>
      <li><p class="text-gray-300">[27] Trusted Computing Group. Trusted platform module main specification. Version 1.2, Revision 103, July 2007.</p></li>
      <li><p class="text-gray-300">[28] A. Yao. Protocols for secure computations. In <em>Proceedings of the IEEE Symposium on Foundations of Computer Science</em>, 1982.</p></li>
      <li><p class="text-gray-300">[29] A. Yao. How to generate and exchange secrets. In <em>Proceedings of the IEEE Symposium on Foundations of Computer Science</em>, 1986.</p></li>
      <li><p class="text-gray-300">[30] B. S. Yee. <em>Using Secure Coprocessors</em>. PhD thesis, Carnegie Mellon University, 1994.</p></li>
    </ul>

    </section>
`;
---

<BaseLayout title="Non-Interactive Verifiable Computing: Outsourcing Computatio... (2009/547)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2009 &middot; eprint 2009/547
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <PaperDisclaimer eprintUrl={EPRINT_URL} />
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <nav id="toc" class="mb-10 p-6 rounded-lg" style="background: rgba(255,255,255,0.03); border: 1px solid rgba(255,255,255,0.06);">
      <h2 class="text-lg font-bold mb-4">Table of Contents</h2>
      <ol class="space-y-1 text-sm text-gray-300
        list-decimal list-inside">
        <li><a href="#abstract" class="hover:text-white">Abstract</a></li>
        <li><a href="#sec-1" class="hover:text-white">Introduction</a></li>
        <li>
          <a href="#sec-2" class="hover:text-white">Background</a>
          <ol class="ml-6 mt-1 space-y-1 list-decimal
            list-inside text-gray-400">
            <li><a href="#sec-2.1" class="hover:text-white">Yao&#x27;s Garbled Circuit Construction</a></li>
            <li><a href="#sec-2.2" class="hover:text-white">The Security of Yao&#x27;s Protocol</a></li>
            <li><a href="#sec-2.3" class="hover:text-white">Fully-Homomorphic Encryption</a></li>
          </ol>
        </li>
        <li>
          <a href="#sec-3" class="hover:text-white">Problem Definition</a>
          <ol class="ml-6 mt-1 space-y-1 list-decimal
            list-inside text-gray-400">
            <li><a href="#sec-3.1" class="hover:text-white">Basic Requirements</a></li>
            <li><a href="#sec-3.2" class="hover:text-white">Input and Output Privacy</a></li>
            <li><a href="#sec-3.3" class="hover:text-white">Efficiency</a></li>
          </ol>
        </li>
        <li>
          <a href="#sec-4" class="hover:text-white">An Efficient Verifiable-Computation Scheme with Input/Output Privacy</a>
          <ol class="ml-6 mt-1 space-y-1 list-decimal
            list-inside text-gray-400">
            <li><a href="#sec-4.1" class="hover:text-white">Protocol Definition</a></li>
            <li><a href="#sec-4.2" class="hover:text-white">Proof of Security</a></li>
            <li><a href="#sec-4.3" class="hover:text-white">Proof Sketch of Yao&#x27;s Security for One Execution</a></li>
            <li><a href="#sec-4.4" class="hover:text-white">Proof of Theorem 1</a></li>
            <li><a href="#sec-4.5" class="hover:text-white">Proof of Input and Output Privacy</a></li>
            <li><a href="#sec-4.6" class="hover:text-white">Efficiency</a></li>
          </ol>
        </li>
        <li><a href="#sec-5" class="hover:text-white">How to Handle Cheating Workers</a></li>
        <li><a href="#sec-6" class="hover:text-white">Conclusions and Future Directions</a></li>
      </ol>
      <p class="text-xs text-gray-500 mt-4 mb-1 font-semibold">
        Additional
      </p>
      <ul class="space-y-1 text-sm text-gray-400
        list-disc list-inside">
        <li><a href="#references" class="hover:text-white">References</a></li>
      </ul>
    </nav>


    <Fragment set:html={CONTENT} />

    <PaperHistory slug="non-interactive-verifiable-computing-outsourcing-2009" />
  </article>
</BaseLayout>
