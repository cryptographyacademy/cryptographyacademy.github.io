---
import BaseLayout from '../../layouts/BaseLayout.astro';
import PaperDisclaimer from '../../components/PaperDisclaimer.astro';
import PaperHistory from '../../components/PaperHistory.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2012/481';
const CRAWLER = 'modal-marker';
const CONVERTED_DATE = '2026-02-18';
const TITLE_HTML = 'Improved Security Bounds for Key-Alternating Ciphers via Hellinger Distance';
const AUTHORS_HTML = 'John Steinberger';

const CONTENT = `    <p class="text-gray-300">John Steinberger
Institute of Theoretical Computer Science, Tsinghua University
jpsteinb@gmail.com</p>

    <h4 id="sec-1" class="text-lg font-semibold mt-6">Abstract</h4>

    <p class="text-gray-300">A t-round key alternating cipher can be viewed as an abstraction of AES. It defines a cipher E from t fixed public permutations  <span class="math">P_1, \\ldots, P_t : \\{0,1\\}^n \\to \\{0,1\\}^n</span>  and a key  <span class="math">k = k_0 \\| \\cdots \\| k_t \\in \\{0,1\\}^{n(t+1)}</span> by setting  <span class="math">E_k(x) = k_t \\oplus P_t(k_{t-1} \\oplus P_{t-1}(\\cdots k_1 \\oplus P_1(k_0 \\oplus x)\\cdots))</span> . The indistinguishability of  <span class="math">E_k</span> from a random truly random permutation by an adversary who also has oracle access to the (public) random permutations  <span class="math">P_1, \\ldots, P_t</span>  was investigated for t=2 by Even and Mansour [5] and, much later, by Bogdanov et al. [1]. The former proved indistinguishability up to  <span class="math">2^{n/2}</span>  queries for t=1 while the latter proved indistinguishability up to  <span class="math">2^{2n/3}</span>  queries for  <span class="math">t \\geq 2</span>  (ignoring low-order terms). Our contribution is to improve the analysis of Bogdanov et al. by showing security up to  <span class="math">2^{3n/4}</span>  queries for  <span class="math">t \\geq 3</span> . Given that security cannot exceed  <span class="math">2^{\\frac{t}{t+1}n}</span>  queries, this is in particular achieves a tight bound for the case t=3, whereas, previously, tight bounds had only been achieved for t=1 (by Even and Mansour) and for t=2 (by Bogdanov et al.). Our main technique is an improved analysis of the elegant sample distinguishability game introduced by Bogdanov et al. [1]. More specifically, we succeed in eliminating adaptivity by considering the Hellinger advantage of an adversary, a notion that we introduce here. To our knowledge, our result constitutes the first time Hellinger distance (a standard measure of &quot;distance&quot; between random variables, and a cousin of statistical distance) is used in a cryptographic indistinguishability proof.</p>

    <h4 id="sec-2" class="text-lg font-semibold mt-6">Introduction</h4>

    <p class="text-gray-300">Given t permutations  <span class="math">P_1, \\ldots, P_t : \\{0,1\\}^n \\to \\{0,1\\}^n</span>  the t-round key-alternating cipher based on  <span class="math">P_1, \\ldots, P_t</span>  is a blockcipher  <span class="math">E : \\{0,1\\}^{(t+1)n} \\times \\{0,1\\}^n \\to \\{0,1\\}^n</span>  of keyspace  <span class="math">\\{0,1\\}^{(t+1)n}</span>  and message space  <span class="math">\\{0,1\\}^n</span> , where for a key  <span class="math">k = k_0 ||k_1|| \\cdots ||k_t \\in \\{0,1\\}^{(t+1)n}</span>  and a message  <span class="math">x \\in \\{0,1\\}^n</span>  we set</p>

    <p class="text-gray-300"><span class="math">$E(k,x) = k_t \\oplus P_t(k_{t-1} \\oplus P_{t-1}(\\cdots P_1(k_0 \\oplus x)\\cdots)). \\tag{1}</span>$</p>

    <p class="text-gray-300">(See Figure 1.) Plainly,  <span class="math">E(k,\\cdot)</span>  is a permutation of  <span class="math">\\{0,1\\}^n</span>  for each fixed  <span class="math">k \\in \\{0,1\\}^{(t+1)n}</span> ; we let  <span class="math">E^{-1}(k,\\cdot)</span>  denote the inverse permutation. The  <span class="math">P_i</span> 's are called the <em>round permutations</em> of E and t is the <em>number of rounds</em> of E. Thus t and the permutations  <span class="math">P_1, \\ldots, P_t</span>  are parameters determining E.</p>

    <p class="text-gray-300">Key-alternating ciphers were first proposed (for values of t greater than 1) by the designers of AES [3,4], the Advanced Encryption Standard. Indeed, AES-128 itself can be viewed as a particular instantiation of the key-alternating cipher paradigm in which the round permutations  <span class="math">P_1, \\ldots, P_t</span>  equal a single permutation P (the Rijndael round function, in this case), in which t = 10, and in which only a subset of the  <span class="math">\\{0,1\\}^{(t+1)n} = \\{0,1\\}^{11n}</span>  possible keys are used (more precisely, the 11n bits of key are derived pseudorandomly from a seed of n bits, making the key space  <span class="math">\\{0,1\\}^n = \\{0,1\\}^{128}</span> ). However, for t = 1 the design was proposed much earlier by Even and Mansour as a means of constructing a blockcipher from a fixed permutation [5].</p>

    <p class="text-gray-300">Even and Mansour accompanied their proposal with &quot;provable security&quot; guarantees by showing that, for t = 1, an adversary needs roughly  <span class="math">2^{n/2}</span>  queries to distinguish  <span class="math">E(k, \\cdot)</span>  for a random key k (k being</p>

    <p class="text-gray-300">    <img src="_page_1_Picture_0.jpeg" alt="" class="my-4 max-w-full" />
</p>

    <p class="text-gray-300">Figure 1: A t-round key alternating cipher.</p>

    <p class="text-gray-300">hidden from the adversary) from a true random permutation, in a model where the adversary is given oracle access to  <span class="math">E(k,\\cdot)</span> ,  <span class="math">E^{-1}(k,\\cdot)</span>  as well as to  <span class="math">P_1</span> ,  <span class="math">P_1^{-1}</span> , where  <span class="math">P_1</span>  is modeled as a random permutation (in the dummy world, the adversary is given oracle access to two independent random permutations and their inverses). Their bound was matched by Daemen [2], who showed a  <span class="math">2^{n/2}</span> -query distinguishing attack for t=1.</p>

    <p class="text-gray-300">For t &gt; 1, we can generalize the Even-Mansour indistinguishability experiment by giving the adversary oracle access to  <span class="math">P_1, \\ldots, P_t</span>  and their inverses and to  <span class="math">E(k, \\cdot)</span> ,  <span class="math">E^{-1}(k, \\cdot)</span>  in the real world (for a randomly chosen, hidden  <span class="math">k \\in \\{0,1\\}^{(t+1)n}</span> ), and to a tuple of t+1 independent random permutations and their inverses in the &quot;ideal&quot; or &quot;dummy&quot; world (see Figure 2). In this case, Daemen's attack can be easily generalized to an attack of query complexity  <span class="math">2^{\\frac{t}{t+1}n}</span> , as pointed out by Bogdanov et al. [1], but the security analysis of Even and Mansour does not similarly generalize to a security bound of  <span class="math">2^{\\frac{t}{t+1}n}</span>  (though security of  <span class="math">2^{n/2}</span>  queries still holds, and is easy to prove in a black-box fashion from the Even-Mansour result).</p>

    <p class="text-gray-300">Despite the advent of AES, further provable security improvements on key-alternating ciphers for t &gt; 1 had to wait for the afore-mentioned paper of Bogdanov et al. [1], who showed (in the same model as Even and Mansour) security of  <span class="math">2^{\\frac{2}{3}n}</span>  queries for  <span class="math">t \\ge 2</span>  (modulo lower-order terms). This bound is tight for t = 2, as it matches the  <span class="math">2^{\\frac{t}{t+1}n}</span> -query attack, but is not sharp for t &gt; 2—e.g., the best known attack for t = 3 has cost  <span class="math">2^{\\frac{3}{4}n}</span> , whereas the best known security bound remains  <span class="math">2^{\\frac{2}{3}n}</span> . In this paper we further this line of work by showing that key-alternating ciphers enjoy security of  <span class="math">2^{\\frac{3}{4}n}</span>  queries for  <span class="math">t \\ge 3</span> . In a nutshell, the Even-Mansour bound is tight for t = 1, the Bogdanov et al. bound is tight for t = 2, and our bound is tight for t = 3. It remains an open problem to prove tight bounds for  <span class="math">t \\ge 4</span>  (though, we emphasize, our bound also improves the previous best of  <span class="math">2^{\\frac{2}{3}n}</span>  up to  <span class="math">2^{\\frac{3}{4}n}</span>  for t &gt; 4).</p>

    <p class="text-gray-300">Our proof follows closely the method of Bogdanov et al. [1]. Essentially, our improvement follows by replacing a certain &quot;loose&quot; statistical distance triangle inequality in [1] by a sharper inequality based on Hellinger distance (a variant of statistical distance). In fact, this technique more generally gives a much improved (and, in a cryptographic sense, sharp) analysis of the elegant sample distinguishability game introduced by Bogdanov et al. The modified sample distinguishability analysis that we present is also interesting because we show (loosely speaking) that the &quot;statistical&quot; advantage of an adaptive adversary can be upper bounded by the &quot;Hellinger&quot; advantage of a non-adaptive adversary. It thus presents a new, interesting instance of the paradigm common in cryptographic proofs that seeks to replace an adaptive adversary by a non-adaptive adversary in order to upper bound advantage [8,9,12–14].</p>

    <p class="text-gray-300">Our work has two main parts: (i) an improved (generic) analysis of sample distinguishability based on the technical concept of Hellinger distance, and (ii) an application of the previous analysis to the sample distinguishability game defined by Bogdanov et al., where the main hurdle is to upper bound the relevant Hellinger distance (whereas Bogdanov et al. only upper bounded statistical distance).</p>

    <p class="text-gray-300">In the paper's first part we (re-)introduce sample distinguishability, Hellinger distance, and their interconnection. This part contains the improved Hellinger-distance-based upper bound for sample distinguishability that is of independent interest from the paper's main result. In the paper's second</p>

    <p class="text-gray-300">part we give more detailed definitions for key-alternating ciphers and the security experiment. We then recall the outline of Bogdanov et al.'s security analysis for key-alternating ciphers and &quot;plug in&quot; our improved sample distinguishability bound to this proof, thus obtaining the improvement from  <span class="math">2^{\\frac{2}{3}n}</span>  to  <span class="math">2^{\\frac{3}{4}n}</span>  queries for  <span class="math">t \\geq 3</span>  rounds.</p>

    <p class="text-gray-300">To the best of our knowledge, this paper represents the first application of Hellinger distance in cryptography, or to indistinguishability in general.</p>

    <p class="text-gray-300">A notational preliminary. The statistical distance  <span class="math">\\Delta(X,Y)</span>  (more accurately known, also, as the total variation distance) between two random variables X, Y of range S is defined as</p>

    <p class="text-gray-300"><span class="math">$\\Delta(X,Y) := \\max_{T \\subseteq S} (\\Pr[X \\in T] - \\Pr[Y \\in T]) = \\sum_{s \\in S} \\frac{1}{2} |\\Pr[X = s] - \\Pr[Y = s]|.</span>$</p>

    <p class="text-gray-300">It is well-known (and easily proved) that</p>

    <p class="text-gray-300"><span class="math">$\\Delta(X,Y) = \\sup_{D} |\\Pr[D(X) = 1] - \\Pr[D(Y) = 1]|</span>$</p>

    <p class="text-gray-300">where the sup is taken over all (probabilistic or deterministic—it doesn't matter) distinguishers  <span class="math">D: S \\to \\{0,1\\}</span> , and where  <span class="math">\\Pr[D(X)=1]</span> ,  <span class="math">\\Pr[D(Y)=1]</span>  are the probabilities that D outputs 1 when given a sample of X and Y, respectively, these probabilities being computed over the randomness in X and Y and over D's coins, if any.</p>

    <h3 id="sec-3" class="text-xl font-semibold mt-8">1 Sample Distinguishability and Hellinger Distance</h3>

    <p class="text-gray-300">We start by recalling the elegant <em>sample distinguishability</em> game originally introduced as a mid-level abstraction in the proof of Bogdanov et al.</p>

    <p class="text-gray-300">Bogdanov et al. consider a family  <span class="math">(X_{\\alpha}, Y_{\\alpha})_{\\alpha \\in A}</span>  of pairs of random variables indexed by some finite&lt;sup&gt;1&lt;/sup&gt; set A. For each  <span class="math">\\alpha \\in A</span> ,  <span class="math">X_{\\alpha}</span>  and  <span class="math">Y_{\\alpha}</span>  take values in some finite set  <span class="math">S_{\\alpha}</span>  (one can assume, for conceptual simplicity and without loss of generality, that all  <span class="math">S_{\\alpha}</span> 's are equal, say  <span class="math">S_{\\alpha} = S</span>  for all  <span class="math">\\alpha</span>  where S is some sufficiently large finite set).</p>

    <p class="text-gray-300">The notation  <span class="math">D^{(X_{\\alpha})_{\\alpha\\in A}}</span>  indicates that a distinguisher D is given oracle access to a family of random variables indexed by the set A, in this case the family  <span class="math">(X_{\\alpha})_{\\alpha\\in A}</span> . More precisely, this means D's query sequence has the form  <span class="math">\\alpha_1, \\ldots, \\alpha_q</span>  where each  <span class="math">\\alpha_i</span>  is in A, query  <span class="math">\\alpha_i</span>  being answered by a sample from  <span class="math">X_{\\alpha_i}</span> . D may repeat queries to the same  <span class="math">\\alpha \\in A</span> , in which case a &quot;fresh&quot; sample of  <span class="math">X_{\\alpha}</span>  is returned each time to D. All samples returned to D are independent (including, thus, samples from repeated queries to the same  <span class="math">\\alpha \\in A</span> ). In general, D can be adaptive.</p>

    <p class="text-gray-300">D's sample distinguishability advantage with respect to the family of pairs  <span class="math">(X_{\\alpha}, Y_{\\alpha})_{\\alpha \\in A}</span>  is defined as</p>

    <p class="text-gray-300"><span class="math">$\\Delta_D^{\\mathsf{samp}}((X_\\alpha, Y_\\alpha)_{\\alpha \\in A}) = |\\Pr[D^{(X_\\alpha)_{\\alpha \\in A}} = 1] - \\Pr[D^{(Y_\\alpha)_{\\alpha \\in A}} = 1]|</span>$</p>

    <p class="text-gray-300">where  <span class="math">D^{(X_{\\alpha})_{\\alpha \\in A}} = 1</span>  indicates the event that D outputs 1 after interacting with its oracle. The q-query sample distinguishability of the family  <span class="math">(X_{\\alpha}, Y_{\\alpha})_{\\alpha \\in A}</span>  is defined as</p>

    <p class="text-gray-300"><span class="math">$\\Delta^{\\mathsf{samp}}(q,(X_\\alpha,Y_\\alpha)_{\\alpha\\in A}) = \\sup_D \\Delta^{\\mathsf{samp}}_D((X_\\alpha,Y_\\alpha)_{\\alpha\\in A})</span>$</p>

    <p class="text-gray-300">where the sup is taken over all distinguishers D making at most q queries.</p>

    <p class="text-gray-300">&lt;sup&gt;&amp;&lt;/sup&gt;lt;sup&gt;1&lt;/sup&gt;The various finiteness assumptions are, of course, made mostly for simplicity; by replacing maximums by supremums and discrete sums by Lebesgue integrals, the game and theory can be generalized to the case of an infinite A and arbitrary measure spaces  <span class="math">S_{\\alpha}</span>  equipped with probability measures  <span class="math">X_{\\alpha}, Y_{\\alpha}</span> .</p>

    <p class="text-gray-300">It might seem, given the independence of the various samples, that adaptivity cannot help the distinguisher. However, as Bogdanov et al. point out, this intuition is mistaken. Bodganov et al. give a somewhat hard-to-follow example with q=2, |A|=2 and  <span class="math">\\max_{\\alpha}|S_{\\alpha}|=3</span>  that shows an adaptive adversary can have better advantage than a non-adaptive one. A much simpler and more intuitive example was provided to us by Liu Tianren [15]. In this example q=2, |A|=3, and  <span class="math">\\max_{\\alpha} |S_{\\alpha}|=2</span> . We put, more precisely,  <span class="math">A = \\{1, 2, 3\\}</span>  and  <span class="math">S_{\\alpha} = \\{0, 1\\}</span>  for all  <span class="math">\\alpha \\in A = \\{1, 2, 3\\}</span> ; the pairs  <span class="math">(X_1, Y_1), (X_2, Y_2),</span>  <span class="math">(X_3, Y_3)</span>  are given by the probability tables</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Pr</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">0</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">1</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Pr</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">(</th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">X_1</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">2/3</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">1/3</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">X_2</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">Y_1</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">1/3</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">2/3</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">Y_2</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">3,</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
          </tr>
        </tbody>
      </table>
    </div>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Pr</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">0</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">1</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Pr</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">0</th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">X_2</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">1</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">0</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">X_3</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">1/4</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">Y_2</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">3/4</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">1/4</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">Y_3</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">0</td>
          </tr>
        </tbody>
      </table>
    </div>

    <p class="text-gray-300"><span class="math">$\\Delta(X_1, Y_1) = 1/3</span>$
<span class="math">\\Delta(X_2, Y_2) = 1/4</span>   <span class="math">\\Delta(X_3, Y_3) = 1/4</span></p>

    <p class="text-gray-300">Here the optimal distinguisher is an adaptive distinguisher D that first queries 1, then queries 2 if the answer to the first query is 0, and otherwise queries 3 if the answer to the first query is 1. One can</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li><p class="text-gray-300">check, then, that  <span class="math">\\Delta_D^{\\mathsf{samp}} = \\frac{1}{2}</span> . On the other hand, one can also verify that</p>

    <p class="text-gray-300">if D non-adaptively queries (1,1),  <span class="math">\\Delta_D^{\\mathsf{samp}} = 1/3</span>  if D non-adaptively queries (1,2),  <span class="math">\\Delta_D^{\\mathsf{samp}} = 5/12</span>  (and similarly if D non-adaptively queries (1,3))</p>

    <p class="text-gray-300">if D non-adaptively queries (2,3),  <span class="math">\\Delta_D^{\\mathsf{samp}} = 1/4</span>  (and similarly if D non-adaptively queries (3,2))</p>

    <p class="text-gray-300">if D non-adaptively queries (2,2),  <span class="math">\\Delta_D^{\\mathsf{samp}} = 7/16</span>  (and similarly if D non-adaptively queries (3,3)).</p>

    <p class="text-gray-300">Thus, the advantage of the best adaptive distinguisher is  <span class="math">\\frac{1}{2}</span>  whereas the advantage of the best nonadaptive distinguisher is  <span class="math">\\frac{7}{16}</span> .</p></li>
    </ul>

    <p class="text-gray-300">We define separately the non-adaptive sample distinguishibility of a family  <span class="math">(X_{\\alpha}, Y_{\\alpha})_{\\alpha \\in A}</span>  as</p>

    <p class="text-gray-300"><span class="math">$\\Delta_{\\mathrm{non}}^{\\mathrm{samp}}(q,(X_{\\alpha},Y_{\\alpha})_{\\alpha\\in A}) = \\sup_{D} \\Delta_{D}^{\\mathrm{samp}}((X_{\\alpha},Y_{\\alpha})_{\\alpha\\in A})</span>$</p>

    <p class="text-gray-300">where the sup, this time, is taken over all non-adaptive distinguishers D making at most q queries (a nonadaptive distinguisher is defined as a distinguisher that announces its sequence of queries  <span class="math">(\\alpha_1, \\ldots, \\alpha_q) \\in</span>  <span class="math">A^q</span>  at the start of the game, before receiving any query answers). We will prove, among others, that</p>

    <p class="text-gray-300"><span class="math">$\\Delta^{\\mathsf{samp}}(q,(X_{\\alpha},Y_{\\alpha})_{\\alpha\\in A}) \\leq \\sqrt{2\\Delta^{\\mathsf{samp}}_{\\mathsf{non}}(q,(X_{\\alpha},Y_{\\alpha})_{\\alpha\\in A})} \\tag{2}</span>$</p>

    <p class="text-gray-300">for any family  <span class="math">(X_{\\alpha}, Y_{\\alpha})_{\\alpha \\in A}</span> . Thus, while adaptivity helps for sample distinguishability, it helps &quot;at most quadratically&quot;. In practice, it seems, the (potentially) quadratic discrepancy between the adaptive and non-adaptive sample distinguishability advantages makes little difference for cryptographic applications (due to the fact that there is typically little difference, e.g., between the number of non-adaptive queries necessary for reaching distinguishing advantage 0.5 and the number necessary for reaching distinguishing advantage  <span class="math">0.5^2/\\sqrt{2}</span> ).</p>

    <p class="text-gray-300">We will prove (2) using Hellinger distance. Coincidentally, Hellinger distance is also the appropriate tool for upper bounding  <span class="math">\\Delta_{\\mathsf{non}}^{\\mathsf{samp}}</span> —thus Hellinger distance will turn out to be &quot;twice useful&quot; in this paper.</p>

    <p class="text-gray-300">Given random variables X and Y of finite range S, the Hellinger distance h(X,Y) between X and Y is defined via</p>

    <p class="text-gray-300"><span class="math">$\\begin{array}{lcl} h^2(X,Y) &amp;:=&amp; \\frac{1}{2}\\sum_{s\\in S}(\\sqrt{\\Pr[X=s]}-\\sqrt{\\Pr[Y=s]})^2\\\\ &amp;=&amp; 1-\\sum_{s\\in S}\\sqrt{\\Pr[X=s]\\Pr[Y=s]} \\end{array}</span>$</p>

    <p class="text-gray-300">where the second equality is easily verified by elementary algebra. (Thus, we emphasize, h(X,Y) is the positive square root of the above quantities.) We note already that</p>

    <p class="text-gray-300"><span class="math">$1 - h^2(X, Y) = \\sum_{s \\in S} \\sqrt{\\Pr[X = s] \\Pr[Y = s]}</span>$</p>

    <p class="text-gray-300">has a fairly simple expression, which will play a role later on.</p>

    <p class="text-gray-300">Hellinger distance can be used to upper bound statistical distance and vice-versa. One has, to be precise,</p>

    <p class="text-gray-300"><span class="math">$h^2(X,Y) \\le \\Delta(X,Y) \\le \\sqrt{2}h(X,Y). \\tag{3}</span>$</p>

    <p class="text-gray-300">(The first inequality follows directly from the fact that  <span class="math">(\\sqrt{a} - \\sqrt{b})^2 \\le |a - b|</span>  for all real numbers  <span class="math">a, b \\ge 0</span> ; the second inequality can be proved from Cauchy-Schwarz.)</p>

    <p class="text-gray-300">A product distribution is a random variable of the form  <span class="math">X = (X_i)_{i=1}^n</span>  where the  <span class="math">X_i</span> 's are (fully) independent. One can note that if  <span class="math">X = (X_i)_{i=1}^n</span> ,  <span class="math">X = (Y_i)_{i=1}^n</span>  are product distributions where  <span class="math">X_i</span>  and  <span class="math">Y_i</span>  have finite range  <span class="math">S_i</span> , then</p>

    <p class="text-gray-300"><span class="math">$1 - h^{2}(X,Y) = \\sum_{(s_{1},\\dots,s_{n})\\in S_{1}\\times\\dots\\times S_{n}} \\sqrt{\\Pr[X = (s_{1},\\dots,s_{n})] \\Pr[Y = (s_{1},\\dots,s_{n})]}</span>$</p>

    <p class="text-gray-300"><span class="math">$= \\sum_{(s_{1},\\dots,s_{n})\\in S_{1}\\times\\dots\\times S_{n}} \\sqrt{\\Pr[X_{1} = s_{1}] \\cdots \\Pr[X_{n} = s_{n}] \\Pr[Y_{1} = s_{1}] \\cdots \\Pr[Y_{n} = s_{n}]}</span>$</p>

    <p class="text-gray-300"><span class="math">$= \\left(\\sum_{s_{1}\\in S_{1}} \\sqrt{\\Pr[X_{1} = s_{1}] \\Pr[Y_{1} = s_{1}]}\\right) \\cdots \\left(\\sum_{s_{n}\\in S_{n}} \\sqrt{\\Pr[X_{n} = s_{n}] \\Pr[Y_{n} = s_{n}]}\\right)</span>$</p>

    <p class="text-gray-300"><span class="math">$= \\prod_{i=1}^{n} (1 - h^{2}(X_{i}, Y_{i}))</span>$</p>

    <p class="text-gray-300"><span class="math">$\\geq 1 - \\sum_{i=1}^{n} h^{2}(X_{i}, Y_{i})</span>$</p>

    <p class="text-gray-300">so, in particular,</p>

    <p class="text-gray-300"><span class="math">$h^{2}(X,Y) \\leq \\sum_{i=1}^{n} h^{2}(X_{i},Y_{i}) \\leq n \\max_{i} h^{2}(X_{i},Y_{i})</span>$</p>

    <p class="text-gray-300">and</p>

    <p class="text-gray-300"><span class="math">$h(X,Y) \\le \\sqrt{\\sum_{i=1}^{n} h^2(X_i, Y_i)} \\le \\sqrt{n} \\max_{i} h(X_i, Y_i).</span>$
(4)</p>

    <p class="text-gray-300">We note that statistical distance admits the similar inequalities</p>

    <p class="text-gray-300"><span class="math">$\\Delta(X,Y) \\le \\sum_{i=1}^{n} \\Delta(X_i, Y_i) \\le n \\max_{i} \\Delta(X_i, Y_i)</span>$
(5)</p>

    <p class="text-gray-300">but the first inequality is typically fairly loose. Usually, the inequality</p>

    <p class="text-gray-300"><span class="math">$\\Delta(X,Y) \\le \\sqrt{2}h(X,Y) \\le \\sqrt{n} \\max_{i} h(X_i, Y_i)</span>$
(6)</p>

    <p class="text-gray-300">obtained by combining (3) and (4) gives a better upper bound on  <span class="math">\\Delta(X,Y)</span> . (On a very simplified level, our paper's improvement boils down to the difference between (6) and (5).) We next illustrate these</p>

    <p class="text-gray-300">concepts with an example.</p>

    <p class="text-gray-300">Example. Let  <span class="math">X = (X_i)_{i=1}^n</span> ,  <span class="math">Y = (Y_i)_{i=1}^n</span>  be product distributions where  <span class="math">X_1, \\ldots, X_n</span>  are identically distributed, and so for  <span class="math">Y_1, \\ldots, Y_n</span> , these distributions being given by</p>

    <p class="text-gray-300"><span class="math">$\\Pr[X_i = 1] = \\frac{1}{2} + \\varepsilon \\qquad \\Pr[X_i = 0] = \\frac{1}{2} - \\varepsilon</span>$</p>

    <p class="text-gray-300"><span class="math">$\\Pr[Y_i = 1] = \\frac{1}{2} - \\varepsilon \\qquad \\Pr[Y_i = 0] = \\frac{1}{2} + \\varepsilon</span>$</p>

    <p class="text-gray-300">for some small  <span class="math">\\varepsilon</span> . Thus, X represents n independent samples of a coin with a  <span class="math">+\\varepsilon</span>  bias towards 1, whereas Y represents n independent samples of a coin with a  <span class="math">+\\varepsilon</span>  bias towards 0. One can show with a Chernoff bound that  <span class="math">n = O(1/\\varepsilon^2)</span>  samples are <em>sufficient</em> to distinguish X and Y; that is,</p>

    <p class="text-gray-300"><span class="math">$\\Delta(X,Y) = \\Delta^{\\mathsf{samp}}(n,(X_1,Y_1)) = \\Omega(1)</span>$</p>

    <p class="text-gray-300">for  <span class="math">n = O(1/\\varepsilon^2)</span> . (Here we write simply  <span class="math">(X_1, Y_1)</span>  for the family  <span class="math">(X_\\alpha, Y_\\alpha)_{\\alpha \\in A}</span>  whose only member is the pair  <span class="math">(X_1, Y_1)</span> .) Also, (5) gives</p>

    <p class="text-gray-300"><span class="math">$\\Delta(X,Y) \\le n\\Delta(X_1,Y_1) = 2n\\varepsilon</span>$</p>

    <p class="text-gray-300">which shows that  <span class="math">n = O(1/\\varepsilon)</span>  samples are necessary to distinguish X and Y—but  <span class="math">O(1/\\varepsilon)</span>  is a far cry from  <span class="math">O(1/\\varepsilon^2)</span> . On the other hand, (6) gives</p>

    <p class="text-gray-300"><span class="math">$\\Delta(X,Y) \\le \\sqrt{n}h(X_1,Y_1) = \\sqrt{n}O(\\varepsilon)</span>$</p>

    <p class="text-gray-300">which, indeed, shows that  <span class="math">n = O(1/\\varepsilon^2)</span>  samples are necessary to distinguish the biased coins. (We skip the straightforward computation showing that  <span class="math">h(X_1, Y_1) = O(\\varepsilon)</span> .) In this case, therefore, (6) gives a substantially better upper bound on  <span class="math">\\Delta(X, Y)</span>  than (5).</p>

    <p class="text-gray-300">As one of our paper's main results, we will prove that</p>

    <p class="text-gray-300"><span class="math">$\\Delta^{\\mathsf{samp}}(q, (X_{\\alpha}, Y_{\\alpha})_{\\alpha \\in A}) \\le \\sqrt{2q} \\max_{\\alpha \\in A} h(X_{\\alpha}, Y_{\\alpha}) \\tag{7}</span>$</p>

    <p class="text-gray-300">for any family  <span class="math">(X_{\\alpha}, Y_{\\alpha})_{{\\alpha} \\in A}</span> , and any  <span class="math">q \\in \\mathbb{N}</span> . Note that (7) generalizes (6) since, obviously,  <span class="math">\\Delta(X, Y) \\leq \\Delta^{\\mathsf{samp}}(n, (X_i, Y_i)_{i \\in [n]})</span>  for product distributions  <span class="math">X = (X_i)_{i=1}^n</span> ,  <span class="math">Y = (Y_i)_{i=1}^n</span> . We continue with, as our main goal, a proof of (7).</p>

    <p class="text-gray-300">By a standard argument</p>

    <p class="text-gray-300"><span class="math">$\\Delta^{\\mathrm{samp}}(q,(X_{\\alpha},Y_{\\alpha})_{\\alpha\\in A})=\\sup_{D}\\Delta^{\\mathrm{samp}}_{D}((X_{\\alpha},Y_{\\alpha})_{\\alpha\\in A})</span>$</p>

    <p class="text-gray-300">with the sup taken over all deterministic (adaptive) distinguishers D making at most q queries. Thus we can restrict our attention to deterministic distinguishers.</p>

    <p class="text-gray-300">The &quot;query strategy&quot; of a deterministic q-query distinguisher D for a family  <span class="math">(X_{\\alpha}, Y_{\\alpha})_{\\alpha \\in A}</span>  can be encoded as a tree of depth q. Each non-leaf node of the tree is labeled by a query  <span class="math">\\alpha \\in A</span>  where, e.g., the root of the tree is labeled by D's first query. For a node labeled  <span class="math">\\alpha</span> , there are  <span class="math">|S_{\\alpha}|</span>  children for that node, each child corresponding to some element of  <span class="math">S_{\\alpha}</span> , where  <span class="math">S_{\\alpha}</span>  (we recall) is the range of  <span class="math">X_{\\alpha}</span>  and  <span class="math">Y_{\\alpha}</span> . When D makes its queries, it follows the tree downward from the root to a leaf according to the answers it receives from its oracle. Finally, each leaf is labeled with a decision: 0 or 1. Such a labeled tree fully describes a deterministic, adaptive distinguisher D.</p>

    <p class="text-gray-300">Take now some arbitrary (deterministic, adaptive, q-query) D, let T be the tree associated to D, and let r be the root of T. We write  <span class="math">T_v</span>  for the subtree of T rooted at  <span class="math">v \\in V(T)</span>  and write  <span class="math">If(T_v)</span>  for the set of leaves of  <span class="math">T_v</span> . Thus  <span class="math">T = T_r</span> . For  <span class="math">\\ell \\in If(T_v)</span>  we write  <span class="math">P_X(v \\to \\ell)</span>  for the probability that leaf</p>

    <p class="text-gray-300"><span class="math">\\ell</span>  is reached if queries start at v and the oracle is  <span class="math">(X_{\\alpha})_{\\alpha \\in A}</span> .  <span class="math">P_Y(v \\to \\ell)</span>  is similarly defined. We also define  <span class="math">P_X(\\ell \\to \\ell) = P_Y(\\ell \\to \\ell) = 1</span>  for any leaf  <span class="math">\\ell</span> . We let  <span class="math">L_X^T</span> ,  <span class="math">L_Y^T</span>  be random variables of range  <span class="math">\\mathrm{lf}(T)</span> , such that  <span class="math">\\Pr[L_X^T = \\ell] = P_X(r \\to \\ell)</span>  and  <span class="math">\\Pr[L_Y^T = \\ell] = P_Y(r \\to \\ell)</span> . In other words,  <span class="math">L_X^T</span>  is distributed according to the probability that D lands at a given leaf when the oracle is  <span class="math">(X_\\alpha)_{\\alpha \\in A}</span> , and likewise with  <span class="math">L_Y^T</span>  and  <span class="math">(Y_\\alpha)_{\\alpha \\in A}</span> . It is easy to see, then, that  <span class="math">\\Delta_D^{\\mathsf{samp}}((X_\\alpha, Y_\\alpha)_{\\alpha \\in A}) \\leq \\Delta(L_X^T, L_Y^T)</span>  (and, moreover,  <span class="math">\\Delta_D^{\\mathsf{samp}}((X_\\alpha, Y_\\alpha)_{\\alpha \\in A}) = \\Delta(L_X^T, L_Y^T)</span>  if D labels the leaves intelligently, e.g., if leaf  <span class="math">\\ell</span>  has label 1 if and only if  <span class="math">\\Pr[L_X^T = \\ell] \\geq \\Pr[L_Y^T = \\ell]</span> ). In particular, by (3),</p>

    <p class="text-gray-300"><span class="math">$\\Delta_D^{\\mathsf{samp}}((X_\\alpha, Y_\\alpha)_{\\alpha \\in A}) \\le \\Delta(L_X^T, L_Y^T) \\le \\sqrt{2}h(L_X^T, L_Y^T). \\tag{8}</span>$</p>

    <p class="text-gray-300">Say a tree T' is non-adaptive if all the nodes at a given level of the tree have the same label. Obviously, the tree associated to a non-adaptive adversary (viewed as a special case of an adaptive adversary) is a non-adaptive tree. We will show:</p>

    <p class="text-gray-300"><span class="math">$\\exists \\text{ non-adaptive } T&#x27; \\text{ of depth } q \\text{ s.t.} \\forall \\text{ adaptive } T \\text{ of depth } q, \\ h(L_X^T, L_Y^T) \\leq h(L_X^{T&#x27;}, L_Y^{T&#x27;}) \\tag{9}</span>$</p>

    <p class="text-gray-300">In other words, adaptivity does not help for maximizing the Hellinger distance between the probability distributions on the leaves (even though it does help for maximizing the statistical distance between the distributions on the leaves).</p>

    <p class="text-gray-300">For convenience, we define</p>

    <p class="text-gray-300"><span class="math">$\\begin{split} H(T) &amp;= 1 - h^2(L_X^T, L_Y^T) \\\\ &amp;= \\sum_{\\ell \\in \\mathrm{lf}(T)} \\sqrt{\\Pr[L_X^T = \\ell] \\Pr[L_Y^T = \\ell]} \\end{split}</span>$</p>

    <p class="text-gray-300">for a tree T. Then (9) is equivalent to</p>

    <p class="text-gray-300"><span class="math">$\\exists \\text{ non-adaptive } T&#x27; \\text{ of depth } q \\text{ s.t.} \\forall \\text{ adaptive } T \\text{ of depth } q, \\ H(L_X^T, L_Y^T) \\geq H(L_X^{T&#x27;}, L_Y^{T&#x27;}). \\tag{10}</span>$</p>

    <p class="text-gray-300">We prove (10) by induction on q. When q=1 the result is obvious, since it suffices to define T' to be the depth-1 tree whose root is labeled by the element  <span class="math">\\beta \\in A</span>  that maximizes  <span class="math">h(X_{\\beta}, Y_{\\beta})</span>  (i.e., that minimizes  <span class="math">H(X_{\\beta}, Y_{\\beta})</span> ). The fact that A is finite guarantees the existence of such a  <span class="math">\\beta</span> .</p>

    <p class="text-gray-300">For the induction step, assume that (10) has been established for  <span class="math">q \\leq t</span> , and let  <span class="math">T&#x27;_t</span>  be the tree T' satisfying (10) for q = t. Let T be an arbitrary tree of depth t + 1. Let T(v) denote the subtree of T rooted at  <span class="math">v \\in V(T)</span>  (thus, for example, T = T(r) where r is the root of T). Let the root r of T have label  <span class="math">\\alpha_r \\in A</span> , where  <span class="math">X_{\\alpha_r}</span> ,  <span class="math">Y_{\\alpha_r}</span>  have range  <span class="math">S_{\\alpha_r}</span> , and assume that r's children in T are  <span class="math">\\{v_s : s \\in S_{\\alpha_r}\\}</span> . Then</p>

    <p class="text-gray-300"><span class="math">$\\begin{split} H(T) &amp;= H(L_X^T, L_Y^T) \\\\ &amp;= \\sum_{\\ell \\in \\operatorname{If}(T)} \\sqrt{\\Pr[L_X^T = \\ell] \\Pr[L_Y^T = \\ell]} \\\\ &amp;= \\sum_{s \\in S_{\\alpha_r}} \\sum_{\\ell \\in \\operatorname{If}(T(v_s))} \\sqrt{\\Pr[X_{\\alpha_r} = v_s] \\Pr[L_X^{T(v_s)} = \\ell] \\Pr[Y_{\\alpha_r} = v_s] \\Pr[L_Y^{T(v_s)} = \\ell]} \\\\ &amp;= \\sum_{s \\in S_{\\alpha_r}} \\sqrt{\\Pr[X_{\\alpha_r} = v_s] \\Pr[Y_{\\alpha_r} = v_s]} \\sum_{\\ell \\in \\operatorname{If}(T(v_s))} \\sqrt{\\Pr[L_X^{T(v_s)} = \\ell] \\Pr[L_Y^{T(v_s)} = \\ell]} \\\\ &amp;= \\sum_{s \\in S_{\\alpha_r}} \\sqrt{\\Pr[X_{\\alpha_r} = v_s] \\Pr[Y_{\\alpha_r} = v_s]} H(T(v_s)) \\\\ &amp;\\geq \\sum_{s \\in S_{\\alpha_r}} \\sqrt{\\Pr[X_{\\alpha_r} = v_s] \\Pr[Y_{\\alpha_r} = v_s]} H(T_t&#x27;) \\end{split}</span>$</p>

    <p class="text-gray-300"><span class="math">$= H(T&#x27;_t) \\sum_{s \\in S_{\\alpha_r}} \\sqrt{\\Pr[X_{\\alpha_r} = v_s] \\Pr[Y_{\\alpha_r} = v_s]}</span>$
<span class="math">$= H(T&#x27;_t) H(X_{\\alpha_r}, Y_{\\alpha_r}).</span>$</p>

    <p class="text-gray-300">On the other hand, if we define  <span class="math">T&#x27;_{t+1}</span>  to be the non-adaptive tree whose root is labeled by the element  <span class="math">\\beta \\in A</span>  that minimizes  <span class="math">H(X_{\\beta}, Y_{\\beta})</span>  and whose first-level subtrees are each equal to  <span class="math">T&#x27;_t</span> , then</p>

    <p class="text-gray-300"><span class="math">$H(T&#x27;_{t+1}) = H(T&#x27;_t)H(X_{\\beta}, Y_{\\beta}) \\le H(T&#x27;_t)H(X_{\\alpha_r}, Y_{\\alpha_r})</span>$</p>

    <p class="text-gray-300">so that  <span class="math">H(T&#x27;_{t+1}) \\leq H(T)</span> , as desired. This establishes (10) and (9).</p>

    <p class="text-gray-300">Now let T' be the non-adaptive tree of depth q whose existence is guaranteed by (9) and let D' be the non-adaptive adversary associated to T', where we label the leaves of T' such that  <span class="math">\\ell \\in lf(T&#x27;)</span>  has label 1 if and only if  <span class="math">\\Pr[L_X^{T&#x27;} = \\ell] \\ge \\Pr[L_Y^{T&#x27;} = \\ell]</span> . Then  <span class="math">\\Delta_{D&#x27;}^{\\mathsf{samp}}((X_\\alpha, Y_\\alpha)_{\\alpha \\in A}) = \\Delta(L_X^{T&#x27;}, L_Y^{T&#x27;})</span>  and for any q-query adaptive distinguisher D with tree T we have</p>

    <p class="text-gray-300"><span class="math">$\\Delta_D^{\\mathsf{samp}}((X_\\alpha, Y_\\alpha)_{\\alpha \\in A}) \\leq \\Delta(L_X^T, L_Y^T) \\leq \\sqrt{2}h(L_X^T, L_Y^T) \\leq \\sqrt{2}h(L_X^{T&#x27;}, L_Y^{T&#x27;}) \\leq \\sqrt{2}q \\max_{\\alpha \\in A} h(X_\\alpha, Y_\\alpha) \\tag{11}</span>$</p>

    <p class="text-gray-300">where: (i) the second inequality follows by (3), (ii) the third inequality follows by the choice of T', (iii) the fourth inequality follows by (6) because  <span class="math">h(L_X^{T&#x27;}, L_Y^{T&#x27;}) = h(X, Y)</span>  where X, Y are the product distributions  <span class="math">(X_{\\alpha_i})_{i=1}^q</span> ,  <span class="math">(Y_{\\alpha_i})_{i=1}^q</span> , where  <span class="math">\\alpha_i</span>  is the label on the <em>i</em>-th level of T'. Thus we have proved (7), which we record as a lemma:</p>

    <p class="text-gray-300"><strong>Lemma 1</strong> For any family  <span class="math">(X_{\\alpha}, Y_{\\alpha})_{\\alpha \\in A}</span> , A finite, we have</p>

    <p class="text-gray-300"><span class="math">$\\Delta^{\\mathsf{samp}}(q,(X_{\\alpha},Y_{\\alpha})_{\\alpha\\in A}) \\leq \\sqrt{2q} \\max_{\\alpha\\in A} h(X_{\\alpha},Y_{\\alpha})</span>$</p>

    <p class="text-gray-300">for all  <span class="math">q \\in \\mathbb{N}</span> .</p>

    <p class="text-gray-300">(One can observe that the case |A| = q = 1 of Lemma 1 is equivalent to the right-hand inequality in (3).)</p>

    <p class="text-gray-300">Moreover, the sequence of inequalities in (11) can be used to directly relate the adaptive and non-adaptive sample distinguishability advantages. More exactly, since</p>

    <p class="text-gray-300"><span class="math">$\\sqrt{2}h(L_X^{T&#x27;},L_Y^{T&#x27;}) \\leq \\sqrt{2\\Delta(L_X^{T&#x27;},L_Y^{T&#x27;})} = \\sqrt{2\\Delta_{D&#x27;}^{\\mathsf{samp}}((X_\\alpha,Y_\\alpha)_{\\alpha \\in A})} = \\sqrt{2\\Delta_{\\mathsf{non}}^{\\mathsf{samp}}(q,(X_\\alpha,Y_\\alpha)_{\\alpha \\in A})}</span>$</p>

    <p class="text-gray-300">with T' and D' as in (11), and since D is an arbitrary q-query distinguisher, we find</p>

    <p class="text-gray-300"><span class="math">$\\Delta^{\\mathsf{samp}}(q,(X_\\alpha,Y_\\alpha)_{\\alpha\\in A}) \\leq \\sqrt{2}h(L_X^{T&#x27;},L_Y^{T&#x27;}) \\leq \\sqrt{2\\Delta^{\\mathsf{samp}}_{\\mathsf{non}}(q,(X_\\alpha,Y_\\alpha)_{\\alpha\\in A})}</span>$</p>

    <p class="text-gray-300">which proves (2). We point out that we do not know the extent to which (2) is sharp. Indeed, it remains an interesting open problem to either find some sequence of examples showing that (2) is sharp (up to a possible constant factor), or else to display an even closer relationship between adaptive and non-adaptive sample distinguishabilities. In all events, however, we shall have no further use for (2), our main tool being Lemma 1.</p>

    <p class="text-gray-300">We finally note, parenthetically, that sample distinguishability is loosely related to the notion of free-start distinguishability defined by Gazl and Maurer [7]. More precisely, sample distinguishability can be seen as taking the idea of free-start indistinguishability to its logical extreme, whereby the distinguisher is &quot;repeatedly given a free start&quot; at each query. (In fact, this establishes a general connection between sample indistinguishability and general indistinguishability.)</p>

    <p class="text-gray-300">    <img src="_page_8_Picture_0.jpeg" alt="" class="my-4 max-w-full" />
</p>

    <p class="text-gray-300">Figure 2: The two worlds for the Even-Mansour security experiment. In World 1 the distinguisher D has oracle access to random permutations  <span class="math">P_1, \\ldots, P_t</span>  and the key-alternating cipher  <span class="math">E_k</span>  (cf. Eq. (1)) for a random key k. In World 2, D has oracle access to t+1 independent random permutations. In either world D also has oracle access to the inverse of each permutation.</p>

    <h2 id="sec-4" class="text-2xl font-bold">2 Applications to Key-Alternating Ciphers</h2>

    <p class="text-gray-300">We define the PRP security of a t-round key-alternating cipher E (cf. (1)) against a distinguisher (or &quot;adversary&quot;) D as</p>

    <p class="text-gray-300"><span class="math">$\\mathbf{Adv}_{E,t}^{\\text{PRP}}(D) = \\Pr[k = k_0 \\cdots k_t \\longleftarrow \\{0,1\\}^{t(n+1)}; D^{E_k,P_1,\\dots,P_t} = 1] - \\Pr[D^{Q,P_1,\\dots,P_t} = 1]</span>$</p>

    <p class="text-gray-300">where in each experiment  <span class="math">Q, P_1, \\ldots, P_t</span>  are independent uniform random permutations, where D is also allowed to query the inverse of each of its oracles (see Figure 2), and where  <span class="math">k = k_0 \\cdots k_t</span>  is selected uniformly at random (and hidden from D). We further define</p>

    <p class="text-gray-300"><span class="math">$\\mathbf{Adv}_{E,t}^{\\mathrm{PRP}}(q) = \\max_{D} \\mathbf{Adv}_{E,t}^{\\mathrm{PRP}}(D)</span>$</p>

    <p class="text-gray-300">where the maximum is taken over all adversaries D making at most q queries. (The notation  <span class="math">\\mathbf{Adv}_{E,t}^{\\mathrm{PRP}}(\\cdot)</span>  is thus overloaded.) We also note that, besides t, n is a parameter on which E (and hence  <span class="math">\\mathbf{Adv}_{E,t}^{\\mathrm{PRP}}(q)</span> ) depends (and naturally enough, since n is the &quot;security parameter&quot;).</p>

    <p class="text-gray-300">Our main result is the following:</p>

    <p class="text-gray-300"><strong>Theorem 1</strong> Let  <span class="math">N=2^n</span>  and let  <span class="math">q=N^{\\frac{t}{t+1}}/Z</span>  for some  <span class="math">Z\\geq 1</span> . Then, for any  <span class="math">t\\geq 1</span>  and assuming  <span class="math">q&lt;\\min(N/100,N/5t)</span> , we have</p>

    <p class="text-gray-300"><span class="math">$Adv_{E,t}^{PRP}(q) \\le \\frac{3q^2t}{N^{1.5}} + \\frac{t+1}{Z^t}.</span>$</p>

    <p class="text-gray-300">The above theorem is void for  <span class="math">q &gt; N^{\\frac{t}{t+1}}</span>  since  <span class="math">Z \\ge 1</span> . Thus, the provable security achieved is always at most  <span class="math">N^{\\frac{t}{t+1}}</span>  queries. On the other hand, the term  <span class="math">3q^2t/N^{1.5}</span>  also caps security at  <span class="math">q = N^{\\frac{3}{4}}</span> . Thus, roughly, Theorem 1 implies indistinguishability of the Even-Mansour cipher up to</p>

    <p class="text-gray-300"><span class="math">$q \\approx \\min(2^{\\frac{3}{4}n}, 2^{\\frac{t}{t+1}n})</span>$</p>

    <p class="text-gray-300">queries, which is sharp for  <span class="math">t \\leq 3</span> . We shall keep  <span class="math">N = 2^n</span>  for the remainder of the article.</p>

    <p class="text-gray-300">For a large part, our proof follows the outline of Bogdanov et al. [1]. As in [1], instead of giving D oracle access to the tuple of oracles</p>

    <p class="text-gray-300"><span class="math">$E_k, P_1, \\ldots, P_t</span>$</p>

    <p class="text-gray-300">in &quot;world 1&quot;, we give it oracle access to the oracles</p>

    <p class="text-gray-300"><span class="math">$E_k^{-1}, P_1, \\dots, P_t</span>$</p>

    <p class="text-gray-300">(since the inverses can be queried, this is clearly a cosmetic change—moreover the corresponding change has no effect in world 2, since Q is anyway a uniform random permutation). We also rename  <span class="math">E_k^{-1}</span>  as  <span class="math">P_0</span>  so that, in &quot;world 1&quot;, D's oracles become a (t+1)-tuple</p>

    <p class="text-gray-300"><span class="math">$P_0, P_1, \\ldots, P_t</span>$</p>

    <p class="text-gray-300">with the property that</p>

    <p class="text-gray-300"><span class="math">$P_t(P_{t-1}(\\dots P_2(P_1(P_0(\\cdot) \\oplus k_0) \\oplus k_1) \\oplus k_2 \\dots) \\oplus k_{t-1}) \\oplus k_t = id.</span>$
(12)</p>

    <p class="text-gray-300">We can indeed think of D's t+1 oracles  <span class="math">P_0, \\ldots, P_t</span>  as being constructed as follows in world 1: first the key  <span class="math">k_0 \\cdots k_t</span>  is sampled, after which permutations  <span class="math">P_0, \\ldots, P_t</span>  are sampled uniformly at random from all (t+1)-tuples  <span class="math">(P_0, \\ldots, P_t)</span>  satisfying (12). (In turn, one way of implementing the latter sampling is to sample t of the t+1 permutations  <span class="math">P_0, \\ldots, P_t</span>  uniformly and independently at random, and to define the remaining (t+1)-th permutation via (12).)</p>

    <p class="text-gray-300">Following [1], we formally implement the interface of oracles  <span class="math">(P_0, \\ldots, P_t)</span>  in world 1 via an oracle O(N,t) taking  <span class="math">k_0, \\ldots, k_t</span>  as implicit parameters. This oracle uses lazy sampling to define  <span class="math">P_0, \\ldots, P_t</span> . Originally the  <span class="math">P_i</span> 's are undefined at all points. Subsequently, when the adversary makes a query  <span class="math">P_i(x)</span>  (the case of a backward query  <span class="math">P_i^{-1}(x)</span>  is similarly handled) the oracle O(N,t) defines  <span class="math">P_i(x)</span>  according to the following procedure:</p>

    <p class="text-gray-300">• Let  <span class="math">\\mathcal{P} = \\mathcal{P}(P_0, \\dots, P_t)</span>  be the set of all (t+1)-tuples of permutations  <span class="math">(\\overline{P}_0, \\dots, \\overline{P}_t)</span>  such that  <span class="math">\\overline{P}_i</span>  extends the currently defined portion of  <span class="math">P_i</span> , and such that</p>

    <p class="text-gray-300"><span class="math">$\\overline{P}_t(\\cdots \\overline{P}_2(\\overline{P}_1(\\overline{P}_0(\\cdot) \\oplus k_0) \\oplus k_1) \\cdots \\oplus k_{t-1}) \\oplus k_t = id.</span>$
(13)</p>

    <p class="text-gray-300">Then O(N,t) samples uniformly at random an element  <span class="math">(\\overline{P}_0,\\ldots,\\overline{P}_t)</span>  from  <span class="math">\\mathcal{P}</span> . The oracle sets  <span class="math">P_i(x) = \\overline{P}_i(x)</span>  and returns this value.</p>

    <p class="text-gray-300">After the above, the oracle &quot;forgets&quot; about  <span class="math">\\overline{P}_0, \\ldots, \\overline{P}_t</span> , and samples these afresh at the next query. It is clear that this lazy sampling process gives the same distribution as sampling the tuple  <span class="math">(P_0, \\ldots, P_t)</span>  at the start of the game.</p>

    <p class="text-gray-300">In view of applying a hybrid argument, Bogdanov et al. also define a second oracle  <span class="math">\\tilde{O}(N,t)</span> . This oracle also defines the permutations  <span class="math">P_0, \\ldots, P_t</span>  via lazy sampling, and also takes the key  <span class="math">k_0 \\cdots k_t</span>  as an implicit input, but this time the lazy sampling process is a bit different.</p>

    <p class="text-gray-300">We say that a sequence of partially defined permutations  <span class="math">P_0, \\ldots, P_t</span>  is consistent if  <span class="math">\\mathcal{P}(P_0, \\ldots, P_t) \\neq \\emptyset</span> , with  <span class="math">\\mathcal{P}(\\cdot)</span>  defined as in the description of O(N,t) above. Initially,  <span class="math">\\tilde{O}(N,t)</span>  sets the permutations  <span class="math">P_0, \\ldots, P_t</span>  to be undefined everywhere. Upon receiving a forward query  <span class="math">P_i(x)</span> ,  <span class="math">\\tilde{O}(N,t)</span>  uses the following lazy sampling procedure:</p>

    <p class="text-gray-300">• Let  <span class="math">U \\subseteq \\{0,1\\}^n</span>  be the set of values y such that defining  <span class="math">P_i(x) = y</span>  maintains the consistency of  <span class="math">P_0, \\ldots, P_t</span> , besides maintaining the fact that  <span class="math">P_i</span>  is a permutation. Then  <span class="math">\\tilde{O}(N,t)</span>  samples a value y uniformly from U, sets  <span class="math">P_i(x) = y</span> , and returns y.</p>

    <p class="text-gray-300">Inverse queries are similarly treated. While it may not be immediately apparent that  <span class="math">\\tilde{O}(N,t)</span> 's answers are distributed any differently from O(N,t)'s, small examples can be constructed to show that, indeed, these two oracles are statistically non-equivalent.</p>

    <p class="text-gray-300">Theorem 1 is, via a hybrid argument, the direct consequence of the following two propositions:</p>

    <p class="text-gray-300"><strong>Proposition 1</strong> Let  <span class="math">q &lt; \\min(N/100, N/5t)</span> . With O(N,t) and  <span class="math">\\tilde{O}(N,t)</span>  defined as above,</p>

    <p class="text-gray-300"><span class="math">$\\Pr[k_0, \\dots, k_t \\leftarrow \\{0, 1\\}^n; D^{O(N, t)} = 1] - \\Pr[k_0, \\dots, k_t \\leftarrow \\{0, 1\\}^n; D^{\\tilde{O}(N, t)} = 1] \\le \\frac{3q^2t}{N^{1.5}}</span>$</p>

    <p class="text-gray-300">for every distinguisher D making at most g queries.</p>

    <p class="text-gray-300"><strong>Proposition 2</strong> Let  <span class="math">q = N^{\\frac{t}{t+1}}/Z</span>  for some  <span class="math">Z \\ge 1</span>  be such that q &lt; N/3. With  <span class="math">\\tilde{O}(N,t)</span>  defined as above,</p>

    <p class="text-gray-300"><span class="math">$\\Pr[k_0, \\dots, k_t \\leftarrow \\{0, 1\\}^n; D^{\\tilde{O}(N, t)} = 1] - \\Pr[D^{Q_0, \\dots, Q_t} = 1] \\le \\frac{t + 1}{Z^{t + 1}}.</span>$</p>

    <p class="text-gray-300">for every distinguisher D making at most q queries, where  <span class="math">Q_0, \\ldots, Q_t</span>  are independent random permutations.</p>

    <p class="text-gray-300">Proposition 2 is proved in [1]. A weaker form of Proposition 1 is also proved in [1], with a bound of  <span class="math">4.3q^3t/N^2</span>  instead of  <span class="math">3q^2t/N^{1.5}</span> . The rest of the paper is devoted to the proof of Proposition 1.</p>

    <p class="text-gray-300">We use a sample distinguishability game to prove Proposition 1. This is the same sample distinguishability game used by Bogdanov et al. for the proof of &quot;their&quot; version of Proposition 1. Only the game's analysis will be different.</p>

    <p class="text-gray-300">In view of defining this game, Bogdanov et al. first note that the adversary D can only have better advantage at distinguishing O(N,t) from  <span class="math">\\tilde{O}(N,t)</span>  if we tell D the keys  <span class="math">k_0,\\ldots,k_t</span>  (indeed, D is free to disregard this information). Nextly, they observe that one can further reduce to the case in which  <span class="math">k_0 = \\ldots = k_t = 0^n</span>  ([1] Proposition 4). (In the latter case, moreover, D obviously does not need to be &quot;told&quot;  <span class="math">k_0,\\ldots,k_t</span> .) Letting</p>

    <p class="text-gray-300"><span class="math">$\\mathbf{Adv}_{N,t}^{O\\tilde{O};0^n}(D) = \\Pr[k_0 = \\dots = k_t = 0^n; D^{O(N,t)} = 1] - \\Pr[k_0 = \\dots = k_t = 0^n; D^{\\tilde{O}(N,t)} = 1].</span>$</p>

    <p class="text-gray-300">denote the advantage of an adversary D at distinguishing O(N,t) and  <span class="math">\\tilde{O}(N,t)</span>  implemented with zero keys, it thus suffices to prove</p>

    <p class="text-gray-300"><span class="math">$\\mathbf{Adv}_{N,t}^{O\\tilde{O};0^{n}}(q) \\le \\frac{3q^{2}t}{N^{1.5}} \\tag{14}</span>$</p>

    <p class="text-gray-300">where  <span class="math">\\mathbf{Adv}_{N,t}^{O\\tilde{O};0^n}(q)</span>  is defined as the sup of  <span class="math">\\mathbf{Adv}_{N,t}^{O\\tilde{O};0^n}(D)</span>  taken over all q-query distinguishers D.</p>

    <p class="text-gray-300">When the keys are zero, O(N,t) and  <span class="math">\\tilde{O}(N,t)</span>  offer, essentially, two slightly different methods of lazy sampling permutations  <span class="math">P_0, \\ldots, P_t</span>  subject to the constraint</p>

    <p class="text-gray-300"><span class="math">$P_t(\\cdots P_1(P_0(\\cdot))\\cdots) = id. \\tag{15}</span>$</p>

    <p class="text-gray-300">Moreover, the oracles O(N,t) and  <span class="math">\\tilde{O}(N,t)</span>  maintain no &quot;internal&quot; data structures between queries that are hidden from the distinguisher. (Indeed, the only data structures that O(N,t) and  <span class="math">\\tilde{O}(N,t)</span>  remember between queries are the tables  <span class="math">P_0, \\ldots, P_t</span> , and the state of these tables is known to the distinguisher D who has made the queries to fill them.) More precisely, if we let  <span class="math">\\ell_i</span>  be the number of points at which  <span class="math">P_i</span>  (or  <span class="math">P_i^{-1}</span> ) is defined before D's i-th query, then  <span class="math">\\ell_0 + \\cdots + \\ell_t = i - 1</span>  when D makes its i-th query, assuming (wlog) that D has made no redundant queries. We can imagine further strengthening D by giving D the ability to reset the state of the data structures  <span class="math">P_0, \\ldots, P_t</span>  arbitrarily between queries, subject to the constraint (say) that each  <span class="math">P_i</span>  is defined at at most q points. Giving this added power to D brings us to a sample distinguishability game  <span class="math">(X_\\alpha, Y_\\alpha)_{\\alpha \\in A_0}</span>  in which in which the index set  <span class="math">A_0</span>  corresponds to all possible partial settings of the permutations  <span class="math">P_0, \\ldots, P_t</span>  available to D and in which the X-world and Y-world correspond to O(N,t) and  <span class="math">\\tilde{O}(N,t)</span> , respectively. (We write  <span class="math">A_0</span>  instead of A in order to distinguish this specific sample distinguishability from the generic sample distinguishability game discussed in Section 1.)</p>

    <p class="text-gray-300">We now make a number of definitions to describe the above sample distinguishability game  <span class="math">(X_{\\alpha}, Y_{\\alpha})_{\\alpha \\in A_0}</span>  more formally. In particular, we will encode the (partially defined) permutations  <span class="math">P_0, \\ldots, P_t</span>  as matchings, to remain consistent with the terminology of [1]. The following few definitions, indeed, are more or less all cut-and-pasted from [1].</p>

    <p class="text-gray-300">We let  <span class="math">V_0, \\ldots, V_t, V_{t+1}</span>  be vertex sets with  <span class="math">|V_i| = N</span>  and where we identify  <span class="math">V_{t+1}</span>  with  <span class="math">V_0</span> . A sequence of matchings  <span class="math">\\overline{M} = (\\overline{M}_0, \\ldots, \\overline{M}_{t+1})</span>  where  <span class="math">\\overline{M}_i</span>  is a perfect matching between  <span class="math">V_i</span>  and  <span class="math">V_{i+1}</span>  is called <em>circular</em></p>

    <p class="text-gray-300">if every path starting at a vertex  <span class="math">v \\in V_0</span>  following the edges in  <span class="math">\\overline{M}_0, \\dots, \\overline{M}_t</span>  ends at the same vertex  <span class="math">v \\in V_{t+1} = V_0</span> . In other words, circularity is the matching equivalent of (15).</p>

    <p class="text-gray-300">Given a sequence  <span class="math">M = (M_0, \\dots, M_t)</span>  where each  <span class="math">M_i</span>  is a partial matching between  <span class="math">V_i</span>  and  <span class="math">V_{i+1}</span> , we let</p>

    <p class="text-gray-300"><span class="math">$\\mathcal{M}(M)</span>$</p>

    <p class="text-gray-300">be the set of all circular sequences  <span class="math">\\overline{M}</span>  extending M, i.e. the set of all sequences  <span class="math">\\overline{M} = (\\overline{M}_0, \\dots, \\overline{M}_t)</span>  such that  <span class="math">\\overline{M}_i</span>  extends  <span class="math">M_i</span>  for each i and such that  <span class="math">\\overline{M}</span>  is circular. We say M is consistent if  <span class="math">\\mathcal{M}(M) \\neq \\emptyset</span> .</p>

    <p class="text-gray-300">A q-configuration is a pair  <span class="math">(v_0, M)</span>  such that (i)  <span class="math">M = (M_0, \\ldots, M_t)</span>  is a consistent sequence of partial matchings such that  <span class="math">|M_i| \\leq q</span>  for all i, (ii)  <span class="math">v_0 \\in V_0</span>  is nonadjacent to  <span class="math">M_0</span> . The index set  <span class="math">A_0</span>  of the sample distinguishability game will be the set of all q-configurations. That is,</p>

    <p class="text-gray-300"><span class="math">$A_0 = \\{(v_0, M) : (v_0, M) \\text{ is a } q\\text{-configuration}\\}.</span>$</p>

    <p class="text-gray-300">We next describe the distributions  <span class="math">X_{\\alpha}</span>  and  <span class="math">Y_{\\alpha}</span> . Let  <span class="math">\\alpha = (v_0, M) \\in A_0</span>  be a q-configuration,  <span class="math">M = (M_0, \\ldots, M_t)</span> . For any vertex  <span class="math">u \\in V_1</span>  nonadjacent to  <span class="math">M_0</span> , we write  <span class="math">M \\cup \\{(v_0, u)\\}</span>  for the sequence of partial matchings  <span class="math">(M_0 \\cup \\{(v_0, u)\\}, M_1, \\ldots, M_t)</span> . Let  <span class="math">U \\subseteq V_1</span>  be the set of vertices u such that  <span class="math">M \\cup \\{(v_0, u)\\}</span>  is consistent. We define</p>

    <p class="text-gray-300"><span class="math">$\\Pr[X_{\\alpha} = u] := \\frac{\\mathcal{M}(M \\cup \\{(v_0, u)\\})}{\\mathcal{M}(M)}.</span>$</p>

    <p class="text-gray-300">We note that  <span class="math">X_{\\alpha}</span>  is a probability distribution on U, and that  <span class="math">X_{\\alpha}</span>  is equidistributed to O(N,t) queried at  <span class="math">P_0(v_0)</span>  with keys  <span class="math">k_0 = \\ldots = k_t = 0^n</span>  and with  <span class="math">P_0, \\ldots, P_t</span>  defined such that  <span class="math">P_i(x) = y \\iff (x,y) \\in M_i</span> . As for  <span class="math">Y_{\\alpha}</span> , it is simply the uniform distribution on U. Thus  <span class="math">Y_{\\alpha}</span>  is equidistributed to  <span class="math">\\tilde{O}(N,t)</span>  under the same correspondence. This completes the description of the sample distinguishability game  <span class="math">(X_{\\alpha}, Y_{\\alpha})_{\\alpha \\in A_0}</span>  (parameterized by N and t).</p>

    <p class="text-gray-300">We note the restriction of the sample distinguishability game to queries of the form  <span class="math">P_0(\\cdot)</span>  is without loss of generality, by symmetry considerations and because the distinguisher can &quot;set up&quot; the matchings as it wants. Thus</p>

    <p class="text-gray-300"><span class="math">$\\mathbf{Adv}_{N,t}^{O\\tilde{O};0^n}(q) \\leq \\Delta^{\\mathsf{samp}}(q,(X_\\alpha,Y_\\alpha)_{\\alpha \\in A_0})</span>$</p>

    <p class="text-gray-300">since the sample distinguishability game only gives more power to the distinguisher and, in particular, it suffices to show</p>

    <p class="text-gray-300"><span class="math">$\\Delta^{\\mathsf{samp}}(q, (X_{\\alpha}, Y_{\\alpha})_{\\alpha \\in A_0}) \\le \\frac{3q^2t}{N^{1.5}} \\text{ when } q &lt; N/100</span>$
(16)</p>

    <p class="text-gray-300">in order to prove Proposition 1. To prove (16) we combine Lemma 1 with a rather technical analysis upper bounding  <span class="math">\\max_{\\alpha \\in A_0} h(X_{\\alpha}, Y_{\\alpha})</span> . The latter analysis has two basic components: a combinatorial characterization of the &quot;shape&quot; of the distribution  <span class="math">X_{\\alpha}</span>  (recall  <span class="math">Y_{\\alpha}</span>  is uniform), and, secondly, an upper bound on  <span class="math">h(X_{\\alpha}, Y_{\\alpha})</span>  based on this combinatorial characterization. These two components correspond, in that order, to the following two propositions:</p>

    <p class="text-gray-300"><strong>Proposition 3</strong> Let q &lt; N/3, let  <span class="math">(X_{\\alpha}, Y_{\\alpha})_{\\alpha \\in A_0}</span>  be the sample distinguishability game described above, and let  <span class="math">\\alpha = (v_0, M) \\in A_0</span>  be a q-configuration. Let  <span class="math">\\rho = 2.05qt/N</span> . Let  <span class="math">U \\subseteq V_1</span>  be the support of  <span class="math">Y_{\\alpha}</span> . Then either |U| = 1 and  <span class="math">h(X_{\\alpha}, Y_{\\alpha}) = \\Delta(X_{\\alpha}, Y_{\\alpha}) = 0</span> , or else  <span class="math">|U| \\ge N - 2q</span>  and there exists a set  <span class="math">R \\subseteq U</span>  such that (i)  <span class="math">|R| \\ge N - 2q</span>  and  <span class="math">|U \\setminus R| \\le q</span> , (ii)  <span class="math">\\Pr[X_{\\alpha} = u] = \\Pr[X_{\\alpha} = v]</span>  for all  <span class="math">u, v \\in R</span> . Moreover,</p>

    <p class="text-gray-300"><span class="math">$\\frac{\\Pr[X_{\\alpha} = u]}{\\Pr[X_{\\alpha} = v]} \\ge 1 - \\rho</span>$</p>

    <p class="text-gray-300">for all  <span class="math">u, v \\in U</span> .</p>

    <p class="text-gray-300"><strong>Proposition 4</strong> Let X, Y be random variables of range U such that Y is uniform on U and such that there exists a set  <span class="math">R \\subseteq U</span> ,  <span class="math">|R| \\ge |U|/2</span> , such that  <span class="math">\\Pr[X = u] = \\Pr[X = v]</span>  for all  <span class="math">u, v \\in R</span>  and such that</p>

    <p class="text-gray-300"><span class="math">$\\frac{\\Pr[X=u]}{\\Pr[X=v]} \\ge 1 - \\varepsilon</span>$</p>

    <p class="text-gray-300">for all  <span class="math">u, v \\in U</span> , for some  <span class="math">\\varepsilon \\leq \\frac{1}{2}</span> . Then</p>

    <p class="text-gray-300"><span class="math">$h^{2}(X,Y) \\leq \\varepsilon^{2} \\left(\\frac{|T|}{|R|}\\right)^{2} + \\frac{\\varepsilon^{2}|T|}{2|U|}</span>$</p>

    <p class="text-gray-300">where  <span class="math">T = U \\backslash R</span> .</p>

    <p class="text-gray-300">Proposition 3 is proved by Bogdanov et al. and, more exactly, is established as an intermediate result within the proof of their Lemma 2 [1]. (As a means of giving some intuition, however, we note the set R corresponds to the set of vertices in  <span class="math">V_1</span>  are neither adjacent to an edge of  <span class="math">M_1</span>  or of  <span class="math">M_2</span> —there are, naturally, at least N-2q such vertices.)</p>

    <p class="text-gray-300">Before proving Proposition 4 (which amounts to a fairly straightforward computation) we show how (16), and hence Proposition 1, can be obtained as a corollary of Propositions 3 and 4:</p>

    <p class="text-gray-300">Proof of Proposition 1. Let U and R be as guaranteed by Proposition 3 (for some given  <span class="math">\\alpha \\in A_0</span> ). Since q &lt; N/100 we have  <span class="math">|R| \\ge N - 2q \\ge \\frac{98}{100}N</span> , and  <span class="math">|U| \\le N</span> , so  <span class="math">|R| \\ge |U|/2</span> . Moreover since q &lt; N/5t we have  <span class="math">\\rho = 2.05qt/N \\le \\frac{1}{2}</span> . We can therefore apply Proposition 4 with  <span class="math">\\varepsilon = \\rho = 2.05qt/N</span> ; we find, since  <span class="math">|U| \\ge N - 2q</span>  and  <span class="math">|T| = |U \\setminus R| \\le q</span> :</p>

    <p class="text-gray-300"><span class="math">$h^2(X_{\\alpha}, Y_{\\alpha}) \\le \\rho^2 \\left(\\frac{q}{N - 2q}\\right)^2 + \\frac{\\rho^2 q}{2(N - q)} \\le \\frac{\\rho^2 q}{(N - q)}</span>$</p>

    <p class="text-gray-300">where we used q &lt; N/100 in the second inequality. Thus, by Lemma 1,</p>

    <p class="text-gray-300"><span class="math">$\\Delta^{\\mathsf{samp}}(q, (X_{\\alpha}, Y_{\\alpha})_{\\alpha \\in A_0}) \\leq \\sqrt{2q} \\max_{\\alpha \\in A_0} h(X_{\\alpha}, Y_{\\alpha}) \\leq \\frac{\\rho q}{\\sqrt{N - q}} \\leq \\frac{3q^2 t}{N^{1.5}}</span>$</p>

    <p class="text-gray-300">again using q &lt; N/100.</p>

    <h4 id="sec-5" class="text-lg font-semibold mt-6">Corollary 1 Theorem 1.</h4>

    <p class="text-gray-300">We finish, finally, with the (prosaic) proof of Proposition 4:</p>

    <p class="text-gray-300">Proof of Proposition 4. Since  <span class="math">\\varepsilon \\leq \\frac{1}{2}</span> ,  <span class="math">(1-\\varepsilon)^{-1} \\leq 1+2\\varepsilon</span> . By an averaging argument, there exists some  <span class="math">u_0, u_0&#x27; \\in U</span>  such that  <span class="math">\\Pr[X = u_0] \\leq 1/|U|</span> ,  <span class="math">\\Pr[X = u_0&#x27;] \\geq 1/|U|</span> . Therefore</p>

    <p class="text-gray-300"><span class="math">$\\Pr[X = u] \\in [(1 - \\varepsilon)/|U|, (1 + 2\\varepsilon)/|U|]</span>$</p>

    <p class="text-gray-300">for all  <span class="math">u \\in U</span> . In particular,</p>

    <p class="text-gray-300"><span class="math">$\\frac{|T|(1-\\varepsilon)}{|U|} \\le \\Pr[X \\in T] \\le \\frac{|T|(1+2\\varepsilon)}{|U|}</span>$</p>

    <p class="text-gray-300">(recall  <span class="math">T = U \\backslash R</span> ) implying</p>

    <p class="text-gray-300"><span class="math">$1 - \\frac{|T|(1-\\varepsilon)}{|U|} \\ge \\Pr[X \\in R] \\ge 1 - \\frac{|T|(1+2\\varepsilon)}{|U|}.</span>$</p>

    <p class="text-gray-300">Let  <span class="math">p_R = \\Pr[X \\in R]/|R|</span> . By definition, then  <span class="math">p_R = \\Pr[X = u]</span>  for all  <span class="math">u \\in R</span> . We have</p>

    <p class="text-gray-300"><span class="math">$p_R \\geq \\frac{1}{|R|} \\left( 1 - \\frac{|T|(1+2\\varepsilon)}{|U|} \\right)</span>$</p>

    <p class="text-gray-300"><span class="math">$= \\frac{1}{|R|} \\left( 1 - \\frac{(|U| - |R|)(1 + 2\\varepsilon)}{|U|} \\right)</span>$</p>

    <p class="text-gray-300"><span class="math">$= \\frac{1}{|R|} \\left( \\frac{|U|}{|U|} - \\frac{(|U| - |R|) + 2\\varepsilon(|U| - |R|)}{|U|} \\right)</span>$</p>

    <p class="text-gray-300"><span class="math">$= \\frac{1}{|R|} \\left( \\frac{|R| - 2\\varepsilon(|U| - |R|)}{|U|} \\right)</span>$</p>

    <p class="text-gray-300"><span class="math">$= \\frac{1}{|U|} - 2\\varepsilon \\left( \\frac{1}{|R|} - \\frac{1}{|U|} \\right)</span>$
(17)</p>

    <p class="text-gray-300">and</p>

    <p class="text-gray-300"><span class="math">$p_R \\le \\frac{1}{|R|} \\left( 1 - \\frac{|T|(1-\\varepsilon)}{|U|} \\right)</span>$</p>

    <p class="text-gray-300">=  <span class="math">\\frac{1}{|U|} + \\varepsilon \\left( \\frac{1}{|R|} - \\frac{1}{|U|} \\right)</span> .</p>

    <p class="text-gray-300">We also note that  <span class="math">p_R \\ge \\frac{1}{2|U|}</span> , as can be seen from (17) and from the fact that  <span class="math">|R| \\ge |U|/2</span> . By concavity,  <span class="math">\\sqrt{x}</span>  is upper bounded by its tangent line approximation at x = b; namely,</p>

    <p class="text-gray-300"><span class="math">$\\sqrt{b+h} \\leq \\sqrt{b} + \\frac{1}{2}hb^{-\\frac{1}{2}}</span>$</p>

    <p class="text-gray-300">for all  <span class="math">b \\geq 0</span>  and all  <span class="math">h \\in [-b, \\infty)</span> . Thus</p>

    <p class="text-gray-300"><span class="math">$\\begin{split} \\sqrt{p_R} - \\sqrt{\\frac{1}{|U|}} &amp; \\leq &amp; \\sqrt{\\frac{1}{|U|} + \\varepsilon \\left(\\frac{1}{|R|} - \\frac{1}{|U|}\\right)} - \\sqrt{\\frac{1}{|U|}} \\\\ &amp; \\leq &amp; \\sqrt{\\frac{1}{|U|} + \\frac{1}{2}\\varepsilon \\left(\\frac{1}{|R|} - \\frac{1}{|U|}\\right)} \\sqrt{|U|} - \\sqrt{\\frac{1}{|U|}} \\\\ &amp; = &amp; \\frac{1}{2}\\varepsilon \\left(\\frac{1}{|R|} - \\frac{1}{|U|}\\right) \\sqrt{|U|} \\end{split}</span>$</p>

    <p class="text-gray-300">and (using  <span class="math">p_R \\ge \\frac{1}{2|U|}</span> )</p>

    <p class="text-gray-300">$$\\sqrt{\\frac{1}{|U|}} - \\sqrt{p_R} = \\sqrt{p_R + (\\frac{1}{|U|} - p_R)} - \\sqrt{p_R}
\\leq \\sqrt{p_R} + \\frac{1}{2} (\\frac{1}{|U|} - p_R) p_R^{-\\frac{1}{2}} - \\sqrt{p_R}
\\leq \\frac{1}{2} (\\frac{1}{|U|} - p_R) \\sqrt{2|U|}
\\leq \\varepsilon \\left(\\frac{1}{|R|} - \\frac{1}{|U|}\\right) \\sqrt{2|U|}.$$</p>

    <p class="text-gray-300">Therefore</p>

    <p class="text-gray-300"><span class="math">$\\left|\\sqrt{\\frac{1}{|U|}} - \\sqrt{p_R}\\right| \\leq \\varepsilon \\Big(\\frac{1}{|R|} - \\frac{1}{|U|}\\Big) \\sqrt{2|U|}</span>$</p>

    <p class="text-gray-300">and</p>

    <p class="text-gray-300"><span class="math">$\\left(\\sqrt{\\frac{1}{|U|}} - \\sqrt{p_R}\\right)^2 \\leq 2\\varepsilon^2 |U| \\left(\\frac{1}{|R|} - \\frac{1}{|U|}\\right)^2</span>$</p>

    <p class="text-gray-300"><span class="math">$= 2\\varepsilon^2 |U| \\left(\\frac{|U| - |R|}{|U||R|}\\right)^2</span>$</p>

    <p class="text-gray-300"><span class="math">$= \\frac{2\\varepsilon^2}{|U|} \\left(\\frac{|T|}{|R|}\\right)^2.</span>$</p>

    <p class="text-gray-300">Therefore</p>

    <p class="text-gray-300"><span class="math">$\\sum_{u \\in R} (\\sqrt{\\Pr[X = u]} - \\sqrt{\\Pr[Y = u]})^2 \\le |R| \\frac{2\\varepsilon^2}{|U|} \\left(\\frac{|T|}{|R|}\\right)^2</span>$</p>

    <p class="text-gray-300"><span class="math">$\\le 2\\varepsilon^2 \\left(\\frac{|T|}{|R|}\\right)^2</span>$</p>

    <p class="text-gray-300">(using |R| ≤ |U|).</p>

    <p class="text-gray-300">Next, let u ∈ T. Then</p>

    <p class="text-gray-300"><span class="math">$\\begin{split} \\sqrt{\\Pr[X=u]} - \\sqrt{\\Pr[Y=u]} &amp; \\leq \\sqrt{(1+2\\varepsilon)/|U|} - \\sqrt{1/|U|} \\\\ &amp; \\leq \\sqrt{1/|U|} + \\frac{1}{2}(2\\varepsilon/|U|)\\sqrt{|U|} - \\sqrt{1/|U|} \\\\ &amp; = \\varepsilon/\\sqrt{|U|}. \\end{split}</span>$</p>

    <p class="text-gray-300">Also,</p>

    <p class="text-gray-300"><span class="math">$\\begin{split} \\sqrt{\\Pr[Y=u]} - \\sqrt{\\Pr[X=u]} &amp; \\leq \\sqrt{1/|U|} - \\sqrt{(1-\\varepsilon)/|U|} \\\\ &amp; = \\sqrt{(1-\\varepsilon)/|U|} + \\varepsilon/|U| - \\sqrt{(1-\\varepsilon)/|U|} \\\\ &amp; \\leq \\sqrt{(1-\\varepsilon)/|U|} + \\frac{1}{2}(\\varepsilon/|U|)\\sqrt{|U|/(1-\\varepsilon)} - \\sqrt{(1-\\varepsilon)/|U|} \\\\ &amp; \\leq \\varepsilon/\\sqrt{2|U|} \\end{split}</span>$</p>

    <p class="text-gray-300">(using 1/(1 − ε) ≤ 2). Therefore</p>

    <p class="text-gray-300"><span class="math">$\\sum_{u \\in T} (\\sqrt{\\Pr[X=u]} - \\sqrt{\\Pr[Y=u]})^2 \\leq |T| \\frac{\\varepsilon^2}{|U|}.</span>$</p>

    <p class="text-gray-300">Altogether, therefore, we find that</p>

    <p class="text-gray-300"><span class="math">$h^2(X,Y) = \\frac{1}{2} \\sum_{u \\in U} (\\sqrt{\\Pr[X=u]} - \\sqrt{\\Pr[Y=u]})^2 \\le \\varepsilon^2 \\left(\\frac{|T|}{|R|}\\right)^2 + \\frac{\\varepsilon^2 |T|}{2|U|}.</span>$</p>

    <p class="text-gray-300">References</p>

    <p class="text-gray-300">[1] Andrey Bogdanov, Lars R. Knudsen, Gregor Leander, Francois-Xavier Standaert, John Steinberger and Elmar Tischhauser: Key-Alternating Ciphers in a Provable Setting: Encryption Using a Small Number of Public Permutations. EUROCRYPT 2012, LNCS 7237, pp. , Springer-Verlag, 2012.</p>

    <p class="text-gray-300">15</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>[2] Joan Daemen: Limitations of the Even-Mansour Construction. ASIACRYPT 1991, LNCS 739, pp. 495-498, Springer-Verlag, 1991.</li>
      <li>[3] Joan Daemen and Vincent Rijmen: The Design of Rijndael. Springer-Verlag, 2002.</li>
      <li>[4] Joan Daemen and Vincent Rijmen: The Wide Trail Design Strategy. IMA Int. Conf., LNCS 2260, pp. 222-238, Springer-Verlag, 2001.</li>
      <li>[5] Shimon Even and Yishay Mansour: A Construction of a Cipher From a Single Pseudorandom Permutation. ASIACRYPT 1991, LNCS 739, pp. 210–224, Springer-Verlag, 1993.</li>
      <li>[6] Shimon Even and Yishay Mansour: A Construction of a Cipher from a Single Pseudorandom Permutation. J. Cryptology, vol. 10, num. 3, pp. 151-162, 1997.</li>
      <li>[7] Peter Gai and Ueli Maurer: Free-Start Distinguishing: Combining Two Types of Indistinguishability Amplification. The 4th International Conference on Information Theoretic Security - ICITS 2009, LNCS 5973, pp. 2844, 2010.</li>
      <li>[8] Michael Luby and Charles Rackoff: How to Construct Pseudorandom Permutations from Pseudorandom Functions. SIAM J. Comput., vol. 17, num. 2, pp. 373-386, 1988.</li>
      <li>[9] Ben Morris, Phillip Rogaway and Till Stegers: How to Encipher Messages on a Small Domain: Deterministic Encryption and the Thorp Shuffle. CRYPTO 2009. LNCS 5677, Springer, pp. 286- 302, 2009.How to Encipher Messages on a Small Domain: Deterministic Encryption and the Thorp Shuffle. CRYPTO 2009, LNCS 5677, Springer, pp. 286-302, 2009.</li>
      <li>[10] Mitsuru Matsui: New Structure of Block Ciphers with Provable Security against Differential and Linear Cryptanalysis. FSE 1996, LNCS 1039, pp. 205-218, Springer-Verlag, 1996.</li>
      <li>[11] Mitsuru Matsui: New Block Encryption Algorithm MISTY. FSE 1997, LNCS 1267, pp. 54-68, Springer-Verlag, 1997.</li>
      <li>[12] Ueli Maurer: Indistinguishability of Random Systems. Advances in Cryptology EUROCRYPT 2002, LNCS 2332, pp. 110132, May 2002.</li>
      <li>[13] Ueli Maurer and Krzysztof Pietrzak: Composition of Random Systems: When Two Weak Make One Strong. TCC 2004, LNCS 2951, pp. 410427, Feb 2004.</li>
      <li>[14] Ueli Maurer, Krzysztof Pietrzak and Renato Renner: Indistinguishability Amplification. CRYPTO 2007, LNCS 4622, pp. 130149, 2007.</li>
      <li>[15] Liu Tianren, personal communication.</li>
      <li>[16] Serge Vaudenay: Decorrelation: A Theory for Block Cipher Security. J. Cryptology, vol. 16, num. 14, pp. 249-286, 2003.</li>
    </ul>

`;
---

<BaseLayout title="Improved Security Bounds for Key-Alternating Ciphers via Hel... (2012/481)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2012 &middot; eprint 2012/481
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <PaperDisclaimer eprintUrl={EPRINT_URL} />
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <Fragment set:html={CONTENT} />

    <PaperHistory slug="improved-security-bounds-for-key-alternating-ciphers-via-2012" />
  </article>
</BaseLayout>
