---
import BaseLayout from '../../layouts/BaseLayout.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2018/772';
const CRAWLER = 'marker';
const CONVERTED_DATE = '2026-02-16';
const TITLE_HTML = 'Linear Equivalence of Block Ciphers with Partial Non-Linear Layers: Application to LowMC';
const AUTHORS_HTML = 'Itai Dinur, Daniel Kales, Angela Promitzer, Sebastian Ramacher, Christian Rechberger';

const CONTENT = `    <p class="text-gray-300">Itai Dinur<sup>1</sup> , Daniel Kales<sup>2</sup> , Angela Promitzer<sup>3</sup> , Sebastian Ramacher<sup>2</sup> , and Christian Rechberger<sup>2</sup></p>

    <p class="text-gray-300"><sup>1</sup> Department of Computer Science, Ben-Gurion University, Israel <sup>2</sup> Graz University of Technology, Austria 3 Independent</p>

    <p class="text-gray-300">Abstract. LowMC is a block cipher family designed in 2015 by Albrecht et al. It is optimized for practical instantiations of multi-party computation, fully homomorphic encryption, and zero-knowledge proofs. LowMC is used in the Picnic signature scheme, submitted to NIST's post-quantum standardization project and is a substantial building block in other novel post-quantum cryptosystems. Many LowMC instances use a relatively recent design strategy (initiated by G´erard et al. at CHES 2013) of applying the non-linear layer to only a part of the state in each round, where the shortage of non-linear operations is partially compensated by heavy linear algebra. Since the high linear algebra complexity has been a bottleneck in several applications, one of the open questions raised by the designers was to reduce it, without introducing additional non-linear operations (or compromising security).</p>

    <p class="text-gray-300">In this paper, we consider LowMC instances with block size n, partial non-linear layers of size s ≤ n and r encryption rounds. We redesign LowMC's linear components in a way that preserves its specification, yet improves LowMC's performance in essentially every aspect. Most of our optimizations are applicable to all SP-networks with partial non-linear layers and shed new light on this relatively new design methodology.</p>

    <p class="text-gray-300">Our main result shows that when s < n, each LowMC instance belongs to a large class of equivalent instances that differ in their linear layers. We then select a representative instance from this class for which encryption (and decryption) can be implemented much more efficiently than for an arbitrary instance. This yields a new encryption algorithm that is equivalent to the standard one, but reduces the evaluation time and storage of the linear layers from r · n <sup>2</sup> bits to about r · n <sup>2</sup> − (r − 1)(n − s) 2 . Additionally, we reduce the size of LowMC's round keys and constants and optimize its key schedule and instance generation algorithms. All of these optimizations give substantial improvements for small s and a reasonable choice of r. Finally, we formalize the notion of linear equivalence of block ciphers and prove the optimality of some of our results.</p>

    <p class="text-gray-300">Comprehensive benchmarking of our optimizations in various LowMC applications (such as Picnic) reveals improvements by factors that typically range between 2x and 40x in runtime and memory consumption.</p>

    <p class="text-gray-300">Keywords: Block cipher, LowMC, Picnic signature scheme, linear equivalence</p>

    <h2 id="sec-2" class="text-2xl font-bold">1 Introduction</h2>

    <p class="text-gray-300">LowMC is a block cipher family designed by Albrecht et al. <a href="#page-29-0">\\[ARS</a>+15], and is heavily optimized for practical instantiations of multi-party computation (MPC), fully homomorphic encryption (FHE), and zero-knowledge proofs. In such applications, non-linear operations incur a higher penalty in communication and computational complexity compared to linear ones. Due to its design strategy, LowMC is a popular building block in post-quantum designs that are based on MPC and zero-knowledge protocols (cf. <a href="#page-29-1">\\[BEF18,</a><a href="#page-29-2">CDG</a>+17a<a href="#page-30-0">,DRS18b</a><a href="#page-30-1">,KKW18,</a>[DRS18a\\]](#page-30-2)). Most notably, it is used in the Picnic signature algorithm <a href="#page-29-3">\\[CDG</a>+17b] which is a candidate in NIST's post-quantum cryptography standardization project.<a href="#page-1-0">4</a></p>

    <p class="text-gray-300">Instances of LowMC are designed to perform well in two particular metrics that measure the complexity of non-linear operations over GF(2). The first metric is multiplicative complexity (MC), which simply counts the number of multiplications (AND gates in our context) in the circuit. The second metric is the multiplicative (AND) depth of the circuit.</p>

    <p class="text-gray-300">The relevance of each metric depends on the specific application. For example, in the context of MPC protocols, Yao's garbled circuits [\\[Yao86\\]](#page-31-0) with the free-XOR technique [\\[KS08\\]](#page-30-3) (and many of their variants) have a constant number of communication rounds. The total amount of communication depends on the MC of the circuit as each AND gate requires communication, whereas XOR operations can be performed locally. In an additional class of MPC protocols (e.g., GMW [\\[GMW87\\]](#page-30-4)), the number of communication rounds is linear in the ANDdepth of the evaluated circuit. The performance of these protocols depends on both the MC and ANDdepth of the circuit.</p>

    <p class="text-gray-300">In order to reduce the complexity of non-linear operations for a certain level of security, LowMC combines very dense linear layers over GF(2)<sup>n</sup> (where n is the block size) with simple non-linear layers containing 3 ×3 Sboxes of algebraic degree 2. The LowMC block cipher family includes a huge number of instances, where for each instance, the linear layer of each round is chosen independently and uniformly at random from all invertible n × n matrices.</p>

    <p class="text-gray-300">The design strategy of LowMC attempts to offer flexibility with respect to both the MC and ANDdepth metrics. In particular, some LowMC instances minimize the MC metric by applying only a partial non-linear layer to the state of the cipher at each round, while the linear layers still mix the entire state. In general, this approach requires to increase the total number of rounds in the scheme in order to maintain a certain security level, but this is compensated by the reduction in the size of the non-linear layers and the total AND count is generally reduced. The global parameters of LowMC that are most relevant for this paper are (1) the block size of n bits, (2) the number of rounds r (which is determined according to the desired security level), and (3) a parameter s which</p>

    <p class="text-gray-300"><span id="page-1-0"></span><sup>4</sup> <a href="https://csrc.nist.gov/Projects/Post-Quantum-Cryptography/Round-1-Submissions">https://csrc.nist.gov/Projects/Post-Quantum-Cryptography/</a> <a href="https://csrc.nist.gov/Projects/Post-Quantum-Cryptography/Round-1-Submissions">Round-1-Submissions</a></p>

    <p class="text-gray-300">denotes the domain length of each non-linear layer, namely, the number of bits on which it operates (which may be smaller than n).<sup>5</sup></p>

    <p class="text-gray-300">While LowMC's design aims to minimize the non-linear complexity of the scheme at the expense of using many linear algebra (XOR) operations, in several practical applications, XORs do not come for free and may become a bottleneck in the implementation. This phenomenon was already noted and demonstrated in the original LowMC paper. Indeed, due to the large computational cost of LowMC's dense linear layers, one of the open problems raised by its designers was to reduce their computational cost, presumably by designing more specific linear layers that offer the same security level with improved efficiency.</p>

    <p class="text-gray-300">More recently, the high cost of LowMC's linear operations influenced the design of the PICNIC signature algorithm, where the most relevant metric is the MC that affects the signature size. In order to minimize the AND count (and the signature size), the LowMC instances used by PICNIC should have a very small partial non-linear layer in each round (perhaps using only a single  <span class="math">3\\times3</span>  Sbox). However, such an instance has a large number of rounds r and each encryption requires computation of r matrix-vector products that increase the signing and verification times. Consequently, the PICNIC designers settled for non-linear layers of intermediate size in order to balance the signature size on one hand and the signing and verification times on the other.</p>

    <p class="text-gray-300">In fact, in Picnic there is another source of inefficiency due to the heavy cost of the linear operations in LowMC's key schedule: the computation of LowMC inside Picnic involves splitting the LowMC instance to 3 related instances which are evaluated with a fresh share of the key in each invocation. Therefore, in contrast to standard applications, the key schedule has to be run before each cipher invocation and it is not possible to hard-code the round keys into the LowMC instance in this specific (and very important) application. In LowMC, each of the r+1 round keys is generated by applying an independent  <span class="math">n \\times \\kappa</span>  random linear transformation to the  <span class="math">\\kappa</span> -bit master key. Therefore, the total complexity of the key schedule is  <span class="math">(r+1) \\cdot n \\cdot \\kappa</span>  in both time and memory, which is a substantial overhead on the signing and verification processes in Picnic.</p>

    <p class="text-gray-300">Our Contribution In this paper we revisit the open problem of the LowMC designers to reduce the complexity of its linear operations, focusing on instances with partial non-linear layers (i.e., s < n). We consider a generalized LowMC construction in which the r linear layers are selected uniformly at random from the set of all invertible matrices and the non-linear layers are arbitrary and applied to s bits of the n-bit internal state in each of the r rounds. Our results are divided into several parts.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>The round keys and constants of a generalized LowMC cipher require memory of  <span class="math">(r+1) \\cdot n</span>  bits. We compress them to  <span class="math">n+r \\cdot s</span>  bits. We then consider LowMC's linear key schedule (with a master key of size  <span class="math">\\kappa</span>  bits) and reduce</li>

    </ol>

    <p class="text-gray-300"><span id="page-2-0"></span><sup>&</sup>lt;sup>5</sup> The LowMC specification denotes by m the number of  <span class="math">3 \\times 3</span>  Sboxes in each non-linear layer and therefore s=3m in our context.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>its complexity from  <span class="math">(r+1) \\cdot n \\cdot \\kappa</span>  to  <span class="math">n \\cdot \\kappa + r \\cdot (s \\cdot \\kappa)</span> . This has a substantial effect on the performance of Picnic, as described above.</li>

      <li>2. The linear algebra of the encryption (and decryption) algorithm requires matrices of size  <span class="math">r \\cdot n^2</span>  bits and performs matrix-vector products with about the same complexity. We describe a new algorithm that uses matrices requiring only  <span class="math">r \\cdot n^2 (r-1)(n-s)^2</span>  bits of storage and about the same linear algebra time complexity (using standard matrix-vector products<sup>6</sup>).</li>

      <li>3. We consider the complexity of generating a generalized LowMC instance, assuming its linear layers are sampled at random. We devise a new sampling algorithm that reduces this complexity<sup>7</sup> from about  <span class="math">r \\cdot n^3</span>  to  <span class="math">n^3 + (r-1) \\cdot (s^2 \\cdot n)</span> . Our sampling algorithm further reduces the number of uniform (pseudo) random bits required to sample the linear layers from about  <span class="math">r \\cdot n^2</span>  to  <span class="math">n^2 + (r-1) \\cdot (n^2 (n-s)^2)</span> . These optimizations are useful in applications that require frequent instance generation, e.g. for the RASTA design strategy [DEG<sup>+</sup>18].</li>

      <li>4. We address the question of whether the linear layer description we use during encryption is optimal (i.e., minimal) or can be further compressed. Indeed, it may seem that the formula  <span class="math">n^2 + (r-1)(n^2 (n-s)^2)</span>  is suboptimal, and the formula  <span class="math">n^2 + (r-1) \\cdot s \\cdot n^2</span>  is more reasonable, as it is linear in s (similarly to the reduction in the size of the round keys). However, we prove (under two assumptions which we argue are natural) that no further optimizations that reduce the linear layer sizes are possible without changing their functionality.</li>

    </ul>

    <p class="text-gray-300">Table 1 summarizes our improvements and the assumptions under which they can be applied to an SP-network with partial non-linear layers. Surprisingly, although the open problem of the LowMC designers presumably involved changing the specification of LowMC's linear layers to reduce its linear algebra complexity, our improvements achieve this without any specification change. All of these improvements are significant for  <span class="math">s \\ll n</span>  and r that is not too small.</p>

    <p class="text-gray-300">We stress that our optimized encryption algorithm is applicable to any SP-network with partial non-linear layers (such as Zorro<sup>8</sup> [GGNS13]) since it does not assume any special property of the linear or non-linear layers. Yet, if the linear layers are not selected uniformly at random, the question of whether our algorithm is more efficient compared to the standard one depends on the specific design. On the other hand, when designing new SP-networks with partial non-linear layers, one may use our optimized linear layers as a starting point for additional improvements. We further note that the reduced complexity of the linear layer evaluation during encryption is also useful for adversaries that attempt to break LowMC instances via exhaustive search.</p>

    <p class="text-gray-300"><span id="page-3-0"></span> <span class="math">&lt;sup&gt;^6&lt;/sup&gt;</span>  Optimizations in matrix-vector multiplications (such as the "method of four Russians" [ABH10]) can be applied to both the standard and to our new encryption algorithm.</p>

    <p class="text-gray-300"><span id="page-3-1"></span>Using asymptotically fast matrix multiplication and invertible matrix sampling algorithms will reduce the asymptotic complexity of both the original and our new algorithm. Nevertheless, it is not clear whether they would reduce their concrete complexity for relevant choices of parameters.</p>

    <p class="text-gray-300"><span id="page-3-2"></span><sup>8</sup> Although Zorro is broken [BDD+15,RASA14,WWGY14], its general design strategy remains valid.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Metric</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Unoptimized</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Optimized</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Sect.</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Assumption</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">RK and RC</td>

            <td class="px-3 py-2 border-b border-gray-700">M</td>

            <td class="px-3 py-2 border-b border-gray-700"><span class="math">(r+1) \\cdot n</span></td>

            <td class="px-3 py-2 border-b border-gray-700"><span class="math">n + s \\cdot r</span></td>

            <td class="px-3 py-2 border-b border-gray-700">3.1</td>

            <td class="px-3 py-2 border-b border-gray-700">None</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">KS</td>

            <td class="px-3 py-2 border-b border-gray-700">T/M</td>

            <td class="px-3 py-2 border-b border-gray-700"><span class="math">(r+1)\\cdot(n\\cdot\\kappa)</span></td>

            <td class="px-3 py-2 border-b border-gray-700"><span class="math">n \\cdot \\kappa + r \\cdot (s \\cdot \\kappa)</span></td>

            <td class="px-3 py-2 border-b border-gray-700">3.2</td>

            <td class="px-3 py-2 border-b border-gray-700">Linear KS</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">LL evaluation</td>

            <td class="px-3 py-2 border-b border-gray-700">T/M</td>

            <td class="px-3 py-2 border-b border-gray-700"><span class="math">r \\cdot n^2</span></td>

            <td class="px-3 py-2 border-b border-gray-700"><span class="math">n^{2} + (r-1) \\cdot (n^{2} - (n-s)^{2})</span></td>

            <td class="px-3 py-2 border-b border-gray-700">5</td>

            <td class="px-3 py-2 border-b border-gray-700">None</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">LL sampling</td>

            <td class="px-3 py-2 border-b border-gray-700">T R</td>

            <td class="px-3 py-2 border-b border-gray-700"><span class="math">r \\cdot n^3</span> <span class="math">r \\cdot n^2</span></td>

            <td class="px-3 py-2 border-b border-gray-700"><span class="math">n^{3} + (r-1) \\cdot (s^{2} \\cdot n)</span><br><span class="math">n^{2} + (r-1) \\cdot (n^{2} - (n-s)^{2})</span></td>

            <td class="px-3 py-2 border-b border-gray-700">7</td>

            <td class="px-3 py-2 border-b border-gray-700">Random LL sampling</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><span id="page-4-0"></span><strong>Table 1.</strong> Improvements in time/memory/randomness (T/M/R) and assumptions under which they are applicable (RK = round keys, RC = round constants, KS = key schedule, LL = linear layer).</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Pa</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">ramet</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">ers</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Memory</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Runtime</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">n</td>

            <td class="px-3 py-2 border-b border-gray-700">s</td>

            <td class="px-3 py-2 border-b border-gray-700">r</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">LowMC</td>

            <td class="px-3 py-2 border-b border-gray-700">PICNIC</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">128</td>

            <td class="px-3 py-2 border-b border-gray-700">30</td>

            <td class="px-3 py-2 border-b border-gray-700">20</td>

            <td class="px-3 py-2 border-b border-gray-700">2.38x</td>

            <td class="px-3 py-2 border-b border-gray-700">1.41x</td>

            <td class="px-3 py-2 border-b border-gray-700">1.34x</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">192</td>

            <td class="px-3 py-2 border-b border-gray-700">30</td>

            <td class="px-3 py-2 border-b border-gray-700">30</td>

            <td class="px-3 py-2 border-b border-gray-700">3.99x</td>

            <td class="px-3 py-2 border-b border-gray-700">2.48x</td>

            <td class="px-3 py-2 border-b border-gray-700">1.72x</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">256</td>

            <td class="px-3 py-2 border-b border-gray-700">30</td>

            <td class="px-3 py-2 border-b border-gray-700">38</td>

            <td class="px-3 py-2 border-b border-gray-700">4.84x</td>

            <td class="px-3 py-2 border-b border-gray-700">2.82x</td>

            <td class="px-3 py-2 border-b border-gray-700">2.01x</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">128</td>

            <td class="px-3 py-2 border-b border-gray-700">3</td>

            <td class="px-3 py-2 border-b border-gray-700">182</td>

            <td class="px-3 py-2 border-b border-gray-700">16.51x</td>

            <td class="px-3 py-2 border-b border-gray-700">6.57x</td>

            <td class="px-3 py-2 border-b border-gray-700">4.74x</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">192</td>

            <td class="px-3 py-2 border-b border-gray-700">3</td>

            <td class="px-3 py-2 border-b border-gray-700">284</td>

            <td class="px-3 py-2 border-b border-gray-700">31.85x</td>

            <td class="px-3 py-2 border-b border-gray-700">11.50x</td>

            <td class="px-3 py-2 border-b border-gray-700">7.97x</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">256</td>

            <td class="px-3 py-2 border-b border-gray-700">3</td>

            <td class="px-3 py-2 border-b border-gray-700">363</td>

            <td class="px-3 py-2 border-b border-gray-700">39.48x</td>

            <td class="px-3 py-2 border-b border-gray-700">16.18x</td>

            <td class="px-3 py-2 border-b border-gray-700">10.83x</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><span id="page-4-1"></span><strong>Table 2.</strong> Multiplicative gains (previous / new) in memory consumption and in runtimes for LowMC encryption and Picnic signing and verification.</p>

    <p class="text-gray-300">Table 2 compares<sup>9</sup> the size of LowMC's linear layers in previous implementations to our new encryption algorithm for several instances. The first three instances are the ones used by the Picnic signature algorithm and for them we obtain a multiplicative gain of between 2.38x and 4.84x in memory consumption. Runtime-wise we obtain an improvement of a factor between 1.41x to 2.82x for LowMC encryption and by a factor between 1.34x to 2.01x for Picnic.</p>

    <p class="text-gray-300">Even more importantly, prior to this work, reducing s (in order to optimize the MC metric) while increasing r (in order to maintain the same security level for a LowMC instance) increased the linear algebra complexity proportionally to the increase in the number of rounds, making those instances impractical. One of the main consequences of this work is that such a reduction in s now also reduces the linear algebra complexity per round, such that the larger number of rounds is no longer a limiting factor. In particular, the last three instances in Table 2 correspond to a choice of parameters with a minimal value of s that minimizes signature sizes in Picnic. For those instances, we reduce the size of the linear layers by a factor between 16.51x to 39.48x and improve runtimes by up to a factor of 16x. Moreover, compared to the <em>original</em> Picnic instances that use s=30, using our optimizations, instances with s=3 reduce memory consumption and achieve comparable runtime results.</p>

    <p class="text-gray-300"><span id="page-4-2"></span><sup>&</sup>lt;sup>9</sup> For key size and the allowed data complexity, we refer to Appendix C.</p>

    <p class="text-gray-300">Our Techniques The first step in reducing the size of the round keys and constants is to exchange the order of the key and constant additions with the application of the linear layer in a round of the cipher. While this is a common technique in symmetric cryptography, we observe that in case s < n, after reordering, the constant and key additions of consecutive rounds can be merged through the n − s bits of the state that do not go through the non-linear transformation. Applying this process recursively effectively eliminates all the key and constant additions on n − s bits of the state (except for the initial key and constant additions). We then exploit the linear key schedule of LowMC and compute the reduced round keys more efficiently from the master key.</p>

    <p class="text-gray-300">In order to derive our new encryption algorithm, we show that each (generalized) LowMC instance belongs to a class of equivalent instances which is of a very large size when s n. We then select a representative member of the equivalence class that can be implemented efficiently using linear algebra optimizations which apply matrices with a special structure instead of random matrices (yet the full cipher remains equivalent). This requires a careful examination of the interaction between linear operations in consecutive rounds which is somewhat related to (but more complex than) the way that round keys and constants of consecutive rounds interact. After devising the encryption algorithm, we show how to sample a representative member of an equivalence class more efficiently than a random member. Our new sampling algorithm breaks dependencies among different parts of the linear layers in a generalized LowMC cipher, shedding further light on its internal structure.</p>

    <p class="text-gray-300">Finally, we formalize the notion of linear equivalence among generalized LowMC ciphers. This allows us to prove (based on two natural assumptions) that we correctly identified the linear equivalence classes and hence our description of the linear layers is optimal in size and we use the minimal amount of randomness to sample it. The formalization requires some care and the proof of optimality is somewhat non-standard (indeed, the claim that we prove is nonstandard).</p>

    <p class="text-gray-300">Related Work Previous works <a href="#page-29-6">\\[BB02,</a>[BCBP03\\]](#page-29-7) investigated equivalent representations of AES and other block ciphers obtained by utilizing the specific structure of their Sboxes (exploiting a property called self-affine equivalence [\\[BCBP03\\]](#page-29-7)). On the other hand, our equivalent representation and encryption algorithm is independent of the non-linear layer and can be applied regardless of its specification. Yet we only deal with block ciphers with partial non-linear layers in this paper.</p>

    <p class="text-gray-300">Paper Organization The rest of the paper is organized as follows. We describe some preliminaries in Section <a href="#page-6-0">2.</a> Our first optimizations regarding round keys, constants, and the key schedule are described in Section <a href="#page-8-1">3.</a> In Section <a href="#page-10-0">4,</a> we prove basic linear algebra properties, which are then used in our optimized encryption algorithm, described in Section <a href="#page-14-0">5.</a> Our evaluation of LowMC implementations that make use of these optimization are detailed in Section <a href="#page-18-0">6.</a> Next, our optimized instance generation algorithm for sampling the linear layers is given in Section 7. Finally, we prove the optimality of our description of the linear layers in Section 8 and conclude in Section 9.</p>

    <h2 id="sec-3" class="text-2xl font-bold">2 Preliminaries</h2>

    <h4 id="sec-4" class="text-lg font-semibold mt-6">2.1 Notation</h4>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Given a string of bits  <span class="math">x \\in \\{0,1\\}^n</span> , denote by x[</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">d] its d most significant bits (MSBs) and by x[d</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">] its d least significant bits (LSBs). Given strings x,y, denote by x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">y their concatenation. Given a matrix A, denote by A[\\<em>,i] its i'th column, by A[\\</em>,d</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">] its first d columns and by A[\\*,</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">d] its last d columns. Given two matrices  <span class="math">A \\in \\mathrm{GF}(2)^{d_1 \\times d_2}</span>  and  <span class="math">B \\in \\mathrm{GF}(2)^{d_1 \\times d_3}</span>  denote by  $A</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">B</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\in \\mathrm{GF}(2)^{d_1 \\times (d_2 + d_3)}<span class="math">  their concatenation. Denote by  </span>I_d \\in \\mathrm{GF}(2)^{d \\times d}$  the identity matrix.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Throughout this paper, addition x + y between bit strings  <span class="math">x, y \\in \\{0, 1\\}^n</span>  is performed bit-wise over  <span class="math">GF(2)^n</span>  (i.e., by XORing them).</p>

    <h2 id="sec-5" class="text-2xl font-bold">2.2 Generalized LowMC Ciphers</h2>

    <p class="text-gray-300">We study generalized LowMC (GLMC) ciphers where the block size is n bits, and each non-linear layer operates on  <span class="math">s \\leq n</span>  bits of the state. Each instance is characterized by a number of rounds r, round keys  <span class="math">k_i</span>  for  <span class="math">i \\in \\{0, ..., r\\}</span>  and round constants  <span class="math">C_i</span> , for  <span class="math">i \\in \\{0, ..., r\\}</span> . The cipher consists of r (partial) invertible nonlinear layers  <span class="math">S_i : \\{0, 1\\}^s \\to \\{0, 1\\}^s</span>  and r invertible linear layers  <span class="math">L_i \\in GF(2)^{n \\times n}</span>  for  <span class="math">i \\in \\{1, ..., r\\}</span> .</p>

    <p class="text-gray-300">A GLMC instance is generated by choosing each  <span class="math">L_i</span>  independently and uniformly at random among all invertible  <span class="math">n \\times n</span>  matrices.<sup>10</sup> However, we note that the main encryption algorithm we devise in Section 5 is applicable regardless of the way that the linear layers are chosen. We do not restrict the invertible non-linear layers.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">The encryption procedure manipulates n-bit words that represent GLMC states, while breaking them down according to their s LSBs (which we call "part 0 of the state") and n-s MSBs (which we call "part 1 of the state"). To simplify our notation, given any n-bit string x, we denote  <span class="math">x^{(0)} = x[s]</span>  and  $x^{(1)} = x[</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">n-s</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">]$ .</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><span id="page-6-2"></span>The basic GLMC encryption procedure is given in Algorithm 1. Decryption is performed by applying the inverse operations to a ciphertext.</p>

    <p class="text-gray-300">Key Schedule The key schedule optimization of Section 3.2 assumes that round keys are generated linearly from the master key (as in LowMC) and we now define appropriate notation. The master key k is of length  <span class="math">\\kappa</span>  bits. It is used to generate round key  <span class="math">k_i</span>  for  <span class="math">i \\in \\{0, 1, \\ldots, r\\}</span>  using the matrix  <span class="math">K_i \\in GF(2)^{n \\times \\kappa}</span> , namely,  <span class="math">k_i = K_i \\cdot k</span> . During instance generation, each matrix  <span class="math">\\{K_i\\}_{i=0}^r</span>  is chosen uniformly at random among all  <span class="math">n \\times \\kappa</span>  matrices.</p>

    <p class="text-gray-300"><span id="page-6-1"></span> <span class="math">&lt;sup&gt;^{10}&lt;/sup&gt;</span>  Alternatively, they can be selected in a pseudo-random way from a short seed, as in LowMC.</p>

    <pre><code class="language-text">\\begin{array}{l} \\textbf{Input} &amp; : x_0 \\\\ \\textbf{Output} : x_{r+1} \\\\ \\textbf{begin} \\\\ &amp; x_1 \\leftarrow x_0 + k_0 + C_0 \\\\ &amp; \\textbf{for} \\ i \\in \\{1, 2, \\dots, r\\} \\ \\textbf{do} \\\\ &amp; &amp; | \\ y_i \\leftarrow S_i(x_i^{(0)}) \\| x_i^{(1)} \\\\ &amp; | \\ x_{i+1} \\leftarrow L_i(y_i) + k_i + C_i \\\\ &amp; \\textbf{end} \\\\ &amp; \\textbf{return} \\ x_{r+1} \\\\ \\textbf{end} \\end{array}</code></pre>

    <p class="text-gray-300">Algorithm 1: Basic encryption.</p>

    <h4 id="sec-6" class="text-lg font-semibold mt-6">2.3 Breaking Down the Linear Layers</h4>

    <p class="text-gray-300">Given  <span class="math">L_i</span>  (which is an  <span class="math">n \\times n</span>  matrix), we partition its n-bit input into the first s LSBs (part 0 of the state that is output by  <span class="math">S_i</span> ) and the remaining n-s bits (part 1 of the state). Similarly, we partition its n-bit output into the first s LSBs (that are inputs of  <span class="math">S_{i+1}</span> ) and the remaining n-s bits. We define 4 sub-matrices of  <span class="math">L_i</span>  that map between the 4 possible pairs of state parts:</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{split} L_i^{00} &amp;\\in \\mathrm{GF}(2)^{s \\times s}, L_i^{01} \\in \\mathrm{GF}(2)^{s \\times (n-s)}, \\\\ L_i^{10} &amp;\\in \\mathrm{GF}(2)^{(n-s) \\times s}, L_i^{11} \\in \\mathrm{GF}(2)^{(n-s) \\times (n-s)}. \\end{split}</span></div>

    <p class="text-gray-300">Thus, in our notation  <span class="math">L_i^{ab}</span>  for  <span class="math">a, b \\in \\{0, 1\\}</span>  maps the part of the state denoted by b to the part of the state denoted by a.</p>

    <div class="my-4 text-center"><span class="math-block">L_{i} = \\left[ \\underbrace{\\frac{L_{i}^{00} \\mid L_{i}^{01}}{L_{i}^{10} \\mid L_{i}^{11}}}_{s} \\right] \\begin{cases} s \\\\ n-s \\end{cases}</span></div>

    <p class="text-gray-300">We extend our notation  <span class="math">L_i^{ab}</span>  by allowing  <span class="math">a,b\\in\\{0,1,<em>\\}</span> , where the symbol '\\</em>' denotes the full state. Therefore,</p>

    <div class="my-4 text-center"><span class="math-block">L_i^{0*} \\in \\mathrm{GF}(2)^{s \\times n}, L_i^{1*} \\in \\mathrm{GF}(2)^{(n-s) \\times n}, L_i^{*0} \\in \\mathrm{GF}(2)^{n \\times s}, L_i^{*1} \\in \\mathrm{GF}(2)^{n \\times (n-s)},</span></div>

    <p class="text-gray-300">are linear transformations which are sub-matrices of  <span class="math">L_i</span> , as shown below.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$L_i = \\left[\\frac{L_i^{0<em>}}{L_i^{1</em>}}\\right], \\, L_i = \\left[L_i^{*0} \\middle</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">L_i^{*1}\\right]$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">These linear transformation satisfy several basic equalities, e.g., for each  <span class="math">y \\in \\{0,1\\}^n</span> :</p>

    <div class="my-4 text-center"><span class="math-block">L_i^{0*}(y) = L_i(y)^{(0)} = L_i^{00}(y^{(0)}) + L_i^{01}(y^{(1)}),</span></div>

    <div class="my-4 text-center"><span class="math-block">L_i^{1*}(y) = L_i(y)^{(1)} = L_i^{10}(y^{(0)}) + L_i^{11}(y^{(1)}).</span></div>

    <h4 id="sec-7" class="text-lg font-semibold mt-6">2.4 Complexity Evaluation</h4>

    <p class="text-gray-300">In this paper, we analyze the complexity of the linear layers of generalized LowMC schemes. We will be interested in the two natural measures of time complexity (measured by the number of bit operations) and memory complexity (measured by the number of stored bits) of a single encryption (or decryption) of an arbitrary plaintext (or ciphertext). The linear layers are naturally represented by matrices, and thus evaluating a linear layer on a state is a simply a matrix-vector product. Since the time and memory complexities of evaluating and storing the linear layers are proportional in this paper, we will typically refer to both as the linear algebra complexity of the linear layers. For algorithms that generate GLMC instances, we will be interested in time complexity and in the number of random bits (or pseudo-random bits) that they use.</p>

    <h2 id="sec-8" class="text-2xl font-bold">3 Optimized Round Key Computation and Constant Addition</h2>

    <p class="text-gray-300">In this section we optimize the round key computation and constant addition in a GLMC cipher. First, we show how to compress the round keys and constants and then we optimize the key schedule of the cipher, assuming it is linear. These optimizations are significant in case we need to run the key schedule for every cipher invocation (which is the case in PICNIC).</p>

    <h4 id="sec-9" class="text-lg font-semibold mt-6">3.1 Compressing the Round Keys and Constants</h4>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">We combine the last two linear operations in encryption Algorithm 1 and obtain  <span class="math">x_{i+1} \\leftarrow L_i(y_i) + k_i + C_i</span> . Moreover,  $y_i \\leftarrow S_i(x_i^{(0)}) \\</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x_i^{(1)}<span class="math"> , namely  </span>S_i<span class="math">  only operates on the first s bits of the state and does not change  </span>x_i^{(1)}$ . Based on this observation, we perform the following:</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Modify  <span class="math">x_{i+1} \\leftarrow L_i(y_i) + k_i + C_i</span>  to  <span class="math">x_{i+1} \\leftarrow L_i(y_i + L_i^{-1} \\cdot k_i) + C_i</span> . - Split  <span class="math">L^{-1} \\cdot k_i</span>  into the lower splits (the "non-linear part" is  <span class="math">C_i = C_i</span> .</li>

      <li>Split  <span class="math">L_i^{-1} \\cdot k_i</span>  into the lower s bits (the "non-linear part", i.e.,  <span class="math">(L_i^{-1} \\cdot k_i)^{(0)}</span> ) and the upper n-s bits (the "linear part", i.e.,  <span class="math">(L_i^{-1} \\cdot k_i)^{(1)}</span> ) and move the addition of the upper n-s bits before the Sbox layer.</li>

    </ul>

    <p class="text-gray-300">Figure 1 demonstrates one round of the cipher with the above modifications (which do not change its output).</p>

    <p class="text-gray-300">Next, we observe that the addition of  <span class="math">(L_i^{-1} \\cdot k_i)^{(1)}</span>  at the beginning of the round can be combined with the addition of  <span class="math">k_{i-1}</span>  in the previous round. We can now perform similar operations to round i-1 and continue recursively until all additions to the linear part of the state have been moved to the start of the algorithm. In general, starting from the last round and iterating this procedure down to the first, we eliminate all additions of the linear parts of the round keys and move them before the first round. For each round  <span class="math">i \\geq 1</span> , we are left with a reduced round key of size s.</p>

    <p class="text-gray-300"><img src="_page_9_Figure_0.jpeg" alt="" class="my-4 max-w-full" /></p>

    <p class="text-gray-300"><span id="page-9-1"></span>Fig. 1. One round before (left) and after (right) splitting the round key addition.</p>

    <p class="text-gray-300">In total, the size of the round keys is reduced from  <span class="math">n \\cdot (r+1)</span>  to  <span class="math">n+s \\cdot r</span> . We remark that the same optimization can be performed to the constant additions, reducing their size by the same amount. We denote the new reduced round key of round i by  <span class="math">k_i&#x27;</span>  and the new reduced round constant by  <span class="math">C_i&#x27;</span> . The new encryption procedure is given in Algorithm 2. Observe that all the values  <span class="math">\\{k_i&#x27; + C_i&#x27;\\}_{i=0}^r</span>  can be computed and stored at the beginning of the encryption and their total size is  <span class="math">n+s \\cdot r</span> .</p>

    <pre><code class="language-text">\\begin{array}{ll} \\textbf{Input} &amp; : x_0 \\\\ \\textbf{Output} : x_{r+1} \\\\ \\textbf{begin} \\\\ &amp; \\left| \\begin{array}{l} x_1 \\leftarrow x_0 + k_0&#x27; + C_0&#x27; \\\\ \\textbf{for} \\ i \\in \\{1, 2, \\dots, r\\} \\ \\textbf{do} \\\\ &amp; \\left| \\begin{array}{l} y_i \\leftarrow (S_i(x_i^{(0)}) + k_i&#x27; + C_i&#x27;) \\|x_i^{(1)} \\\\ x_{i+1} \\leftarrow L_i(y_i) \\end{array} \\right. \\\\ &amp; \\text{end} \\\\ &amp; \\text{return} \\ x_{r+1} \\end{array}</code></pre>

    <p class="text-gray-300"><span id="page-9-2"></span><strong>Algorithm 2:</strong> Encryption with reduced round keys and constants.</p>

    <h3 id="sec-10" class="text-xl font-semibold mt-8">3.2 Optimizing the Key Schedule</h3>

    <p class="text-gray-300">We now deal with optimizing the round key computation of Algorithm 2, assuming a linear key schedule. The original key schedule applies r+1 round key matrices  <span class="math">K_i</span>  to the  <span class="math">\\kappa</span> -bit key k in order to compute the round keys  <span class="math">k_i = K_i \\cdot k</span> . It therefore has a complexity of  <span class="math">(r+1) \\cdot (n \\cdot \\kappa)</span>  (using a similar amount of memory). We show how to reduce this complexity to  <span class="math">n \\cdot \\kappa + r \\cdot (s \\cdot \\kappa)</span> .</p>

    <p class="text-gray-300">The main observation is that all transformations performed in Section 3.1 in order to calculate the new round keys from the original ones are linear. These linear transformations can be composed with the linear transformations  <span class="math">K_i</span>  in order to define linear transformations that compute the new round keys directly from the master key k. Since the total size of the round keys is  <span class="math">n+s\\cdot r</span>  bits, we can define matrices of total size  <span class="math">n\\cdot \\kappa + r\\cdot (s\\cdot \\kappa)</span>  that calculate all round keys from the master  <span class="math">\\kappa</span> -bit key.</p>

    <p class="text-gray-300">More specifically, we define the matrix  <span class="math">\\overline{L_i^{-1}}</span>  which is the inverse of the linear layer matrix  <span class="math">L_i</span> , with the first s rows of this inverse set to 0. Applying the iterative procedure defined in Section 3.1 from round r down to round i, we obtain</p>

    <div class="my-4 text-center"><span class="math-block">P_{N,i} = \\sum_{j=i}^{r} \\left( \\prod_{\\ell=i}^{j} \\overline{L_{\\ell}^{-1}} \\right) \\cdot K_{j}.</span></div>

    <p class="text-gray-300">For  <span class="math">i \\geq 1</span> , the new round key  <span class="math">k&#x27;_i</span>  (for the non-linear part of the state) is computed by taking the s least significant bits of  <span class="math">P_{N,i} \\cdot k</span> . Using the notation of Section 2.3, we have</p>

    <div class="my-4 text-center"><span class="math-block">k_i&#x27; = (P_{N,i})^{0*} \\cdot k.</span></div>

    <p class="text-gray-300">Observe that the total size of all  <span class="math">\\{(P_{N,i})^{0*}\\}_{i=1}^r</span>  is  <span class="math">r \\cdot (s \\cdot \\kappa)</span>  bits. Finally, the new round key  <span class="math">k&#x27;_0</span>  is calculated by summing the contributions from the linear parts of the state, using the matrix</p>

    <div class="my-4 text-center"><span class="math-block">P_L = K_0 + \\sum_{j=1}^r \\left( \\prod_{\\ell=1}^j \\overline{L_{\\ell}^{-1}} \\right) \\cdot K_j.</span></div>

    <p class="text-gray-300">Therefore, we have  <span class="math">k&#x27;_0 = P_L \\cdot k</span> , where  <span class="math">P_L</span>  is an  <span class="math">n \\times \\kappa</span>  matrix. All matrices  <span class="math">\\{(P_{N,i})^{0*}\\}_{i=1}^r, P_L</span>  can be precomputed after instance generation and we do not need to store the original round key matrices  <span class="math">K_i</span> .</p>

    <h2 id="sec-11" class="text-2xl font-bold">4 Linear Algebra Properties</h2>

    <p class="text-gray-300">In this section we describe the linear algebra properties that are relevant for the rest of this paper. We begin by describing additional notational conventions.</p>

    <h2 id="sec-12" class="text-2xl font-bold">4.1 General Matrix Notation</h2>

    <p class="text-gray-300">The superscript of  <span class="math">L_i^{ab}</span>  introduce in Section 2.3 has a double interpretation, as specifying both the dimensions of the matrix and its location in  <span class="math">L_i</span> . We will use this notation more generally to denote sub-matrices of some  <span class="math">n \\times n</span>  matrix A, or simply to define a matrix with appropriate dimensions (e.g.,  <span class="math">A^{01} \\in \\mathrm{GF}(2)^{s \\times (n-s)}</span>  may be defined without defining A and this should be clear from the context). Therefore, dimensions of the matrices in the rest of the paper will be explicitly specified in superscript as  <span class="math">A^{ab}</span> , where  <span class="math">a, b \\in \\{0, 1, *\\}</span>  (we do not deal with matrices of other dimensions). In case the matrix  <span class="math">A^{ab}</span>  is a sub-matrix of a larger</p>

    <p class="text-gray-300">matrix A, the superscript has a double interpretation as specifying both the dimensions of  <span class="math">A^{ab}</span>  and its location in A. When no superscript is given, the relevant matrix is of dimensions  <span class="math">n \\times n</span> . There will be two exceptions to this rule which will be specified separately.</p>

    <h2 id="sec-13" class="text-2xl font-bold">4.2 Invertible Binary Matrices</h2>

    <p class="text-gray-300">Denote by  <span class="math">\\alpha_n</span>  the probability that an  <span class="math">n \\times n</span>  uniformly chosen binary matrix is invertible. We will use the following well-known fact:</p>

    <p class="text-gray-300">Fact 1 [[Kol99], page 126, adapted] The probability that an  <span class="math">n \\times n</span>  uniform binary matrix is invertible is  <span class="math">\\alpha_n = \\prod_{i=1}^n (1-1/2^i) &gt; 0.2887</span> . More generally, for positive integers  <span class="math">d \\leq n</span> , the probability that a  <span class="math">d \\times n</span>  binary matrix, chosen uniformly at random, has full row rank of d is  <span class="math">\\prod_{i=n-d+1}^n (1-1/2^i) = (\\prod_{i=1}^n (1-1/2^i))/(\\prod_{i=1}^{n-d} (1-1/2^i)) = \\alpha_n/\\alpha_{n-d}</span> .</p>

    <p class="text-gray-300">We will be interested in invertibility of matrices of a special form, described in the following fact (which follows from basic linear algebra).</p>

    <p class="text-gray-300"><strong>Fact 2</strong> An  <span class="math">n \\times n</span>  binary matrix of the form</p>

    <p class="text-gray-300"><span id="page-11-4"></span><span id="page-11-1"></span></p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$\\left[ \\frac{A^{00}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A^{01}}{A^{10}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">I_{n-s}} \\right]$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">is invertible if and only if the  <span class="math">s \\times s</span>  matrix  <span class="math">B^{00} = A^{00} + A^{01}A^{10}</span>  is invertible and its inverse is given by</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$\\left\\lceil \\frac{(B^{00})^{-1}}{-A^{10}\\cdot(B^{00})^{-1}} \\middle</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\frac{-(B^{00})^{-1}\\cdot A^{01}}{I_{n-s}-A^{10}\\cdot(B^{00})^{-1}\\cdot A^{01}} \\right\\rceil.$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><span id="page-11-3"></span>Finally, we prove (in Appendix A) a simple proposition regarding random matrices.</p>

    <p class="text-gray-300"><strong>Proposition 1.</strong> Let  <span class="math">A \\in GF(2)^{n \\times n}</span>  be an invertible matrix chosen uniformly at random and let  <span class="math">B^{11} \\in GF(2)^{(n-s) \\times (n-s)}</span>  be an arbitrary invertible matrix (for  <span class="math">s \\leq n</span> ) that is independent from A. Then the matrix</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$C = \\left[ \\frac{A^{00}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A^{01} \\cdot B^{11}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}{A^{10}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A^{11} \\cdot B^{11}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">} \\right]$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">is a uniform invertible matrix.</p>

    <h4 id="sec-14" class="text-lg font-semibold mt-6">4.3 Normalized Matrices</h4>

    <p class="text-gray-300"><span id="page-11-2"></span><span id="page-11-0"></span><strong>Definition 1.</strong> Let  <span class="math">A^{1<em>}</span>  be a Boolean matrix with full row rank of n-s (and therefore it has n-s linearly independent columns). Let  <span class="math">\\mathrm{COL}(A)</span>  denote the first set of n-s linearly independent columns of  <span class="math">A^{1</em>}</span>  in a fixed lexicographic ordering of columns sets. Then, these columns form an  <span class="math">(n-s)\\times (n-s)</span>  invertible matrix which is denoted by  <span class="math">\\dot{A}</span> , while the remaining columns form an  <span class="math">(n-s)\\times s</span>  matrix which is denoted by  <span class="math">\\ddot{A}</span> . Moreover, denote  <span class="math">\\dot{A}=\\dot{A}^{-1}\\cdot A^{1*}\\in\\mathrm{GF}(2)^{(n-s)\\times}</span>  (in this matrix  <span class="math">\\dot{A}</span> , the columns of  <span class="math">\\mathrm{COL}(A)</span>  form the identity matrix).</p>

    <p class="text-gray-300">Remark 1. The only exception to the rule of Section 4.1 has to do with Definition 1 (and later with the related Definition 2). In this paper, the decomposition of Definition 1 is always applied to matrices  <span class="math">A^{1<em>} \\in \\mathrm{GF}(2)^{(n-s)\\times n}</span>  (in case  <span class="math">A^{1</em>}</span>  is a sub-matrix of A, it contains the bottom n-s rows of A). Hence the resulting matrices  <span class="math">\\dot{A} \\in \\mathrm{GF}(2)^{(n-s)\\times (n-s)}</span> ,  <span class="math">\\ddot{A} \\in \\mathrm{GF}(2)^{(n-s)\\times s}</span>  and  <span class="math">\\hat{A} \\in \\mathrm{GF}(2)^{(n-s)\\times n}</span>  have fixed dimensions and do not need any superscript. On the other hand, we will use superscript notation to denote sub-matrices of these. For example  <span class="math">\\hat{A}^{10} \\in \\mathrm{GF}(2)^{(n-s)\\times s}</span>  is a sub-matrix of  <span class="math">\\hat{A}</span> , consisting of its first s columns.</p>

    <p class="text-gray-300">It will be convenient to consider a lexicographic ordering in which the columns indices of  <span class="math">A^{1<em>}</span>  are reversed, i.e., the first ordered set of n-s columns is  <span class="math">\\{n,n-1,\\ldots,s+1\\}</span> , the second is  <span class="math">\\{n,n-1,\\ldots,s+2,s\\}</span> , etc. To demonstrate the above definition, assume that  <span class="math">\\mathrm{COL}(A) = \\{n,n-1,\\ldots,s+1\\}</span>  is a consecutive set of linearly independent columns. Then, the matrix  <span class="math">A^{1</em>}</span>  is shown below.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$A^{1*} = \\left[\\underbrace{\\ddot{A}}_{s} \\middle</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\underbrace{\\dot{A}}_{n-s} \\right] \\quad \\right\\} n - s$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">We can write  <span class="math">A = (\\dot{A} \\cdot \\dot{A}^{-1}) \\cdot A = \\dot{A} \\cdot (\\dot{A}^{-1} \\cdot A) = \\dot{A} \\cdot \\hat{A}</span> , where</p>

    <p class="text-gray-300"><span id="page-12-1"></span> <span class="math-block">\\hat{A} = \\dot{A}^{-1} \\cdot A^{1*} = \\left[\\underbrace{\\dot{A}^{-1} \\cdot \\ddot{A}}_{s} \\underbrace{I_{n-s}}_{n-s}\\right] \\quad \\right\\} n - s. \\tag{1}</span></p>

    <p class="text-gray-300">Normalized Equivalence Classes Given an invertible matrix  <span class="math">A \\in GF(2)^{n \\times n}</span> , define</p>

    <div class="my-4 text-center"><span class="math-block">N(A) = \\left[\\frac{A^{0*}}{\\hat{A}}\\right] = \\left[\\frac{A^{0*}}{\\dot{A}^{-1} \\cdot A^{1*}}\\right] = \\left[\\frac{I_s \\mid \\mathbf{0}^{01}}{\\mathbf{0}^{10} \\mid \\dot{A}^{-1}}\\right] \\cdot A.</span></div>

    <p class="text-gray-300">The transformation  <span class="math">N(\\cdot)</span>  partitions the set of invertible  <span class="math">n \\times n</span>  boolean matrices into normalized equivalence classes, where A, B are in the same normalized equivalence class if N(A) = N(B). We denote  <span class="math">A \\leftrightarrow_N B</span>  the relation N(A) = N(B).</p>

    <p class="text-gray-300"><span id="page-12-0"></span><strong>Proposition 2.</strong> Two invertible  <span class="math">n \\times n</span>  boolean matrices A, B satisfy  <span class="math">A \\leftrightarrow_N B</span>  if and only if there exists an invertible matrix  <span class="math">C^{11}</span>  such that</p>

    <p class="text-gray-300"><span id="page-12-2"></span> <span class="math-block">A = \\left[ \\frac{I_s \\mid \\mathbf{0}^{01}}{\\mathbf{0}^{10} \\mid C^{11}} \\right] \\cdot B.</span></p>

    <p class="text-gray-300">For the proof of Proposition 2, we refer the reader to Appendix A.</p>

    <p class="text-gray-300">Let  <span class="math">\\Phi = \\{N(A) \\mid A \\in \\mathrm{GF}(2)^{n \\times n} \\text{ is invertible}\\}</span>  contain a representative from each normalized equivalence class. Using Fact 1 and Proposition 2, we deduce the following corollary.</p>

    <p class="text-gray-300">Corollary 1. The following properties hold for normalized equivalence classes:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>1. Each member of  <span class="math">\\Phi</span>  represents a normalized equivalence class whose size is equal to the number of invertible  <span class="math">(n-s)\\times (n-s)</span>  matrices  <span class="math">C^{11}</span> , which is  <span class="math">\\alpha_{n-s}\\cdot 2^{(n-s)^2}</span> .</li>

      <li>2. The size of  <span class="math">\\Phi</span>  is</li>

    </ul>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\Phi</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">= \\frac{\\alpha_n \\cdot 2^{n^2}}{\\alpha_{n-s} \\cdot 2^{(n-s)^2}} = \\alpha_n / \\alpha_{n-s} \\cdot 2^{n^2 - (n-s)^2}.$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h4 id="sec-15" class="text-lg font-semibold mt-6">4.4 Matrix-Vector Product</h4>

    <p class="text-gray-300"><span id="page-13-0"></span><strong>Definition 2.</strong> Let  <span class="math">A^{1<em>}</span>  and  <span class="math">B^{</em>1}</span>  be two Boolean matrices such that  <span class="math">A^{1*}</span>  has full row rank of n-s. Define  <span class="math">\\check{B}_A=B\\cdot\\dot{A}\\in\\mathrm{GF}(2)^{n\\times(n-s)}</span> .</p>

    <p class="text-gray-300">When A is understood from the context, we simply write  <span class="math">\\check{B}</span>  instead of  <span class="math">\\check{B}_A</span> .</p>

    <p class="text-gray-300">Remark 2. The notational conventions that apply to Definition 1 also apply Definition 2 (see Remark 1), as it is always applied to matrices  <span class="math">A^{1<em>} \\in \\mathrm{GF}(2)^{(n-s)\\times n}</span>  and  <span class="math">B^{</em>1} \\in \\mathrm{GF}(2)^{n\\times (n-s)}</span> , where  <span class="math">\\check{B} \\in \\mathrm{GF}(2)^{n\\times (n-s)}</span>  (and its sub-matrices are denoted using superscript).</p>

    <p class="text-gray-300"><span id="page-13-2"></span><strong>Proposition 3.</strong> Let  <span class="math">A^{1<em>}</span>  and  <span class="math">B^{</em>1}</span>  be two Boolean matrices such that  <span class="math">A^{1<em>}</span>  has full row rank of n-s. Let  <span class="math">C=B^{</em>1}\\cdot A^{1<em>}\\in \\mathrm{GF}(2)^{n\\times n}</span> . Then, after preprocessing  <span class="math">A^{1</em>}</span>  and  <span class="math">B^{*1}</span> , C can be represented using  <span class="math">b=n^2-s^2+n</span>  bits. Moreover, given  <span class="math">x\\in \\mathrm{GF}(2)^n</span> , the matrix-vector product Cx can be computed using O(b) bit operations.</p>

    <p class="text-gray-300">Note that the above representation of the  <span class="math">n\\times n</span>  matrix C is more efficient than the trivial representation that uses  <span class="math">n^2</span>  bits (ignoring the additive lower order term n). It is also more efficient than a representation that uses the decomposition  <span class="math">C=B^{<em>1}\\cdot A^{1</em>}</span>  which requires  <span class="math">2n(n-s)=(n^2-s^2)+(n-s)^2\\geq n^2-s^2</span>  bits. Proof. The optimized representation is obtained by "pushing" linear algebra operations from  <span class="math">A^{1<em>}</span>  into  <span class="math">B^{</em>1}</span> , which "consumes" them, as formally described next. Note that since  <span class="math">A^{1<em>}</span>  has full row rank of n-s, we use definitions 1 and 2, and write  <span class="math">C=B^{</em>1}\\cdot A^{1<em>}=B^{</em>1}\\cdot (\\dot{A}\\cdot\\dot{A}^{-1})\\cdot A^{1<em>}=(B^{</em>1}\\cdot\\dot{A})\\cdot (\\dot{A}^{-1}\\cdot A^{1<em>})=\\check{B}\\cdot\\hat{A}</span> , where  <span class="math">\\check{B}</span>  and  <span class="math">\\hat{A}</span>  can be computed during preprocessing. Let us assume that the last n-s columns of  <span class="math">A^{1</em>}</span>  are linearly independent (namely,  <span class="math">\\mathrm{COL}(A^{1*})=\\{n,n-1,\\ldots,s+1\\}</span> ). Then due to (1),  <span class="math">\\hat{A}</span>  can be represented using s(n-s) bits and the matrix-vector product s(n-s) are computed using s(n-s) bits and the matrix-vector product s(n-s) computing s(n-s) bits and the matrix-vector product s(n-s) computing s(n-s) bits operations by computing s(n-s) bits operations by computing s(n-s) bits operations by computing s(n-s) bits and s(n-s) bits operations by computing s(n-s) bits operations by computing s(n-s) bits operations by computing s(n-s) bits operations by computing s(n-s) bits operations by computing s(n-s) bits operations by computing s(n-s) bits operations by computing s(n-s) bits operations by computing s(n-s) bits operations by computing s(n-s) bits operations by computing s(n-s) bits operations by computing s(n-s) bits operations by computing s(n-s) bits operations by computing s(n-s) bits operations of s(n-s) bits operations by computing s(n-s) bits operations of s(n-s) bits operations of s(n-s) bits operations of s(n-s) bits operations of s(n-s) bits operations of s(n-s) bits operations of s(n-s) bits operations of s(n-s) bits operations of s(n-s) bits</p>

    <p class="text-gray-300">We assumed that the last n-s columns of  <span class="math">A^{1<em>}</span>  are linearly independent. If this is not the case, then  <span class="math">COL(A^{1</em>})</span>  can be specified explicitly (to indicate the columns of  <span class="math">\\hat{A}</span>  that form the identity) using at most n additional bits. The product  <span class="math">\\hat{A}x</span>  is computed by decomposing x according to  <span class="math">COL(A^{1*})</span>  (rather than according to its s LSBs).</p>

    <p class="text-gray-300"><span id="page-13-3"></span>Remark 3. Consider the case that  <span class="math">A^{1<em>}</span>  is selected uniformly at random among all matrices of full row rank. Then, using simple analysis based on Fact 1, n-s linearly independent columns of  <span class="math">A^{1</em>}</span>  are very likely to be found among its n-s+3 last columns. Consequently, the additive low-order term n in the representation size of C can be reduced to an expected size of about  <span class="math">3 \\log n</span>  (specifying the 3 indices among are final n-s+3 that do not belong in  <span class="math">\\mathrm{COL}(A^{1*})</span> ). Moreover, computing the product  <span class="math">\\hat{A}x</span>  requires permuting only 3 pairs of bits of x on average (and then decomposing it as in the proof above).</p>

    <p class="text-gray-300">Remark 4. Instead of simplifying  <span class="math">A^{1<em>}</span>  to contain the identity matrix, we can alternatively simplify  <span class="math">B^{</em>1}</span>  assuming it has full column rank.<sup>11</sup> It is easy to verify that both simplifications give essentially the same result in terms of linear algebra complexity.</p>

    <p class="text-gray-300"><span id="page-13-1"></span><sup>&</sup>lt;sup>11</sup> One can also simplify both  <span class="math">A^{1<em>}</span>  and  <span class="math">B^{</em>1}</span> , but this is never useful in our application.</p>

    <pre><code class="language-text">\\begin{array}{l} \\textbf{Input} \\quad : x_0 \\\\ \\textbf{Output} : x_{r+1} \\\\ \\textbf{begin} \\\\ \\mid \\quad x_1 \\leftarrow x_0 + k_0 \\\\ \\mid \\quad \\text{for } i \\in \\{1, 2, \\dots, r\\} \\ \\textbf{do} \\\\ \\mid \\quad y_i \\leftarrow S_i(x_i^{(0)}) \\| x_i^{(1)} \\\\ \\mid \\quad x_{i+1} \\leftarrow L_i(y_i) \\\\ \\mid \\quad \\textbf{end} \\\\ \\mid \\quad \\textbf{return } x_{r+1} \\\\ \\textbf{end} \\end{array}</code></pre>

    <p class="text-gray-300"><strong>Algorithm 3:</strong> Simplified encryption.</p>

    <h2 id="sec-16" class="text-2xl font-bold">5 Optimized Linear Layer Evaluation</h2>

    <p class="text-gray-300">In this section, we describe our encryption algorithm that optimizes the linear algebra of Algorithm 2. We begin by optimizing the implementation of a 2-round GLMC cipher and then consider a general r-round cipher.</p>

    <p class="text-gray-300">It will be convenient to further simplify Algorithm 2 by defining  <span class="math">k_0&#x27;&#x27; = k_0&#x27; + C_0&#x27;</span> . For i > 0, we move the addition of  <span class="math">k_i&#x27; + C_i&#x27;</span>  into  <span class="math">S_i</span>  by redefining  <span class="math">S_i&#x27;&#x27;(x_i^{(0)}) = S_i(x_i^{(0)}) + k_i&#x27; + C_i&#x27;</span> . This makes the Sbox key-dependent, which is not important for the rest of the paper. Finally, we abuse notation for simplicity and rename  <span class="math">k_0&#x27;&#x27;</span>  and  <span class="math">S_i&#x27;&#x27;</span>  back to  <span class="math">k_0</span>  and  <span class="math">S_i</span> , respectively. The outcome is given in Algorithm 3.</p>

    <h2 id="sec-17" class="text-2xl font-bold">5.1 Basic 2-Round Encryption Algorithm</h2>

    <p class="text-gray-300">We start with a basic algorithm that attempts to combine the linear algebra computation of two rounds. This computation can be written as</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$\\left(\\frac{x_3^{(0)}}{x_3^{(1)}}\\right) = \\left[\\frac{L_2^{00}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">L_2^{01}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}{L_2^{10}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">L_2^{1T}}\\right] \\left(\\frac{y_2^{(0)}}{y_2^{(1)}}\\right), \\left(\\frac{x_2^{(0)}}{x_2^{(1)}}\\right) = \\left[\\frac{L_1^{00}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">L_1^{01}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}{L_1^{10}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">L_1^{1T}}\\right] \\left(\\frac{y_1^{(0)}}{y_1^{(1)}}\\right).$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Note that  <span class="math">x_2^{(0)}</span>  and  <span class="math">y_2^{(0)}</span>  are related non-linearly as  <span class="math">y_2^{(0)} = S_2(x_2^{(0)})</span> . On the other hand, since  <span class="math">x_2^{(1)} = y_2^{(1)}</span>  we can compute the contribution of  <span class="math">y_2^{(1)}</span>  to  <span class="math">x_3</span>  at once from  <span class="math">y_1</span>  by partially combining the linear operations of the two rounds as</p>

    <p class="text-gray-300"><span id="page-14-2"></span></p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$\\left( \\frac{t_3^{(0)}}{t_3^{(1)}} \\right) = \\left[ \\frac{L_2^{01} L_1^{10} \\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">L_2^{01} L_1^{11} \\right</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}{L_2^{11} L_1^{10} \\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">L_2^{11} L_1^{11} \\right</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">} \\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\frac{y_1^{(0)}}{y_1^{(1)}} \\right</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">.$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"> (2)</p>

    <p class="text-gray-300">The linear transformation of (2) is obtained from the product  <span class="math">L_2 \\cdot L_1</span>  by ignoring the terms involving  <span class="math">L_2^{00}</span>  and  <span class="math">L_2^{10}</span>  (that operate on  <span class="math">y_2^{(0)}</span> ). Note that (2) defines an  <span class="math">n \\times n</span>  matrix that can be precomputed.</p>

    <p class="text-gray-300">We are left to compute the contribution of  <span class="math">y_2^{(0)}</span>  to  <span class="math">x_3</span> , which is done directly as in Algorithm 3 by</p>

    <p class="text-gray-300"><span id="page-14-3"></span> <span class="math-block">x_2^{(0)} \\leftarrow L_1^{0<em>}(y_1), y_2^{(0)} \\leftarrow S_2(x_2^{(0)}), t_3&#x27; \\leftarrow L_2^{</em>0}(y_2^{(0)}).</span>  (3)</p>

    <p class="text-gray-300">This calculation involves  <span class="math">s \\times n</span>  and  <span class="math">n \\times s</span>  matrices. Finally, combining the contributions of (2) and (3), we obtain</p>

    <p class="text-gray-300"><span class="math-block">x_3 \\leftarrow t_3 + t_3&#x27;</span> .</p>

    <p class="text-gray-300">Overall, the complexity of linear algebra in the two rounds is  <span class="math">n^2 + 2sn</span>  instead of  <span class="math">2n^2</span>  of Algorithm 3. This is an improvement provided that s < n/2, but is inefficient otherwise.</p>

    <h2 id="sec-18" class="text-2xl font-bold">5.2 Optimized 2-Round Encryption Algorithm</h2>

    <p class="text-gray-300">The optimized algorithm requires a closer look at the linear transformation of (2). Note that this matrix can be rewritten as the product</p>

    <p class="text-gray-300"><span id="page-15-1"></span></p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$\\left(\\frac{t_3^{(0)}}{t_3^{(1)}}\\right) = \\left[\\frac{L_2^{01}}{L_2^{11}}\\right] \\left[L_1^{10} \\middle</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">L_1^{11}\\right] \\left(\\frac{y_1^{(0)}}{y_1^{(1)}}\\right).$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">(4)</p>

    <p class="text-gray-300">More compactly, this  <span class="math">n \\times n</span>  linear transformation is decomposed as  <span class="math">L_2^{<em>1} \\cdot L_1^{1</em>}</span> , namely, it is a product of matrices with dimensions  <span class="math">(n-s) \\times n</span>  and  <span class="math">n \\times (n-s)</span> . In order to take advantage of this decomposition, we use Proposition 3 which can be applied since  <span class="math">L_1^{1<em>}</span>  has full row rank of n-s. This reduces linear algebra complexity of  <span class="math">L_2^{</em>1} \\cdot L_1^{1*}</span>  from  <span class="math">n^2</span>  to  <span class="math">n(n-s)+n(n-s)-(n-s)^2=n^2-s^2</span> , ignoring an additive low order term of n - s = n + 1 in Remark 3.</p>

    <pre><code class="language-text">\\begin{array}{llllllllllllllllllllllllllllllllllll</code></pre>

    <p class="text-gray-300">Algorithm 4 exploits the decomposition  <span class="math">L_2^{<em>1} \\cdot L_1^{1</em>} = \\check{L}_2 \\cdot \\hat{L}_1</span> . Altogether, the linear algebra complexity of 2 rounds is reduced to</p>

    <div class="my-4 text-center"><span class="math-block">n^2 + 2sn - s^2 = 2n^2 - (n - s)^2</span></div>

    <p class="text-gray-300">(or  <span class="math">2n^2 - (n-s)^2 + 3\\log n</span>  after taking Remark 3 into account). This is an improvement by an additive factor of about  <span class="math">s^2</span>  compared to the basic 2-round algorithm above and is an improvement over the standard complexity of  <span class="math">2n^2</span>  for essentially all s < n.</p>

    <h4 id="sec-19" class="text-lg font-semibold mt-6">5.3 Towards an Optimized r-Round Encryption Algorithm</h4>

    <p class="text-gray-300">The optimization applied in the 2-round algorithm does not seem to generalize to an arbitrary number of rounds in a straightforward manner. In fact, there is more than one way to generalize this algorithm (and obtain improvements over the standard one in some cases) using variants of the basic algorithm of Section 5.1 which directly combines more that two rounds. These variants are sub-optimal since they do not exploit the full potential of Proposition 3.</p>

    <p class="text-gray-300">The optimal algorithm is still not evident since the structure of the rounds of Algorithm 4 does not resemble their structure in Algorithm 3 that we started with. Consequently, we rewrite it in Algorithm 5 such that  <span class="math">z_2^{(1)} = \\hat{L}_1(y_1)</span>  is computed already in round 1 instead of round 2. The linear algebra in round 2 of Algorithm 5 can now be described using the  <span class="math">n \\times n</span>  transformation</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$\\begin{pmatrix} x_3^{(0)} \\\\ \\overline{x_3^{(1)}} \\end{pmatrix} = \\begin{bmatrix} \\underline{L_2^{00}} \\middle</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\check{L}_2^{01} \\\\ \\overline{L_2^{10}} \\middle</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\check{L}_2^{11} \\end{bmatrix} \\begin{pmatrix} \\underline{y_2^{(0)}} \\\\ \\overline{z_2^{(1)}} \\end{pmatrix}.$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Note that  <span class="math">z_2^{(1)}</span>  is a value that is never computed by the original Algorithm 3.</p>

    <p class="text-gray-300">When we add additional encryption rounds, we can apply Proposition 3 again and "push" some of the linear algebra of round 2 into round 3, then "push" some of the linear algebra of round 3 into round 4, etc. The full algorithm is described in detail next.</p>

    <h2 id="sec-20" class="text-2xl font-bold">5.4 Optimized r-Round Encryption Algorithm</h2>

    <p class="text-gray-300">In this section, we describe our optimized algorithm for evaluating r rounds of a GLMC cipher. We begin by defining the following sequence of matrices.</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{aligned} \\text{For } i = 1: &amp; R_1^{1*} = L_1^{1*} \\\\ \\hat{R}_1 &amp;= (\\dot{R}_1)^{-1} \\cdot R_1^{1*}. \\end{aligned}</span></div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$\\begin{aligned} \\hat{T}_i &= L_i^{<em>1} \\cdot \\dot{R}_{i-1} \\\\ R_i^{1</em>} &= L_i^{10}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\check{T}_i^{11}. \\\\ \\hat{R}_i &= (\\dot{R}_i)^{-1} \\cdot R_i^{1*}. \\end{aligned}$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="my-4 text-center"><span class="math-block">\\begin{aligned} \\hat{T}_i &amp;= L_i^{10} \\cdot \\dot{R}_i^{1*}. \\\\ \\hat{T}_i &amp;= L_i^{10} \\cdot \\dot{R}_i^{1*}. \\end{aligned}$$  For  $i = r$ : $$\\dot{T}_r &amp;= L_r^{*1} \\cdot \\dot{R}_{r-1}. \\end{aligned}</span></div>

    <p class="text-gray-300">Basically, the matrix  <span class="math">\\check{T}_i</span>  combines the linear algebra of round i with the linear algebra that is pushed from the previous round (represented by  <span class="math">\\dot{R}_{i-1}</span> ). The matrix  <span class="math">\\hat{R}_i</span>  is the source of optimization, computed by normalizing the updated round matrix (after computing  <span class="math">\\check{T}_i</span> ). The byproduct of this normalization is  <span class="math">\\dot{R}_i</span> , which is pushed into round i+1, and so forth.</p>

    <p class="text-gray-300">Before we continue, we need to prove the following claim (the proof is given in Appendix B).</p>

    <p class="text-gray-300"><span id="page-16-1"></span><strong>Proposition 4.</strong> The matrix  <span class="math">R_i^{1*}</span>  has full row rank of n-s for all  <span class="math">i \\in \\{1, \\ldots, r-1\\}</span> , hence  <span class="math">(\\dot{R}_i)^{-1}</span>  exists.</p>

    <pre><code class="language-text">\\begin{array}{l} \\text{Input} &amp; : x_0 \\\\ \\text{Output} : x_{r+1} \\\\ \\text{begin} \\\\ &amp; \\quad x_1 \\leftarrow x_0 + k_0 \\\\ &amp; \\quad y_1 \\leftarrow S_1(x_1^{(0)}) \\| x_1^{(1)} \\\\ &amp; \\quad y_1 \\leftarrow S_1(x_1^{(0)}) \\| x_1^{(1)} \\\\ &amp; \\quad z_2^{(0)} \\leftarrow L_1^{0*}(y_1) \\\\ &amp; \\quad z_2^{(1)} \\leftarrow \\hat{R}_1(y_1) \\\\ &amp; \\quad z_2^{(1)} \\leftarrow \\hat{R}_1(y_1) \\\\ &amp; \\quad for \\ i \\in \\{2, \\dots, r-1\\} \\ \\text{do} \\\\ &amp; \\quad \\quad \\left| \\begin{array}{c} y_i^{(0)} \\leftarrow S_i(x_i^{(0)}) \\\\ x_{i+1}^{(0)} \\leftarrow L_i^{(0)}(y_i^{(0)}) + \\check{T}_i^{(0)}(z_i^{(1)}) \\\\ &amp; \\quad z_{i+1}^{(1)} \\leftarrow \\hat{R}_i(y_i^{(0)} \\| z_i^{(1)}) \\\\ &amp; \\quad \\text{end} \\\\ &amp; \\quad y_r^{(0)} \\leftarrow S_r(x_r^{(0)}) \\\\ &amp; \\quad x_{r+1} \\leftarrow L_r^{*0}(y_r^{(0)}) + \\check{T}_r(z_r^{(1)}) \\\\ &amp; \\quad \\text{return} \\ x_{r+1} \\end{array} \\right. \\Rightarrow \\text{Round} \\ r</code></pre>

    <p class="text-gray-300"><strong>Algorithm 6:</strong> Optimized r-round encryption.</p>

    <p class="text-gray-300"><span id="page-17-0"></span>The general optimized encryption algorithm is given in Algorithm 6. At a high level, the first round can be viewed as mapping the "real state"  <span class="math">(y_1^{(0)}, y_1^{(1)})</span>  into the "shadow state"  <span class="math">(x_2^{(0)}, z_2^{(1)})</span>  using the linear transformation</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$\\left(\\frac{x_2^{(0)}}{z_2^{(1)}}\\right) = \\left[\\frac{L_1^{00} \\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">L_1^{01} \\right</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}{\\hat{R}_1^{10} \\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\hat{R}_1^{11} \\right</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">} \\left(\\frac{y_1^{(0)}}{y_1^{(1)}}\\right).$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">In rounds  <span class="math">i \\in \\{2, \\ldots, r-1\\}</span> , the shadow state  <span class="math">(y_i^{(0)}, z_i^{(1)})</span>  (obtained after applying  <span class="math">S_i(x_i^{(0)})</span> ) is mapped to the next shadow state  <span class="math">(x_{i+1}^{(0)}, z_{i+1}^{(1)})</span>  using the linear transformation</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$\\begin{pmatrix} x_{i+1}^{(0)} \\\\ z_{i+1}^{(1)} \\end{pmatrix} = \\begin{bmatrix} L_i^{00} \\middle</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\check{T}_i^{01} \\\\ \\hat{R}_i^{10} \\middle</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\hat{R}_i^{11} \\end{bmatrix} \\begin{pmatrix} y_i^{(0)} \\\\ z_i^{(1)} \\end{pmatrix}.$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Finally, in round r, the shadow state  <span class="math">(y_r^{(0)}, z_r^{(1)})</span>  is mapped to the final real state  <span class="math">(x_{r+1}^{(0)}, x_{r+1}^{(1)})</span>  using the linear transformation</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$\\begin{pmatrix} x_{r+1}^{(0)} \\\\ x_{r+1}^{(1)} \\end{pmatrix} = \\begin{bmatrix} L_r^{00}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\check{T}_r^{01} \\\\ L_r^{10}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\check{T}_r^{11} \\end{bmatrix} \\begin{pmatrix} y_r^{(0)} \\\\ z_r^{(1)} \\end{pmatrix}.$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Complexity Evaluation As noted above, Algorithm 6 applies r linear transformation, each of dimension  <span class="math">n \\times n</span> . Hence, ignoring the linear algebra optimizations for each  <span class="math">\\hat{R}_i</span> , the linear algebra complexity of each round is  <span class="math">n^2</span> , leading to a total complexity of  <span class="math">r \\cdot n^2</span> . Taking the optimizations into account, for each  <span class="math">i \\in \\{1, \\ldots, r-1\\}</span> , the actual linear algebra complexity of  <span class="math">\\hat{R}_i</span>  is reduced by  <span class="math">(n-s)^2</span>  to  <span class="math">n^2 - (n-s)^2</span></p>

    <p class="text-gray-300">(as  <span class="math">\\hat{R}_i</span>  contains the  <span class="math">(n-s)\\times(n-s)</span>  identity matrix). Therefore, the total linear algebra complexity is</p>

    <p class="text-gray-300"><span class="math-block">r \\cdot n^2 - (r-1)(n-s)^2</span> .</p>

    <p class="text-gray-300">Taking Remark 3 into account, we need to add another factor of  <span class="math">3(r-1)\\log n</span> .</p>

    <p class="text-gray-300">Remark 5. Note that Algorithm 6 is obtained from Algorithm 3 independently of how the instances of the cipher are generated. Hence, Algorithm 6 is applicable in principle to all SP-networks with partial non-linear layers.</p>

    <p class="text-gray-300">Correctness We now prove correctness of Algorithm 6 by showing that its output value is identical to a standard implementation of the scheme in Algorithm 3. For each  <span class="math">i \\in \\{0, 1, \\ldots, r+1\\}</span> , denote by  <span class="math">\\bar{x}_i</span>  the state value at the beginning of round i in a standard implementation and by  <span class="math">\\bar{y}_i</span>  the state after the application of  <span class="math">S_i</span> . The proof of Proposition 5 are given in Appendix B.</p>

    <p class="text-gray-300"><strong>Proposition 5.</strong> For each <span class="math-block">i \\in \\{1, ..., r-1\\}</span>  in Algorithm 6,  <span class="math">y_i^{(0)} = \\bar{y}_i^{(0)}, x_{i+1}^{(0)} = \\bar{x}_{i+1}^{(0)}</span>  and  <span class="math">z_{i+1}^{(1)} = (\\dot{R}_i)^{-1}(\\bar{x}_{i+1}^{(1)})</span> .</p>

    <p class="text-gray-300"><strong>Proposition 6.</strong> Algorithm 6 is correct, namely  <span class="math">x_{r+1} = \\bar{x}_{r+1}</span> .</p>

    <p class="text-gray-300">Proof. By Algorithm 6 and using Proposition 5,</p>

    <div class="my-4 text-center"><span class="math-block">x_{r+1} = L_r^{*0}(y_r^{(0)}) + \\check{T}_r(z_r^{(1)}) = L_r^{*0}(\\bar{y}_r^{(0)}) + L_r^{*1} \\cdot \\dot{R}_{r-1}((\\dot{R}_{r-1})^{-1}(\\bar{x}_r^{(1)})) = L_r^{*0}(\\bar{y}_r^{(0)}) + L_r^{*1}(\\bar{y}_r^{(1)}) = L_r(\\bar{y}_r) = \\bar{x}_{r+1}.</span></div>

    <p class="text-gray-300"><span id="page-18-1"></span></p>

    <p class="text-gray-300">To verify the expected performance and memory improvements, we evaluate both suggested optimizations in three scenarios: LowMC encryption, the digital signature scheme Picnic, and in the context of Yao's garbled circuits. We discuss the details on the choice of LowMC instances and how LowMC is used in Picnic and garbled circuits and their applications in Appendix C. Throughout this section, we benchmark LowMC instances with block size n, non-linear layer size s and r rounds and simply refer to them as LowMC-n-s-r. For the evaluation in the context of Picnic, we integrated our optimizations in the SIMD-optimized implementation available on GitHub. For the evaluation in a garbled circuit framework, we implement it from scratch. All benchmarks presented in this section were performed on an Intel Core i7-4790 running Ubuntu 18.04.</p>

    <p class="text-gray-300"><span id="page-18-2"></span><sup>&</sup>lt;sup>12</sup> See https://github.com/IAIK/Picnic for the integration in PICNIC and https://github.com/IAIK/Picnic-LowMC for the matrix generation.</p>

    <h4 id="sec-22" class="text-lg font-semibold mt-6">6.1 LowMC</h4>

    <p class="text-gray-300">We first present benchmarking results for encryption of LowMC instances selected for the Picnic use-case, i.e., with data complexity 1, and s=3, as well as the instances currently used in Picnic with s=30. While the optimized round key computation and constant addition (ORKC, Section 3) already reduces the runtime of a single encryption by half, which we would also obtain by pre-computing the round keys (when not used inside Picnic), the optimized linear layer evaluation (OLLE, Section 5) significantly reduces the runtime even using a SIMD optimized implementation. For s=30, we achieve improvements by a factor up to 2.82x and for s=3 up to a factor of 16.18x, bringing the performance of the instances with only one Sbox close to ones with more Sboxes.</p>

    <p class="text-gray-300">Memory-wise we observe huge memory reductions for the instances used in Picnic. While ORKC reduces the required storage for the LowMC matrices and constants to about a half, OLLE further reduces memory requirements substantially. As expected, the instances with a small number of Sboxes benefit most significantly from both optimizations. For example, for LowMC-256-10-38 the matrices and constants shrink from 620.8 KB to 128.3 KB, a reduction by 79 %, whereas for LowMC-256-1-363 instead of 5861.4 KB encryption requires only 148.5 KB, i.e., only 2.5 % of the original size. The full benchmark results and sizes of the involved matrices and constants are given in Table 3.</p>

    <h4 id="sec-23" class="text-lg font-semibold mt-6">6.2 Picnic</h4>

    <p class="text-gray-300">We continue with evaluating our optimizations in Picnic itself. In Table 4 we present the numbers obtained from benchmarking Picnic with the original LowMC instances, as well as those with  <span class="math">s=3.^{13}</span>  For instances with 10 Sboxes we achieve an improvement of up to a factor of 2.01x. For the extreme case using only 1 Sbox, even better improvements of up to a factor of 10.83x are possible. With OLLE those instances are close to the performance numbers of the instances with 10 Sboxes, reducing the overhead from a factor 8.4x to a factor 1.6x. Thus those instances become practically useful alternatives to obtain the smallest possible signatures.</p>

    <h2 id="sec-24" class="text-2xl font-bold">6.3 Garbled Circuits</h2>

    <p class="text-gray-300">Finally, we evaluated LowMC in the context of garbled circuits, where we compare an implementation using the standard linear layer and round-key computation (utilizing the method of four Russians to speed up the matrix-vector products) to an implementation using our optimizations. In Table 5 we present the results of our evaluation. We focus on LowMC instances with 1 Sbox, since</p>

    <p class="text-gray-300"><span id="page-19-0"></span><sup>&</sup>lt;sup>13</sup> PICNIC instances may internally use the Fiat-Shamir (FS) or Unruh (UR) transforms. However, as both evaluate LowMC exactly in the same way, only numbers for PICNIC instances using the FS transform are given. Namely, improvements to LowMC encryption apply to PICNIC-FS and PICNIC-UR in the same way.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">LowMC-n-s-r</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">w/o opt. with ORKC with OLLE Improv. (old / new)</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">R</td>

            <td class="px-3 py-2 border-b border-gray-700">3.29</td>

            <td class="px-3 py-2 border-b border-gray-700">2.36</td>

            <td class="px-3 py-2 border-b border-gray-700">2.33</td>

            <td class="px-3 py-2 border-b border-gray-700">1.41x</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">128-30-20</td>

            <td class="px-3 py-2 border-b border-gray-700">S</td>

            <td class="px-3 py-2 border-b border-gray-700">84.2</td>

            <td class="px-3 py-2 border-b border-gray-700">55.0</td>

            <td class="px-3 py-2 border-b border-gray-700">35.4</td>

            <td class="px-3 py-2 border-b border-gray-700">2.38x</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">R</td>

            <td class="px-3 py-2 border-b border-gray-700">10.03</td>

            <td class="px-3 py-2 border-b border-gray-700">5.64</td>

            <td class="px-3 py-2 border-b border-gray-700">4.04</td>

            <td class="px-3 py-2 border-b border-gray-700">2.48x</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">192-30-30</td>

            <td class="px-3 py-2 border-b border-gray-700">S</td>

            <td class="px-3 py-2 border-b border-gray-700">369.8</td>

            <td class="px-3 py-2 border-b border-gray-700">211.2</td>

            <td class="px-3 py-2 border-b border-gray-700">92.8</td>

            <td class="px-3 py-2 border-b border-gray-700">3.99x</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">R</td>

            <td class="px-3 py-2 border-b border-gray-700">16.41</td>

            <td class="px-3 py-2 border-b border-gray-700">9.21</td>

            <td class="px-3 py-2 border-b border-gray-700">5.81</td>

            <td class="px-3 py-2 border-b border-gray-700">2.82x</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">256-30-38</td>

            <td class="px-3 py-2 border-b border-gray-700">S</td>

            <td class="px-3 py-2 border-b border-gray-700">620.8</td>

            <td class="px-3 py-2 border-b border-gray-700">353.5</td>

            <td class="px-3 py-2 border-b border-gray-700">128.3</td>

            <td class="px-3 py-2 border-b border-gray-700">4.84x</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">128-3-182</td>

            <td class="px-3 py-2 border-b border-gray-700">R</td>

            <td class="px-3 py-2 border-b border-gray-700">30.93</td>

            <td class="px-3 py-2 border-b border-gray-700">17.13</td>

            <td class="px-3 py-2 border-b border-gray-700">4.71</td>

            <td class="px-3 py-2 border-b border-gray-700">6.57x</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">S</td>

            <td class="px-3 py-2 border-b border-gray-700">749.9</td>

            <td class="px-3 py-2 border-b border-gray-700">383.9</td>

            <td class="px-3 py-2 border-b border-gray-700">45.4</td>

            <td class="px-3 py-2 border-b border-gray-700">16.51x</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">R</td>

            <td class="px-3 py-2 border-b border-gray-700">90.99</td>

            <td class="px-3 py-2 border-b border-gray-700">47.32</td>

            <td class="px-3 py-2 border-b border-gray-700">7.91</td>

            <td class="px-3 py-2 border-b border-gray-700">11.50x</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">192-3-284</td>

            <td class="px-3 py-2 border-b border-gray-700">S</td>

            <td class="px-3 py-2 border-b border-gray-700">3449.5</td>

            <td class="px-3 py-2 border-b border-gray-700">1743.2</td>

            <td class="px-3 py-2 border-b border-gray-700">108.3</td>

            <td class="px-3 py-2 border-b border-gray-700">31.85x</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">R</td>

            <td class="px-3 py-2 border-b border-gray-700">167.05</td>

            <td class="px-3 py-2 border-b border-gray-700">78.64</td>

            <td class="px-3 py-2 border-b border-gray-700">10.32</td>

            <td class="px-3 py-2 border-b border-gray-700">16.18x</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">256-3-363</td>

            <td class="px-3 py-2 border-b border-gray-700">S</td>

            <td class="px-3 py-2 border-b border-gray-700">5861.4</td>

            <td class="px-3 py-2 border-b border-gray-700">2963.7</td>

            <td class="px-3 py-2 border-b border-gray-700">148.5</td>

            <td class="px-3 py-2 border-b border-gray-700">39.48x</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><span id="page-20-1"></span>Table 3. Benchmarks (R) of LowMC-n-s-r instances using SIMD, without optimization, with ORKC, and OLLE (in µs). Sizes (S) of matrices and constants stored in compiled implementation (in KB).</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">w/o opt.</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">with ORKC with OLLE</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Improv. (old / new)</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Parameters</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">Sign Verify</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">Sign Verify</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">Sign Verify</td>

            <td class="px-3 py-2 border-b border-gray-700">Sign</td>

            <td class="px-3 py-2 border-b border-gray-700">Verify</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Picnic-128-30-20</td>

            <td class="px-3 py-2 border-b border-gray-700">3.56</td>

            <td class="px-3 py-2 border-b border-gray-700">2.41</td>

            <td class="px-3 py-2 border-b border-gray-700">2.71</td>

            <td class="px-3 py-2 border-b border-gray-700">1.89</td>

            <td class="px-3 py-2 border-b border-gray-700">2.65</td>

            <td class="px-3 py-2 border-b border-gray-700">1.87</td>

            <td class="px-3 py-2 border-b border-gray-700">1.34x</td>

            <td class="px-3 py-2 border-b border-gray-700">1.29x</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Picnic-192-30-30</td>

            <td class="px-3 py-2 border-b border-gray-700">10.91</td>

            <td class="px-3 py-2 border-b border-gray-700">7.76</td>

            <td class="px-3 py-2 border-b border-gray-700">7.52</td>

            <td class="px-3 py-2 border-b border-gray-700">5.22</td>

            <td class="px-3 py-2 border-b border-gray-700">6.33</td>

            <td class="px-3 py-2 border-b border-gray-700">4.44</td>

            <td class="px-3 py-2 border-b border-gray-700">1.72x</td>

            <td class="px-3 py-2 border-b border-gray-700">1.75x</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Picnic-256-30-38</td>

            <td class="px-3 py-2 border-b border-gray-700">22.80</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">15.63 15.41</td>

            <td class="px-3 py-2 border-b border-gray-700">10.82 11.37</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">7.88</td>

            <td class="px-3 py-2 border-b border-gray-700">2.01x</td>

            <td class="px-3 py-2 border-b border-gray-700">1.98x</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Picnic-128-3-182</td>

            <td class="px-3 py-2 border-b border-gray-700">20.49</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">14.23 11.78</td>

            <td class="px-3 py-2 border-b border-gray-700">8.28</td>

            <td class="px-3 py-2 border-b border-gray-700">4.32</td>

            <td class="px-3 py-2 border-b border-gray-700">3.11</td>

            <td class="px-3 py-2 border-b border-gray-700">4.74x</td>

            <td class="px-3 py-2 border-b border-gray-700">4.57x</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Picnic-192-3-284</td>

            <td class="px-3 py-2 border-b border-gray-700">80.76</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">58.23 42.85</td>

            <td class="px-3 py-2 border-b border-gray-700">29.94 10.13</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">7.29</td>

            <td class="px-3 py-2 border-b border-gray-700">7.97x</td>

            <td class="px-3 py-2 border-b border-gray-700">7.99x</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">Picnic-256-3-363</td>

            <td class="px-3 py-2 border-b border-gray-700">192.65 139.62 91.77</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">64.45 18.47 12.89 10.43x</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

            <td class="px-3 py-2 border-b border-gray-700">10.83x</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><span id="page-20-2"></span>Table 4. Benchmarks of Picnic-n-s-r using SIMD without optimizations, with ORKC, and OLLE (in ms).</p>

    <p class="text-gray-300">in the context of garbled circuits, the number of AND gates directly relates to the communication overhead. Instances with only 1 Sbox thus minimize the size of communicated data. In terms of encryption time, we observe major improvements of up to a factor of 24.72x when compared to an implementation without any optimizations, and a factor of 15.9x when compared to an implementation using the method of four Russians. Since in this type of implementation we have to operate on a bit level instead of a word or 256-bit register as in Picnic, the large reduction of XORs has a greater effect in this scenario, especially since up to 99% of the runtime of the unoptimized GC protocol is spent evaluating the LowMC encryption circuit.</p>

    <h2 id="sec-25" class="text-2xl font-bold">7 Optimized Sampling of Linear Layers</h2>

    <p class="text-gray-300">In this section we optimize the sampling of linear layers of generalized LowMC ciphers, assuming they are chosen uniformly at random from the set of all invert-</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Parameters</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">w/o opt.</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">with M4RM</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">with OLLE</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Improv. (old / new)</th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">LowMC-128-3-287</td>

            <td class="px-3 py-2 border-b border-gray-700">8.46</td>

            <td class="px-3 py-2 border-b border-gray-700">8.01</td>

            <td class="px-3 py-2 border-b border-gray-700">0.69</td>

            <td class="px-3 py-2 border-b border-gray-700">12.26x</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">LowMC-192-3-413</td>

            <td class="px-3 py-2 border-b border-gray-700">25.26</td>

            <td class="px-3 py-2 border-b border-gray-700">20.59</td>

            <td class="px-3 py-2 border-b border-gray-700">1.54</td>

            <td class="px-3 py-2 border-b border-gray-700"></td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">LowMC-256-3-537</td>

            <td class="px-3 py-2 border-b border-gray-700">66.50</td>

            <td class="px-3 py-2 border-b border-gray-700">40.88</td>

            <td class="px-3 py-2 border-b border-gray-700">2.69</td>

            <td class="px-3 py-2 border-b border-gray-700">24.72x</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><span id="page-21-0"></span><strong>Table 5.</strong> Benchmarks of LowMC-<em>n-s-r</em> instances with standard linear layer using method of four Russians (M4RM) and OLLE (in seconds for 2<sup>10</sup> circuit evaluations).</p>

    <p class="text-gray-300">ible matrices. Sampling the linear layers required by Algorithm 6 in a straightforward manner involves selecting r invertible matrices and applying additional linear algebra operations that transform them to normalized form. This increases the complexity compared to merely sampling these r matrices in complexity  <span class="math">O(r \\cdot n^3)</span>  using a simple rejection sampling algorithm (or asymptotically faster using the algorithm of [Ran93]) and encrypting with Algorithm 3.</p>

    <p class="text-gray-300">We show how to reduce the complexity from  <span class="math">O(r \\cdot n^3)</span>  to<sup>14</sup></p>

    <div class="my-4 text-center"><span class="math-block">O(n^3 + (r-1)(s^2 \\cdot n)).</span></div>

    <p class="text-gray-300">We also reduce the amount of (pseudo) random bits requires to sample the linear layers from about  <span class="math">r \\cdot n^2</span>  to about  <span class="math">r \\cdot n^2 - (r-1) \\left( (n-s)^2 - 2(n-s) \\right)</span> . We note that similar (yet simpler) optimizations can be applied to sampling the key schedule matrices of the cipher (in case it is linear and its matrices are selected at random, as considered in Section 3.2).</p>

    <p class="text-gray-300">The linear layer sampling complexity is reduced in three stages. The first stage breaks the dependency between matrices of different rounds. The second stage breaks the dependency in sampling the bottom part of each round matrix (containing n-s rows) from its top part. Finally, the substantial improvement in complexity for small s is obtained in the third stage that optimizes the sampling of the bottom part of the round matrices. Although the first two stages do not significantly reduce the complexity, they are necessary for applying the third stage and are interesting in their own right.</p>

    <h2 id="sec-26" class="text-2xl font-bold">7.1 Breaking Dependencies Among Different Round Matrices</h2>

    <p class="text-gray-300">Recall that for  <span class="math">i \\in \\{2, ..., r\\}</span> , the linear transformation of round i is generated from the matrix</p>

    <p class="text-gray-300"><span id="page-21-2"></span></p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$\\left[\\frac{L_i^{00}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\check{T}_i^{01}}{L_i^{10}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\check{T}_i^{11}}\\right]$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">(5)</p>

    <p class="text-gray-300">where</p>

    <div class="my-4 text-center"><span class="math-block">\\check{T}_i = L_i^{*1} \\cdot \\dot{R}_{i-1}.</span></div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">For i=r, this gives the final linear transformation, while for i< r, the final transformation involves applying the decomposition of Definition 1 to  $L_i^{10} \\</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\check{T}_i^{11}<span class="math"> . Since  </span>\\check{T}_i<span class="math">  depends on the invertible  </span>(n-s) \\times (n-s)<span class="math">  matrix  </span>\\dot{R}_{i-1}$  (computed in the</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><span id="page-21-1"></span><sup>14</sup> Further asymptotic improvements are possible using fast matrix multiplication.</p>

    <p class="text-gray-300">previous round), a naive linear transformation sampling algorithm would involve computing the linear transformations in their natural order by computing  <span class="math">\\dot{R}_{i-1}</span>  in round i-1 and using it in round i. However, this is not required, as the linear transformation of each round can be sampled independently. Indeed, by using Proposition 1 with the invertible matrix  <span class="math">B^{11} = \\dot{R}_{i-1}</span> , we conclude that in round i we can simply sample the matrix given in (5) as a uniform invertible  <span class="math">n \\times n</span>  matrix without ever computing  <span class="math">\\dot{R}_{i-1}</span> . Therefore, the linear transformation sampling for round r simplifies to selecting a uniform invertible  <span class="math">n \\times n</span>  matrix,  <span class="math">L_r</span> . For rounds  <span class="math">i \\in \\{1, \\ldots, r-1\\}</span> , we can select a uniform invertible  <span class="math">n \\times n</span>  matrix,  <span class="math">L_i</span> , and then normalize it and discard  <span class="math">\\dot{R}_i</span>  after the process. This simplifies Algorithm 6, and it can be rewritten as in Algorithm 7. Note that we have renamed the sequence  <span class="math">\\{z_i^{(1)}\\}</span>  to  <span class="math">\\{x_i^{(1)}\\}</span>  for convenience.</p>

    <p class="text-gray-300">We stress that the dependency between the round matrices could be broken in Algorithm 7 only since the linear transformation in each round is a uniform invertible matrix. If this is not the case, one can still rename the matrices of Algorithm 6 and derive an algorithm of the form of Algorithm 7. However, computing these matrices would still require deriving  <span class="math">\\check{T}_i</span>  and  <span class="math">\\hat{R}_i</span>  as defined in Section 5.4.</p>

    <pre><code class="language-text">\\begin{array}{l} \\text{Input} &amp; : x_0 \\\\ \\text{Output} : x_{r+1} \\\\ \\text{begin} \\\\ &amp; \\left| \\begin{array}{l} x_1 \\leftarrow x_0 + k_0 \\\\ \\text{for } i \\in \\{1, \\dots, r-1\\} \\text{ do} \\\\ &amp; \\left| \\begin{array}{l} y_i \\leftarrow S_i(x_i^{(0)}) \\| x_i^{(1)} \\\\ &amp; x_{i+1} \\leftarrow L_i^{(0*)}(y_i) \\| \\hat{L}_i(y_i) \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} x_{r+1} \\leftarrow L_i^{(0*)}(y_i) \\| \\hat{L}_i(y_i) \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} x_{r+1} \\leftarrow L_r(y_r) \\\\ x_{r+1} \\leftarrow L_r(y_r) \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ \\\\ \\\\ &amp; \\left| \\begin{array}{l} \\text{Found } r \\\\ \\end{array} \\right. \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\</code></pre>

    <p class="text-gray-300"><strong>Algorithm 7:</strong> Simplified and optimized <em>r</em>-round encryption.</p>

    <h4 id="sec-27" class="text-lg font-semibold mt-6">7.2 Reduced Sampling Space</h4>

    <p class="text-gray-300">We examine the sample space of the linear layers more carefully.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">For each of the first r-1 rounds, the sampling procedure for Algorithm 7 involves selecting a uniform invertible matrix and then normalizing it according to Definition 1. However, by Corollary 1, since each normalized equivalence class contains the same number of  <span class="math">\\alpha_{n-s} \\cdot 2^{(n-s)^2}</span>  invertible matrices, this is equivalent to directly sampling a uniform member from  <span class="math">\\Phi</span>  to represent its normalized equivalence class. If we order all the matrices in  <span class="math">\\Phi</span> , then sampling from it can be done using  $\\log</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\Phi</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$  uniform bits. However, encrypting with Algorithm 7 requires</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">an explicit representation of the matrices and using an arbitrary ordering is not efficient in terms of complexity. In the rest of this section, our goal is to optimize the complexity of sampling from Φ, but first we introduce notation for the full sampling space.</p>

    <p class="text-gray-300">Let the set Λ<sup>r</sup> contain r-tuples of matrices defined as</p>

    <p class="text-gray-300"><span id="page-23-0"></span> <span class="math-block">\\Lambda_r = \\Phi^{r-1} \\times \\{ A \\in \\mathrm{GF}(2)^{n \\times n} \\text{ is invertible} \\},</span></p>

    <p class="text-gray-300">where <span class="math-block">\\Phi^{r-1} = \\underbrace{\\Phi \\times \\Phi \\dots \\times \\Phi}_{r-1 \\text{ times}}</span> .</p>

    <p class="text-gray-300">The following corollary is a direct continuation of Corollary <a href="#page-12-2">1.</a></p>

    <p class="text-gray-300">Corollary 2. The following properties hold:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Each r-tuple (L1, . . . , Lr−1, Lr) ∈ Λ<sup>r</sup> represents a set of size (αn−s) r−1 · 2 (r−1)(n−s) containing r-tuples of matrices (L 0 1 , . . . , L<sup>0</sup> r−1 , L<sup>0</sup> r ) such that</li>

    </ol>

    <div class="my-4 text-center"><span class="math-block">(N(L&#x27;_1), \\dots, N(L&#x27;_{r-1}), L&#x27;_r) = (L_1, \\dots, L_{r-1}, L_r).</span></div>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Λ<sup>r</sup> contains</li>

    </ol>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A_r</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">= \\frac{(\\alpha_n)^r \\cdot 2^{n^2}}{(\\alpha_{n-s})^{r-1} \\cdot 2^{(r-1)(n-s)^2}} = (\\alpha_n)^r / (\\alpha_{n-s})^{r-1} \\cdot 2^{r \\cdot n^2 - (r-1)(n-s)^2}$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">r-tuples of matrices.</p>

    <p class="text-gray-300">As noted above, sampling from Λ<sup>r</sup> reduces to sampling the first r−1 matrices uniformly from Φ and using a standard sampling algorithm for the r'th matrix.</p>

    <h3 id="sec-28" class="text-xl font-semibold mt-8">7.3 Breaking Dependencies Between Round Sub-Matrices</h3>

    <p class="text-gray-300">We describe how to further simplify the algorithm for sampling the linear layers by breaking the dependency between sampling the bottom and top sub-matrices in each round. From this point, we will rename the round matrix L<sup>i</sup> to a general matrix A ∈ GF(2)n×<sup>n</sup> for convenience. In order to sample from Φ, the main idea is to sample the bottom n − s linearly independent rows of A first, apply the decomposition of Definition <a href="#page-11-0">1</a> and then use this decomposition in order to efficiently sample the remaining s linearly independent rows of A. Therefore, we never directly sample the larger n × n matrix, but obtain the same distribution on output matrices as the original sampling algorithm.</p>

    <p class="text-gray-300">Sampling the Bottom Sub-Matrix We begin by describing in Algorithm <a href="#page-24-0">8</a> how to sample and compute Bˆ (which will be placed in the bottom n − s rows of A) and COL(B<sup>1</sup><sup>∗</sup> ) using simple rejection sampling. It uses the sub-procedure GenRand(n1, n2) that samples an n<sup>1</sup> × n<sup>2</sup> binary matrix uniformly at random.</p>

    <p class="text-gray-300">Correctness of the algorithm follows by construction. In terms of complexity, we keep track of the span of B˙ using simple Gaussian elimination. Based on Fact <a href="#page-11-1">1,</a> the expected complexity of (a naive implementation of) the algorithm until it succeeds is O((n − s) <sup>3</sup> + s 2 (n − s)) due to Gaussian elimination and matrix multiplication.</p>

    <pre><code class="language-text">Output: Round matrix for
  Output: \\hat{B}, COL(B^{1*})
                                                                                       Algorithm 7
  begin
                                                                       begin
       B^{1*} \\leftarrow \\mathbf{0}^{(n-s)\\times n}, \\dot{B} \\leftarrow \\mathbf{0}^{(n-s)\\times (n-s)}
                                                                             \\hat{B}, COL(B^{1*}) \\leftarrow
       COL(B^{1*}) \\leftarrow \\emptyset, rank \\leftarrow 0
                                                                              SampleBottom()
       for i \\in \\{n, n-1, ..., 1\\} do
                                                                             A^{1*} \\leftarrow \\hat{B}
             B^{1*}[*,i] \\leftarrow GenRand(n-s,1)
                                                                             C^{00} \\leftarrow GenInv(s)
             if rank = n - s or
                                                                             A&#x27;^{01} \\leftarrow GenRand(s, n-s)
               B^{1*}[*,i] \\in \\operatorname{span}(\\dot{B}) then
                                                                             D^{10} \\leftarrow (\\hat{B} \\cdot P)^{10}
              continue
                                                                             A&#x27;^{00} \\leftarrow C^{00} + A&#x27;^{01} \\cdot D^{10}
                                                                             A^{0*} \\leftarrow (A&#x27;^{00} || A&#x27;^{01}) \\cdot P^{-1}
             rank \\leftarrow rank + 1
             COL(B^{1*}) \\leftarrow COL(B^{1*}) \\cup \\{i\\}
                                                                             \\mathbf{return}\\ A
                                                                       end
            \\dot{B}[*, rank] \\leftarrow B^{1*}[*, i]
                                                                     Algorithm
                                                                                             9:
                                                                                                    Optimized
                                                                    round matrix sampling.
       if rank = n - s then
             \\hat{B} \\leftarrow (\\dot{B})^{-1} \\cdot B^{1*}
             return \\hat{B}, COL(B^{1*})
       else
        ∣ return FAIL
       end
Algorithm 8: SampleBottom() itera-</code></pre>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">The Optimized Round Matrix Sampling Algorithm Let us first assume that after application of Algorithm 8, we obtain  <span class="math">\\hat{B}</span> ,  <span class="math">COL(B^{1<em>})</span>  such that  <span class="math">COL(B^{1</em>})</span>  includes the n-s last columns (which form the identity matrix in  <span class="math">\\hat{B}</span> ). The matrix A is built by placing  <span class="math">\\hat{B}</span>  in its bottom n-s columns, and in this case it will be of the block form considered in Fact 2. There is a simple formula (stated in Fact 2) that determines if such matrices are invertible, and we can use this formula to efficiently sample the top s rows of A, while making sure that the full  <span class="math">n \\times n</span>  matrix is invertible. In case  <span class="math">COL(B^{1<em>})</span>  does not include the n-s last columns, then a similar idea still applies since A would be in the special form after applying a column permutation determined by  <span class="math">COL(B^{1</em>})</span> . Therefore, we assume that A is of the special form, sample the top s rows accordingly and then apply the inverse column permutation to these rows. Algorithm 9 gives the details of this process. It uses a column permutation matrix, denoted by P (computed from  <span class="math">COL(B^{1*})</span> , such that  $\\hat{B} \\cdot P = ((\\dot{B})^{-1} \\cdot \\ddot{B})</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">I_{n-s}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$  is of the required form. The algorithm also uses two sub-procedures:</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>1.  <span class="math">GenRand(n_1, n_2)</span>  samples an  <span class="math">n_1 \\times n_2</span>  binary matrix uniformly at random.</li>

      <li>2.  <span class="math">GenInv(n_1)</span>  samples a uniform invertible  <span class="math">n_1 \\times n_1</span>  matrix.</li>

    </ul>

    <p class="text-gray-300"><span id="page-24-2"></span>The complexity of the algorithm is  <span class="math">O((n-s)^3+s^2(n-s)+s^3+s^2(n-s)+sn) = O((n-s)^3+s^2(n-s)+s^3)</span>  (using naive matrix multiplication and invertible matrix sampling algorithms), where the dominant factor for small s is  <span class="math">(n-s)^3</span> . The algorithm requires about  <span class="math">sn+n(n-s)=n^2</span>  random bits.</p>

    <p class="text-gray-300"><strong>Proposition 7.</strong> Algorithm 9 selects a uniform matrix in  <span class="math">\\Phi</span> , namely, the distribution of the output A is identical to the distribution generated by sampling a uniform invertible  <span class="math">n \\times n</span>  matrix and applying the transformation of Definition 1 to its bottom n-s rows.</p>

    <p class="text-gray-300">For the proof of Proposition 7 we refer the reader to Appendix D.</p>

    <h2 id="sec-29" class="text-2xl font-bold">7.4 Optimized Sampling of the Bottom Sub-Matrix</h2>

    <p class="text-gray-300">For small values of s, the complexity of Algorithm 9 is dominated by Algorithm 8 (SampleBottom()), whose complexity is  <span class="math">O((n-s)^3 + s^2(n-s))</span> . We now show how to reduce this complexity to O(s(n-s)) on average. Thus, the total expected complexity of Algorithm 9 becomes</p>

    <div class="my-4 text-center"><span class="math-block">O(s^{2}(n-s) + s^{3}) = O(s^{2} \\cdot n)</span></div>

    <p class="text-gray-300">(using naive matrix multiplication and invertible matrix sampling algorithms). Moreover, the randomness required by the algorithm is reduced from about  <span class="math">sn+n(n-s)=n^2</span>  to about</p>

    <div class="my-4 text-center"><span class="math-block">sn + (s+2)(n-s) = n^2 - (n-s)^2 + 2(n-s).</span></div>

    <p class="text-gray-300">Below, we give an overview of the algorithm. Its formal description and analysis are given in Appendix E.</p>

    <p class="text-gray-300">Recall that the output of SampleBottom() consists of  <span class="math">\\hat{B}</span> ,  <span class="math">COL(B^{1<em>})</span> , where  <span class="math">\\hat{B}</span>  contains  <span class="math">I_{n-s}</span>  and s additional columns of n-s bits. The main idea is to directly sample  <span class="math">\\hat{B}</span>  without ever sampling the full  <span class="math">B^{1</em>}</span>  and normalizing it. In order to achieve this, we have to artificially determine the column set  <span class="math">COL(B^{1*})</span>  (which contains the identity matrix in  <span class="math">\\hat{B}</span> ), and the values of the remaining s columns.</p>

    <p class="text-gray-300">Remark 6. In general, the distribution of  <span class="math">\\hat{B}</span>  in some alternative SampleBottom() implementation does not have to be identical to the one of Algorithm 8, as we can select  <span class="math">COL(B^{1<em>})</span>  in a different way (i.e., using a different method to enumerate the columns). The important requirement is that under any enumeration,  <span class="math">\\dot{B} \\cdot \\hat{B} = B^{1</em>}</span>  should be a uniform matrix of full row rank. Consider the following trivial optimization attempt of SampleBottom(): sample  <span class="math">COL(B^{1*})</span>  uniformly at random among all column sets of n-s indices (and then sample the remaining columns of  <span class="math">\\hat{B}</span>  uniformly). This algorithm does not satisfy the requirement, as the distribution of  <span class="math">\\dot{B} \\cdot \\dot{B}</span>  for  <span class="math">\\dot{B}</span>  sampled with this algorithm gives more weight to any matrix with many sets of n-s linearly independent columns over any matrix with fewer such sets.</p>

    <p class="text-gray-300">The optimized algorithm simulates SampleBottom() (Algorithm 8). This is performed by maintaining and updating the  <span class="math">COL(B^{1<em>})</span>  and rank variables as in SampleBottom() and sampling concrete vectors only when necessary. For example, the columns of  <span class="math">COL(B^{1</em>})</span>  are not sampled at all and will simply consist of the identity matrix in the output of the algorithm. There are 3 important cases to simulate in the optimized algorithm when considering column i:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>1. In SampleBottom(), full rank is not reached (i.e., rank < n − s) and column i is added to COL(B1<sup>∗</sup> ). Equivalently, the currently sampled vector in SampleBottom() is not in the subspace spanned by the previously sampled vectors (whose size is 2rank). This occurs with probability 1 − 2 rank/2 <sup>n</sup>−<sup>s</sup> = 1 − 2 (n−s)−rank and can be simulated exactly by (at most) (n − s) − rank coin tosses in the optimized algorithm (without sampling any vector).</li>

      <li>2. In SampleBottom(), full rank is not reached (i.e., rank < n − s) and column i is not added to COL(B1<sup>∗</sup> ). This is the complementary event to the first, which occurs with probability 2(n−s)−rank. In SampleBottom(), such a column i is sampled uniformly from the subspace spanned by the previously sampled vectors whose size is 2rank. The final multiplication with (B˙) −1 is a change of basis which transforms the basis of the previously sampled columns to the last rank vectors in the standard basis e(n−s)−rank+1, e(n−s)−rank+2, . . . , en−s. Hence, column i is a uniform vector in the subspace spanned by e(n−s)−rank+1, e(n−s)−rank+2, . . . , en−<sup>s</sup> and the optimized algorithm samples a vector from this space (using rank coin tosses).</li>

      <li>3. In SampleBottom(), full rank is reached (i.e., rank = n−s). The optimized algorithm samples a uniform column using n − s coin tosses. This can be viewed as a special case of the previously considered one, for rank = n − s.</li>

    </ul>

    <p class="text-gray-300">Note that no linear algebra operations are performed by the optimized algorithm and it consists mainly of sampling operations.</p>

    <p class="text-gray-300">Decryption We conclude this section by considering efficient sampling of linear layers for decryption. The inverse of the round encryption matrix is of the form shown in Fact <a href="#page-11-4">2</a> after a row permutation (which is the inverse of a column permutation induced by COL(B1<sup>∗</sup> )). This inverse is generated as a byproduct of Algorithm <a href="#page-24-1">9</a> above for sampling the encryption matrix (which uses the optimized sampling algorithm). Furthermore, matrix-vector product with the inverse matrix (during decryption) can be computed in about n <sup>2</sup> − (n − s) <sup>2</sup> bit operations, hence decryption can be performed in about the same complexity as encryption.</p>

    <h2 id="sec-30" class="text-2xl font-bold">8 Optimality of Linear Representation</h2>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">In this section, we prove that the representation of the linear layers used by Algorithm <a href="#page-22-0">7</a> for a GLMC cipher is essentially optimal. Furthermore, we show that the number of uniform (pseudo) random bits used by the sampling algorithm derived in Section <a href="#page-20-0">7</a> is close to optimal. More specifically, we formulate two assumptions and prove the following theorem under these assumptions, recalling the value of</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Λr</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">from Corollary <a href="#page-23-0">2.</a></th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Theorem 1. Sampling an instance of a GLMC cipher with uniform linear layers must use at least</p>

    <p class="text-gray-300"><span id="page-26-1"></span></p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$b = \\log</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\Lambda_r</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">= \\log \\left( (\\alpha_n)^r / (\\alpha_{n-s})^{r-1} \\cdot 2^{r \\cdot n^2 - (r-1)(n-s)^2} \\right) \\ge r \\cdot n^2 - (r-1)(n-s)^2 - 3.5r.$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">uniform random bits and its encryption (or decryption) algorithm requires at least b bits of storage on average. Moreover, if a secure PRG is used to generate the randomness for sampling, then it must produce at least b pseudo-random bits and the encryption (and decryption) process requires at least b bits of storage on average, assuming that it does not have access to the PRG.</p>

    <p class="text-gray-300">We mention that the theorem does not account for the storage required by the non-linear layers. The theorem implies that the code size of Algorithm 7 is optimal up to an additive factor of about  <span class="math">r \\cdot (3.5 + 3 \\log n)</span> , which is negligible (less than  <span class="math">0.01 \\cdot b</span>  for reasonable choices of parameters).</p>

    <h2 id="sec-31" class="text-2xl font-bold">8.1 Basic Assumptions</h2>

    <p class="text-gray-300">The proof relies on the following two assumptions regarding a GLMC cipher, which are further discussed in Appendix F.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>1. If a PRG is used for the sampling process, it is not used during encryption.</li>

      <li>2. The linear layers are stored in a manner which is independent of the specification of the non-linear layers. Namely, changing the specification of the non-linear layers does not affect the way that the linear layers are stored.</li>

    </ul>

    <h4 id="sec-32" class="text-lg font-semibold mt-6">8.2 Model Formalization</h4>

    <p class="text-gray-300">We now define our model which formalizes the assumptions above and allows to prove the optimality of our representation.</p>

    <p class="text-gray-300"><strong>Definition 3.</strong> Given a triplet of global parameters (n, s, r), a (simplified) standard representation of a GLMC cipher is a triplet  <span class="math">\\mathcal{R} = (k_0, \\mathcal{S}, \\mathcal{L})</span>  such that  <span class="math">k_0 \\in \\{0, 1\\}^n</span> ,  <span class="math">\\mathcal{S} = (S_1, S_2, \\dots, S_r)</span>  is an r-tuple containing the specifications of r non-linear invertible layers  <span class="math">S_i : \\{0, 1\\}^s \\to \\{0, 1\\}^s</span>  and  <span class="math">\\mathcal{L} = (L_1, L_2, \\dots, L_r)</span>  is an r-tuple of invertible matrices  <span class="math">L_i \\in \\mathrm{GF}(2)^{n \\times n}</span> . The r-tuple  <span class="math">\\mathcal{L}</span>  is called a standard linear representation.</p>

    <p class="text-gray-300">To simplify notation, given a standard representation  <span class="math">\\mathcal{R} = (k_0, \\mathcal{S}, \\mathcal{L})</span> , we denote the encryption algorithm defined by Algorithm 3 as  <span class="math">E_{\\mathcal{R}} : \\{0, 1\\}^n \\to \\{0, 1\\}^n</span> .</p>

    <p class="text-gray-300"><strong>Definition 4.</strong> Two standard cipher representations  <span class="math">\\mathcal{R}, \\mathcal{R}&#x27;</span>  are equivalent (denoted  <span class="math">\\mathcal{R} \\equiv \\mathcal{R}&#x27;</span> ) if for each  <span class="math">x \\in \\{0,1\\}^n</span> ,  <span class="math">E_{\\mathcal{R}}(x) = E_{\\mathcal{R}&#x27;}(x)</span> .</p>

    <p class="text-gray-300"><span id="page-27-1"></span><strong>Definition 5.</strong> Two standard linear representations  <span class="math">\\mathcal{L}, \\mathcal{L}&#x27;</span>  are equivalent (denoted  <span class="math">\\mathcal{L} \\equiv \\mathcal{L}&#x27;</span> ) if for each tuple of non-linear layers  <span class="math">\\mathcal{S}</span> , and key  <span class="math">k_0</span> ,  <span class="math">(k_0, \\mathcal{S}, \\mathcal{L}) \\equiv (k_0, \\mathcal{S}, \\mathcal{L}&#x27;)</span> .</p>

    <p class="text-gray-300">The requirement that  <span class="math">(k_0, \\mathcal{S}, \\mathcal{L}) \\equiv (k_0, \\mathcal{S}, \\mathcal{L}&#x27;)</span>  for any  <span class="math">\\mathcal{S}, k_0</span>  captures the second assumption of Section 8.1 that a standard representation of the linear layers is independent of the non-linear layers (and the key).</p>

    <p class="text-gray-300">Clearly, the linear equivalence relation partitions the r-tuples of standard linear representations into linear equivalence classes. It is important to mention that Theorem 1 does not assume that the encryption algorithm uses Algorithm 3 or represents the linear layers as an r-tuple of matrices. These definitions are merely used in its proof, as shown next.</p>

    <h4 id="sec-33" class="text-lg font-semibold mt-6">8.3 Proof of Theorem 1</h4>

    <p class="text-gray-300">We will prove the following lemma regarding linear equivalence classes, from which Theorem 1 is easily derived.</p>

    <p class="text-gray-300"><span id="page-28-1"></span><strong>Lemma 1.</strong> For any  <span class="math">\\mathcal{L} \\neq \\mathcal{L}&#x27; \\in \\Lambda_r</span> ,  <span class="math">\\mathcal{L} \\not\\equiv \\mathcal{L}&#x27;</span> .</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">The lemma states that each r-tuple of  <span class="math">\\Lambda_r</span>  is a member of a distinct equivalence class, implying that we have precisely identified the equivalence classes. Proof (of Theorem 1). Lemma 1 asserts that there are at least  $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\Lambda_r</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math">  linear equivalence classes. Corollary 2 asserts that each r-tuple in  </span>\\Lambda_r<span class="math">  represents a set of linear layers of size  </span>(\\alpha_{n-s})^{r-1} \\cdot 2^{(r-1)(n-s)^2}<span class="math"> , hence every r-tuple in  </span>\\Lambda_r<span class="math">  has the same probability weight when sampling the r linear layers uniformly at random. The theorem follows from the well-known information theoretic fact that sampling and representing a uniform string (an r-tuple in  </span>\\Lambda_r<span class="math"> ) chosen out of a set of  </span>2^t$  strings requires at least t bits on average (regardless of any specific sampling or representation methods).</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">The proof of Lemma 1 relies on two propositions which are implications of the definition of equivalence of standard linear representations (Definition 5).</p>

    <p class="text-gray-300"><span id="page-28-2"></span><strong>Proposition 8.</strong> Let  <span class="math">\\mathcal{L} \\equiv \\mathcal{L}&#x27;</span>  be two equivalent standard linear representations. Given  <span class="math">k_0, \\mathcal{S}</span> , let  <span class="math">\\mathcal{R} = (k_0, \\mathcal{S}, \\mathcal{L})</span>  and  <span class="math">\\mathcal{R}&#x27; = (k_0, \\mathcal{S}, \\mathcal{L}&#x27;)</span> . Fix any  <span class="math">x \\in \\{0, 1\\}^n</span>  and  <span class="math">i \\in \\{0, 1, \\ldots, r+1\\}</span> , and denote by  <span class="math">x_i</span>  (resp.  <span class="math">x_i&#x27;</span> ) the value  <span class="math">E_{\\mathcal{R}}(x)</span>  (resp.  <span class="math">E_{\\mathcal{R}&#x27;}(x)</span> ) at the beginning of round i. Then  <span class="math">x_i^{(0)} = x_i&#x27;^{(0)}</span> .</p>

    <p class="text-gray-300">Namely, non-linear layer inputs (and outputs) have to match at each round when encrypting the same plaintext with ciphers instantiated with equivalent standard linear representations (and use the same key and non-linear layers).</p>

    <p class="text-gray-300"><span id="page-28-3"></span><strong>Proposition 9.</strong> Let  <span class="math">\\mathcal{L} \\equiv \\mathcal{L}&#x27;</span>  be two equivalent standard linear representations. Given  <span class="math">k_0, \\mathcal{S}</span> , let  <span class="math">\\mathcal{R} = (k_0, \\mathcal{S}, \\mathcal{L})</span>  and  <span class="math">\\mathcal{R}&#x27; = (k_0, \\mathcal{S}, \\mathcal{L}&#x27;)</span> . Fix any  <span class="math">x \\in \\{0, 1\\}^n</span>  and  <span class="math">i \\in \\{0, 1, \\ldots, r+1\\}</span> , and denote by  <span class="math">x_i</span>  (resp.  <span class="math">x_i&#x27;</span> ) the value  <span class="math">E_{\\mathcal{R}}(x)</span>  (resp.  <span class="math">E_{\\mathcal{R}&#x27;}(x)</span> ) at the beginning of round i. Moreover, fix  <span class="math">\\bar{x} \\neq x</span>  such that  <span class="math">\\bar{x}_i = \\bar{x}_i^{(0)}, \\bar{x}_i^{(1)}</span> , where  <span class="math">\\bar{x}_i^{(0)} \\neq x_i^{(0)}</span> , but  <span class="math">\\bar{x}_i^{(1)} = x_i^{(1)}</span> . Then,  <span class="math">\\bar{x}_i&#x27;^{(1)} = x_i&#x27;^{(1)}</span> .</p>

    <p class="text-gray-300">The proposition considers two plaintexts x and  <span class="math">\\bar{x}</span>  whose encryptions under the first cipher in round i differ only in the 0 part of the state. We then look at the second cipher (formed using equivalent standard linear representations) and claim that the same property must hold for it as well. Namely, the encryptions of x and  <span class="math">\\bar{x}</span>  under the second cipher in round i differ only on the 0 part of the state. For the proofs of Propositions 8 and 9 and Lemma 1, we refer the reader to Appendix G.</p>

    <h4 id="sec-34" class="text-lg font-semibold mt-6">9 Conclusions</h4>

    <p class="text-gray-300">SP-networks with partial non-linear layers (i.e., s < n) have shown to be beneficial in several applications that require minimizing the AND count of the</p>

    <p class="text-gray-300">cipher. Initial cryptanalytic results analyzing ciphers built with this recent design strategy contributed to our understanding of their security. In this paper, we contribute to the efficient implementation of these SP-networks. In particular, we redesign the linear layers of LowMC instances with s < n in a way that does not change their specifications, but significantly improves their performance. We believe that our work will enable designing even more efficient SP-networks with s < n by using our optimizations as a starting point, allowing to use this design strategy in new applications.</p>

    <p class="text-gray-300">Acknowledgements We thank Tyge Tiessen for interesting ideas and discussions on optimizing LowMC's round key computation. I. Dinur has been supported by the Israeli Science Foundation through grant n◦573/16 and by the European Research Council under the ERC starting grant agreement n◦757731 (LightCrypt). D. Kales has been supported by IOV42. S. Ramacher, and C. Rechberger have been supported by EU H2020 project Prismacloud, grant agreement n◦644962. S. Ramacher has additionally been supported by A-SIT. C. Rechberger has additionally been supported by EU H2020 project PQCRYPTO, grant agreement n◦645622.</p>

    <h2 id="sec-35" class="text-2xl font-bold">References</h2>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span id="page-29-4"></span>ABH10. Martin R. Albrecht, Gregory V. Bard, and William Hart. Algorithm 898: Efficient multiplication of dense matrices over GF(2). ACM Trans. Math. Softw., 37(1):9:1–9:14, 2010.</li>

      <li><span id="page-29-0"></span>ARS<sup>+</sup>15. Martin R. Albrecht, Christian Rechberger, Thomas Schneider, Tyge Tiessen, and Michael Zohner. Ciphers for MPC and FHE. In EURO-CRYPT (1), volume 9056 of LNCS, pages 430–454. Springer, 2015.</li>

      <li><span id="page-29-6"></span>BB02. Elad Barkan and Eli Biham. In how many ways can you write rijndael? In ASIACRYPT, volume 2501 of LNCS, pages 160–175. Springer, 2002.</li>

      <li><span id="page-29-7"></span>BCBP03. Alex Biryukov, Christophe De Canni\`ere, An Braeken, and Bart Preneel. A toolbox for cryptanalysis: Linear and affine equivalence algorithms. In EUROCRYPT, volume 2656 of LNCS, pages 33–50. Springer, 2003.</li>

      <li><span id="page-29-5"></span>BDD<sup>+</sup>15. Achiya Bar-On, Itai Dinur, Orr Dunkelman, Virginie Lallemand, Nathan Keller, and Boaz Tsaban. Cryptanalysis of SP networks with partial nonlinear layers. In EUROCRYPT (1), volume 9056 of LNCS, pages 315–342. Springer, 2015.</li>

      <li><span id="page-29-1"></span>BEF18. Dan Boneh, Saba Eskandarian, and Ben Fisch. Post-quantum group signatures from symmetric primitives. IACR ePrint, 2018:261, 2018.</li>

      <li><span id="page-29-2"></span>CDG<sup>+</sup>17a. Melissa Chase, David Derler, Steven Goldfeder, Claudio Orlandi, Sebastian Ramacher, Christian Rechberger, Daniel Slamanig, and Greg Zaverucha. Post-quantum zero-knowledge and signatures from symmetric-key primitives. In CCS, pages 1825–1842. ACM, 2017.</li>

      <li><span id="page-29-3"></span>CDG<sup>+</sup>17b. Melissa Chase, David Derler, Steven Goldfeder, Claudio Orlandi, Sebastian Ramacher, Christian Rechberger, Daniel Slamanig, and Greg Zaverucha. The Picnic Signature Algorithm Specification, 2017. <a href="https://github.com/Microsoft/Picnic/blob/master/spec.pdf">https://github.com/</a> <a href="https://github.com/Microsoft/Picnic/blob/master/spec.pdf">Microsoft/Picnic/blob/master/spec.pdf</a>.</li>

    </ul>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span id="page-30-5"></span>DEG<sup>+</sup>18. Christoph Dobraunig, Maria Eichlseder, Lorenzo Grassi, Virginie Lallemand, Gregor Leander, Eik List, Florian Mendel, and Christian Rechberger. Rasta: A cipher with low anddepth and few ands per bit. In CRYPTO (1), volume 10991 of LNCS, pages 662–692. Springer, 2018.</li>

      <li><span id="page-30-2"></span>DRS18a. David Derler, Sebastian Ramacher, and Daniel Slamanig. Generic doubleauthentication preventing signatures and a post-quantum instantiation. In ProvSec, volume 11192 of LNCS, pages 258–276. Springer, 2018.</li>

      <li><span id="page-30-0"></span>DRS18b. David Derler, Sebastian Ramacher, and Daniel Slamanig. Post-quantum zero-knowledge proofs for accumulators with applications to ring signatures from symmetric-key primitives. In PQCrypto, volume 10786 of LNCS, pages 419–440. Springer, 2018.</li>

      <li><span id="page-30-14"></span>FIPR05. Michael J. Freedman, Yuval Ishai, Benny Pinkas, and Omer Reingold. Keyword search and oblivious pseudorandom functions. In TCC, volume 3378 of LNCS, pages 303–324. Springer, 2005.</li>

      <li><span id="page-30-12"></span>FS86. Amos Fiat and Adi Shamir. How to prove yourself: Practical solutions to identification and signature problems. In CRYPTO, volume 263 of LNCS, pages 186–194. Springer, 1986.</li>

      <li><span id="page-30-6"></span>GGNS13. Benoˆıt G´erard, Vincent Grosso, Mar´ıa Naya-Plasencia, and Fran¸cois-Xavier Standaert. Block ciphers that are easier to mask: How far can we go? In CHES, volume 8086 of LNCS, pages 383–399. Springer, 2013.</li>

      <li><span id="page-30-11"></span>GMO16. Irene Giacomelli, Jesper Madsen, and Claudio Orlandi. Zkboo: Faster zeroknowledge for boolean circuits. In USENIX Security Symposium, pages 1069–1083. USENIX Association, 2016.</li>

      <li><span id="page-30-4"></span>GMW87. Oded Goldreich, Silvio Micali, and Avi Wigderson. How to play any mental game or A completeness theorem for protocols with honest majority. In STOC, pages 218–229. ACM, 1987.</li>

      <li><span id="page-30-15"></span>HL08. Carmit Hazay and Yehuda Lindell. Efficient protocols for set intersection and pattern matching with security against malicious and covert adversaries. In TCC, volume 4948 of LNCS, pages 155–175. Springer, 2008.</li>

      <li><span id="page-30-10"></span>IKOS07. Yuval Ishai, Eyal Kushilevitz, Rafail Ostrovsky, and Amit Sahai. Zeroknowledge from secure multiparty computation. In STOC, pages 21–30. ACM, 2007.</li>

      <li><span id="page-30-1"></span>KKW18. Jonathan Katz, Vladimir Kolesnikov, and Xiao Wang. Improved noninteractive zero knowledge with applications to post-quantum signatures. In CCS, pages 525–537. ACM, 2018.</li>

      <li><span id="page-30-8"></span>Kol99. Valentin F. Kolchin. Random Graphs. Cambridge Univ. Press, 1999.</li>

      <li><span id="page-30-3"></span>KS08. Vladimir Kolesnikov and Thomas Schneider. Improved garbled circuit: Free XOR gates and applications. In ICALP (2), volume 5126 of LNCS, pages 486–498. Springer, 2008.</li>

      <li><span id="page-30-16"></span>LTW13. Sven Laur, Riivo Talviste, and Jan Willemson. From oblivious AES to efficient and secure database join in the multiparty setting. In ACNS, volume 7954 of LNCS, pages 84–101. Springer, 2013.</li>

      <li><span id="page-30-13"></span>Rab81. Michael Rabin. How to exchange secrets with oblivious transfer. In Technical Report TR-81. Aiken Computation Laboratory: Harvard University, 1981.</li>

      <li><span id="page-30-9"></span>Ran93. Dana Randall. Efficient generation of random nonsingular matrices. Random Struct. Algorithms, 4(1):111–118, 1993.</li>

      <li><span id="page-30-7"></span>RASA14. Shahram Rasoolzadeh, Zahra Ahmadian, Mahmood Salmasizadeh, and Mohammad Reza Aref. Total Break of Zorro using Linear and Differential Attacks. ISeCure, The ISC International Journal of Information Security, 6(1):23–34, 2014.</li>

    </ul>

    <p class="text-gray-300"><span id="page-31-3"></span>RST18. Christian Rechberger, Hadi Soleimany, and Tyge Tiessen. Cryptanalysis of low-data instances of full lowmcv2. <em>IACR Trans. Symmetric Cryptol.</em>, 2018(3):163–181, Sep. 2018.</p>

    <p class="text-gray-300"><span id="page-31-4"></span>Unr<br/>15. Dominique Unruh. Non-interactive zero-knowledge proofs in the quantum random oracle model. In EUROCRYPT (2), volume 9057 of LNCS, pages 755–784. Springer, 2015.</p>

    <p class="text-gray-300"><span id="page-31-1"></span>WWGY14. Yanfeng Wang, Wenling Wu, Zhiyuan Guo, and Xiaoli Yu. Differential cryptanalysis and linear distinguisher of full-round zorro. In ACNS, volume 8479 of LNCS, pages 308–323. Springer, 2014.</p>

    <p class="text-gray-300"><span id="page-31-0"></span>Yao86. Andrew Chi-Chih Yao. How to generate and exchange secrets (extended abstract). In <em>FOCS</em>, pages 162–167. IEEE Computer Society, 1986.</p>

    <h2 id="sec-36" class="text-2xl font-bold">A Proofs from Section 4</h2>

    <p class="text-gray-300">Proof (of Proposition 1). The proof follows from the bijection</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$C = \\begin{bmatrix} \\frac{A^{00}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A^{01} \\cdot B^{11}}{A^{10}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A^{11} \\cdot B^{11}} \\end{bmatrix} = \\begin{bmatrix} \\frac{A^{00}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A^{01}}{A^{10}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A^{11}} \\end{bmatrix} \\cdot \\begin{bmatrix} \\frac{I_s</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathbf{0}^{01}}{\\mathbf{0}^{10}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">B^{11}} \\end{bmatrix}$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">between the uniform matrix A and C. This is indeed a bijection since  <span class="math">B^{11}</span>  is invertible as the inverse is given by</p>

    <div class="my-4 text-center"><span class="math-block">\\left[ \\frac{I_s \\mid \\mathbf{0}^{01}}{\\mathbf{0}^{10} \\mid (B^{11})^{-1}} \\right].</span></div>

    <p class="text-gray-300">Proof (of Proposition 2). If  <span class="math">A \\leftrightarrow_N B</span>  then</p>

    <div class="my-4 text-center"><span class="math-block">\\left\\lceil \\frac{I_s \\mid \\mathbf{0}^{01}}{\\mathbf{0}^{10} \\mid \\dot{A}^{-1}} \\right\\rceil \\cdot A = \\left\\lceil \\frac{I_s \\mid \\mathbf{0}^{01}}{\\mathbf{0}^{10} \\mid \\dot{B}^{-1}} \\right\\rceil \\cdot B</span></div>

    <p class="text-gray-300">or</p>

    <div class="my-4 text-center"><span class="math-block">A = \\left[ \\frac{I_s \\mid \\mathbf{0}^{01}}{\\mathbf{0}^{10} \\mid \\dot{A} \\cdot \\dot{B}^{-1}} \\right] \\cdot B</span></div>

    <p class="text-gray-300">For the other direction, if</p>

    <div class="my-4 text-center"><span class="math-block">A = \\left\\lceil \\frac{I_s \\mid \\mathbf{0}^{01}}{\\mathbf{0}^{10} \\mid C^{11}} \\right\\rceil \\cdot B</span></div>

    <p class="text-gray-300">then</p>

    <div class="my-4 text-center"><span class="math-block">\\left[ \\frac{A^{0*}}{A^{1*}} \\right] = \\left[ \\frac{B^{0*}}{C^{11} \\cdot B^{1*}} \\right].</span></div>

    <p class="text-gray-300">Observe that  <span class="math">COL(A^{1<em>}) = COL(B^{1</em>})</span> , as left multiplication with the invertible  <span class="math">C^{11}</span>  does not affect the linear dependencies among the columns of  <span class="math">B^{1*}</span> . Furthermore,</p>

    <div class="my-4 text-center"><span class="math-block">\\hat{A} = \\dot{A}^{-1} \\cdot A^{1*} = \\dot{A}^{-1} \\cdot C^{11} \\cdot B^{1*} =</span></div>

    <div class="my-4 text-center"><span class="math-block">\\dot{A}^{-1} \\cdot C^{11} \\cdot \\dot{B} \\cdot \\hat{B} = (\\dot{A}^{-1} \\cdot C^{11} \\cdot \\dot{B}) \\cdot \\hat{B}.</span></div>

    <p class="text-gray-300">Thus, the normalized forms  <span class="math">\\hat{A}, \\hat{B}</span>  are related by the invertible linear transformation  <span class="math">\\dot{A}^{-1} \\cdot C^{11} \\cdot \\dot{B}</span> . Since  <span class="math">\\mathrm{COL}(A^{1<em>}) = \\mathrm{COL}(B^{1</em>})</span> , this linear transformation maps the identity matrix in  <span class="math">\\hat{B}</span>  to the identity matrix in  <span class="math">\\hat{A}</span> , implying that  <span class="math">\\dot{A}^{-1} \\cdot C^{11} \\cdot \\dot{B} = I_{n-s}</span>  and  <span class="math">\\hat{A} = \\hat{B}</span> . Combined with the equality  <span class="math">A^{0<em>} = B^{0</em>}</span>  we obtain  <span class="math">A \\leftrightarrow_N B</span> .</p>

    <h2 id="sec-37" class="text-2xl font-bold">B Proofs from Section 5</h2>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Proof (of Proposition 4). The proof is by induction on i. For  <span class="math">i=1,\\,R_1^{1<em>}=L_1^{1</em>}</span>  has full row rank by the invertibility of  <span class="math">L_1</span> . For  $i\\in\\{2,\\ldots,r-1\\},\\,R_i^{1*}=L_i^{10}\\</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\check{T}_i^{11}<span class="math"> . Observe that the matrix  </span>L_i^{1*}=L_i^{10}\\</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">L_i^{11}<span class="math">  has full row rank and thus column rank of n-s. The matrix  </span>\\check{T}_i^{11}<span class="math">  is obtained from  </span>L_i^{11}<span class="math">  by right multiplication with the matrix  </span>\\dot{R}_{i-1}<span class="math">  which has full rank by the induction hypothesis. Hence  </span>\\check{T}_i^{11}<span class="math">  and  </span>L_i^{11}<span class="math">  have the same column span. Therefore, the column spans of  </span>L_i^{10}\\</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">L_i^{11}<span class="math">  and  </span>R_i^{1*}=L_i^{10}\\</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\check{T}_i^{11}$  are identical, implying that their column and row ranks are n-s.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><em>Proof (of Proposition 5).</em> The proof is by induction on i. For i=1, clearly  <span class="math">y_1^{(0)}=\\bar{y}_1^{(0)},x_2^{(0)}=\\bar{x}_2^{(0)}</span>  and</p>

    <div class="my-4 text-center"><span class="math-block">z_2^{(1)} = \\hat{R}_1(y_1) = (\\dot{R}_1)^{-1} \\cdot R_1^{1*}(\\bar{y}_1) = (\\dot{R}_1)^{-1} \\cdot L_1^{1*}(\\bar{y}_1) = (\\dot{R}_1)^{-1}(\\bar{x}_2^{(1)}).</span></div>

    <p class="text-gray-300">For  <span class="math">i \\in \\{2, ..., r-1\\}</span> , using the induction hypothesis we obtain</p>

    <div class="my-4 text-center"><span class="math-block">y_i^{(0)} = S_i(x_i^{(0)}) = S_i(\\bar{x}_i^{(0)}) = \\bar{y}_i^{(0)},</span></div>

    <p class="text-gray-300">and</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{split} x_{i+1}^{(0)} &amp;= L_i^{00}(y_i^{(0)}) + \\check{T}_i^{01}(z_i^{(1)}) = \\\\ L_i^{00}(\\bar{y}_i^{(0)}) + \\check{T}_i^{01} \\left( (\\dot{R}_{i-1})^{-1}(\\bar{x}_i^{(1)}) \\right) = \\\\ L_i^{00}(\\bar{y}_i^{(0)}) + L_i^{01} \\cdot \\dot{R}_{i-1} \\cdot (\\dot{R}_{i-1})^{-1}(\\bar{x}_i^{(1)}) = \\\\ L_i^{00}(\\bar{y}_i^{(0)}) + L_i^{01}(\\bar{y}_i^{(1)}) = \\\\ L_i^{0*}(\\bar{y}_i) = \\\\ \\bar{x}_{i+1}^{(0)}. \\end{split}</span></div>

    <p class="text-gray-300">Finally,</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$z_{i+1}^{(1)} = \\hat{R}_i(y_i^{(0)}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">z_i^{(1)}) =$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$(\\dot{R}_i)^{-1} \\cdot R_i^{1*} (\\bar{y}_i^{(0)}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">(\\dot{R}_{i-1})^{-1} (\\bar{x}_i^{(1)})) =$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$(\\dot{R}_i)^{-1} \\cdot \\left( L_i^{10}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\check{T}_i^{11} \\right) (\\bar{y}_i^{(0)}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">(\\dot{R}_{i-1})^{-1} (\\bar{x}_i^{(1)})) =$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="my-4 text-center"><span class="math-block">(\\dot{R}_i)^{-1} \\cdot \\left( L_i^{10} (\\bar{y}_i^{(0)}) + \\check{T}_i^{11} \\cdot (\\dot{R}_{i-1})^{-1} (\\bar{x}_i^{(1)}) \\right) =</span></div>

    <div class="my-4 text-center"><span class="math-block">(\\dot{R}_i)^{-1} \\cdot \\left( L_i^{10} (\\bar{y}_i^{(0)}) + L_i^{11} \\cdot \\dot{R}_{i-1} \\cdot (\\dot{R}_{i-1})^{-1} (\\bar{x}_i^{(1)}) \\right) =</span></div>

    <div class="my-4 text-center"><span class="math-block">(\\dot{R}_i)^{-1} \\cdot \\left( L_i^{10} (\\bar{y}_i^{(0)}) + L_i^{11} (\\bar{x}_i^{(1)}) \\right) =</span></div>

    <div class="my-4 text-center"><span class="math-block">(\\dot{R}_i)^{-1} \\cdot \\left( L_i^{1*} (\\bar{y}_i) \\right) =</span></div>

    <div class="my-4 text-center"><span class="math-block">(\\dot{R}_i)^{-1} \\cdot \\left( L_i^{1*} (\\bar{y}_i) \\right) =</span></div>

    <p class="text-gray-300">LowMC instances used for benchmarking are depicted in Table 6. The number of rounds in all these instances was selected according to the round formula presented in [RST18], such that they provide the desired security level (for a certain data complexity, available to the attacker). The first two sets of instances were selected for PICNIC, the third set of parameters for evaluation in a garbled circuit. Round key matrices and constant pre-computations for ORKC (Section 3) as well as the optimized linear layer for OLLE (Section 5) were implemented on top of the LowMC reference implementation. <sup>15</sup></p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Block size <span class="math">(n)</span> Non-linear</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">layer size <span class="math">(s)</span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Key size <span class="math">(\\kappa)</span> Da</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">ata complexity (d)</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Rounds <span class="math">(r)</span></th>

          </tr>

        </thead>

        <tbody>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">128</td>

            <td class="px-3 py-2 border-b border-gray-700">30</td>

            <td class="px-3 py-2 border-b border-gray-700">128</td>

            <td class="px-3 py-2 border-b border-gray-700">1</td>

            <td class="px-3 py-2 border-b border-gray-700">20</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">192</td>

            <td class="px-3 py-2 border-b border-gray-700">30</td>

            <td class="px-3 py-2 border-b border-gray-700">192</td>

            <td class="px-3 py-2 border-b border-gray-700">1</td>

            <td class="px-3 py-2 border-b border-gray-700">30</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">256</td>

            <td class="px-3 py-2 border-b border-gray-700">30</td>

            <td class="px-3 py-2 border-b border-gray-700">256</td>

            <td class="px-3 py-2 border-b border-gray-700">1</td>

            <td class="px-3 py-2 border-b border-gray-700">38</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">128</td>

            <td class="px-3 py-2 border-b border-gray-700">3</td>

            <td class="px-3 py-2 border-b border-gray-700">128</td>

            <td class="px-3 py-2 border-b border-gray-700">1</td>

            <td class="px-3 py-2 border-b border-gray-700">182</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">192</td>

            <td class="px-3 py-2 border-b border-gray-700">3</td>

            <td class="px-3 py-2 border-b border-gray-700">192</td>

            <td class="px-3 py-2 border-b border-gray-700">1</td>

            <td class="px-3 py-2 border-b border-gray-700">284</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">256</td>

            <td class="px-3 py-2 border-b border-gray-700">3</td>

            <td class="px-3 py-2 border-b border-gray-700">256</td>

            <td class="px-3 py-2 border-b border-gray-700">1</td>

            <td class="px-3 py-2 border-b border-gray-700">363</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">128</td>

            <td class="px-3 py-2 border-b border-gray-700">3</td>

            <td class="px-3 py-2 border-b border-gray-700">128</td>

            <td class="px-3 py-2 border-b border-gray-700">128</td>

            <td class="px-3 py-2 border-b border-gray-700">287</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">192</td>

            <td class="px-3 py-2 border-b border-gray-700">3</td>

            <td class="px-3 py-2 border-b border-gray-700">192</td>

            <td class="px-3 py-2 border-b border-gray-700">192</td>

            <td class="px-3 py-2 border-b border-gray-700">413</td>

          </tr>

          <tr>

            <td class="px-3 py-2 border-b border-gray-700">256</td>

            <td class="px-3 py-2 border-b border-gray-700">3</td>

            <td class="px-3 py-2 border-b border-gray-700">256</td>

            <td class="px-3 py-2 border-b border-gray-700">256</td>

            <td class="px-3 py-2 border-b border-gray-700">537</td>

          </tr>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><span id="page-33-1"></span>Table 6. LowMC parameters used for benchmarking.</p>

    <p class="text-gray-300"><span id="page-33-2"></span><sup>15</sup> https://github.com/LowMC/LowMC</p>

    <h2 id="sec-39" class="text-2xl font-bold">C.1 Picnic - Post-Quantum Secure Signatures from Symmetric-Key Primitives</h2>

    <p class="text-gray-300">The post-quantum digital signature scheme PICNIC [CDG<sup>+</sup>17a,CDG<sup>+</sup>17b] is based on zero-knowledge proofs of knowledge of pre-images of one-way functions. There, the one-way functions are instantiated using LOWMC. PICNIC relies on a proof system called ZKB++, which is based on the "MPC-in-the-head" [IKOS07] paradigm. In the following, we shortly discuss some aspects of ZKB++ that explain the relation of signature size and multiplicative complexity of the underlying symmetric primitives, and give an overview of the computational overhead caused by the zero-knowledge proof system.</p>

    <p class="text-gray-300">At the core ZKB++, circuit decomposition is used as protocol for jointly computing a circuit. In a (2,3)-decomposition there are three players and the protocol has 2-privacy, i.e., it remains secure even if two of the three players are corrupted. We discuss some definitions from [GMO16] and the instantiation in PICNIC [CDG<sup>+</sup>17a].</p>

    <p class="text-gray-300"><strong>Definition 6 ((2,3)-decomposition).</strong> Let f be a function that is computed by an n-gate circuit  <span class="math">\\phi</span>  such that  <span class="math">f(x) = \\phi(x) = y</span> . Let  <span class="math">k_1, k_2</span> , and  <span class="math">k_3</span>  be tapes of length  <span class="math">\\kappa</span>  chosen uniformly at random from  <span class="math">\\{0,1\\}^{\\kappa}</span>  corresponding to players  <span class="math">P_1, P_2</span>  and  <span class="math">P_3</span> , respectively. The tuple of algorithms (Share, Update, Output, Reconstruct) are defined as follows:</p>

    <p class="text-gray-300">Share <span class="math">(x, k_1, k_2, k_3)</span> : On input of the secret value x, outputs the initial views for each player containing the secret share  <span class="math">x_i</span>  of x.</p>

    <p class="text-gray-300">Update(view<sub>i</sub><sup>(j)</sup>, view<sub>i+1</sub><sup>(j)</sup>,  <span class="math">k_i</span> ,  <span class="math">k_i</span> ,  <span class="math">k_i</span> ): On input of the views view<sub>i</sub><sup>(j)</sup>, view<sub>i+1</sub><sup>(j)</sup> and random tapes  <span class="math">k_i</span> ,  <span class="math">k_{i+1}</span> , compute wire values for the next gate and returns the updated view view<sub>i</sub><sup>(j+1)</sup>.</p>

    <p class="text-gray-300">Output(view<sub>i</sub><sup>(n)</sup>): On input of the final view view<sub>i</sub><sup>(n)</sup>, returns the output share  <span class="math">y_i</span> . Reconstruct( <span class="math">y_1, y_2, y_3</span> ): On input of output shares  <span class="math">y_i</span> , reconstructs and returns y.</p>

    <p class="text-gray-300">Correctness requires that reconstructing a (2,3)-decomposed evaluation of a circuit  <span class="math">\\phi</span>  yields the same value as directly evaluating  <span class="math">\\phi</span>  on the input value. The 2-privacy property requires that revealing the values from two shares reveals nothing about the input value.</p>

    <p class="text-gray-300">The  <span class="math">\\Sigma</span> -protocol ZKB++ constructs the (2,3)-decomposition of a circuit as follows: Let R be an arbitrary finite ring and  <span class="math">\\phi</span>  a function such that  <span class="math">\\phi: R^m \\to R^\\ell</span>  can be expressed by an n-gate arithmetic circuit over the ring using addition (respectively multiplications) by constants, and binary addition and binary multiplication gates. A (2,3)-decomposition of  <span class="math">\\phi</span>  is then given by:</p>

    <p class="text-gray-300">Share <span class="math">(x, k_1, k_2, k_3)</span> : Samples random  <span class="math">x_1, x_2 \\in \\mathbb{R}^m</span>  from  <span class="math">k_1</span>  and  <span class="math">k_2</span>  and computes  <span class="math">x_3</span>  such that  <span class="math">x_1 + x_2 + x_3 = x</span> . Returns views containing  <span class="math">x_1, x_2, x_3</span> .</p>

    <p class="text-gray-300"> <span class="math">\\mathsf{Update}_{i}^{(j)}(\\mathsf{view}_{i}^{(j)},\\mathsf{view}_{i+1}^{(j)},k_{i},k_{i+1})\\colon \\mathsf{Computes}\\ P_{i}</span> 's view of the output wire of gate  <span class="math">g_{j}</span>  and appends it to the view. For the k-th wire  <span class="math">w_{k}</span>  where  <span class="math">w_{k}^{(i)}</span>  denotes  <span class="math">P_{i}</span> 's view, the update operation is defined as follows:</p>

    <pre><code class="language-text">Addition by constant (w_b = w_a + c): w_b^{(i)} = w_a^{(i)} + c if i = 1 and w_b^{(i)} = w_a^{(i)} otherwise.

Multiplication by constant (w_b = c \\cdot w_a): w_b^{(i)} = c \\cdot w_a^{(i)}

Binary addition (w_c = w_a + w_b): w_c^{(i)} = w_a^{(i)} + w_b^{(i)}

Binary multiplication (w_c = w_a \\cdot w_b): w_c^{(i)} = w_a^{(i)} \\cdot w_b^{(i)} + w_a^{(i+1)} \\cdot w_b^{(i)} + w_a^{(i)} \\cdot w_b^{(i+1)} + R_i(c) - R_{i+1}(c) where R_i(c) is the c-th output of a pseudorandom generator seeded with k_i.</code></pre>

    <p class="text-gray-300">Output<sub>i</sub>(view<sub>i</sub><sup>(n)</sup>): Return the  <span class="math">\\ell</span>  output wires stored in the view view<sub>i</sub><sup>(n)</sup>. Reconstruct <span class="math">(y_1, y_2, y_3)</span> : Computes  <span class="math">y = y_1 + y_2 + y_3</span>  and returns y.</p>

    <p class="text-gray-300">Note that  <span class="math">P_i</span>  can compute all gate types with the exception of binary multiplication gates locally. The latter however requires inputs from  <span class="math">P_{i+1}</span> .</p>

    <p class="text-gray-300">With the (2,3)-decomposition in place, the  <span class="math">\\Sigma</span> -protocols works as follows: the prover computes fresh shares of the secret input, computes the circuit and then commits to the so-obtained views. Those commitments are then sent to the verifier. The verifier then selects two views to be opened uniformly at random. The prover opens the commitments to those two views and the verifier checks the consistency of the opened views. The  <span class="math">\\Sigma</span> -protocol can then be transformed into a non-interactive zero-knowledge proof system using standard techniques, e.g. the Fiat-Shamir transform [FS86] (in the ROM) or the Unruh transform [Unr15] (in the QROM). Additionally binding the proofs to a message allows one to obtain a signature scheme.</p>

    <p class="text-gray-300">As noted above, only binary multiplications gates require communication. Thus for every binary multiplication gate, one element from R needs to be stored in the view. Consequently, the proof and also signature size directly relate to the number of binary multiplication gates times the size of the ring R. Hence suitable symmetric-key primitives ideally have a small number of multiplication gates, so that the signature sizes are minimal. For LowMC this means that view size is  <span class="math">3 \\cdot m \\cdot r</span>  bits with m Sboxes and r rounds.</p>

    <p class="text-gray-300">Runtime performance-wise, additions by constant cost exactly the same as before. The cost of multiplications by constants and binary additions triples and for binary multiplications 9 multiplications and 12 additions have to be performed. The two matrix multiplications in LowMC thus contribute even more two the overall performance. One has to essentially perform three matrix-vector multiplications to derive the round key from the key shares and then again perform three matrix-vector multiplications to apply the linear layer to the current state.</p>

    <h4 id="sec-40" class="text-lg font-semibold mt-6">C.2 Garbled Circuits</h4>

    <p class="text-gray-300">Yao's garbled circuits (GC) protocol [Yao86] a prominent technique for secure two-party function evaluation. The two parties, called the <em>garbler</em> and <em>evaluator</em>, communicate to evaluate a function f(x, y) on their respective inputs x and y, so that they only learn the result of the evaluation and nothing more about the other parties' input than can be inferred by the function output. The basic</p>

    <p class="text-gray-300">working principle behind garbled circuits is the following: The function f is represented as a Boolean circuit which is garbled into a garbled circuit that is sent to the evaluator, along with the garbled input of the garbler. The evaluator then uses oblivious transfer [\\[Rab81\\]](#page-30-13) to retrieve the garbled labels corresponding to his input. Using the garbled input labels the evaluator evaluates the garbled circuit and computes the output of the computation.</p>

    <p class="text-gray-300">Several optimizations to Yao's garbled circuits have been proposed in the past, with one the most important techniques being the free XOR optimization [\\[KS08\\]](#page-30-3). This optimization allows to evaluate XOR gates without cryptographic operations and even more importantly, without any communication between the parties. While LowMC is designed to have a low number of AND gates and is therefore well suited to be evaluated using garbled circuits, the large number of XOR gates in a standard LowMC circuit means that the XORgates "are no longer free" <a href="#page-29-0">\\[ARS</a><sup>+</sup>15], but take up a considerable fraction of the generation and evaluation time of the garbled circuit.</p>

    <p class="text-gray-300">One application of garbled circuits is the evaluation of a pseudo-random function in an oblivious way, where one party provides the key to the PRF and the other party provides the input. Such an oblivious pseudo-random function (OPRF) [\\[FIPR05\\]](#page-30-14) can be used to build private set-intersection [\\[HL08\\]](#page-30-15) and secure database join [\\[LTW13\\]](#page-30-16) protocols. We briefly discuss the private-set intersection construction: The server samples a random PRF key and applies the PRF to its set and sends the encrypted set to the client. The client then evaluates each of his items using the OPRF and performs an intersection of the encrypted server set and his encrypted client items. Since the communication of the OPRF scales with the number of AND gates in circuit of the evaluated PRF, instances of LowMC with only one Sbox are interesting for this use-case.</p>

    <p class="text-gray-300">Proof (of Proposition <a href="#page-24-2">7\\)</a>. First, note that the bottom n−s rows of A are in normalized form after the transformation A1<sup>∗</sup> = Bˆ = (B˙) −1 ·B1<sup>∗</sup> in SampleBottom(). We show that A is sampled correctly by showing that reversing this transformation gives a uniform invertible matrix. Namely,</p>

    <div class="my-4 text-center"><span class="math-block">H = \\left[\\frac{A^{0*}}{B^{1*}}\\right] = \\left[\\frac{A^{0*}}{\\dot{B} \\cdot \\dot{B}}\\right]</span></div>

    <p class="text-gray-300">is a uniform invertible random matrix.</p>

    <p class="text-gray-300">In order to show that H is invertible, note that</p>

    <div class="my-4 text-center"><span class="math-block">\\left[\\frac{I_s \\mid \\mathbf{0}^{01}}{\\mathbf{0}^{10} \\mid (\\dot{B})^{-1}}\\right] \\cdot H = \\left[\\frac{I_s \\mid \\mathbf{0}^{01}}{\\mathbf{0}^{10} \\mid (\\dot{B})^{-1}}\\right] \\cdot \\left[\\frac{A^{0*}}{\\dot{B} \\cdot \\hat{B}}\\right] = \\left[\\frac{A^{0*}}{\\hat{B}}\\right] = A.</span></div>

    <p class="text-gray-300">Therefore, it suffices to prove that A is invertible, which is true if and only if A · P is invertible. We have</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$A^{1*} \\cdot P = \\hat{B} \\cdot P = ((\\dot{B})^{-1} \\cdot \\ddot{B}) \\</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">I_{n-s} = D^{10} \\</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">I_{n-s}.$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Hence</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$A \\cdot P = \\left[ \\frac{A^{0<em>} \\cdot P}{A^{1</em>} \\cdot P} \\right] = \\left[ \\frac{A'^{00} \\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A'^{01} \\right</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}{D^{10} \\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">I_{n-s} \\right</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">} \\right] = \\left[ \\frac{C^{00} + A'^{01} \\cdot D^{10} \\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A'^{01} \\right</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}{D^{10} \\left</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">I_{n-s} \\right</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">} \\right].$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">By Fact 2 (and change of notation), in order to show that  <span class="math">A \\cdot P</span>  is an invertible matrix, we need to verify that  <span class="math">(C^{00} + A&#x27;^{01} \\cdot D^{10}) + A&#x27;^{01} \\cdot D^{10} = C^{00}</span>  is invertible. This indeed holds by the way that  <span class="math">C^{00}</span>  is sampled.</p>

    <p class="text-gray-300">It remains to show that H is sampled uniformly among invertible matrices. For this purpose, we show that (a) the number of possible triplets of  <span class="math">B^{1*}</span> ,  <span class="math">C^{00}</span> ,  <span class="math">A&#x27;^{01}</span>  sampled uniformly in Algorithm 9 is  <span class="math">\\prod_{i=1}^{n} (1-1/2^i) \\cdot 2^{n^2} = \\alpha_n \\cdot 2^{n^2}</span> , and (b) every such triplet gives a different (invertible) matrix H. Since the number of invertible matrices is  <span class="math">\\alpha_n \\cdot 2^{n^2}</span>  by Fact 1, (a) and (b) combined with the fact that the algorithm only samples invertible matrices H completes the proof.</p>

    <p class="text-gray-300">We begin by counting the number of triplets  <span class="math">B^{1<em>}</span> ,  <span class="math">C^{00}</span> ,  <span class="math">A&#x27;^{01}</span> . By Fact 1, the number of possible values for  <span class="math">B^{1</em>}</span>  is  <span class="math">\\alpha_n/\\alpha_s \\cdot 2^{n(n-s)}</span> , as it is a uniform  <span class="math">(n-s) \\times n</span>  matrix with full row rank. Again, by Fact 1, the number of invertible matrices  <span class="math">C^{00}</span>  is  <span class="math">\\alpha_s \\cdot 2^{s^2}</span> , while the number of values of  <span class="math">A&#x27;^{01}</span>  is  <span class="math">2^{s(n-s)}</span> . Altogether, the number of triplets is</p>

    <div class="my-4 text-center"><span class="math-block">\\alpha_n/\\alpha_s \\cdot 2^{n(n-s)} \\cdot \\alpha_s \\cdot 2^{s^2} \\cdot 2^{s(n-s)} = \\alpha_n \\cdot 2^{n^2},</span></div>

    <p class="text-gray-300">as claimed.</p>

    <p class="text-gray-300">Finally, we show that every such triplet gives a different matrix H. Let  <span class="math">B_1^{1<em>}, C_1^{00}, A_1^{\\prime 01}</span>  and  <span class="math">B_2^{1</em>}, C_2^{00}, A_2^{\\prime 01}</span>  be two triplets sampled by executions of Algorithm 9 and assume that they both give rise to the same matrix H. We show that these triplets are equal. First, note that  <span class="math">H^{1<em>} = B_1^{1</em>} = B_2^{1<em>}</span>  which also implies that the value of  <span class="math">D^{10}</span>  and P computed in Algorithm 9 (which only depends on  <span class="math">B^{1</em>}</span> ) is identical for both executions. Next, the value of  <span class="math">H^{0<em>} = A^{0</em>}</span>  is computed in two different ways as</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$A^{0*} = (A_1^{\\prime 00} \\</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A_1^{\\prime 01}) \\cdot P^{-1} = (A_2^{\\prime 00} \\</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A_2^{\\prime 01}) \\cdot P^{-1} \\rightarrow$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$$A_1^{\\prime 00} \\</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A_1^{\\prime 01} = A_2^{\\prime 00} \\</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A_2^{\\prime 01} \\rightarrow$$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="my-4 text-center"><span class="math-block">A_1^{\\prime 01} = A_2^{\\prime 01} \\text{ and } A_1^{\\prime 00} = A_2^{\\prime 00} \\rightarrow</span></div>

    <div class="my-4 text-center"><span class="math-block">A_1^{\\prime 01} = A_2^{\\prime 01} \\text{ and } C_1^{00} + A_1^{\\prime 01} \\cdot D^{10} = C_2^{00} + A_2^{\\prime 01} \\cdot D^{10} \\rightarrow</span></div>

    <div class="my-4 text-center"><span class="math-block">A_1^{\\prime 01} = A_2^{\\prime 01} \\text{ and } C_1^{00} = C_2^{00},</span></div>

    <p class="text-gray-300">as claimed.</p>

    <p class="text-gray-300"><span id="page-37-1"></span>Algorithm 10 gives the full procedure for sampling the bottom sub-matrix as part of the sampling algorithm of Section 7.</p>

    <pre><code class="language-text">Output : B, ˆ COL(B
                  1∗
                   )
begin
   COL(B
         1∗
           ) ← ∅
   rank ← 0
   for i ∈ {n, n − 1, . . . , 1} do
      if rank = n − s then
                                           . already obtained full rank
         Bˆ[∗, i] ← GenRand(n − s, 1) . sample column uniformly
         continue
      end
      for j ∈ {1, . . . ,(n − s) − rank} do
         b ←rand
               {0, 1} . sample uniform bit
         if b = 1 then
                                                . column in COL(B
                                                                 1∗
                                                                   )
             rank ← rank + 1
             COL(B
                   1∗
                     ) ← COL(B
                               1∗
                                 ) ∪ {i}
             continue . sample next column
         end
      end
                                            . column not in COL(B
                                                                 1∗
                                                                   )
      if rank = 0 then
         Bˆ[∗, i] ← 0
         continue
      end
      v ←rand
            {0, 1}
                 rank . sample rank uniform bits
      Bˆ[∗, i] ← 0
               (n−s)−rankkv . prepend (n − s) − rank zeros to v
   end
   if rank &lt; n − s then
      return FAIL
   end
   return B, ˆ COL(B
                   1∗
                     )
end</code></pre>

    <p class="text-gray-300">Algorithm 10: OptSampleBottom() iteration</p>

    <p class="text-gray-300">Proposition 10. The output distribution of OptSampleBottom() (Algorithm <a href="#page-37-1">10\\)</a> is identical to the output distribution of a single iteration of SampleBottom() (Algorithm <a href="#page-24-0">8\\)</a>.</p>

    <p class="text-gray-300">Proof. First, we show that the distributions of COL(B1<sup>∗</sup> ) in each stage i ∈ {n, n − 1, . . . , 1} of the algorithms are identical. Note that in both algorithms, the size of COL(B1<sup>∗</sup> ) at each stage is rank, hence this will also show that the distributions of rank are identical (and imply that both algorithms have the same failure probability). At the beginning, COL(B1<sup>∗</sup> ) = ∅ in both and it suffices to show that this variable is updated correctly in OptSampleBottom() for each i ∈ {n, n − 1, . . . , 1}. Indeed, in SampleBottom() column i is added to COL(B1<sup>∗</sup> ) if the currently sampled vector is not in the subspace spanned by the previously sampled vectors (whose size is 2rank). This occurs with probability 1−2 rank/2 <sup>n</sup>−<sup>s</sup> = 1−2 (n−s)−rank and is simulated exactly by the (n−s)−rank coin tosses of OptSampleBottom().</p>

    <p class="text-gray-300">It remains to show that the output distributions of Bˆ in the two algorithms are identical. Since we showed that the distributions of COL(B1<sup>∗</sup> ) are identical at each stage i ∈ {n, n − 1, . . . , 1}, it is sufficient to show that the output distributions of Bˆ are identical, conditioned on COL(B1<sup>∗</sup> ). Clearly, the columns of COL(B1<sup>∗</sup> ), which contain the identity matrix in both algorithms, are identical. Moreover, the columns sampled after rank = n − s are uniformly distributed in both algorithms (as multiplication with the independent invertible matrix (B˙) −1 does not change their distribution in SampleBottom()). It remains to consider the columns of i ∈ {n, n − 1, . . . , 1} which are sampled when rank < n − s and are not added to COL(B1<sup>∗</sup> ). In SampleBottom(), such a column i is sampled uniformly from the subspace spanned by the previously sampled vectors whose size is 2rank. The final multiplication with (B˙) −1 is a change of basis which transforms the basis of the previously sampled columns to the last rank vectors in the standard basis e(n−s)−rank+1, e(n−s)−rank+2, . . . , en−s. Hence, after fixing COL(B1<sup>∗</sup> ), in the output Bˆ of SampleBottom(), column i is a uniform vector in the subspace spanned by e(n−s)−rank+1, e(n−s)−rank+2, . . . , en−s, which is identical to its distribution in OptSampleBottom().</p>

    <p class="text-gray-300">Complexity The computational effort of the algorithm is proportional to its number of coin tosses (note that it does not involve any linear algebra). Hence, to analyze the complexity, we count the expected number of coin tosses. Simple probabilistic analysis shows that when the iteration succeeds, the expected number of coin tosses can be upper bounded by 2(n − s) + s(n − s) = (s + 2)(n − s), where the factor 2(n − s) accounts for coin tosses for the n − s columns in COL(B<sup>1</sup><sup>∗</sup> ) and the s(n − s) accounts for sampling the s columns outside of COL(B<sup>1</sup><sup>∗</sup> ). For a reasonable<a href="#page-39-0">16</a> value of s ≥ 3, by Fact <a href="#page-11-1">1,</a> the first iteration will succeed with probability of more than 0.87. Moreover, we can reduce the expected number of coin tosses (while maintaining the output distribution) to less</p>

    <p class="text-gray-300"><span id="page-39-0"></span><sup>16</sup> For s = 1 or s = 2, the "non-linear" permutation layer is actually linear which makes the cipher insecure.</p>

    <p class="text-gray-300">than (s + 2)(n − s) + 0.3n ≈ (s + 2)(n − s) by sampling the columns that are not in COL(B1<sup>∗</sup> ) only if an iteration succeeds. In fact, it is possible to sample COL(B1<sup>∗</sup> ) directly without using rejection sampling, but this is more complicated and does not lead to a substantial improvement in complexity.</p>

    <h2 id="sec-43" class="text-2xl font-bold">F Discussion About the Assumptions of Section <a href="#page-27-0">8.1</a></h2>

    <p class="text-gray-300">We motivate the assumptions of Section <a href="#page-27-0">8.1</a> by arguing that implementations that bypass these assumptions are either not useful in practice, or they do not decrease by much the total amount of memory required to encrypt with an instance of a GLMC cipher (even though they may reduce the storage size of the linear layers).</p>

    <p class="text-gray-300">Regarding the first assumption, it is possible to encrypt (or decrypt) using very little memory by generating the linear layers from the seed on-the-fly during the encryption (or decryption) process. However, this is very inefficient in terms time complexity. Therefore, we assume that the encryption maintains an explicit representation of the linear layers (as indeed maintained by current LowMC implementations), and for this purpose, they can be assumed to be truly random based on the pseudo-randomness of the generator.</p>

    <p class="text-gray-300">The second assumption is bypassed (for example) by standard AES software implementations that combine the linear layers with the AES Sbox and form look-up tables which clearly depend on the AES Sbox specification. However, this only increases the total size of the code (in exchange for improved performance on some platforms) and is not a useful way to bypass the second assumption with respect to implementation size. A more meaningful way to bypass the second assumption consists of optimizations that make use of equivalent representations of the cipher which are dependent on its non-linear layers. For example, if the "non-linear" layers are actually linear, then all the linear layers can be combined into a single matrix, equivalent to all combined matrices. Of course, this results in a linear and insecure cipher, but a more realistic approach would consider non-linear layers that are self-affine equivalent (cf. [\\[BCBP03\\]](#page-29-7)), which implies that they have several equivalent representations. However, unless the non-linear layers are close to being truly linear, the number of such equivalent representations is small compared to the total possible number of linear layers in a GLMC cipher (and in the specific case of LowMC) and they do not allow a substantial saving in the implementation size.<a href="#page-40-1">17</a></p>

    <p class="text-gray-300">Finally, we note that implementations which try to combine the linear layers in various ways (that are independent of the non-linear layers), or manipulate them (such as the "method of four Russians") do fall within our model and we prove that they cannot reduce the storage size.</p>

    <p class="text-gray-300"><span id="page-40-1"></span><sup>17</sup> It is possible to optimize the representation of Algorithm <a href="#page-22-0">7</a> to take advantage of selfaffine equivalent non-linear layers. However, this is out of the scope of this paper and as noted above, has very limited effect for a reasonable choice of non-linear layers.</p>

    <h2 id="sec-44" class="text-2xl font-bold">G Proofs from Section 8</h2>

    <p class="text-gray-300">Proof (of Proposition 8). The proposition clearly holds for i=0 and i=r+1 (as the outputs of equivalent ciphers have to match for any x). Assume towards contradiction that  <span class="math">x_i^{(0)} \\neq x_i&#x27;^{(0)}</span>  for some  <span class="math">i \\in \\{1, ..., r\\}</span> . Recall that linear equivalence implies that  <span class="math">(k_0, \\mathcal{S}, \\mathcal{L}) \\equiv (k_0, \\mathcal{S}, \\mathcal{L}&#x27;)</span>  for any  <span class="math">k_0, \\mathcal{S}</span> . In particular, consider  <span class="math">\\mathcal{S}^<em></span>  for which the only change from  <span class="math">\\mathcal{S}</span>  is that  <span class="math">S_i</span>  is modified to  <span class="math">S_i^</em></span>  and exchanges the output values of  <span class="math">x_i&#x27;^{(0)}</span>  and some  <span class="math">u_i^{(0)}</span>  (for  <span class="math">u_i^{(0)} \\neq x_i^{(0)}</span>  and  <span class="math">u_i^{(0)} \\neq x_i&#x27;^{(0)}</span> ), namely,  <span class="math">S_i^<em>(x_i&#x27;^{(0)}) = S_i(u_i^{(0)})</span>  and  <span class="math">S_i^</em>(u_i^{(0)}) = S_i(x_i&#x27;^{(0)})</span> .</p>

    <p class="text-gray-300">Since  <span class="math">S_i(x_i&#x27;^{(0)}) \\neq S_i^<em>(x_i&#x27;^{(0)})</span> , then  <span class="math">E_{(k_0,\\mathcal{S},\\mathcal{L}&#x27;)}(x) \\neq E_{(k_0,\\mathcal{S}^</em>,\\mathcal{L}&#x27;)}(x)</span> . Indeed, the state values up to the i'th non-linear layer match in  <span class="math">E_{(k_0,\\mathcal{S},\\mathcal{L}&#x27;)}(x)</span>  and  <span class="math">E_{(k_0,\\mathcal{S}^*,\\mathcal{L}&#x27;)}(x)</span> ,</p>

    <p class="text-gray-300">Since  <span class="math">S_i(x_i&#x27;^{(0)}) \\neq S_i^<em>(x_i&#x27;^{(0)})</span> , then  <span class="math">E_{(k_0,\\mathcal{S},\\mathcal{L}&#x27;)}(x) \\neq E_{(k_0,\\mathcal{S}^</em>,\\mathcal{L}&#x27;)}(x)</span> . Indeed, the state values up to the <em>i</em>'th non-linear layer match in  <span class="math">E_{(k_0,\\mathcal{S},\\mathcal{L}&#x27;)}(x)</span>  and  <span class="math">E_{(k_0,\\mathcal{S}^<em>,\\mathcal{L}&#x27;)}(x)</span>  but then diverge as  <span class="math">S_i(x_i&#x27;^{(0)}) \\neq S_i^</em>(x_i&#x27;^{(0)})</span> . Since the remaining partial encryption algorithm (consisting of  <span class="math">L_i&#x27;</span>  and rounds  <span class="math">i+1,\\ldots,r</span> ) is a permutation which is identical in both ciphers, the state values cannot converge.</p>

    <p class="text-gray-300">On the other hand  <span class="math">E_{(k_0,\\mathcal{S}^<em>,\\mathcal{L})}(x) = E_{(k_0,\\mathcal{S},\\mathcal{L})}(x)</span>  as  <span class="math">S_i(x_i^{(0)}) = S_i^</em>(x_i^{(0)})</span> . Therefore, either  <span class="math">E_{(k_0,\\mathcal{S},\\mathcal{L})}(x) \\neq E_{(k_0,\\mathcal{S},\\mathcal{L}&#x27;)}(x)</span>  or</p>

    <div class="my-4 text-center"><span class="math-block">E_{(k_0,\\mathcal{S}^*,\\mathcal{L})}(x) = E_{(k_0,\\mathcal{S},\\mathcal{L})}(x) = E_{(k_0,\\mathcal{S},\\mathcal{L}&#x27;)}(x) \\neq E_{(k_0,\\mathcal{S}^*,\\mathcal{L}&#x27;)}(x),</span></div>

    <p class="text-gray-300">and in any case  <span class="math">\\mathcal{L} \\not\\equiv \\mathcal{L}&#x27;</span>  in contradiction.</p>

    <p class="text-gray-300">Proof (of Proposition 9). Consider  <span class="math">\\mathcal{S}^<em></span>  for which the only change from  <span class="math">\\mathcal{S}</span>  is that  <span class="math">S_i</span>  is modified to  <span class="math">S_i^</em></span>  and exchanges the output values of  <span class="math">x_i^{(0)}</span>  and  <span class="math">\\bar{x}_i^{(0)}</span>  (in particular  <span class="math">S_i^<em>(\\bar{x}_i^{(0)}) = S_i(x_i^{(0)})</span> ). Consider  <span class="math">E_{(k_0,\\mathcal{S}^</em>,\\mathcal{L})}(\\bar{x})</span>  and note that the state obtained after i rounds (that are unchanged from  <span class="math">(k_0,\\mathcal{S},\\mathcal{L})</span> ) is equal to</p>

    <div class="my-4 text-center"><span class="math-block">\\bar{x}_i = \\bar{x}_i^{(0)}, \\bar{x}_i^{(1)} = \\bar{x}_i^{(0)}, x_i^{(1)}.</span></div>

    <p class="text-gray-300">After application of  <span class="math">S_i^<em></span> , the 0 part of the state is modified to  <span class="math">S_i^</em>(\\bar{x}_i^{(0)}) = S_i(x_i^{(0)}) = y_i^{(0)}</span> . Hence, the full state of  <span class="math">E_{(k_0, S^*, \\mathcal{L})}(\\bar{x})</span>  after i encryption rounds and the nonlinear layer application is  <span class="math">y_i^{(0)}, x_i^{(1)} = y_i^{(0)}, y_i^{(1)} = y_i</span> .</p>

    <p class="text-gray-300">Since the mappings from  <span class="math">y_i</span>  to the output in the remaining <em>i</em>'th linear layer and last r - i + 1 round functions in  <span class="math">(k_0, \\mathcal{S}^*, \\mathcal{L})</span>  and  <span class="math">(k_0, \\mathcal{S}, \\mathcal{L})</span>  are identical, we have</p>

    <div class="my-4 text-center"><span class="math-block">E_{(k_0,\\mathcal{S}^*,\\mathcal{L})}(\\bar{x}_i) = E_{(k_0,\\mathcal{S},\\mathcal{L})}(x_i).</span></div>

    <p class="text-gray-300">In other words, exchanging the non-linear layer values canceled the change caused the exchanging the plaintexts.</p>

    <p class="text-gray-300">Since  <span class="math">\\mathcal{L} \\equiv \\mathcal{L}&#x27;</span> , then</p>

    <div class="my-4 text-center"><span class="math-block">E_{(k_0,\\mathcal{S}^*,\\mathcal{L}&#x27;)}(\\bar{x}_i) = E_{(k_0,\\mathcal{S}^*,\\mathcal{L})}(\\bar{x}_i) = E_{(k_0,\\mathcal{S},\\mathcal{L})}(x_i) = E_{(k_0,\\mathcal{S},\\mathcal{L}&#x27;)}(x_i).</span></div>

    <p class="text-gray-300">Once again, the last r-i+1 round functions after applications of  <span class="math">S_i^<em></span>  and  <span class="math">S_i</span>  in  <span class="math">(k_0, \\mathcal{S}^</em>, \\mathcal{L}&#x27;)</span>  and  <span class="math">(k_0, \\mathcal{S}, \\mathcal{L}&#x27;)</span>  (respectively) are identical permutations and their outputs are identical by the above equality. Hence, the inputs to the final</p>

    <p class="text-gray-300">rounds (obtained after i encryption rounds and application of the i'th non-linear layer) are identical, and in particular the values of their part 1 of the state are identical. In  <span class="math">E_{(k_0,\\mathcal{S}^<em>,\\mathcal{L}&#x27;)}(\\bar{x}_i)</span>  it is  <span class="math">\\bar{y}_i&#x27;^{(1)} = \\bar{x}_i&#x27;^{(1)}</span>  as the first r rounds of  <span class="math">E_{(k_0,\\mathcal{S}^</em>,\\mathcal{L}&#x27;)}</span>  and  <span class="math">E_{(k_0,\\mathcal{S},\\mathcal{L}&#x27;)}</span>  are identical. In  <span class="math">E_{(k_0,\\mathcal{S},\\mathcal{L}&#x27;)}(x_i)</span>  is it  <span class="math">y_i&#x27;^{(1)} = x_i&#x27;^{(1)}</span>  by definition. We conclude that  <span class="math">\\bar{x}_i&#x27;^{(1)} = x_i&#x27;^{(1)}</span>  as claimed.</p>

    <p class="text-gray-300">Proof (of Lemma 1). The proof is by induction of r.</p>

    <p class="text-gray-300">For r=1,  <span class="math">\\Lambda_1</span>  is the set of invertible matrices. We show that every different invertible matrix  <span class="math">\\mathcal{L}=(L_1)</span>  forms a linear equivalence class. Indeed, assume that for some  <span class="math">K, \\mathcal{S}</span>  and all x,  <span class="math">E_{(k_0,\\mathcal{S},(L_1))}(x)=E_{(k_0,\\mathcal{S},(L_1&#x27;))}(x).^{18}</span>  Then after adding  <span class="math">k_0</span>  and applying  <span class="math">S_1</span>  (which are the same for both schemes), we get  <span class="math">L_1(y_1)=L_1&#x27;(y_1)</span> . In particular, this holds for the n vectors of the standard basis  <span class="math">y_1 \\in \\{e_1,e_2,\\ldots,e_n\\}</span>  which give (in matrix notation)  <span class="math">L_1 \\cdot I_n = L_1&#x27; \\cdot I_n</span>  or  <span class="math">L_1 = L_1&#x27;</span> .</p>

    <p class="text-gray-300">Assume correctness for i=r and add a round at the beginning, noticing that rounds  <span class="math">2, \\ldots, r+1</span>  form an r-round scheme with zero initial round key.</p>

    <p class="text-gray-300">Let  <span class="math">(L_1, \\mathcal{L}) \\neq (L&#x27;_1, \\mathcal{L}&#x27;) \\in \\Lambda_{r+1}</span>  such that  <span class="math">\\mathcal{L}, \\mathcal{L}&#x27; \\in \\Lambda_r</span> . We need to prove  <span class="math">(L_1, \\mathcal{L}) \\not\\equiv (L&#x27;_1, \\mathcal{L}&#x27;)</span> . We divide the proof into three cases.</p>

    <p class="text-gray-300">If  <span class="math">\\mathcal{L} = \\mathcal{L}&#x27;</span>  but  <span class="math">L_1 \\neq L&#x27;_1</span> , then for any corresponding ciphers, we have</p>

    <div class="my-4 text-center"><span class="math-block">E_{(k_0,S,(L_1,\\mathcal{L}))}(x) \\neq E_{(k_0,S,(L&#x27;_1,\\mathcal{L}))}(x)</span></div>

    <p class="text-gray-300">on every x for which the outputs of the first round differs (since rounds  <span class="math">2, \\ldots, r+1</span>  are identical).</p>

    <p class="text-gray-300">If  <span class="math">\\mathcal{L} \\neq \\mathcal{L}&#x27;</span>  and  <span class="math">L_1 = L_1&#x27;</span> , then by the induction hypothesis,  <span class="math">\\mathcal{L} \\not\\equiv \\mathcal{L}&#x27;</span> . If we start with non-equivalent r-round ciphers and add an identical round at the beginning, clearly the schemes remain non-equivalent.</p>

    <p class="text-gray-300">Finally, it remains to prove the induction step for the case that  <span class="math">\\mathcal{L} \\neq \\mathcal{L}&#x27;</span>  and  <span class="math">L_1 \\neq L&#x27;_1</span> . Assume towards contradiction that  <span class="math">(L_1, \\mathcal{L}) \\equiv (L&#x27;_1, \\mathcal{L}&#x27;)</span> . We will prove that in this case,  <span class="math">L_1 \\leftrightarrow_N L&#x27;_1</span> . However, by the definition of  <span class="math">\\Lambda_{r+1}</span> , we select only one matrix from each normalized equivalence class and hence  <span class="math">L_1 = L&#x27;_1</span>  in contradiction.</p>

    <p class="text-gray-300">In order to derive  <span class="math">L_1 \\leftrightarrow_N L&#x27;_1</span> , fix a key  <span class="math">k_0</span> , an (r+1)-tuple of non-linear layers S and pick some plaintext  <span class="math">x \\in \\{0,1\\}^n</span> . Denote the state values for round i of the first cipher  <span class="math">E_{(k_0,S,(L_1,\\mathcal{L}))}(x)</span>  by  <span class="math">x_i</span>  and  <span class="math">y_i</span>  and similarly, denote by  <span class="math">x&#x27;_i</span>  and  <span class="math">y&#x27;_i</span>  these values for  <span class="math">E_{(k_0,S,(L&#x27;_1,\\mathcal{L}&#x27;))}(x)</span> . Since the initial round keys and  <span class="math">S_1</span>  are identical,  <span class="math">y_1 = y&#x27;_1</span>  and therefore,</p>

    <div class="my-4 text-center"><span class="math-block">x_2 = L_1 \\cdot \\left(\\frac{y_1^{(0)}}{y_1^{(1)}}\\right), \\ x_2&#x27; = L_1&#x27; \\cdot \\left(\\frac{y_1^{(0)}}{y_1^{(1)}}\\right),</span></div>

    <p class="text-gray-300">for the encryption of any  <span class="math">x \\in \\{0,1\\}^n</span> . We consider the transformation</p>

    <div class="my-4 text-center"><span class="math-block">(L_1&#x27; \\cdot (L_1)^{-1})(x_2) = x_2&#x27;.</span></div>

    <p class="text-gray-300"><span id="page-42-0"></span>The definition of linear equivalence requires that  <span class="math">E_{(k_0,\\mathcal{S},(L_1))}(x) = E_{(k_0,\\mathcal{S},(L_1&#x27;))}(x)</span>  holds for any  <span class="math">k_0,\\mathcal{S}</span> , so it obviously must hold for an arbitrary choice of  <span class="math">k_0,\\mathcal{S}</span> .</p>

    <p class="text-gray-300">Our goal is to show that</p>

    <p class="text-gray-300"><span id="page-43-0"></span> <span class="math-block">L_1&#x27; \\cdot (L_1)^{-1} = \\left[ \\frac{I_s \\mid \\mathbf{0}^{01}}{\\mathbf{0}^{10} \\mid C^{11}} \\right]</span>  (6)</p>

    <p class="text-gray-300">for some invertible  <span class="math">C^{11}</span> , which proves  <span class="math">L_1 \\leftrightarrow_N L&#x27;_1</span>  by Proposition 2. Since we assume  <span class="math">(L_1, \\mathcal{L}) \\equiv (L&#x27;_1, \\mathcal{L}&#x27;)</span> , then by Proposition 8 we must have</p>

    <p class="text-gray-300">Since we assume  <span class="math">(L_1, L) \\equiv (L_1, L)</span> , then by Proposition 8 we must have  <span class="math">x_2^{\\prime(0)} = x_2^{(0)}</span>  at the input of the second linear layer. By setting  <span class="math">x_2 \\in \\{e_1, e_2, \\dots, e_n\\}</span>  to be the n standard basis vectors and using the equality  <span class="math">x_2^{\\prime(0)} = x_2^{(0)}</span> , we conclude that the top s rows of  <span class="math">L_1&#x27; \\cdot (L_1)^{-1}</span>  are of the form of (6).</p>

    <p class="text-gray-300">It remains to prove that the bottom-left  <span class="math">(n-s) \\times s</span>  block of  <span class="math">L_1&#x27; \\cdot (L_1)^{-1}</span>  is equal to  <span class="math">\\mathbf{0}^{10}</span> . Equivalently, we need to show that changing  <span class="math">x_2^{(0)}</span>  (without changing  <span class="math">x_2^{(1)}</span> ) at the input of  <span class="math">L_1&#x27; \\cdot (L_1)^{-1}</span>  does not change  <span class="math">x_2&#x27;^{(1)}</span>  at the output (but will obviously set  <span class="math">x_2&#x27;^{(0)} = x_2^{(0)}</span> ). Indeed, this property is directly implied by Proposition 9.</p>`;
---

<BaseLayout title="Linear Equivalence of Block Ciphers with Partial Non-Linear ... (2018/772)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2018 &middot; eprint 2018/772
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <p class="mt-4 text-xs text-gray-500">
        All content below belongs to the original authors. This page
        reproduces the paper for educational purposes. Always
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >cite the original</a>.
      </p>
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <Fragment set:html={CONTENT} />

  </article>
</BaseLayout>
