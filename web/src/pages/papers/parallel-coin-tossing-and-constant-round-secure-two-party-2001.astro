---
import BaseLayout from '../../layouts/BaseLayout.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2001/107';
const CRAWLER = 'mistral';
const CONVERTED_DATE = '2026-02-16';
const TITLE_HTML = 'Parallel Coin-Tossing and Constant-Round Secure Two-Party Computation';
const AUTHORS_HTML = 'Yehuda Lindell';

const CONTENT = `    <p class="text-gray-300">Parallel Coin-Tossing and Constant-Round Secure Two-Party Computation</p>

    <p class="text-gray-300">Yehuda Lindell</p>

    <p class="text-gray-300">Dept. of Computer Science and Applied Math. The Weizmann Institute of Science Rehovot 76100, ISRAEL. lindell@wisdom.weizmann.ac.il</p>

    <p class="text-gray-300">October 9, 2003</p>

    <h2 id="sec-1" class="text-2xl font-bold">Abstract</h2>

    <p class="text-gray-300">In this paper we show that any two-party functionality can be securely computed in a constant number of rounds, where security is obtained against (polynomial-time) malicious adversaries that may arbitrarily deviate from the protocol specification. This is in contrast to Yao's constant-round protocol that ensures security only in the face of semi-honest adversaries, and to its malicious adversary version that requires a polynomial number of rounds.</p>

    <p class="text-gray-300">In order to obtain our result, we present a constant-round protocol for secure coin-tossing of polynomially many coins (in parallel). We then show how this protocol can be used in conjunction with other existing constructions in order to obtain a constant-round protocol for securely computing any two-party functionality. On the subject of coin-tossing, we also present a constant-round almost perfect coin-tossing protocol, where by "almost perfect" we mean that the resulting coins are guaranteed to be statistically close to uniform (and not just pseudorandom).</p>

    <p class="text-gray-300">Keywords: Secure computation, Constant-round protocols, Coin-tossing.</p>

    <p class="text-gray-300">*An extended abstract of this work appeared in CRYPTO 2001.</p>

    <p class="text-gray-300">0</p>

    <p class="text-gray-300">1 Introduction</p>

    <h3 id="sec-2" class="text-xl font-semibold mt-8">1.1 Secure Two-Party Computation</h3>

    <p class="text-gray-300">In the setting of two-party computation, two parties with respective private inputs <span class="math">x</span> and <span class="math">y</span>, wish to jointly compute a functionality <span class="math">f(x,y)=(f_{1}(x,y),f_{2}(x,y))</span>, such that the first party receives <span class="math">f_{1}(x,y)</span> and the second party receives <span class="math">f_{2}(x,y)</span>. This functionality may be probabilistic, in which case <span class="math">f(x,y)</span> is a random variable. Loosely speaking, the security requirements are that nothing is learned from the protocol other than the output (privacy), and that the output is distributed according to the prescribed functionality (correctness). The actual definition <em>[22, 27, 3, 11]</em> blends these two conditions (see Section 2.2). The above security must be guaranteed even when one of the parties is adversarial. Such an adversary may be semi-honest (or passive), in which case it correctly follows the protocol specification, yet attempts to learn additional information by analyzing the transcript of messages received during the execution. On the other hand, an adversary may be malicious (or active), in which case it can arbitrarily deviate from the protocol specification.</p>

    <p class="text-gray-300">The first general solutions for the problem of secure computation were presented by Yao <em>[33]</em> for the two-party case (with security against semi-honest adversaries) and Goldreich, Micali and Wigderson <em>[25]</em> for the multi-party case (with security even against malicious adversaries). Thus, despite the stringent security requirements placed on such protocols, wide-ranging completeness results were established, demonstrating that any probabilistic polynomial-time functionality can be securely computed (assuming the existence of trapdoor permutations).</p>

    <h5 id="sec-3" class="text-base font-semibold mt-4">Yao’s protocol.</h5>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">In <em>[33]</em>, Yao presented a constant-round protocol for securely computing any functionality, where the adversary may be semi-honest. Denote Party 1 and Party 2’s respective inputs by <span class="math">x</span> and <span class="math">y</span> and let <span class="math">f</span> be the functionality that they wish to compute (for simplicity, assume that both parties wish to receive <span class="math">f(x,y)</span>). Loosely speaking, Yao’s protocol works by having one of the parties (say Party 1) first generate an “encrypted” circuit computing <span class="math">f(x,\\cdot)</span> and send it to Party 2. The circuit is such that it reveals nothing in its encrypted form and therefore Party 2 learns nothing from this stage. However, Party 2 can obtain the output <span class="math">f(x,y)</span> by “decrypting” the circuit. In order to ensure that Party 2 learns nothing more than the output itself, this decryption must be “partial” and must reveal <span class="math">f(x,y)</span> only. Without going into unnecessary details, this is accomplished by Party 2 obtaining a series of keys corresponding to its input <span class="math">y</span>, such that given these keys and the circuit, the output value <span class="math">f(x,y)</span> (and only this value) may be obtained. Of course, Party 2 must obtain these keys without revealing anything about <span class="math">y</span> and this can be done by running $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">y</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$ instances of a (semi-honest) secure 2-out-of-1 Oblivious Transfer protocol <em>[14]</em>, which is constant-round. By running the Oblivious Transfer protocols in parallel, this protocol requires only a constant number of rounds.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Now consider what happens if Yao’s protocol is run when the adversary may be malicious. Firstly, we have no guarantee that Party 1 constructed the circuit so that it correctly computes <span class="math">f(x,\\cdot)</span>. Thus, correctness may be violated. Secondly, the Oblivious Transfer protocol must satisfy the requirements for secure computation (in the face of malicious adversaries), and must maintain its security when run in parallel. We know of no such (highly secure) oblivious transfer protocol that runs in a constant number of rounds. Finally, if the functionality <span class="math">f</span> is probabilistic, then Party 1 must be forced to input a truly random string into the circuit. Thus, some type of coin-tossing protocol is also required.</p>

    <h5 id="sec-4" class="text-base font-semibold mt-4">Secure protocol compilation.</h5>

    <p class="text-gray-300">As we have mentioned, Goldreich, Micali and Wigderson <em>[24, 25]</em> showed that assuming the existence of trapdoor permutations, there exist protocols for securely</p>

    <p class="text-gray-300">computing any multi-party functionality, where the adversary may be malicious. They achieve this in two stages. First, they show a protocol for securely computing any functionality in the semi-honest adversarial model. Next, they construct a protocol compiler that takes any semi-honest protocol and “converts” it into a protocol that is secure in the malicious model. As this compiler is generic, it can be applied to any semi-honest protocol and in particular, to the constant-round two-party protocol of Yao. However, due to the nature of their compilation, the output protocol is no longer constant-round.</p>

    <h3 id="sec-5" class="text-xl font-semibold mt-8">1.2 Our Results</h3>

    <p class="text-gray-300">The focus of this paper is to construct a protocol compiler such that the round-complexity of the compiled protocol is of the same order as that of the original protocol. We observe that the only component of the GMW compiler for which there does not exist a constant-round construction is that of coin-tossing in the well <em>[8]</em>. In particular, in the GMW compiler the coins are tossed sequentially, and thus polynomially many rounds are required. Our technical contribution is therefore in constructing a constant-round protocol for coin-tossing in the well of polynomially many coins. That is, we obtain the following theorem (informally stated):</p>

    <h6 id="sec-6" class="text-base font-medium mt-4">Theorem 1</h6>

    <p class="text-gray-300">(constant-round coin-tossing): Assuming the existence of one-way functions, there exists a constant-round protocol for the coin-tossing functionality (as required by the GMW compiler).</p>

    <p class="text-gray-300">In order to construct such a constant-round protocol we introduce a technique relating to the use of commitment schemes, which we believe may be useful in other settings as well. Commitment schemes are a basic building block and are used in the construction of many protocols. Consider, for example, Blum’s protocol for coin-tossing a single bit <em>[8]</em>. In this protocol, Party 1 sends a commitment to a random-bit; then, Party 2 replies with its own random bit and finally Party 1 decommits. Loosely speaking, the security of such a protocol is guaranteed by providing a simulator who receives a uniformly chosen bit and “interacts” with the adversarial party in order to generate a transcript of the protocol execution that is consistent with this bit. The difficulty in simulating this protocol with an adversarial Party 2 is that the simulator only knows the correct value to commit to after Party 2 sends its message. However, since the simulator is bound to its commitment, it must somehow guess the correct value before this message is sent. In case the messages are long (say <span class="math">n</span> bits rather than a single bit or <span class="math">\\log n</span> bits), this may be problematic. Thus, rather than decommitting, we propose to have the party reveal the committed value and then prove (in zero-knowledge) the validity of this revealed value. In a real execution, this is equivalent to decommitting, since the committing party is effectively bound to the committed value by the zero-knowledge proof. However, the simulator is able to provide a simulated zero-knowledge proof (rather than a real one). Furthermore, this proof remains indistinguishable from a real proof even if the revealed value is incorrect (and thus the statement is false). Therefore, the simulator can effectively “decommit” to any value it wishes and is not bound in any way by the original commitment that it sends.</p>

    <p class="text-gray-300">Combining the constant-round protocol of Theorem 1 with other known constructions, we obtain the following theorem:</p>

    <h6 id="sec-7" class="text-base font-medium mt-4">Theorem 2</h6>

    <p class="text-gray-300">Assume the existence of one-way functions. Then, there exists a protocol compiler that given a two-party protocol <span class="math">\\Pi</span> for securely computing <span class="math">f</span> in the semi-honest model produces a two-party protocol <span class="math">\\Pi^{\\prime}</span> that securely computes <span class="math">f</span> in the malicious model, so that the number of rounds of communication in <span class="math">\\Pi^{\\prime}</span> is within a constant multiplicative factor of the number of rounds of communication in <span class="math">\\Pi</span>.</p>

    <p class="text-gray-300">We stress that, when ignoring the “round preservation” clause, the existence of a protocol compiler is not new and has been shown in <em>[24, 25]</em> (in fact, as we have mentioned, we use most of the components of their compiler). Our contribution is in reducing the overhead of the compiler, in terms of the round-complexity, to a constant factor. The main result, stated in the following theorem, is obtained by applying the compiler of Theorem 2 to the constant-round protocol of Yao.</p>

    <h6 id="sec-8" class="text-base font-medium mt-4">Theorem 3</h6>

    <p class="text-gray-300">Assuming the existence of trapdoor permutations, any two-party functionality can be securely computed in the malicious model in a constant number of rounds.</p>

    <p class="text-gray-300">While on the subject of coin-tossing, we also present a constant-round protocol for the “almost perfect” coin-tossing (of polynomially many coins) that guarantees that the output of the coin-tossing protocol is statistically close to uniform, and not just computationally indistinguishable.</p>

    <h3 id="sec-9" class="text-xl font-semibold mt-8">1.3 Related Work</h3>

    <p class="text-gray-300">Although generic feasibility results for the problem of secure computation have been established, these protocols are not very efficient. For example, in the <em>[25]</em> protocol, both the number of rounds and communication complexity are polynomial in the size of the circuit computing the functionality. Thus some research has focused on finding efficient protocols for specific problems of secure computation. See <em>[9, 12, 15, 31]</em> for just a few examples. This direction is not the focus of our work.</p>

    <p class="text-gray-300">Other research has considered the efficiency of generic solutions themselves and as such also addresses fundamental questions regarding efficiency considerations (e.g., the possibility of obtaining protocols with only a constant number of rounds or with sub-linear communication complexity). Specifically, in the setting of multi-party computation with an honest majority, Beaver, Micali and Rogaway <em>[5]</em> showed that any functionality can be securely computed in a constant number of rounds, where the adversary may be malicious. Unfortunately, their techniques rely heavily on the fact that a majority of the parties are honest and as such cannot be applied to the case of two-party protocols. The above addresses the issue of the round complexity of protocols. Another important question relates to the communication complexity (in terms of the bandwidth) of secure protocols. See <em>[29]</em> for a recent work in this direction.</p>

    <p class="text-gray-300">As we have described, in this paper we establish the analogous result of <em>[5]</em> for the setting of two-party computation.</p>

    <h3 id="sec-10" class="text-xl font-semibold mt-8">1.4 Organization</h3>

    <p class="text-gray-300">In Sections 2 and 3, definitions and the cryptographic tools used in our protocols are presented (most of these are standard, although Section 3.2 contains some important discussions and Section 3.3 contains a new technical lemma, see below). Then, in Section 4 we discuss the protocol compiler of Goldreich, Micali and Wigderson and observe that in order to achieve “round-preserving” compilation, one needs only to construct a constant-round coin-tossing protocol. Our technical contribution in this paper thus begins in Section 5 where we present such a constant-round coin-tossing protocol. Then, in Section 6 we present a secure protocol for “almost perfect” coin-tossing, for which the output is guaranteed to be statistically close to uniform (and not just pseudorandom).</p>

    <p class="text-gray-300">In addition, we provide two useful technical results. In Section 3.3 we present a general lemma that can be used to simplify the analysis of protocols that use zero-knowledge proofs of knowledge as subprotocols (as indeed we do in our coin-tossing protocols). Also, in Appendix A we consider zero-knowledge arguments and arguments of knowledge in a setting where the adversarial party</p>

    <p class="text-gray-300">may run in <em>expected</em> polynomial-time (rather than being limited to strict polynomial-time). In particular, we show that the zero-knowledge arguments of knowledge of <em>[17]</em> remain zero-knowledge in this setting, whereas the zero-knowledge proof system of <em>[20]</em> seems not to.</p>

    <h2 id="sec-11" class="text-2xl font-bold">2 Definitions</h2>

    <p class="text-gray-300">Most of the definitions presented below are standard. The only difference is that in the setting of secure computation, we allow the adversary to run in <em>expected</em> polynomial-time (rather than strict polynomial-time).</p>

    <h3 id="sec-12" class="text-xl font-semibold mt-8">2.1 Preliminaries</h3>

    <p class="text-gray-300">In this section we present some basic definitions and notations. We begin by recalling the definitions of statistical closeness and computational indistinguishability.</p>

    <h6 id="sec-13" class="text-base font-medium mt-4">Definition 4</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">(Statistical Closeness): Let <span class="math">S\\subseteq\\{0,1\\}^{*}</span> be a set of strings (<span class="math">S</span> is called the index set). Let <span class="math">\\{X_{s}\\}_{s\\in S}</span> and <span class="math">\\{Y_{s}\\}_{s\\in S}</span> be probability ensembles, so that for any <span class="math">s\\in S</span> the distribution <span class="math">X_{s}</span> (resp., <span class="math">Y_{s}</span>) ranges over strings of length polynomial in $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">s</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math">. We say that the ensembles are statistically close, denoted </span>\\{X_{s}\\}\\stackrel{{\\scriptstyle s}}{{=}}\\{Y_{s}\\}<span class="math">, if for every polynomial </span>p(\\cdot)<span class="math"> and all sufficiently long </span>s\\in S$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$\\sum_{\\alpha}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\Pr[X_{s}=\\alpha]-\\Pr[Y_{s}=\\alpha]</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><\\frac{1}{p(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">s</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)}$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h6 id="sec-14" class="text-base font-medium mt-4">Definition 5</h6>

    <p class="text-gray-300">(Computational Indistinguishability): Let <span class="math">S</span>, <span class="math">\\{X_{s}\\}_{s\\in S}</span> and <span class="math">\\{Y_{s}\\}_{s\\in S}</span> be as above. We say that the ensembles are computationally indistinguishable, denoted <span class="math">\\{X_{s}\\}\\stackrel{{\\scriptstyle s}}{{=}}\\{Y_{s}\\}</span>, if for every probabilistic polynomial time distinguisher <span class="math">D</span>, every polynomial <span class="math">p(\\cdot)</span>, all sufficiently long <span class="math">s\\in S</span> and all auxiliary information <span class="math">z\\in\\{0,1\\}^{\\operatorname{poly}(n)}</span></p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\Pr[D(X_{s},s,z)=1]-\\Pr[D(Y_{s},s,z)=1]</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><\\frac{1}{p(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">s</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)}$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">We denote the uniform distribution over strings of length <span class="math">m</span> by <span class="math">U_{m}</span>. Thus a distribution ensemble <span class="math">\\{X_{n}\\}</span> ranging over strings of length <span class="math">m</span> is said to be pseudorandom if it is computationally indistinguishable from <span class="math">\\{U_{m}\\}</span>. (When considering ensembles indexed by N, we associate N and <span class="math">\\{1^{n}:n\\in\\textsf{N}\\}</span>.) For a set <span class="math">S</span>, we denote <span class="math">s\\in_{R}S</span> when <span class="math">s</span> is chosen uniformly from <span class="math">S</span>. Finally, the security parameter is denoted by <span class="math">n</span>.</p>

    <h4 id="sec-15" class="text-lg font-semibold mt-6">Uniform versus non-uniform presentation.</h4>

    <p class="text-gray-300">For the sake of simplicity, the definition of secure computation is presented in the non-uniform model of computation (in line with the presentation of <em>[18]</em>). However, the proof of the coin-tossing protocol itself is in the uniform model of computation. That is, the protocol is shown to be secure when the adversary is any probabilistic (expected) polynomial-time machine. This enables us to assume a weaker assumption; namely that of one-way functions that are hard to invert for uniform machines. However, in such a case, security is only obtained for a weaker, uniform adversary. Nevertheless, our proof also implies security in the non-uniform model, and thus security can be formulated against non-uniform adversaries and assuming one-way functions that are hard to invert for non-uniform machines.</p>

    <p class="text-gray-300">###</p>

    <h4 id="sec-16" class="text-lg font-semibold mt-6">Expected polynomial-time machines.</h4>

    <p class="text-gray-300">In this work, we consider adversaries that are modeled by expected polynomial-time interactive machines. An interactive machine <span class="math">M_{1}</span> is expected polynomial-time if there exists a (single) polynomial <span class="math">p(\\cdot)</span> such that for <em>every</em> (possibly unbounded) machine <span class="math">M_{2}</span>, the expected running time of <span class="math">M_{1}</span> (upon input of length <span class="math">n</span>) when interacting with <span class="math">M_{2}</span> is bounded by <span class="math">p(n)</span>.</p>

    <h4 id="sec-17" class="text-lg font-semibold mt-6">Discussion - expected polynomial-time.</h4>

    <p class="text-gray-300">An alternative definition to the above one would be to say that the adversary must run in expected polynomial-time when interacting with an honest party only, but is not restricted in the case that it interacts with any other machine. This is the definition of expected polynomial-time suggested by Feige <em>[16, Section 3.3]</em>. However, Feige shows that obtaining zero-knowledge against such expected polynomial-time verifiers is problematic. We therefore adopt the above definition, for which we <em>can</em> obtain zero-knowledge, as is shown in Appendix A.</p>

    <p class="text-gray-300">We stress that we do not claim that a definition allowing for expected polynomial-time adversaries is “superior” to an analogous definition that restricts adversaries to strict polynomial-time. In fact, we personally adopt the view that expected polynomial-time is very problematic and it is far preferable to stay within the relm of strict polynomial-time (without adopting either of the above definitions). Nevertheless, we consider expected polynomial-time here for the following reasons:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>At the time that this work was conducted, no constant-round zero-knowledge protocols with <em>strict</em> polynomial-time simulators were known to exist. Likewise, all extractors for constant-round zero-knowledge proofs of knowledge ran in expected polynomial-time. Subsequent to this work, constant-round protocols with strict polynomial-time simulators and extractors were demonstrated <em>[1, 2]</em>. Thus, relying on that work, the results of this paper can be modified to consider strict polynomial-time adversaries only. We note, however, that <em>[1, 2]</em> assume the existence of trapdoor clawfree permutations, whereas the protocols we use (with expected polynomial-time simulators and extractors) assume only the existence of one-way functions.</li>

      <li>Despite the problematic nature of equating efficient computation with expected polynomial-time, we believe that it is important to have rigorous proofs of security also in this model. We feel that the results presented in Appendix A justify this view.</li>

    </ol>

    <p class="text-gray-300">We conclude by noting that the standard way of bypassing this issue is to restrict the adversary to strict polynomial-time, and allow for its simulation to take expected polynomial-time. Thus, the security provided states that anything the adversary could see in a strict polynomial-time attack on the protocol, it could generate in expected polynomial-time. On the other hand, in this work we prove that anything that the adversary could see in an expected polynomial-time attack, it could generate in that same time. This seems to be a preferable security claim.</p>

    <h3 id="sec-18" class="text-xl font-semibold mt-8">2.2 Secure Computation</h3>

    <p class="text-gray-300">In this section we present the definition for secure two-party computation. The following description and definition is based on <em>[18]</em>, which in turn follows <em>[22, 27, 3, 11]</em>.</p>

    <h4 id="sec-19" class="text-lg font-semibold mt-6">Two-party computation.</h4>

    <p class="text-gray-300">A two-party protocol problem is cast by specifying a random process that maps pairs of inputs to pairs of outputs (one for each party). We refer to such a process as a functionality and denote it <span class="math">f:\\{0,1\\}^{<em>}\\times\\{0,1\\}^{</em>}\\rightarrow\\{0,1\\}^{<em>}\\times\\{0,1\\}^{</em>}</span>, where <span class="math">f=(f_{1},f_{2})</span>. That is, for every pair of inputs <span class="math">(x,y)</span>, the output-pair is a random variable <span class="math">(f_{1}(x,y),f_{2}(x,y))</span> ranging over pairs</p>

    <p class="text-gray-300">of strings. The first party (with input  <span class="math">x</span> ) wishes to obtain  <span class="math">f_{1}(x,y)</span>  and the second party (with input  <span class="math">y</span> ) wishes to obtain  <span class="math">f_{2}(x,y)</span> . We often denote such a functionality by  <span class="math">(x,y) \\mapsto (f_{1}(x,y), f_{2}(x,y))</span> . Thus, for example, the basic coin-tossing functionality is denoted by  <span class="math">(1^{n}, 1^{n}) \\mapsto (U_{n}, U_{n})</span> .</p>

    <p class="text-gray-300">Adversarial behavior. Loosely speaking, the aim of a secure two-party protocol is to protect an honest party against dishonest behavior by the other party. This "dishonest behavior" can manifest itself in a number of ways; in particular, we focus on what are known as semi-honest and malicious adversaries. A semi-honest adversary follows the prescribed protocol, yet attempts to learn more information than "allowed" from the execution. Specifically, the adversary may record the entire message transcript of the execution and attempt to learn something beyond the protocol output. On the other hand, a malicious adversary may arbitrarily deviate from the specified protocol. When considering malicious adversaries, there are certain undesirable actions that cannot be prevented. Specifically, a party may refuse to participate in the protocol, may substitute its local input (and enter with a different input) and may abort the protocol prematurely. One ramification of the adversary's ability to abort, is that it is impossible to achieve "fairness". That is, the adversary may obtain its output while the honest party does not.[2]</p>

    <p class="text-gray-300">As is standard for two-party computation, in this work we consider a static corruption model, where one of the parties is adversarial and the other is honest, and this is fixed before the execution begins. (This is in contrast to an adaptive corruption model where an adversary can corrupt one or both of the parties during the execution, based on what it sees. Such a model makes sense for two-party computation when one considers a large network of users in which pairs of parties run two-party protocols.)</p>

    <p class="text-gray-300">Security of protocols (informal). The security of a protocol is analyzed by comparing what an adversary can do in the protocol to what it can do in an ideal scenario that is secure by definition. This is formalized by considering an ideal computation involving an incorruptible trusted third party to whom the parties send their inputs. The trusted party computes the functionality on the inputs and returns to each party its respective output. Loosely speaking, a protocol is secure if any adversary interacting in the real protocol (where no trusted third party exists) can do no more harm than if it was involved in the above-described ideal computation.</p>

    <p class="text-gray-300">Execution in the ideal model. The ideal model differs for semi-honest and malicious parties. First, for semi-honest parties, an ideal execution involves each party sending their respective input to the trusted party and receiving back their prescribed output. An honest party then outputs this output, whereas a semi-honest party may output an arbitrary (probabilistic polynomial-time computable) function of its initial input and the message it obtained from the trusted party. (See [18] for a formal definition of the ideal and real models for the case of semi-honest adversaries.)</p>

    <p class="text-gray-300">We now turn to the ideal model for malicious parties. Since some malicious behavior cannot be prevented (for example, early aborting), the definition of the ideal model in this case is somewhat more involved. An ideal execution proceeds as follows:</p>

    <p class="text-gray-300">Inputs: Each party obtains an input, denoted  <span class="math">z</span> .</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">An honest party always sends <span class="math">z</span> to the trusted party. A malicious party may, depending on <span class="math">z</span>, either abort or sends some $z^{\\prime}\\in\\{0,1\\}^{</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">z</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}$ to the trusted party.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">In case it has obtained an input pair, <span class="math">(x,y)</span>, the trusted party (for computing <span class="math">f</span>), first replies to the first party with <span class="math">f_{1}(x,y)</span>. Otherwise (i.e., in case it receives only one valid input), the trusted party replies to both parties with a special symbol <span class="math">\\bot</span>. In case the first party is malicious it may, depending on its input and the trusted party’s answer, decide to <em>stop</em> the trusted party. In this case the trusted party sends <span class="math">\\bot</span> to the second party. Otherwise (i.e., if not stopped), the trusted party sends <span class="math">f_{2}(x,y)</span> to the second party. An honest party always outputs the message it has obtained from the trusted party. A malicious party may output an arbitrary (probabilistic polynomial-time computable) function of its initial input and the message obtained from the trusted party.</p>

    <p class="text-gray-300">Let <span class="math">f:\\{0,1\\}^{<em>}\\times\\{0,1\\}^{</em>}\\mapsto\\{0,1\\}^{<em>}\\times\\{0,1\\}^{</em>}</span> be a functionality, where <span class="math">f=(f_{1},f_{2})</span>, and let <span class="math">\\overline{M}=(M_{1},M_{2})</span> be a pair of non-uniform probabilistic <em>expected polynomial-time</em> machines (representing parties in the ideal model). Such a pair is admissible if for at least one <span class="math">i\\in\\{1,2\\}</span> we have that <span class="math">M_{i}</span> is honest (i.e., follows the honest party instructions in the above-described ideal execution). Then, the joint execution of <span class="math">f</span> under <span class="math">\\overline{M}</span> in the ideal model (on input pair <span class="math">(x,y)</span>), denoted <span class="math">\\textsc{ideal}_{f,\\overline{M}}(x,y)</span>, is defined as the output pair of <span class="math">M_{1}</span> and <span class="math">M_{2}</span> from the above ideal execution. For example, in the case that <span class="math">M_{1}</span> is malicious and always aborts at the outset, the joint execution is defined as <span class="math">(M_{1}(x,\\bot),\\bot)</span>. Whereas, in case <span class="math">M_{1}</span> never aborts, the joint execution is defined as <span class="math">(M_{1}(x,f_{1}(x^{\\prime},y)),f_{2}(x^{\\prime},y))</span> where <span class="math">x^{\\prime}=M_{1}(x)</span> is the input that <span class="math">M_{1}</span> gives to the trusted party.</p>

    <h4 id="sec-20" class="text-lg font-semibold mt-6">Execution in the real model.</h4>

    <p class="text-gray-300">We next consider the real model in which a real (two-party) protocol is executed (and there exists no trusted third party). In this case, a malicious party may follow an arbitrary feasible strategy; that is, any strategy implementable by non-uniform expected polynomial-time machines. In particular, the malicious party may abort the execution at any point in time (and when this happens prematurely, the other party is left with no output).</p>

    <p class="text-gray-300">Let <span class="math">f</span> be as above and let <span class="math">\\Pi</span> be a two-party protocol for computing <span class="math">f</span>. Furthermore, let <span class="math">\\overline{M}=(M_{1},M_{2})</span> be a pair of non-uniform probabilistic <em>expected polynomial-time</em> machines (representing parties in the real model). Such a pair is admissible if for at least one <span class="math">i\\in\\{1,2\\}</span> we have that <span class="math">M_{i}</span> is honest (i.e., follows the strategy specified by <span class="math">\\Pi</span>). Then, the joint execution of <span class="math">\\Pi</span> under <span class="math">\\overline{M}</span> in the real model (on input pair <span class="math">(x,y)</span>), denoted <span class="math">\\textsc{real}_{\\Pi,\\overline{M}}(x,y)</span>, is defined as the output pair of <span class="math">M_{1}</span> and <span class="math">M_{2}</span> resulting from the protocol interaction.</p>

    <h4 id="sec-21" class="text-lg font-semibold mt-6">Security as emulation of a real execution in the ideal model.</h4>

    <p class="text-gray-300">Having defined the ideal and real models, we can now define security of protocols. Loosely speaking, the definition asserts that a secure two-party protocol (in the real model) emulates the ideal model (in which a trusted party exists). This is formulated by saying that admissible pairs in the ideal model are able to simulate admissible pairs in an execution of a secure real-model protocol.</p>

    <h6 id="sec-22" class="text-base font-medium mt-4">Definition 6 (security in the malicious model)</h6>

    <p class="text-gray-300">Let <span class="math">f</span> and <span class="math">\\Pi</span> be as above. Protocol <span class="math">\\Pi</span> is said to securely compute <span class="math">f</span> (in the malicious model) if for every pair of admissible non-uniform probabilistic expected polynomial-time machines <span class="math">\\overline{A}=(A_{1},A_{2})</span> for the real model, there exists a pair of admissible</p>

    <p class="text-gray-300">non-uniform probabilistic expected polynomial-time machines <span class="math">\\overline{B}=(B_{1},B_{2})</span> for the ideal model, such that</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$\\left\\{\\textsc{ideal}_{f,\\overline{B}}(x,y)\\right\\}_{x,y\\text{ s.t. }</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">=</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">y</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}\\stackrel{{\\scriptstyle e}}{{\\equiv}}\\left\\{\\textsc{real}_{\\overline{\\Pi},\\overline{A}}(x,y)\\right\\}_{x,y\\text{ s.t. }</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">=</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">y</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">We note that the above definition assumes that the parties know the input lengths (this can be seen from the requirement that $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">=</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">y</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$). Some restriction on the input lengths is unavoidable, see <em>[18, Section 2.1]</em> for discussion.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h5 id="sec-23" class="text-base font-semibold mt-4">Remark:</h5>

    <p class="text-gray-300">The above definition is different from the standard definition in that the adversary (in both the ideal and real models) is allowed to run in expected polynomial-time (rather than strict polynomial-time). This seems to be inevitable given that currently known constant-round zero-knowledge proofs require expected polynomial-time simulation. We note that the honest party strategies in our protocols run in strict polynomial-time only.</p>

    <h2 id="sec-24" class="text-2xl font-bold">3 Cryptographic Tools</h2>

    <p class="text-gray-300">In this section we provide (informal) definitions of perfectly-binding and perfectly-hiding commitment schemes, and present the definition of zero-knowledge arguments of knowledge. All the above is standard. However, Section 3.2 also contains important discussions that we recommend not skipping (even for the familiar reader), and Section 3.3 contains a new technical lemma that we rely on in proving the security of our coin-tossing protocol.</p>

    <h3 id="sec-25" class="text-xl font-semibold mt-8">3.1 String Commitment</h3>

    <h4 id="sec-26" class="text-lg font-semibold mt-6">3.1.1 Perfectly binding commitment schemes</h4>

    <p class="text-gray-300">Commitment schemes are a basic ingredient in many cryptographic protocols. They are used to enable a party, known as the sender, to commit itself to a value while keeping it secret from the receiver (this property is called hiding). Furthermore, the commitment is binding, and thus in a later stage when the commitment is opened, it is guaranteed that the “opening” can yield only a single value determined in the committing phase. In a perfectly binding commitment scheme, the binding property holds even for an all-powerful sender, while the hiding property is only guaranteed with respect to a polynomial-time bounded receiver.</p>

    <p class="text-gray-300">For simplicity, we present the definition for a non-interactive, commitment scheme for a single bit. String commitment can be obtained by separately committing to each bit in the string. We denote by <span class="math">C(b;r)</span> the output of the commitment scheme <span class="math">C</span> upon input <span class="math">b\\in\\{0,1\\}</span> and using the random string <span class="math">r\\in_{R}\\{0,1\\}^{n}</span> (for simplicity, we assume that <span class="math">C</span> uses <span class="math">n</span> random bits where <span class="math">n</span> is the security parameter).</p>

    <h6 id="sec-27" class="text-base font-medium mt-4">Definition 7 (perfectly-binding bit commitment)</h6>

    <p class="text-gray-300">A perfectly binding commitment scheme is a probabilistic algorithm <span class="math">C</span> satisfying the following two conditions:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Perfect Binding: <span class="math">C(0;r)\\neq C(1;s)</span> for every <span class="math">r,s\\in\\{0,1\\}^{n}</span> and <span class="math">n\\in\\mathsf{N}</span>.</li>

      <li>Computational Hiding: The probability ensembles <span class="math">\\{C(0;U_{n})\\}_{n\\in\\mathsf{N}}</span> and <span class="math">\\{C(1;U_{n})\\}_{n\\in\\mathsf{N}}</span> are computationally indistinguishable.</li>

    </ol>

    <p class="text-gray-300">Non-interactive perfectly-binding commitment schemes can be constructed using any 1–1 one-way function (see <em>[19, Section 4.4.1]</em>). Allowing some minimal interaction (in which the receiver first sends a single message), almost perfectly-binding commitment schemes can be obtained from any one-way function <em>[28]</em>.</p>

    <h4 id="sec-28" class="text-lg font-semibold mt-6">3.1.2 Perfectly hiding commitment schemes</h4>

    <p class="text-gray-300">We now informally describe the requirements for a <em>perfectly hiding</em> commitment scheme. In such a scheme, the binding property is guaranteed to hold only with respect to a polynomial-time sender. On the other hand, the hiding property is unconditional. That is, the distributions of commitments to 0 and commitments to 1 are identical, and thus even an all-powerful receiver cannot know the value committed to by the sender. We stress that the binding property guarantees that a cheating probabilistic polynomial-time sender can find only one decommitment, even though decommitments to both 0 and 1 exist. See <em>[19, Section 4.8.2]</em> for a full definition.</p>

    <p class="text-gray-300">Perfectly hiding commitment schemes can be constructed from any one-way permutation <em>[30]</em>. However, <em>constant-round</em> schemes are only known to exist under seemingly stronger assumptions; specifically that of collision-resistant hash functions <em>[32, 13]</em> or families of certified clawfree functions (see <em>[19, Section 4.8.2.3]</em>). (We note that certified clawfree functions exist under the discrete-log assumption.)</p>

    <h3 id="sec-29" class="text-xl font-semibold mt-8">3.2 Constant Round Zero-Knowledge Arguments of Knowledge</h3>

    <p class="text-gray-300">Feige and Shamir <em>[17]</em> show that there exist constant-round zero-knowledge arguments (with negligible error probability) for membership in any <span class="math">\\mathcal{NP}</span>-language, assuming that there exist one-way functions. Furthermore their argument system is actually a system of <em>arguments of knowledge</em>.</p>

    <p class="text-gray-300">We first recall the definition of zero-knowledge proof systems <em>[23]</em>. Below, we denote by <span class="math">\\langle P,V^{<em>}\\rangle(x)</span> the output of <span class="math">V^{</em>}</span> after interacting with <span class="math">P</span> on common input <span class="math">x</span>.</p>

    <h6 id="sec-30" class="text-base font-medium mt-4">Definition 8 (auxiliary-input zero-knowledge)</h6>

    <p class="text-gray-300">Let <span class="math">(P,V)</span> be an interactive proof for a language <span class="math">L</span>; denote by <span class="math">P_{L}(x)</span> the set of strings <span class="math">y</span> satisfying the completeness condition with respect to <span class="math">x\\in L</span> <span class="math">(</span>i.e., <span class="math">\\Pr[\\langle P(y_{x}),V(z)\\rangle(x)=1]=1</span> for every <span class="math">z\\in\\{0,1\\}^{<em>}</span>). We say that <span class="math">(P,V)</span> is auxiliary-input zero-knowledge if for every probabilistic expected polynomial-time machine <span class="math">V^{</em>}</span>, there exists a probabilistic algorithm <span class="math">M^{*}</span>, running in expected-time that is polynomial in its first input, so that the following two probability ensembles are computationally indistinguishable:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">\\{\\langle P(y_{x}),V^{<em>}(z)\\rangle(x)\\}_{x\\in L,z\\in\\{0,1\\}^{</em>}}</span> for an arbitrary <span class="math">y_{x}\\in P_{L}(x)</span>.</li>

      <li><span class="math">\\{M^{<em>}(x,z)\\}_{x\\in L,z\\in\\{0,1\\}^{</em>}}</span></li>

    </ol>

    <p class="text-gray-300">In the above definition, <span class="math">z</span> represents the auxiliary information given to the verifier. Note that whereas <span class="math">P</span> and <span class="math">V^{<em>}</span> above are interactive strategies, <span class="math">M^{</em>}</span> is a non-interactive machine. We now present the definition for arguments of knowledge (as defined in <em>[7, 19]</em>):</p>

    <h6 id="sec-31" class="text-base font-medium mt-4">Definition 9 (system of arguments of knowledge)</h6>

    <p class="text-gray-300">Let <span class="math">R</span> be a binary relation and <span class="math">\\kappa:\\mathsf{N}\\mapsto[0,1]</span>. We say that a probabilistic polynomial-time <span class="math">V</span> is a knowledge verifier for the relation <span class="math">R</span> with knowledge error <span class="math">\\kappa</span> if the following two conditions hold:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Non-triviality: There exists a probabilistic polynomial-time interactive machine <span class="math">P</span> so that for every <span class="math">(x,y)\\in R</span>, all possible interactions of <span class="math">V</span> with <span class="math">P(y)</span> on common-input <span class="math">x</span> are accepting.</li>

    </ul>

    <p class="text-gray-300">-</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Validity (or knowledge soundness) with error <span class="math">\\kappa(\\cdot)</span>: <em>There exists a polynomial <span class="math">q(\\cdot)</span> and a probabilistic oracle machine <span class="math">K</span>, called a knowledge extractor, such that for every probabilistic</em> expected <em>polynomial-time machine <span class="math">P^{</em>}</span> and every <span class="math">x,y,r\\in\\{0,1\\}^{<em>}</span></em>, <em>machine <span class="math">K</span> satisfies the following condition:</em></li>

    </ul>

    <blockquote class="border-l-4 border-gray-600 pl-4 my-4 text-gray-400 italic">
      <p><em>Denote by <span class="math">p(x,y,r)</span> the probability that <span class="math">V</span> accepts, on input <span class="math">x</span>, when interacting with the prover specified by <span class="math">P^{</em>}_{x,y,r}</span> (where <span class="math">P^{<em>}_{x,y,r}</span> denotes the strategy of <span class="math">P^{</em>}</span> on common-input <span class="math">x</span>, auxiliary-input <span class="math">y</span> and random-tape <span class="math">r</span>). Then, if <span class="math">p(x,y,r)&gt;\\kappa(|x|)</span> then, <span class="math">(x,K^{P^{<em>}_{x,y,r}}(x))\\in R</span>, where <span class="math">K^{P^{</em>}_{x,y,r}}</span> runs within an expected number of steps bounded by*</p>
    </blockquote>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$\\frac{q(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)}{p(x,y,r)-\\kappa(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)}$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300"><em>The oracle machine <span class="math">K</span> is called a knowledge extractor.</em></p>

    <p class="text-gray-300"><em>An interactive pair <span class="math">(P,V)</span> so that <span class="math">V</span> is a knowledge verifier for a relation <span class="math">R</span> and <span class="math">P</span> is a machine satisfying the non-triviality condition (with respect to <span class="math">V</span> and <span class="math">R</span>) is called a system of arguments of knowledge for the relation <span class="math">R</span>. The argument system <span class="math">(P,V)</span> is a system of </em>zero-knowledge arguments of knowledge<em> if it is also zero-knowledge.</em></p>

    <h4 id="sec-32" class="text-lg font-semibold mt-6">A remark on alternative definitions of proofs of knowledge.</h4>

    <p class="text-gray-300">The definition of a proof of knowledge used by Feige and Shamir in <em>[17]</em> is different from the above (specifically, it states that the extractor runs in expected polynomial-time and the probability of successful extraction is at most negligibly smaller than the probability that the verifier accepts). However, as shown in <em>[7]</em>, these definitions are equivalent for <span class="math">\\mathcal{NP}</span> relations.</p>

    <h4 id="sec-33" class="text-lg font-semibold mt-6">Soundness.</h4>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">We note that the above validity requirement implies <em>soundness</em> with error of at most <span class="math">\\kappa</span>. That is, if there does not exist a <span class="math">y</span> such that <span class="math">(x,y)\\in R</span>, then the probability that the verifier accepts is at most <span class="math">\\kappa</span>. This is because no witness can be extracted (as none exists), yet the validity requirement demands that extraction succeeds in the case that $p(x)>\\kappa(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math">. As mentioned above, the system of arguments of Feige and Shamir <em>[17]</em> is such that the error function </span>\\kappa$ is negligible.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Formally, the soundness property of interactive proofs is stated as follows: there exists a <em>single</em> negligible function <span class="math">\\kappa(\\cdot)</span>, such that for every probabilistic polynomial-time (malicious) prover <span class="math">P^{<em>}</span>, the probability that <span class="math">V</span> accepts after interacting with <span class="math">P^{</em>}</span> upon input <span class="math">x</span> and <span class="math">x\\not\\in L</span>, is at most $\\kappa(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math">. However, the definition of soundness used in <em>[17]</em> states that for every prover </span>P^{<em>}<span class="math"> there exists a (possibly different) negligible function </span>\\kappa_{P^{</em>}}<span class="math">, such that the probability that the verifier accepts upon common input </span>x\\not\\in L<span class="math"> is at most </span>\\kappa_{P^{*}}(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)$. The fact that these soundness requirements are equivalent was shown in <em>[6]</em>.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h4 id="sec-34" class="text-lg font-semibold mt-6">On expected polynomial-time adversaries.</h4>

    <p class="text-gray-300">The standard definition of zero-knowledge arguments refers to strict polynomial-time verifiers. Thus, a simulator is required to generate a transcript that is indistinguishable to a real transcript for such a verifier. On the other hand, our definition above refers to <em>expected</em> polynomial-time verifiers. This extension is needed since in our definition of secure computation, the real-model adversary in a two-party protocol is allowed to run in expected polynomial-time. In the protocol simulation, this adversary plays the verifier and we must therefore be able to simulate the zero-knowledge protocol when the verifier runs in expected (rather than strict) polynomial-time. This is not immediate. In fact, in Appendix A, we show that there exist expected polynomial-time verifiers for which the simulator for the protocol of <em>[20]</em> runs for an exponential number of steps. Thus, the zero-knowledge property of the <em>[20]</em> protocol seems</p>

    <p class="text-gray-300">not to hold for expected polynomial-time verifiers (we do not know how to “fix” the simulator of <em>[20]</em> so that it does succeed for such verifiers). A similar issue arises with respect to extraction from an expected polynomial-time prover.</p>

    <p class="text-gray-300">In Appendix A, we show that the simulator (resp., extractor) of the zero-knowledge arguments of knowledge of <em>[17]</em> does run in expected polynomial-time for any expected polynomial-time verifier (resp., prover). Thus, we can use the <em>[17]</em> argument system in our protocols, where security must be guaranteed against expected polynomial-time adversaries. See Appendix A for a detailed treatment of this issue.</p>

    <h4 id="sec-35" class="text-lg font-semibold mt-6">Perfect zero-knowledge arguments.</h4>

    <p class="text-gray-300">A perfect zero-knowledge argument is one for which there exists a simulator that generates a transcript that is identically distributed to the transcript of a real proof (and not just computationally indistinguishable). Constant-round perfect zero-knowledge arguments and arguments of knowledge can be constructed using families of clawfree functions or any perfectly hiding string commitment <em>[10, 17]</em>.</p>

    <h3 id="sec-36" class="text-xl font-semibold mt-8">3.3 Proofs of Knowledge as Subprotocols: A Useful Lemma</h3>

    <p class="text-gray-300">Zero-knowledge proofs (or arguments) of knowledge are often used as subprotocols within larger protocols. The property desired by the use of a proof of knowledge in such a scenario is that a simulator can run the extractor for the proof of knowledge and thus obtain some secret information. This secret information is then used in order to simulate the rest of the larger protocol.</p>

    <p class="text-gray-300">However, a technical problem arises when using a proof of knowledge in such a way. In order to illustrate this problem, consider a two-party protocol consisting of a number of stages. In one of the stages, party <span class="math">A</span> provides a proof of knowledge of some witness <span class="math">w</span> to party <span class="math">B</span>. Then, in the proof of security, a simulator <span class="math">S</span> needs to simulate <span class="math">A</span>’s view and as part of this simulation, it must extract the witness <span class="math">w</span>. The natural way to construct such a simulator is to simply have it run the knowledge extractor at the appropriate point, thus obtaining <span class="math">w</span>. However, <span class="math">S</span> needs to simulate <span class="math">A</span>’s entire view and this includes <span class="math">A</span>’s view within the proof of knowledge. (Recall that by the definition of proofs of knowledge, the extractor only outputs a witness and not a view of the execution.) Therefore, in order to obtain both <span class="math">A</span>’s view within the proof of knowledge and the witness, <span class="math">S</span> works as follows.</p>

    <p class="text-gray-300">First, <span class="math">S</span> verifies the proof given by <span class="math">A</span>, by playing the honest verifier. Then, if the proof is rejected, <span class="math">S</span> halts (exactly as party <span class="math">B</span> in a real interaction with <span class="math">A</span> would). On the other hand, if <span class="math">S</span> accepts the proof, it proceeds by running the extractor for the proof of knowledge in order to obtain <span class="math">w</span>. In this way, <span class="math">S</span> obtains both <span class="math">A</span>’s view in the proof (from the initial verification), and the witness (from the extraction). Now, let <span class="math">p</span> denote the probability that <span class="math">A</span> convinces the verifier in the proof of knowledge. Then, by the definition of proofs of knowledge (Definition 9), there exists an extractor <span class="math">K</span> and a negligible function <span class="math">\\kappa</span> (this is the knowledge-error function) such that if <span class="math">p&gt;\\kappa</span>, then <span class="math">K</span> successfully obtains the witness <span class="math">w</span> within an expected number of steps bounded by <span class="math">poly/(p-\\kappa)</span>. Since the simulator runs the extractor with probability <span class="math">p</span> (i.e., only when the proof is accepted), we have that the overall expected running-time of the simulator is <span class="math">poly\\cdot p/(p-\\kappa)</span>. This brings us to the problem: the function <span class="math">p/(p-\\kappa)</span> may not be polynomial (for example, if <span class="math">\\kappa=2^{-n/2}</span> and <span class="math">p=2^{-n/2}+2^{-n}</span>, then <span class="math">p/(p-\\kappa)\\approx 2^{n/2}</span>).</p>

    <p class="text-gray-300">This technical difficulty was solved by Goldreich and Kahan in <em>[20]</em> and therefore there is no inherent problem here. However, their techniques are complex and thus applying them every time</p>

    <p class="text-gray-300">we wish to use a proof of knowledge as a subprotocol is cumbersome. Our goal here is therefore to present a general lemma that will enable the use of proofs of knowledge as subprotocols <em>without</em> requiring any complicated analysis. Rather, the analysis (indeed using the techniques of <em>[20]</em>) is needed only once in proving the lemma (established below).</p>

    <h4 id="sec-37" class="text-lg font-semibold mt-6">Witness-extended emulation.</h4>

    <p class="text-gray-300">Loosely speaking, the lemma below states that if there exists a proof of knowledge with a knowledge extractor <span class="math">K</span> (by Def. 9), then there exists an expected polynomial-time <em>witness-extended emulator</em> <span class="math">E</span> who outputs the information required by the simulator to continue the simulation of the larger protocol. That is, <span class="math">E</span> outputs the following two items:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>The honest verifier’s view of an execution of the proof of knowledge with the specified prover. (Recall that a party’s view contains the contents of its input, auxiliary input and random tape, along with the transcript of messages that it sees during the execution.)</li>

      <li>The witness for the statement being proved in the proof of knowledge (in the case that the above verifier view is accepting).</li>

    </ol>

    <p class="text-gray-300">We note that given such a lemma, simulation in the context of secure computation as described above is simple. The simulator <span class="math">S</span> runs the witness-extended emulator <span class="math">E</span> at the appropriate point and obtains the verifier’s view <span class="math">v</span> and (possibly) the secret witness <span class="math">w</span>. Simulator <span class="math">S</span> can then compute from <span class="math">v</span> whether or not the proof was accepting: if not, then <span class="math">S</span> halts. Otherwise, <span class="math">S</span> has the witness <span class="math">w</span> and can continue the simulation. In order to do this, it computes the transcript of messages <span class="math">t_{pok}</span> that the prover would receive in this execution (such a transcript can easily be computed from <span class="math">v</span>). Then, it feeds the prover with these messages and continues the simulation of the rest of the protocol, as required. We note that since <span class="math">E</span> runs in expected polynomial-time, the overall running-time of <span class="math">S</span> is also expected polynomial-time.</p>

    <h4 id="sec-38" class="text-lg font-semibold mt-6">Notation:</h4>

    <p class="text-gray-300">Recall that <span class="math">P^{<em>}_{x,y,r}</span> denotes the strategy of <span class="math">P^{</em>}</span> upon common-input <span class="math">x</span>, auxiliary-input <span class="math">y</span> and random-tape <span class="math">r</span>. We denote by <span class="math">\\mathsf{view}^{P^{<em>}_{x,y,r}}_{V}(x)</span> a random variable describing the view of the honest verifier <span class="math">V</span> in an execution of the proof of knowledge with <span class="math">P^{</em>}_{x,y,r}</span> (this random variable depends only on the coins of <span class="math">V</span>). Furthermore, let <span class="math">\\mathsf{accept}_{V}(\\cdot)</span> be a (deterministic) function that takes a specific verifier view, and outputs <span class="math">1</span> if and only if <span class="math">V</span> accepts in the proof execution in which this is its view.</p>

    <p class="text-gray-300">We are now ready to present the definition:</p>

    <h6 id="sec-39" class="text-base font-medium mt-4">Definition 10</h6>

    <p class="text-gray-300">(witness-extended emulator): Let <span class="math">R</span> be a binary relation and let <span class="math">(P,V)</span> be an interactive proof system. Consider a probabilistic expected polynomial-time machine <span class="math">E</span> that is given input <span class="math">x</span> and access to the oracle <span class="math">P^{<em>}_{x,y,r}</span>. Let <span class="math">E^{P^{</em>}_{x,y,r}}_{1}(x)</span> and <span class="math">E^{P^{<em>}_{x,y,r}}_{2}(x)</span> denote the random variables representing the first and second elements of the output of <span class="math">E</span>, respectively. We say that <span class="math">E</span> is a witness-extended emulator for <span class="math">(P,V)</span> and <span class="math">R</span> if for every interactive function <span class="math">P^{</em>}</span>, every <span class="math">x,y,r\\in\\{0,1\\}^{*}</span>, every polynomial <span class="math">p(\\cdot)</span> and all sufficiently large <span class="math">x</span>’s,</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">E_{1}</span> outputs the distribution of <span class="math">V</span>’s view in a real execution with <span class="math">P^{*}_{x,y,r}</span>. That is,</li>

    </ol>

    <p class="text-gray-300"><span class="math">\\left\\{E^{P^{<em>}_{x,y,r}}_{1}(x)\\right\\}_{x,y,r}\\equiv\\left\\{\\mathsf{view}^{P^{</em>}_{x,y,r}}_{V}(x)\\right\\}_{x,y,r}</span></p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>The probability that <span class="math">V</span>’s view (as output by <span class="math">E_{1}</span>) is accepting, and yet <span class="math">E_{2}</span> does not output a correct witness, is negligible. That is,</li>

    </ol>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$\\Pr[\\mathsf{accept}_{V}(E_{1}^{P^{<em>}_{x,y,r}}(x))=1\\ \\&\\ (x,E_{2}^{P^{</em>}_{x,y,r}}(x))\\not\\in R]<\\frac{1}{p(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)}$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">As we have mentioned above, the aim of this section is to prove the existence of a witness-extended emulator for <em>any</em> proof of knowledge. That is,</p>

    <h6 id="sec-40" class="text-base font-medium mt-4">Lemma 3.1 (witness-extended emulation lemma):</h6>

    <p class="text-gray-300">Let <span class="math">R</span> be a binary relation and let <span class="math">(P,V)</span> be a proof of knowledge for <span class="math">R</span> with negligible knowledge error. Then, there exists a witness-extended emulator <span class="math">E</span> for <span class="math">(P,V)</span> and <span class="math">R</span>.</p>

    <p class="text-gray-300">Proof: Let <span class="math">P^{<em>}_{x,y,r}</span> be an arbitrary prover strategy and let <span class="math">x</span> be the common-input (i.e., <span class="math">P^{</em>}</span> is proving that it knows <span class="math">w</span> such that <span class="math">(x,w)\\in R</span>). We construct a witness-extended emulator <span class="math">E</span> as follows: <span class="math">E</span> works in a similar manner as described in the motivating discussion above. That is, <span class="math">E</span> first verifies the proof from <span class="math">P^{<em>}_{x,y,r}</span> by playing the role of the honest verifier. Following this step, <span class="math">E</span> can already define its first output. That is, let <span class="math">r_{V}</span> be the coins used by <span class="math">E</span> in the verification of the proof from <span class="math">P^{</em>}_{x,y,r}</span>. Furthermore, let <span class="math">t</span> equal the transcript of messages received by <span class="math">V</span> in the proof execution between <span class="math">P^{<em>}_{x,y,r}</span> and <span class="math">V</span> (who has random-tape <span class="math">r_{V}</span>). Then, <span class="math">E</span> defines <span class="math">V</span>’s view as <span class="math">E_{1}=(x,r_{V},t)</span>. It is clear that the output of <span class="math">E_{1}</span> is distributed exactly like that of the honest verifier’s view (when interacting with <span class="math">P^{</em>}_{x,y,r}</span>). Therefore, at this point, item (1) of Definition 10 is fulfilled.</p>

    <p class="text-gray-300">Now, if the above proof is rejected by <span class="math">E</span> (i.e., if <span class="math">\\mathsf{accept}_{V}(E_{1})=0</span>), then <span class="math">E</span> outputs the pair <span class="math">(E_{1},\\bot)</span> and halts. In this case, there is no requirement that <span class="math">E</span> outputs a witness and this is therefore sufficient. Otherwise, if the proof is accepting (i.e., if <span class="math">\\mathsf{accept}_{V}(E_{1})=1</span>), then <span class="math">E</span> proceeds to attempt extraction of the witness. However, as described above, the naive approach of simply running the extractor <span class="math">K</span> for the proof of knowledge <span class="math">(P,V)</span> may result in <span class="math">E</span> not being expected polynomial-time.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">This problem is solved by ensuring that the extractor never runs “too long”. Specifically, as in <em>[20]</em>, <span class="math">E</span> first estimates the value of <span class="math">p(x,y,r)</span>, where <span class="math">p(x,y,r)</span> denotes the probability that <span class="math">P^{*}_{x,y,r}</span> successfully proves that it knows a witness <span class="math">w</span> such that <span class="math">(x,w)\\in R</span>. This is done by repeating the verification until a fixed polynomial number, denoted $q^{\\prime}(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math">, of successful verifications occur. By choosing </span>q^{\\prime}(\\cdot)<span class="math"> to be large enough, it is possible to obtain an estimate </span>\\tilde{p}<span class="math"> such that with overwhelming probability </span>\\tilde{p}<span class="math"> is within a constant factor of </span>p(x,y,r)<span class="math">. We set </span>q^{\\prime}<span class="math"> so that the probability that </span>\\tilde{p}<span class="math"> is not within a constant factor of </span>p<span class="math"> is at most </span>2^{-</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">^{2}}$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Next, <span class="math">E</span> repeats the following up to $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> times: </span>E<span class="math"> runs the extractor </span>K<span class="math"> and answers all of </span>K<span class="math">’s oracle queries with the oracle </span>P^{*}_{x,y,r}<span class="math">. However, </span>E<span class="math"> limits the number of steps taken by </span>K<span class="math"> to </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">{\\cdot}q(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)/\\tilde{p}<span class="math"> steps, where </span>q(\\cdot)<span class="math"> is the polynomial from the <em>validity</em> condition in the definition of proofs of knowledge (every extractor </span>K<span class="math"> has a single polynomial </span>q(\\cdot)<span class="math"> associated with it and so </span>E<span class="math"> can use the appropriate </span>q<span class="math">). Note that a call to the oracle </span>P^{*}_{x,y,r}<span class="math"> is counted by </span>E<span class="math"> as a single step. Now, if within this time </span>K<span class="math"> outputs a witness </span>w<span class="math">, then </span>E<span class="math"> outputs the pair </span>(E_{1},w)<span class="math"> (where </span>E_{1}<span class="math"> is the value generated above). (We note that </span>E<span class="math"> does not need to check if </span>w<span class="math"> is a valid witness because by the definition of </span>K<span class="math">, it only outputs valid witnesses.) If </span>K<span class="math"> does not output a witness within this time, then </span>E<span class="math"> aborts this attempt and tries again. As mentioned above, this is repeated up to </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> times; we stress that in each attempt, </span>K<span class="math"> is provided independent coins by </span>E<span class="math">. If the extractor </span>K<span class="math"> did not output a witness in any of the </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> attempts, then </span>E<span class="math"> halts and outputs the pair </span>(E_{1},\\mathsf{time-out})<span class="math">. We will show that this strategy ensures that the probability that </span>E<span class="math"> outputs </span>\\mathsf{time-out}$ is negligible.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">erefore, the probability that the initial verification of the proof succeeded, yet <span class="math">E</span> does not output a valid witness, is negligible.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">In addition to the above, <span class="math">E</span> keeps a count of the overall running time of <span class="math">K</span> and if it reaches $2^{</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}<span class="math"> steps, then it halts outputting the pair </span>(E_{1},\\texttt{time-out})<span class="math">. (This additional time-out is needed to ensure that </span>E<span class="math"> does not run too long in the case that the estimate </span>\\tilde{p}<span class="math"> is not within a constant factor of </span>p(x,y,r)<span class="math">. Recall that this “bad event” can only happen with probability </span>2^{-</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">^{2}}$.)</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">We first claim that <span class="math">E</span> runs in expected polynomial-time.</p>

    <h6 id="sec-41" class="text-base font-medium mt-4">Claim 3.2</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">The emulator <span class="math">E</span> runs in expected-time that is polynomial in $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Proof: Recall that <span class="math">E</span> initially verifies the proof provided by <span class="math">P^{*}_{x,y,r}</span>. Since <span class="math">E</span> merely plays an honest verifier, this takes a strict polynomial number of steps. Next, <span class="math">E</span> obtains an estimate <span class="math">\\tilde{p}</span> of <span class="math">p(x,y,r)</span>. This involves repeating the verification until $q^{\\prime}(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math"> successes are obtained. Therefore, the expected number of repetitions equals exactly </span>q^{\\prime}(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)/p(x,y,r)<span class="math"> (since the expected number of trials for a single success is </span>1/p(x,y,r)<span class="math">). After the estimation </span>\\tilde{p}<span class="math"> has been obtained, </span>E<span class="math"> runs the extractor </span>K<span class="math"> for a maximum of </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> times, each time for at most </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\cdot q(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)/\\tilde{p}$ steps.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Given the above, we are ready to compute the expected running-time of <span class="math">E</span>. In order to do this, we differentiate between two cases. In the first case, we consider what happens if <span class="math">\\tilde{p}</span> is <em>not</em> within a constant factor of <span class="math">p(x,y,r)</span>. The only thing we can say about <span class="math">E</span>’s running-time in this case is that it is bound by $2^{</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}<span class="math"> (since this is an overall bound on its running-time). However, since this event happens with probability at most </span>2^{-</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">^{2}}<span class="math">, this case adds only a negligible factor to the overall expected running-time. We now consider the second case, where </span>\\tilde{p}<span class="math"> <em>is</em> within a constant factor of </span>p(x,y,r)<span class="math">. In this case, we can bound the expected running-time of </span>E$ by</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$p(x,y,r)\\cdot\\left(\\frac{q^{\\prime}(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)}{p(x,y,r)}+\\frac{</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">^{2}q(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)}{\\tilde{p}}\\right)=\\mathrm{poly}(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)\\cdot\\frac{p(x,y,r)}{\\tilde{p}}=\\mathrm{poly}(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">and this concludes the analysis.</p>

    <p class="text-gray-300">As we have mentioned above, <span class="math">E</span>’s first output <span class="math">E_{1}</span> is distributed exactly as required by Definition 10. Therefore, it remains to show that the probability that <span class="math">P^{<em>}</span>’s proof is accepting and yet <span class="math">E</span> does not output a valid witness, is negligible. Notice that whenever <span class="math">P^{</em>}</span>’s proof is accepting, <span class="math">E</span> runs the extractor <span class="math">K</span> and either obtains a proper witness <span class="math">w</span> or it outputs time-out. That is, in the case of accepting proofs, if <span class="math">E</span> does not output time-out, then it outputs a proper witness. Therefore, it suffices to show that the probability that <span class="math">E</span> outputs time-out is negligible.</p>

    <h6 id="sec-42" class="text-base font-medium mt-4">Claim 3.3</h6>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">The probability that <span class="math">E</span> outputs the time-out symbol is a negligible function in $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Proof: Notice that the probability that <span class="math">E</span> outputs time-out is less than or equal to the probability that the extractor <span class="math">K</span> does not succeed in outputting a witness <span class="math">w</span> in any of the $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> extraction attempts <em>plus</em> the probability that </span>E<span class="math"> runs for </span>2^{</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}$ steps.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">We first claim that the probability that <span class="math">E</span> runs for $2^{</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}<span class="math"> steps is negligible. We have already shown in Claim 3.2, that </span>E<span class="math"> runs in expected polynomial-time. Therefore, the probability that an execution will deviate so far from its expectation and run for </span>2^{</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}$ steps is negligible. (It is enough to use Markov’s inequality to establish this fact.)</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">We now continue by considering the probability that in all $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> extraction attempts, the extractor </span>K<span class="math"> does not output a witness within </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">q(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)/\\tilde{p}<span class="math"> steps. Consider the following two possible cases (recall that </span>p(x,y,r)<span class="math"> equals the probability that </span>P^{*}_{x,y,r}<span class="math"> succeeds in proving the proof, and that </span>\\kappa$ is the negligible knowledge-error function of the proof system):</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">1. Case 1: $p(x,y,r) \\leq 2\\kappa(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math">: In this case, </span>P^*<span class="math"> succeeds in proving the proof with only negligible probability. This means that the probability that </span>E<span class="math"> even reaches the stage that it runs </span>K<span class="math"> is negligible (and thus </span>E$ outputs time-out with negligible probability only).</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">2. Case 2: $p(x,y,r) &gt; 2\\kappa(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math">: Recall that by the definition of proofs of knowledge, the polynomial </span>q(\\cdot)<span class="math"> is such that the expected number of steps taken by </span>K<span class="math"> to output a witness is at most </span>q(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)/(p(x,y,r) - \\kappa(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">))<span class="math">. Now, since in this case </span>p(x,y,r) &gt; 2\\kappa(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math">, it holds that the expected number of steps required by </span>K<span class="math"> is less than </span>2q(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)/p(x,y,r)<span class="math">. Assuming that </span>\\tilde{p}<span class="math"> is within a constant factor of </span>p(x,y,r)<span class="math">, we have that the expected number of steps is bound by </span>O(q(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)/\\tilde{p})<span class="math">. Therefore, by Markov&#x27;s inequality, the probability that </span>K<span class="math"> runs longer than </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">q(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)/\\tilde{p}<span class="math"> steps in any given extraction attempt is at most </span>O(1/</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math">. It follows that the probability that </span>K<span class="math"> runs longer than this time in </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> independent attempts is negligible in </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> (specifically, it is bound by </span>O(1/</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)^{</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}<span class="math">). This covers the case that </span>\\tilde{p}<span class="math"> is within a constant factor of </span>p(x,y,r)<span class="math">. However, the probability that </span>\\tilde{p}<span class="math"> is not within a constant factor of </span>p(x,y,r)<span class="math"> is also negligible. Putting this together, we have that </span>E$ outputs time-out with negligible probability only.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">This completes the proof of the lemma.</p>

    <h2 id="sec-43" class="text-2xl font-bold">3.3.1 On Expected Polynomial-Time Provers</h2>

    <p class="text-gray-300">We proved the witness-extended emulation lemma for <span class="math">E</span> who is given oracle access to an arbitrary prover strategy <span class="math">P^{<em>}</span>. However, in some contexts (like in secure computation), <span class="math">P^{</em>}</span> runs in polynomial-time (or expected polynomial-time) and the running-time of <span class="math">P^{<em>}</span> must be included in the analysis of the overall running-time of <span class="math">E</span>. This is a non-issue in the case that <span class="math">P^{</em>}</span> runs in strict polynomial-time, because we can simply multiply <span class="math">E</span>'s running-time by a single polynomial. However, in the case that <span class="math">P^{*}</span> runs in expected polynomial-time, we must ensure that <span class="math">E</span> remains expected polynomial-time. This extension must therefore be dealt with when considering expected polynomial-time adversaries (as is the case in this work). The only place that this arises in the proof of Lemma 3.1 is in Claim 3.2 (i.e., when analyzing the running-time of <span class="math">E</span>).</p>

    <p class="text-gray-300">We therefore reprove Claim 3.2 here for the case that <span class="math">P^<em></span> is an expected polynomial-time machine. Before proceeding, we note that in this context, it does not make sense to consider a prover <span class="math">P_{x,y,r}^</em></span> with fixed randomness. This is because the expectation with respect to <span class="math">P^<em></span>'s running-time is taken over its random-tape <span class="math">r</span> as well.⁶ Thus, we modify <span class="math">E</span> so that it first chooses a random-tape <span class="math">r</span> for <span class="math">P_{x,y}^</em></span>. This then defines the prover <span class="math">P_{x,y,r}^<em></span>, and <span class="math">E</span> continues as described above. We consider extractors who receive <span class="math">P_{x,y,r}^</em></span> and whose expected running-time is polynomial, when the expectation is both over its own coins and the choice of <span class="math">r</span>. Furthermore, we include the running-time of <span class="math">P_{x,y,r}^<em></span> in the running-time of the extractor. We say that such an extractor "remains expected polynomial-time when including the running-time of an expected polynomial-time prover <span class="math">P^</em></span>". We note that in Appendix A, we show that such extractors exist (in particular, the extractor of the [17] zero-knowledge arguments of knowledge).</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">⁶ We stress that for a fixed <span class="math">r</span>, the machine <span class="math">P_{x,y,r}^{*}</span> may not be polynomial-time. For example, consider a prover which runs in strict polynomial-time except when its random-tape equals a single random string <span class="math">r</span>, in which case it runs for $2^{</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">r</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}<span class="math"> steps. Clearly, </span>P_{x,y}^{<em>}<span class="math"> runs in expected polynomial-time. However, the machine </span>P_{x,y,r}^{</em>}<span class="math"> with fixed random-tape </span>r$, runs for an exponential number of steps.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Claim 3.4 (Claim 3.2 - restated): If <span class="math">K</span> remains expected polynomial-time when including the running-time of an expected polynomial-time prover <span class="math">P^{<em>}</span>, then <span class="math">E</span> also remains expected polynomial-time when including the running-time of an expected polynomial-time prover <span class="math">P^{</em>}</span>.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Proof: Recall that <span class="math">E</span> first verifies the proof from <span class="math">P_{x,y,r}^{*}</span> and if it is accepting then repeats the verification until $q^{\\prime}(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math"> successes (i.e., accepting executions) are obtained. Following this, </span>E<span class="math"> runs the knowledge extractor </span>K<span class="math"> for exactly </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> extraction attempts (while limiting its running-time). By the assumption that </span>K<span class="math"> remains expected polynomial-time in this setting, it is enough to analyze the running-time of the first stage of </span>E<span class="math">. We stress that in each verification of the first stage of the emulation, the emulator </span>E<span class="math"> (playing the verifier) uses new and independent randomness, whereas </span>P_{x,y}^{<em>}<span class="math">&#x27;s randomness is fixed to </span>r<span class="math">. In Claim A.1, we show that the expected running-time for obtaining a single success in such a scenario, is </span>1/p(x,y,r)<span class="math"> times the expected running-time of </span>P_{x,y,r}^{</em>}<span class="math"> (where the expectation is taken over </span>K<span class="math">&#x27;s coins). Therefore, by the linearity of expectations, the expected running-time for obtaining </span>q^{\\prime}(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math"> successes equals </span>q^{\\prime}(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)/p(x,y,r)<span class="math"> times the expected running-time of </span>P_{x,y,r}^{*}<span class="math">. By continuing as in the analysis of Claim 3.2, we obtain that </span>E<span class="math"> concludes the first stage in an expected number of steps bounded by </span>\\mathrm{poly}(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math"> times the expected running-time of </span>P_{x,y,r}^{<em>}<span class="math">. We conclude by noticing that since </span>P_{x,y}^{</em>}<span class="math">&#x27;s random-tape </span>r<span class="math"> is chosen uniformly, the overall expected running-time of </span>E<span class="math"> (where the expectation is also over the choice of </span>r<span class="math">) is bounded by </span>\\mathrm{poly}(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math"> times the expected running-time of </span>P_{x,y}^{*}$, as required.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h2 id="sec-45" class="text-2xl font-bold">4.1 The Compiler of Goldreich, Micali and Wigderson [25]</h2>

    <p class="text-gray-300">Goldreich, Micali and Wigderson [25] showed that assuming the existence of trapdoor permutations, there are secure protocols (in the malicious model) for any multi-party functionality. Their methodology works by first presenting a protocol secure against semi-honest adversaries. Next, a compiler is applied that transforms any protocol secure against semi-honest adversaries into a protocol secure against malicious adversaries. Thus, their compiler can also be applied to the constant-round two-party protocol of Yao [33] (as it is secure against semi-honest adversaries). However, as we shall see, the output protocol has a polynomial number of rounds, and in particular is not constant-round. In this section, we describe the [25] compiler and show what should be modified in order to obtain a constant-round compiler instead.</p>

    <p class="text-gray-300">Enforcing semi-honest behavior. The GMW compiler takes for input a protocol secure against semi-honest adversaries; from here on we refer to this as the "basic protocol". Recall that this protocol is secure in the case that each party follows the protocol specification exactly, using its input and uniformly chosen random tape. Thus, in order to obtain a protocol secure against malicious adversaries, we need to enforce potentially malicious parties to behave in a semi-honest manner. First and foremost, this involves forcing the parties to follow the prescribed protocol. However, this only makes sense relative to a given input and random tape. Furthermore, a malicious party must be forced into using a uniformly chosen random tape. This is because the security of the basic protocol may depend on the fact that the party has no freedom in setting its own randomness.7</p>

    <p class="text-gray-300">7A clear example of this is the semi-honest 1-out-of-<span class="math">k</span> Oblivious Transfer protocol of [14] (see Construction 2.2.5 in [18]). The oblivious transfer functionality is defined by <span class="math">((b_{1},\\ldots ,b_{k}),i)\\mapsto (\\lambda ,b_{i})</span>. In the [14] protocol, the receiver gives the sender <span class="math">k</span> images of a trapdoor permutation, where the receiver knows only the <span class="math">i</span>'th preimage. The protocol</p>

    <p class="text-gray-300">An informal description of the GMW compiler. In light of the above discussion, the compiler begins by having each party commit to its input. Next, the parties run a coin-tossing protocol in order to fix their random tapes (clearly, this protocol must be secure against malicious adversaries). A regular coin-tossing protocol in which both parties receive the same uniformly distributed string is not sufficient here. This is because the parties' random tapes must remain secret. This is solved by augmenting the coin-tossing protocol so that one party receives a uniformly distributed string (to be used as its random tape) and the other party receives a commitment to that string. Now, following these two steps, each party holds its own uniformly distributed random-tape and a commitment to the other party's input and random-tape. Therefore, each party can be "forced" into working consistently with the committed input and random-tape.</p>

    <p class="text-gray-300">We now describe how this behavior is enforced. A protocol specification is a deterministic function of a party's view consisting of its input, random tape and messages received so far. As we have seen, each party holds a commitment to the input and random tape of the other party. Furthermore, the messages sent so far are public. Therefore, the assertion that a new message is computed according to the protocol is of the  <span class="math">\\mathcal{NP}</span>  type (and the party sending the message knows an adequate NP-witness to it). Thus, the parties can use zero-knowledge proofs to show that their steps are indeed according to the protocol specification. As the proofs used are zero-knowledge, they reveal nothing. On the other hand, due to the soundness of the proofs, even a malicious adversary cannot deviate from the protocol specification without being detected. We thus obtain a reduction of the security in the malicious case to the given security of the basic protocol against semi-honest adversaries.</p>

    <p class="text-gray-300">In summary, the components of the compiler are as follows (from here on "secure" refers to security against malicious adversaries):</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Input Commitment: In this phase the parties execute a secure protocol for the following functionality:</li>

    </ol>

    <div class="my-4 text-center"><span class="math-block">\\left(\\left(x, r\\right), 1 ^ {n}\\right) \\mapsto (\\lambda , C (x; r))</span></div>

    <p class="text-gray-300">where  <span class="math">x</span>  is the party's input string (and  <span class="math">r</span>  is the randomness chosen by the committing party).</p>

    <p class="text-gray-300">A secure protocol for this functionality involves the committing party sending  <span class="math">C(x; r)</span>  to the other party followed by a zero-knowledge proof of knowledge of  <span class="math">(x, r)</span> . Informally, this functionality ensures that the committing party "knows" the value being committed to.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Coin Generation: The parties generate  <span class="math">t</span> -bit long random tapes (and corresponding commitments) by executing a secure protocol in which one party receives a commitment to a uniform string of length  <span class="math">t</span>  and the other party receives the string itself (to be used as its</li>

    </ol>

    <p class="text-gray-300">works so that the receiver obtains  <span class="math">b_{j}</span>  if it knows the  <span class="math">j</span> 'th preimage (and otherwise he learns nothing of the value of  <span class="math">b_{j}</span> ). Thus, were the receiver to know more than one of the  <span class="math">k</span>  preimages, it would learn more than a single  <span class="math">b_{i}</span> , in contradiction to the security of the protocol. Now, if the receiver can "alter" its random tape, then it can influence the choice of the images of the permutation so that it knows more than one preimage. Thus, the fact that the receiver uses a truly random tape is crucial to the security.</p>

    <p class="text-gray-300">We mention that the above weakness is not a peculiarity of the above specific protocol, and in fact, any protocol secure in the face of semi-honest adversaries can be modified so that it is still "semi-honest" secure, yet is completely breakable in the case that the adversary can fix its own randomness. In order to see this, consider the following addition to the beginning of some semi-honest protocol. One of the parties (say Party 1) randomly chooses an  <span class="math">n</span> -bit string. Then, if the string is all zeros, it asks Party 2 for its input (and Party 2 complies). Otherwise, the parties continue with the original protocol. Since a semi-honest adversary cannot fix its own randomness (and cannot request Party 2's input if the random string is not all zeros), the modified protocol is still secure. On the other hand, if Party 1 is adversarial and can fix its own randomness, then it can obtain Party 2's input, in contradiction to the security requirements.</p>

    <p class="text-gray-300">random tape) and the decommitment (to be used later for proving “proper behavior”). That is, the parties compute the functionality:</p>

    <p class="text-gray-300"><span class="math">(1^{n},1^{n})\\mapsto((U_{t},U_{t\\cdot n}),C(U_{t};U_{t\\cdot n}))</span></p>

    <p class="text-gray-300">(where we assume that to commit to a <span class="math">t</span>-bit string, <span class="math">C</span> requires <span class="math">t\\cdot n</span> random bits).</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Protocol Emulation: In this phase, the parties run the basic protocol whilst proving (in zero-knowledge) that their steps are consistent with their input string, random tape and prior messages received.</li>

    </ol>

    <p class="text-gray-300">A detailed description of each phase of the compiler and a full proof that the resulting protocol is indeed secure against malicious adversaries can be found in <em>[18]</em>.</p>

    <h3 id="sec-46" class="text-xl font-semibold mt-8">4.2 Achieving Round-Preserving Compilation</h3>

    <p class="text-gray-300">As we have mentioned, our aim in this work is to show that the GMW compiler can be implemented so that the number of rounds in the resulting compiled protocol is within a constant factor of the number of rounds in the original semi-honest protocol. We begin by noting that using currently known constructions, Phases 1 and 3 of the GMW compiler can be implemented in a constant number of rounds. That is,</p>

    <h6 id="sec-47" class="text-base font-medium mt-4">Proposition 4.1</h6>

    <p class="text-gray-300">Assuming the existence of one-way functions, both the input-commitment and protocol-emulation phases can be securely implemented in a constant number of rounds.</p>

    <p class="text-gray-300">First consider the input-commitment phase. As mentioned above, this phase can be securely implemented by having the committing party send a perfectly binding commitment of its input to the other party, followed by a zero-knowledge proof of knowledge of the committed value. Both constant-round commitment schemes and constant-round zero-knowledge arguments of knowledge are known to exist by the works of Naor <em>[28]</em> and Feige and Shamir <em>[17]</em>, respectively (these constructions can also be based on any one-way function). Thus the input-commitment phase can be implemented as required for Proposition 4.1. Next, we recall that a secure implementation of the protocol emulation phase requires zero-knowledge proofs for <span class="math">\\mathcal{NP}</span> only. Thus, once again, using the argument system of <em>[17]</em>, this can be implemented in a constant number of rounds (using any one-way function).</p>

    <h4 id="sec-48" class="text-lg font-semibold mt-6">Constant-round coin tossing.</h4>

    <p class="text-gray-300">In contrast to the input-commitment and protocol-emulation phases of the GMW compiler, known protocols for tossing polynomially many coins do not run in a constant number of rounds. Rather, single coins are tossed sequentially (and thus <span class="math">poly(n)</span> rounds are needed). In particular, the proof of <em>[18]</em> does not extend to the case that many coins are tossed in parallel. Thus, in order to obtain a round-preserving compiler, it remains to present a secure protocol for the coin-generation functionality that works in a constant number of rounds. Furthermore, it is preferable to base this protocol on the existence of one-way functions only (so that this seemingly minimal assumption is all that is needed for the entire compiler). In the next section we present such a coin-tossing protocol, thereby obtaining Theorem 2 (as stated in the introduction).</p>

    <p class="text-gray-300">4.3 Constant-Round Secure Computation</p>

    <p class="text-gray-300">Recall that by Yao <em>[33]</em>, assuming the existence of trapdoor permutations, any two-party functionality can be securely computed in the semi-honest model in a constant-number of rounds. Thus, applying the constant-round compiler of Theorem 2 to Yao’s protocol, we obtain a constant-round protocol that is secure in the malicious model, and prove Theorem 3. That is, assuming the existence of trapdoor permutations, any two-party functionality can be securely computed in the malicious model in a constant-number of rounds.</p>

    <h2 id="sec-49" class="text-2xl font-bold">5 The Augmented Coin-Tossing Protocol</h2>

    <p class="text-gray-300">In this section we present our coin-tossing protocol, thus proving Theorem 1.</p>

    <h3 id="sec-50" class="text-xl font-semibold mt-8">5.1 The Augmented Coin-Tossing Functionality</h3>

    <p class="text-gray-300">In a basic coin-tossing functionality, both parties receive identical uniformly distributed strings. That is, the functionality is defined as: <span class="math">(1^{n},1^{n})\\mapsto(U_{m},U_{m})</span> for some <span class="math">m=\\operatorname{poly}(n)</span>. This basic coin-tossing is augmented as follows. Let <span class="math">F</span> be any deterministic function. Then, define the augmented coin-tossing functionality by:</p>

    <p class="text-gray-300"><span class="math">(1^{n},1^{n})\\mapsto(U_{m},F(U_{m}))</span></p>

    <p class="text-gray-300">That is, the first party indeed receives a uniformly distributed string. However, the second party receives <span class="math">F</span> applied to that string (rather than the string itself). Setting <span class="math">F</span> to the identity function, we obtain basic coin-tossing. However, recall that the coin-generation component of the GMW compiler requires the following functionality:</p>

    <p class="text-gray-300"><span class="math">(1^{n},1^{n})\\mapsto((U_{t},U_{t\\cdot n}),C(U_{t};U_{t\\cdot n}))</span></p>

    <p class="text-gray-300">where <span class="math">C</span> is a commitment scheme (and we assume that <span class="math">C</span> requires <span class="math">n</span> random bits for every bit committed to). Then, this functionality can be realized with our augmentation by setting <span class="math">m=t+t\\cdot n</span> and <span class="math">F(U_{m})=C(U_{t};U_{t\\cdot n})</span>. Thus, the second party receives a commitment to a uniformly distributed string of length <span class="math">t</span> and the first party receives the string and its decommitment. Recall that in the compiler, the party uses the <span class="math">t</span>-bit string as its random tape and the decommitment in order to prove in zero-knowledge that it is acting consistently with this random tape (and its input).</p>

    <h3 id="sec-51" class="text-xl font-semibold mt-8">5.2 Motivating Discussion</h3>

    <p class="text-gray-300">In order to motivate our construction of a constant-round coin-tossing protocol, we consider the special case of basic coin-tossing (i.e., where <span class="math">F</span> is the identity function). A natural attempt at a coin-tossing protocol follows:</p>

    <h6 id="sec-52" class="text-base font-medium mt-4">Protocol 1</h6>

    <p class="text-gray-300">(Attempt at Basic Coin-Tossing):</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Party <span class="math">1</span> chooses a random string <span class="math">s_{1}\\in_{R}\\{0,1\\}^{m}</span> and sends <span class="math">c=\\operatorname{Commit}(s_{1})=C(s_{1};r)</span> for a random <span class="math">r</span>.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Party <span class="math">2</span> chooses a random string <span class="math">s_{2}\\in_{R}\\{0,1\\}^{m}</span> and sends it to Party <span class="math">1</span>.</li>

      <li>Party <span class="math">1</span> decommits to <span class="math">s_{1}</span> sending the pair <span class="math">(s_{1},r)</span>.</li>

    </ol>

    <p class="text-gray-300">Party <span class="math">1</span> always outputs <span class="math">s\\stackrel{{\\scriptstyle\\rm def}}{{=}}s_{1}\\oplus s_{2}</span>, whereas Party <span class="math">2</span> outputs <span class="math">s_{1}\\oplus s_{2}</span> if Party <span class="math">1</span>’s decommitment is correct and <span class="math">\\perp</span> otherwise.</p>

    <p class="text-gray-300">We note that when <span class="math">m=1</span> (i.e., a single bit), the above protocol is the basic coin-tossing protocol of Blum <em>[8]</em> (a rigorous proof of the security of this protocol can be found in <em>[18]</em>). However, here we are interested in a parallelized version where the parties attempt to simultaneously generate an <span class="math">m</span>-bit random string (for any <span class="math">m=\\operatorname{poly}(n)</span>). Intuitively, due to the secrecy of the commitment scheme, the string <span class="math">s_{2}</span> chosen by (a possibly malicious) Party <span class="math">2</span> cannot be dependent on the value of <span class="math">s_{1}</span>. Thus if <span class="math">s_{1}</span> is chosen uniformly, the resulting string <span class="math">s=s_{1}\\oplus s_{2}</span> is close to uniform. On the other hand, consider the case that Party <span class="math">1</span> may be malicious. Then, by the protocol, Party <span class="math">1</span> is committed to <span class="math">s_{1}</span> before Party <span class="math">2</span> sends <span class="math">s_{2}</span>. Thus, if <span class="math">s_{2}</span> is chosen uniformly, the string <span class="math">s=s_{1}\\oplus s_{2}</span> is uniformly distributed. We note that due to the binding property of the commitment scheme, Party <span class="math">1</span> cannot alter the initial string committed to. We conclude that neither party is able to bias the output string.</p>

    <p class="text-gray-300">However, the infeasibility of either side to bias the resulting string is not enough to show that the protocol is secure. This is because the definition of secure computation requires that the protocol simulate an ideal execution in which a trusted third party chooses a random string <span class="math">s</span> and gives it to both parties. Loosely speaking, this means that there exists a simulator that works in the ideal model and simulates an execution with a (possibly malicious) party such that the joint output distribution (in this ideal scenario) is indistinguishable from when the parties execute the real protocol.</p>

    <p class="text-gray-300">Protocol <span class="math">1</span> seems not to fulfill this more stringent requirement. That is, our problem in proving the security of Protocol <span class="math">1</span> is with constructing the required simulator. The main problem that occurs is regarding the simulation of Party <span class="math">2</span>.</p>

    <h4 id="sec-53" class="text-lg font-semibold mt-6">Simulating a malicious Party <span class="math">2</span>:</h4>

    <p class="text-gray-300">The simulator receives a uniformly distributed string <span class="math">s</span> from the trusted party and must generate an execution consistent with <span class="math">s</span>. That is, the commitment <span class="math">c=C(s_{1})</span> given by the simulator to Party <span class="math">2</span> must be such that <span class="math">s_{1}\\oplus s_{2}=s</span> (where <span class="math">s_{2}</span> is the string sent by Party <span class="math">2</span> in Step <span class="math">2</span> of the protocol). However, <span class="math">s_{1}</span> is chosen and fixed (via a perfectly binding commitment) before <span class="math">s_{2}</span> is chosen by Party <span class="math">2</span>. Since the commitment is perfectly binding, even an all-powerful simulator cannot “cheat” and decommit to a different value. This problem is compounded by the fact that Party <span class="math">2</span> may choose <span class="math">s_{2}</span> based on the commitment received to <span class="math">s_{1}</span> (by say invoking a pseudorandom function on <span class="math">c</span>). Therefore, rewinding Party <span class="math">2</span> and setting <span class="math">s_{1}</span> to equal <span class="math">s\\oplus s_{2}</span> will not help (as <span class="math">s_{2}</span> will change and thus once again <span class="math">s_{1}\\oplus s_{2}</span> will equal <span class="math">s</span> with only negligible probability). We note that this problem does not arise in the single-bit case as there are only two possible values for <span class="math">s_{2}</span> and thus the simulator succeeds with probability <span class="math">1/2</span> each time.</p>

    <h4 id="sec-54" class="text-lg font-semibold mt-6">A problem relating to abort:</h4>

    <p class="text-gray-300">The above problem arises even when the parties never abort. However, another problem in simulation arises due to the ability of the parties to abort. In particular, simulation of Party <span class="math">1</span> in Protocol <span class="math">1</span> is easy assuming that Party <span class="math">1</span> never aborts. On the other hand, when Party <span class="math">1</span>’s abort probability is unknown (and specifically when it is neither negligible nor noticeable), we do not know how to construct a simulator that does not skew the real probability of abort in the simulated execution. Once again, this problem is considerably easier in the single-bit</p>

    <p class="text-gray-300">case since Party 1’s decision of whether or not to abort is based on only a single bit sent by Party 2 in Step 2 of the protocol.</p>

    <p class="text-gray-300">We note that basic coin-tossing is a special case of the augmented coin-tossing functionality. Thus, the same problems (and possibly others) must be solved in order to obtain an augmented coin-tossing protocol. As we will show, our solutions for these problems are enough for the augmented case as well.</p>

    <h3 id="sec-55" class="text-xl font-semibold mt-8">5.3 The Actual Protocol</h3>

    <p class="text-gray-300">Before presenting the protocol itself, we discuss how we solve the problems described in the above motivating discussion.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><em>Party <span class="math">1</span> is malicious:</em> As described, when Party <span class="math">1</span> is malicious, the problem that arises is that of aborting. In particular, Party <span class="math">1</span> may decide to abort depending on the string <span class="math">s_{2}</span> sent to it by Party <span class="math">2</span>. This causes a problem in ensuring that the probability of abort in the simulation is negligibly close to that in a real execution. This is solved by having Party <span class="math">1</span> send a proof of knowledge of <span class="math">s_{1}</span> after sending the commitment. Then, the simulator can extract <span class="math">s_{1}</span> from the proof of knowledge and can send <span class="math">s_{2}=s_{1}\\oplus s</span> (where <span class="math">s</span> is the string chosen by the trusted party) without waiting for Party <span class="math">1</span> to decommit in a later step.</li>

      <li><em>Party <span class="math">2</span> is malicious:</em> As described, the central problem here is that Party <span class="math">1</span> must commit itself to <span class="math">s_{1}</span> before <span class="math">s_{2}</span> is known (yet <span class="math">s_{1}\\oplus s_{2}</span> must equal <span class="math">s</span>). This cannot be solved by rewinding because Party <span class="math">2</span> may choose <span class="math">s_{2}</span> based on the commitment to <span class="math">s_{1}</span> that it receives (and thus changing the commitment changes the value of <span class="math">s_{2}</span>). We solve this problem by not having Party <span class="math">1</span> decommit at all; rather, it sends <span class="math">s=s_{1}\\oplus s_{2}</span> (or <span class="math">F(s_{1}\\oplus s_{2})</span> in the augmented case) and proves in zero-knowledge that the value sent is consistent with its commitment and <span class="math">s_{2}</span>. Thus, the simulator (who can generate proofs to false statements of this type) is able to “cheat” and send <span class="math">s</span> (or <span class="math">F(s)</span>) irrespective of the <em>real</em> value committed to in Step 1.</li>

    </ul>

    <p class="text-gray-300">This technique of not decommitting, but rather revealing the committed value and proving (in zero-knowledge) that this value is correct, is central to our simulation strategy. Specifically, it enables us to “decommit” to a value that is unknown at the time of the commitment. (As we have mentioned, in order for the simulation to succeed, Party <span class="math">2</span> must be convinced that the commitment of Step <span class="math">1</span> is to <span class="math">s_{1}</span>, where <span class="math">s_{1}\\oplus s_{2}=s</span>. However, the correct value of <span class="math">s_{1}</span> is only known to the simulator after Step <span class="math">2</span>.)</p>

    <p class="text-gray-300">We now present our constant-round protocol for the augmented secure coin-tossing functionality: <span class="math">(1^{n},1^{n})\\mapsto(U_{m},F(U_{m}))</span>, for <span class="math">m=\\text{poly}(n)</span>. For the sake of simplicity, our presentation uses a non-interactive commitment scheme (which is easily constructed given any 1–1 one-way function). However, the protocol can easily be modified so that an interactive commitment scheme is used instead (in particular, the two-round scheme of Naor <em>[28]</em>, which is based on any one-way function).</p>

    <h6 id="sec-56" class="text-base font-medium mt-4">Protocol 2 (Augmented Parallel Coin-Tossing):</h6>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Party <span class="math">1</span> chooses <span class="math">s_{1}\\in_{R}\\{0,1\\}^{m}</span> and sends <span class="math">c=C(s_{1};r)</span> for a random <span class="math">r</span> to Party <span class="math">2</span> (using a perfectly binding commitment scheme).</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Party 1 proves knowledge of <span class="math">(s_1, r)</span> with a (constant round) zero-knowledge argument of knowledge with negligible error. If the proof fails, then Party 2 aborts with output <span class="math">\\bot</span>.</li>

      <li>Party 2 chooses <span class="math">s_2 \\in_{R} \\{0,1\\}^m</span> and sends <span class="math">s_2</span> to Party 1.</li>

      <li>If until this point Party 1 received an invalid message from Party 2, then Party 1 aborts, outputting <span class="math">\\bot</span>.</li>

    </ol>

    <p class="text-gray-300">Otherwise, Party 1 sends <span class="math">y = F(s_1 \\oplus s_2)</span>.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Party 1 proves to Party 2 using a (constant round) zero-knowledge argument that there exists a pair <span class="math">(s_1, r)</span> such that <span class="math">c = C(s_1; r)</span> and <span class="math">y = F(s_1 \\oplus s_2)</span> (that is, Party 1 proves that <span class="math">y</span> is consistent with <span class="math">c</span> and <span class="math">s_2</span>). If the proof fails, then Party 2 aborts with output <span class="math">\\bot</span>.</li>

      <li>Output:</li>

    </ol>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Party 1 outputs <span class="math">s_1 \\oplus s_2</span> (even if Party 2 fails to correctly complete the verification of the proof in Step 5).</li>

      <li>Party 2 outputs <span class="math">y</span>.</li>

    </ul>

    <p class="text-gray-300">Round complexity: Using the constant-round zero-knowledge argument system of Feige and Shamir [17] and the constant-round commitment scheme of Naor [28], Protocol 2 requires a constant number of rounds only. We stress that the argument system of [17] is also an argument of knowledge.</p>

    <p class="text-gray-300">Sufficient assumptions: All the components of Protocol 2 can be implemented using one-way functions. In particular the string commitment of Naor [28] can be used (this requires an additional pre-step in which Party 2 sends a random string to Party 1; however this step is of no consequence to the proof). Furthermore, the zero-knowledge argument of knowledge of [17] can be used in both Steps 2 and 5. Since both the [28] and [17] protocols only assume the existence of one-way functions, this is the only assumption required for the protocol.</p>

    <p class="text-gray-300">Theorem 11 Assuming the existence of one-way functions, Protocol 2 is a secure protocol for augmented parallel coin-tossing.</p>

    <p class="text-gray-300">Proof: We need to show how to efficiently transform any admissible pair of machines <span class="math">(A_1, A_2)</span> for the real model into an admissible pair of machines <span class="math">(B_1, B_2)</span> for the ideal model. We denote the trusted third party by <span class="math">T</span>, the coin-tossing functionality by <span class="math">f</span> and Protocol 2 by <span class="math">\\Pi</span>. We first consider the case that <span class="math">A_1</span> is adversarial.</p>

    <p class="text-gray-300">22</p>

    <h6 id="sec-57" class="text-base font-medium mt-4">Lemma 5.1</h6>

    <p class="text-gray-300">Let <span class="math">(A_{1},A_{2})</span> be a pair of probabilistic expected polynomial-time machines for the real model in which <span class="math">A_{2}</span> is honest. Then there exists a pair of probabilistic expected polynomial-time machines <span class="math">(B_{1},B_{2})</span> for the ideal model in which <span class="math">B_{2}</span> is honest, such that</p>

    <p class="text-gray-300"><span class="math">\\left\\{\\text{\\sc ideal}_{f,\\overline{B}}(1^{n},1^{n})\\right\\}_{n\\in\\mathsf{N}}\\stackrel{{\\scriptstyle\\scriptscriptstyle{\\mathbb{E}}}}{{=}}\\left\\{\\text{\\sc real}_{\\Pi,\\overline{A}}(1^{n},1^{n})\\right\\}_{n\\in\\mathsf{N}}</span></p>

    <p class="text-gray-300">Proof: In this case the second party is honest and thus <span class="math">B_{2}</span> is determined. We now transform the real-model adversary <span class="math">A_{1}</span> into an ideal-model adversary <span class="math">B_{1}</span>, where the transformation is such that <span class="math">B_{1}</span> uses black-box access to <span class="math">A_{1}</span>. Specifically, <span class="math">B_{1}</span> chooses a uniform random tape, denoted <span class="math">R</span>, for <span class="math">A_{1}</span> and invokes <span class="math">A_{1}</span> on input <span class="math">1^{n}</span> and random tape <span class="math">R</span>. Once the input and random tape are fixed, <span class="math">A_{1}</span> is a deterministic function of messages received during a protocol execution. Thus <span class="math">A_{1}(1^{n},R,\\overline{m})</span> denotes the message sent by <span class="math">A_{1}</span> with input <span class="math">1^{n}</span>, random-tape <span class="math">R</span> and sequence <span class="math">\\overline{m}</span> of incoming messages to <span class="math">A_{1}</span>.</p>

    <p class="text-gray-300">The transformation works by having <span class="math">B_{1}</span> emulate an execution of <span class="math">A_{1}</span>, while playing <span class="math">A_{2}</span>’s role. Machine <span class="math">B_{1}</span> does this when interacting with the trusted third party <span class="math">T</span> and its aim is to obtain an execution with <span class="math">A_{1}</span> that is consistent with the output received from <span class="math">T</span>. Therefore, <span class="math">B_{1}</span> has both external communication with <span class="math">T</span> and “internal” emulated communication with <span class="math">A_{1}</span>. Machine <span class="math">B_{1}</span> works as follows:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>The ideal adversary <span class="math">B_{1}</span> chooses a uniformly distributed random tape <span class="math">R</span> for the real adversary <span class="math">A_{1}</span>, invokes the function <span class="math">A_{1}(1^{n},R)</span>, and obtains <span class="math">c</span> (where <span class="math">c</span> is supposed to equal <span class="math">C(s_{1};r)</span> for some <span class="math">s_{1}\\in\\{0,1\\}^{m}</span>, and <span class="math">C</span> is the perfectly binding commitment scheme specified in the protocol).</li>

      <li><span class="math">B_{1}</span> runs the witness-extended emulator <span class="math">E</span> guaranteed by Lemma 3.1 for the proof of knowledge of Step 2, with the prover function determined by <span class="math">A_{1}(1^{n},R)</span>. See Section 3.3 for the definition of a witness-extended emulator and its proof of existence for every proof of knowledge. Loosely speaking, such an emulator receives for input the prover strategy and statement being proved, and outputs (in expected polynomial-time) the verifier’s view of a proof system execution along with a witness in the case that the proof is “accepting”. That is, the emulator <span class="math">E</span> outputs a pair <span class="math">(v,w)</span>, where <span class="math">v</span> denotes the verifier’s view of a proof given by <span class="math">A_{1}</span> to an honest verifier, and <span class="math">w</span> is (possibly) a witness for the statement being proved (in this case, the witness is the decommitment of <span class="math">c</span>).</li>

    </ol>

    <p class="text-gray-300"><span class="math">B_{1}</span> derives from <span class="math">v</span> the series of incoming messages to <span class="math">A_{1}</span> in the proof, denote this series by <span class="math">t_{pok}</span>. Then, <span class="math">B_{1}</span> checks whether or not <span class="math">\\mathsf{accept}_{V}(v)=1</span> (where <span class="math">\\mathsf{accept}_{V}(v)=1</span> if and only if the verifier would accept the proof associated with the view <span class="math">v</span>).</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>If <span class="math">\\mathsf{accept}_{V}(v)=0</span>, then <span class="math">B_{1}</span> aborts and outputs <span class="math">A_{1}(1^{n},R,t_{pok})</span>.</li>

      <li>If <span class="math">\\mathsf{accept}_{V}(v)=1</span>, then <span class="math">B_{1}</span> checks that <span class="math">w</span> is a correct witness; that is, <span class="math">B_{1}</span> checks that <span class="math">w=(s_{1},r)</span> and <span class="math">c=C(s_{1};r)</span>. If this is not the case, then <span class="math">B_{1}</span> outputs a special failure symbol.</li>

      <li>Otherwise (if the proof from <span class="math">A_{1}</span> is accepting and <span class="math">w</span> is a proper decommitment to <span class="math">c</span>), <span class="math">B_{1}</span> sends <span class="math">1^{n}</span> to the (external) trusted third party <span class="math">T</span> and receives the output <span class="math">s</span>. (Note that <span class="math">B_{2}</span> does not as of yet receive <span class="math">F(s)</span>.)</li>

    </ol>

    <p class="text-gray-300"><span class="math">B_{1}</span> sets <span class="math">s_{2}=s\\oplus s_{1}</span>, passes <span class="math">s_{2}</span> to <span class="math">A_{1}</span>, and receives some string <span class="math">y</span> from <span class="math">A_{1}</span> (<span class="math">y</span> “should” equal <span class="math">F(s)</span> by the protocol, but this may not be the case). Formally, <span class="math">B_{1}</span> obtains <span class="math">y</span> by computing the function <span class="math">A_{1}(1^{n},R,t_{pok},s_{2})</span>.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">B_{1}</span> (internally) interacts with <span class="math">A_{1}(1^{n},R,t_{pok},s_{2})</span> in the (zero-knowledge) argument of Step 5 of the protocol, in which <span class="math">A_{1}</span> plays the prover and <span class="math">B_{1}</span> plays the part of the verifier. (Denote the incoming messages to <span class="math">A_{1}</span> from the zero-knowledge argument by <span class="math">t_{pf}</span>.)</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>If the verification fails, then <span class="math">B_{1}</span> instructs <span class="math">T</span> to send <span class="math">\\bot</span> (abort) to <span class="math">B_{2}</span>.</li>

      <li>If the verification succeeds, then <span class="math">B_{1}</span> instructs <span class="math">T</span> to send <span class="math">F(s)</span> to <span class="math">B_{2}</span>.</li>

      <li><span class="math">B_{1}</span> outputs <span class="math">A_{1}(1^{n},R,t_{pok},s_{2},t_{pf})</span>.</li>

    </ol>

    <p class="text-gray-300">We need to show that</p>

    <p class="text-gray-300"><span class="math">\\left\\{\\textsc{ideal}_{f,\\overline{B}}(1^{n},1^{n})\\right\\}_{n\\in\\mathsf{N}}\\stackrel{{\\scriptstyle c}}{{\\equiv}}\\left\\{\\textsc{real}_{\\overline{\\Pi,\\overline{A}}}(1^{n},1^{n})\\right\\}_{n\\in\\mathsf{N}}</span></p>

    <p class="text-gray-300">Clearly, if <span class="math">A_{1}</span> follows the instructions of Protocol 2, then the output distributions in the ideal and real models are identical. This is because <span class="math">A_{1}</span>’s view in the ideal-model emulation with <span class="math">B_{1}</span> is identical to that of a real execution with <span class="math">A_{2}</span>. Furthermore, <span class="math">A_{2}</span> would output <span class="math">F(s_{1}\\oplus s_{2})</span> in such a real execution and this equals <span class="math">F(s)</span>, exactly as output by <span class="math">B_{2}</span> in the ideal model. However, <span class="math">A_{1}</span> may not follow the instructions of the protocol and we must show that nevertheless, the real and ideal output distributions are computationally indistinguishable (in fact, we will show that they are statistically close).</p>

    <p class="text-gray-300">Loosely speaking, differences between the ideal and real distributions can occur either if <span class="math">B_{1}</span> outputs failure (which occurs when <span class="math">\\mathsf{accept}_{V}(v)=1</span> and <span class="math">w</span> is not a proper decommitment), or if <span class="math">y\\neq F(s)</span> and yet the verification of the second proof succeeds.</p>

    <p class="text-gray-300">In the proof, we separate <span class="math">A_{1}</span>’s actions into two distinct stages. In the first stage <span class="math">A_{1}</span> sends <span class="math">c</span> and proves knowledge of the decommitment. In the second stage, <span class="math">A_{1}</span> sends <span class="math">y</span> and proves that it is the “correct” value (i.e., it is consistent with <span class="math">c</span> and <span class="math">s_{2}</span>). The proof proceeds by analyzing all possible scenarios for these stages.</p>

    <p class="text-gray-300">Stage 1 – the commitment and proof of knowledge: By the witness-extended emulation lemma (Lemma 3.1), the verifier view output by <span class="math">E</span> is identically distributed to <span class="math">A_{2}</span>’s view of a real execution of this proof provided by <span class="math">A_{1}</span> (and verified by <span class="math">A_{2}</span>). Therefore, the series of messages <span class="math">t_{pok}</span> received by <span class="math">A_{1}</span> from <span class="math">B_{1}</span> is identically distributed to the messages it would receive from <span class="math">A_{2}</span> in a real execution. This means that the view (in Stage 1) of <span class="math">A_{1}</span> in a real execution is identical to its view in the emulation from <span class="math">B_{1}</span>. Thus, this stage can contribute a difference between the distributions only in the case that <span class="math">B_{1}</span> outputs <span class="math">\\mathsf{failure}</span>. However, by Lemma 3.1, the probability that <span class="math">\\mathsf{accept}_{V}(v)=1</span> and yet <span class="math">w\\neq(s_{1},r)</span> where <span class="math">c=C(s_{1};r)</span>, is negligible. Thus, the probability that <span class="math">B_{1}</span> outputs <span class="math">\\mathsf{failure}</span> is negligible and the real and ideal executions (until this point) are statistically close.</p>

    <p class="text-gray-300">By the above analysis, we have that if <span class="math">B_{1}</span> reaches Step 3 of its specification, then <span class="math">B_{1}</span> has obtained the (unique) pair <span class="math">(s_{1},r)</span> such that <span class="math">c=C(s_{1};r)</span>. Recall that in Step 4, <span class="math">B_{1}</span> passes <span class="math">s_{2}=s\\oplus s_{1}</span> to <span class="math">A_{1}</span>, where <span class="math">s</span> is the output received from <span class="math">T</span>. We continue by analyzing the emulation of the second stage.</p>

    <p class="text-gray-300">Stage 2 – <span class="math">y</span> and the proof of consistency: First consider the case that <span class="math">A_{1}</span> sends <span class="math">y=F(s)</span> in Step 4 of the emulation (i.e., <span class="math">A_{1}</span> sends the “correct” value). There are two possible sub-cases here – either <span class="math">B_{1}</span> accepts the proof from <span class="math">A_{1}</span> in Step 5 or it rejects the proof. If the proof is accepted then <span class="math">B_{2}</span></p>

    <p class="text-gray-300">outputs <span class="math">F(s)</span> (as <span class="math">B_{1}</span> instructs <span class="math">T</span> to give <span class="math">F(s)</span> to <span class="math">B_{2}</span>), otherwise <span class="math">B_{2}</span> outputs <span class="math">\\perp</span> (again, as instructed by <span class="math">B_{1}</span>). In a real execution, <span class="math">A_{2}</span> would also output <span class="math">y = F(s)</span> in the case that the proof is accepted and <span class="math">\\perp</span> otherwise. The fact that the resulting distributions are identical is derived from the fact that the probability that <span class="math">B_{1}</span> accepts the (internally generated) proof from <span class="math">A_{1}</span> in the emulation, equals the probability that <span class="math">A_{2}</span> accepts the proof in a real execution (with <span class="math">A_{1}</span>).</p>

    <p class="text-gray-300">Finally, we consider the case that <span class="math">A_{1}</span> sends <span class="math">y \\neq F(s)</span> in Step 4 of the emulation (i.e., <span class="math">A_{1}</span> attempts to "cheat"). If the verification of the proof in Step 5 fails, then both <span class="math">A_{2}</span> and <span class="math">B_{2}</span> output <span class="math">\\perp</span> (as in the previous case). On the other hand, if the verification succeeds then the real-party <span class="math">A_{2}</span> outputs <span class="math">y \\neq F(s_{1} \\oplus s_{2})</span>, whereas the ideal-party <span class="math">B_{2}</span> outputs <span class="math">F(s_{1} \\oplus s_{2})</span> (and thus the real and ideal output distributions differ). However, by the soundness property of the proof system, a verifier can be convinced of a false assertion with at most negligible probability.</p>

    <p class="text-gray-300">We thus conclude that unless the extraction fails or <span class="math">A_{1}</span> successfully "cheats" in the last proof, the output distributions are identical. Since these events can occur with at most negligible probability, we have that the distributions are statistically close. That is,</p>

    <div class="my-4 text-center"><span class="math-block">\\left\\{\\mathrm{IDEAL}_{f, \\overline{B}}(1^n, 1^n)\\right\\}_{n \\in \\mathbb{N}} \\stackrel{\\mathrm{s}}{=} \\left\\{\\mathrm{REAL}_{\\Pi, \\overline{A}}(1^n, 1^n)\\right\\}_{n \\in \\mathbb{N}}</span></div>

    <p class="text-gray-300">We conclude by noting that <span class="math">B_{1}</span> runs in expected polynomial-time (the "expected" part coming from the execution of the witness-extended emulator <span class="math">E</span> that runs in expected polynomial-time).</p>

    <p class="text-gray-300">We now consider the case that <span class="math">A_{2}</span> is adversarial.</p>

    <p class="text-gray-300"><strong>Lemma 5.2</strong> Let <span class="math">(A_1, A_2)</span> be a pair of probabilistic expected polynomial-time machines for the real model in which <span class="math">A_1</span> is honest. Then there exists a pair of probabilistic expected polynomial-time machines <span class="math">(B_1, B_2)</span> for the ideal model in which <span class="math">B_1</span> is honest, such that</p>

    <div class="my-4 text-center"><span class="math-block">\\left\\{\\mathrm{IDEAL}_{f, \\overline{B}}(1^n, 1^n)\\right\\}_{n \\in \\mathbb{N}} \\stackrel{\\mathrm{r}}{=} \\left\\{\\mathrm{REAL}_{\\Pi, \\overline{A}}(1^n, 1^n)\\right\\}_{n \\in \\mathbb{N}}</span></div>

    <p class="text-gray-300"><strong>Proof:</strong> In this case the first party is honest and thus <span class="math">B_{1}</span> is determined. We now transform the real-model adversary <span class="math">A_{2}</span> into an ideal-model adversary <span class="math">B_{2}</span>. As before, <span class="math">B_{2}</span> works by using black-box access to <span class="math">A_{2}</span>. The notation used here is the same as in the previous lemma. Machine <span class="math">B_{2}</span> works as follows:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>The ideal adversary <span class="math">B_{2}</span> chooses a uniformly distributed random tape <span class="math">R</span> for the real adversary <span class="math">A_{2}</span>, invokes <span class="math">A_{2}(1^{n}, R)</span> and (internally) passes to <span class="math">A_{2}</span> the commitment <span class="math">c = C(0^{m}; r)</span> for a random <span class="math">r</span> (recall that in a real execution, <span class="math">A_{2}</span> expects to receive <span class="math">C(s_{1}; r)</span> for <span class="math">s_{1} \\in_{R} \\{0, 1\\}^{m}</span>).</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">B_{2}</span> invokes the simulator for the zero-knowledge argument of knowledge of the decommitment of <span class="math">c</span>, using <span class="math">A_{2}(1^{n}, R, c)</span> as the verifier.¹³ (That is, this is a simulation of the proof of knowledge that <span class="math">A_{1}</span> is supposed to prove to <span class="math">A_{2}</span> in a real execution.)</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">B_{2}</span> obtains <span class="math">s_{2}</span> from <span class="math">A_{2}</span>. (Recall that this is formally stated by having <span class="math">B_{2}</span> compute the function <span class="math">A_{2}(1^{n}, R, c, t_{pok})</span>, where <span class="math">t_{pok}</span> is the resulting transcript from the simulation of the zero-knowledge proof of knowledge in the previous step).</li>

    </ol>

    <p class="text-gray-300">If at any point until here <span class="math">A_{2}</span> sent an invalid message, then <span class="math">B_{2}</span> aborts and outputs <span class="math">A_{2}(1^{n}, R, c, t_{pok})</span>.</p>

    <p class="text-gray-300">¹³There is a significant difference between the transformation (of <span class="math">A_{2}</span>) here and the transformation (of <span class="math">A_{1}</span>) described in Lemma 5.1. There, <span class="math">B_{1}</span> emulated a true execution of the zero-knowledge proofs between <span class="math">A_{1}</span> and <span class="math">A_{2}</span> by playing <span class="math">A_{2}</span>'s role as an honest verifier. However, here <span class="math">B_{2}</span> runs the zero-knowledge simulator and does not attempt to really prove the statement.</p>

    <p class="text-gray-300">25</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>The ideal adversary <span class="math">B_{2}</span> sends <span class="math">1^{n}</span> to the (external) trusted third party <span class="math">T</span> and receives the output <span class="math">F(s)</span>. (Note that this means that <span class="math">B_{1}</span> also receives its output <span class="math">s</span> from <span class="math">T</span> at this point.) Next, <span class="math">B_{2}</span> (internally) passes to <span class="math">A_{2}</span> the string <span class="math">y=F(s)</span>.</li>

      <li><span class="math">B_{2}</span> invokes the simulator for the zero-knowledge proof of Step 5 of the Protocol with the verifier role being played by <span class="math">A_{2}(1^{n},R,c,t_{pok},y)</span>. Denote the transcript from the simulation of the zero-knowledge proof by <span class="math">t_{pf}</span>.</li>

    </ol>

    <p class="text-gray-300">(Recall that the statement being proved is that there exists a pair <span class="math">(s_{1},r)</span> such that <span class="math">c=C(s_{1};r)</span> and <span class="math">y=F(s_{1}\\oplus s_{2})</span>, where <span class="math">s_{2}</span> is the string obtained from <span class="math">A_{2}</span> in Step 3. Note that with overwhelming probability this statement is false; nevertheless as we shall see the simulation generated is still indistinguishable from a real proof.)</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">B_{2}</span> outputs <span class="math">A_{2}(1^{n},R,c,t_{pok},y,t_{pf})</span>.</li>

    </ol>

    <p class="text-gray-300">We need to show that</p>

    <p class="text-gray-300"><span class="math">\\left\\{\\textsc{ideal}_{f,\\overline{B}}(1^{n},1^{n})\\right\\}_{n\\in\\mathsf{N}}\\stackrel{{\\scriptstyle c}}{{=}}\\left\\{\\textsc{real}_{\\overline{\\Pi,\\overline{A}}}(1^{n},1^{n})\\right\\}_{n\\in\\mathsf{N}}</span> (1)</p>

    <p class="text-gray-300">The following differences are evident between the ideal and real executions:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>The commitment received by <span class="math">A_{2}</span> (in the internal emulation by <span class="math">B_{2}</span>) is to <span class="math">0^{m}</span>, rather than to a random string consistent with <span class="math">y=F(s)</span> and <span class="math">s_{2}</span> (as is the case in a real execution). However, by the indistinguishability of commitments, this should not make a difference.</li>

      <li>In the internal emulation by <span class="math">B_{2}</span>, the zero-knowledge proofs are simulated and not real proofs. However, by the indistinguishability of simulated proofs, this should also not make a difference. As mentioned above, this holds even though one of the statements is false with overwhelming probability; details follow in the proof.</li>

    </ul>

    <p class="text-gray-300">The natural way to proceed at this point would be to define a hybrid experiment in which the commitment given by <span class="math">B_{2}</span> to <span class="math">A_{2}</span> is to <span class="math">s_{1}</span> and yet the zero-knowledge proofs are simulated. (In this hybrid experiment, <span class="math">s_{1}</span> must be such that <span class="math">y=F(s_{1}\\oplus s_{2})</span>.) However, such a hybrid experiment is problematic because the value of <span class="math">s_{1}</span> that is consistent with both <span class="math">y</span> (from <span class="math">T</span>) and <span class="math">s_{2}</span> is unknown at the point that <span class="math">B_{2}</span> generates the commitment. We must therefore bypass this problem before defining the hybrid experiment. We do this by defining the following mental experiment with a modified party <span class="math">B_{2}^{\\prime}</span> (replacing Step 4 only of <span class="math">B_{2}</span> above):</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">B_{2}^{\\prime}</span> chooses <span class="math">s_{1}\\in_{R}\\{0,1\\}^{m}</span> (independently of what it has previously seen) and computes <span class="math">y=F(s_{1}\\oplus s_{2})</span> (rather than obtaining <span class="math">y=F(s)</span> from <span class="math">T</span>).</li>

    </ol>

    <p class="text-gray-300">Next, <span class="math">B_{2}^{\\prime}</span> (internally) passes <span class="math">A_{2}</span> the string <span class="math">y</span>.</p>

    <p class="text-gray-300">We also modify the output of the honest party so that the output of (a virtual party) <span class="math">B_{1}^{\\prime}</span> is defined as follows: if the execution of <span class="math">B_{2}^{\\prime}</span> halts at Step 3 (i.e., if <span class="math">A_{2}</span> sent an invalid message), then <span class="math">\\text{output}(B_{1}^{\\prime})=\\bot</span>, otherwise <span class="math">\\text{output}(B_{1}^{\\prime})=s_{1}\\oplus s_{2}</span> (rather than the value <span class="math">s</span> handed by <span class="math">T</span> to <span class="math">B_{1}</span>).</p>

    <p class="text-gray-300">Notice that <span class="math">B_{2}^{\\prime}</span> does not interact with any trusted third party at all. Rather, it chooses a uniformly distributed <span class="math">s</span>, and computes <span class="math">F(s)</span> itself (choosing <span class="math">s_{1}</span> uniformly and setting <span class="math">s=s_{1}\\oplus s_{2}</span> is equivalent to uniformly choosing <span class="math">s</span>). We stress that <span class="math">B_{2}^{\\prime}</span> does not work in the ideal model, but is rather a mental experiment. Despite this, we will show that (in some sense) <span class="math">B_{2}^{\\prime}</span> simulates the ideal model perfectly. To this end, define a mental experiment mental with parties <span class="math">B_{1}^{\\prime}</span> and <span class="math">B_{2}^{\\prime}</span> by</p>

    <p class="text-gray-300"><span class="math">\\textsc{mental}_{\\overline{B^{\\prime}}}(1^{n},1^{n})\\stackrel{{\\scriptstyle\\text{def}}}{{=}}(\\text{output}(B_{1}^{\\prime}),\\text{output}(B_{2}^{\\prime}))</span></p>

    <p class="text-gray-300">where</p>

    <p class="text-gray-300">We first claim that for the above <span class="math">\\overline{B}</span> and <span class="math">\\overline{B}^{\\prime}</span>, we have that</p>

    <div class="my-4 text-center"><span class="math-block">\\left\\{\\mathrm{IDEAL}_{f,\\overline{B}}(1^n, 1^n)\\right\\}_{n \\in \\mathbb{N}} \\equiv \\left\\{\\mathrm{MENTAL}_{\\overline{B}^{\\prime}}(1^n, 1^n)\\right\\}_{n \\in \\mathbb{N}} \\tag{2}</span></div>

    <p class="text-gray-300">This can be seen as follows. Since <span class="math">B_2&#x27;</span> chooses <span class="math">s_1</span> uniformly and independently of <span class="math">s_2</span>, we have that <span class="math">s = s_1 \\oplus s_2</span> is uniformly distributed. Therefore, this is exactly the same as when the trusted third party <span class="math">T</span> uniformly chooses <span class="math">s</span> and gives it to <span class="math">B_2</span>. This means that the outputs of <span class="math">B_2</span> and <span class="math">B_2&#x27;</span> are identically distributed. Now, in a non-aborting execution, the outputs of <span class="math">B_1</span> and <span class="math">B_1&#x27;</span> are defined to be <span class="math">s</span> and <span class="math">s_1 \\oplus s_2</span>, respectively. Since <span class="math">s = s_1 \\oplus s_2</span>, and these are the same strings viewed by <span class="math">B_2</span> and <span class="math">B_2&#x27;</span>, we have that in such a case, the joint distributions of <span class="math">\\{B_1, B_2\\}</span> and <span class="math">\\{B_1&#x27;, B_2&#x27;\\}</span> are identical. On the other hand, in aborting executions, <span class="math">B_1</span> and <span class="math">B_1&#x27;</span> both output <span class="math">\\perp</span>. Therefore, this case does not change the above joint distributions. We conclude that <span class="math">\\{B_1, B_2\\}</span> and <span class="math">\\{B_1&#x27;, B_2&#x27;\\}</span> are identically distributed.</p>

    <p class="text-gray-300">In essence, the above states that the mental experiment is exactly the same as the ideal model (with <span class="math">B_2</span>). However, as we have mentioned, this transition to the mental experiment is needed before defining an appropriate hybrid experiment.</p>

    <p class="text-gray-300">By the above, it is enough to show that</p>

    <div class="my-4 text-center"><span class="math-block">\\left\\{\\mathrm{MENTAL}_{\\overline{B}^{\\prime}}(1^n, 1^n)\\right\\}_{n \\in \\mathbb{N}} \\stackrel{\\mathrm{c}}{=} \\left\\{\\mathrm{REAL}_{\\Pi,\\overline{A}}(1^n, 1^n)\\right\\}_{n \\in \\mathbb{N}} \\tag{3}</span></div>

    <p class="text-gray-300">We begin by defining a hybrid setting (with a party <span class="math">B_2&#x27;&#x27;</span>) in which the initial commitment is to <span class="math">s_1</span> (rather than to <span class="math">0^m</span> as in the mental experiment), yet the zero-knowledge proofs are simulated (rather than being actual proofs as in the real model). First, define <span class="math">B_2&#x27;&#x27;</span> as follows:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">B_2&#x27;&#x27;</span> chooses <span class="math">s_1 \\in_{R} \\{0,1\\}^m</span>.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">B_2&#x27;&#x27;</span> invokes <span class="math">A_2(1^n, R)</span> for a uniformly chosen <span class="math">R</span> and (internally) passes <span class="math">A_2</span> the commitment <span class="math">c = C(s_1; r)</span> for a random <span class="math">r</span>. (Recall that in contrast, in Step 1 of <span class="math">B_2&#x27;</span>, the commitment was to <span class="math">0^m</span>.)</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">B_2&#x27;&#x27;</span> invokes the simulator for the zero-knowledge argument of knowledge of the decommitment of <span class="math">c</span>, using <span class="math">A_2(1^n, R, c)</span> as the verifier.</li>

    </ol>

    <p class="text-gray-300">This is like Step 2 of <span class="math">B_2</span> (and <span class="math">B_2&#x27;</span>) except that here the commitment has a different distribution.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">B_2&#x27;&#x27;</span> obtains <span class="math">s_2</span> from <span class="math">A_2</span>. (This, as well as the rest of this step, is like in <span class="math">B_2</span>.)</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">B_2&#x27;&#x27;</span> computes <span class="math">y = F(s_1 \\oplus s_2)</span>.</li>

    </ol>

    <p class="text-gray-300">Next, <span class="math">B_2&#x27;&#x27;</span> (internally) passes <span class="math">A_2</span> the string <span class="math">y</span>.</p>

    <p class="text-gray-300">(This is like Step 4' in the mental experiment, except that the first part of the step involving choosing <span class="math">s_1</span> has been moved to Step 0.)</p>

    <p class="text-gray-300">The remaining Steps (5 and 6) are exactly the same as in the specification of <span class="math">B_2</span> and <span class="math">B_2&#x27;</span>. Finally, the output of (a virtual party) <span class="math">B_1&#x27;&#x27;</span> is defined as for <span class="math">B_1&#x27;</span>. Now define:</p>

    <div class="my-4 text-center"><span class="math-block">\\mathrm{HYBRID}_{\\overline{B}&#x27;&#x27;}(1^n, 1^n) \\stackrel{\\mathrm{def}}{=} (\\mathrm{output}(B_1&#x27;&#x27;), \\mathrm{output}(B_2&#x27;&#x27;))</span></div>

    <p class="text-gray-300">Note that in HYBRID, <span class="math">B_2&#x27;&#x27;</span> chooses <span class="math">s_1</span> at the outset rather than after receiving <span class="math">s_2</span>. However, this makes no difference to the distribution of <span class="math">s_1</span> because in the mental experiment <span class="math">B_2&#x27;</span> chooses <span class="math">s_1</span> independently of any messages seen. Thus, the only difference between the MENTAL and HYBRID</p>

    <p class="text-gray-300">27</p>

    <p class="text-gray-300">settings is with respect to the initial commitment (which in hybrid is to <span class="math">s_1</span> and in mental is to <span class="math">0^m</span>). On the other hand, the only difference between the hybrid and real settings is with respect to the zero-knowledge proofs that are simulated in hybrid rather than being actual proofs as in real. Below we shall show that both differences are indistinguishable and thus Eq. (3) follows.</p>

    <p class="text-gray-300">We first show that</p>

    <div class="my-4 text-center"><span class="math-block">\\left\\{\\text{MENTAL}_{\\overline{B}&#x27;}(1^n, 1^n)\\right\\}_{n \\in \\mathbb{N}} \\stackrel{c}{=} \\left\\{\\text{HYBRID}_{\\overline{B}&#x27;&#x27;}(1^n, 1^n)\\right\\}_{n \\in \\mathbb{N}} \\tag{4}</span></div>

    <p class="text-gray-300">As we have mentioned, the only difference between these two settings is with respect to the initial commitment (i.e., the commitment given to <span class="math">B_2&#x27;&#x27;</span> is either to <span class="math">0^m</span> or to a uniformly chosen string <span class="math">s_1</span>). Any distinguishable difference in the above two distributions can thus be used to distinguish such commitments. In particular, let <span class="math">D&#x27;</span> be a probabilistic polynomial-time distinguisher that attempts to distinguish between the mental and hybrid experiments. We now construct a probabilistic polynomial-time machine <span class="math">D</span> that uses <span class="math">D&#x27;</span> in order to "break" the commitment scheme, specifically by distinguishing a commitment to <span class="math">0^m</span> from a commitment to <span class="math">s_1 \\in_R \\{0, 1\\}^m</span>. Given <span class="math">c</span> (that is either a commitment to <span class="math">0^m</span> or <span class="math">s_1</span>), <span class="math">D</span> internally invokes <span class="math">A_2(1^n, R)</span> and gives <span class="math">A_2</span> the commitment <span class="math">c</span>. Next, <span class="math">D</span> continues by simulating the rest of the mental experiment, which is identical to the rest of hybrid (i.e., <span class="math">D</span> follows the specification of <span class="math">B_2&#x27;</span> or equivalently <span class="math">B_2&#x27;&#x27;</span> for Steps 2 to 6 of these experiments). The key point is that <span class="math">D</span> need not know the value committed to in <span class="math">c</span> in order to carry out this simulation. That is, <span class="math">D</span> runs the zero-knowledge simulators and computes <span class="math">y = F(s_1 \\oplus s_2)</span> exactly as defined for <span class="math">B_2&#x27;</span> and <span class="math">B_2&#x27;&#x27;</span>. Then, upon concluding the simulation, <span class="math">D</span> generates the pair <span class="math">(\\text{output}(B_1&#x27;), A_2(1^n, R, c, t_{pok}, y, t_{pf}))</span> and outputs <span class="math">D&#x27;(\\text{output}(B_1&#x27;), A_2(1^n, R, c, t_{pok}, y, t_{pf}))</span>.</p>

    <p class="text-gray-300">Now, if <span class="math">c</span> is a commitment to <span class="math">0^m</span>, then the pair generated by <span class="math">D</span> is distributed exactly according to <span class="math">\\text{MENTAL}_{\\overline{B}&#x27;}(1^n, 1^n)</span>. On the other hand, if <span class="math">c</span> is a commitment to <span class="math">s_1</span>, then the pair is distributed exactly according to <span class="math">\\text{HYBRID}_{\\overline{B}&#x27;&#x27;}(1^n, 1^n)</span>. Thus, <span class="math">D</span> can distinguish commitments to <span class="math">0^m</span> from commitments to <span class="math">s_1</span> with the same success as <span class="math">D&#x27;</span> can distinguish the mental and hybrid distributions. By the indistinguishability of commitments, Eq. (4) follows.<span class="math">^{14}</span></p>

    <p class="text-gray-300">We now prove that</p>

    <div class="my-4 text-center"><span class="math-block">\\left\\{\\text{HYBRID}_{\\overline{B}&#x27;&#x27;}(1^n, 1^n)\\right\\}_{n \\in \\mathbb{N}} \\stackrel{c}{=} \\left\\{\\text{REAL}_{\\Pi, \\overline{A}}(1^n, 1^n)\\right\\}_{n \\in \\mathbb{N}} \\tag{5}</span></div>

    <p class="text-gray-300">The only difference between the two settings is with respect to the zero-knowledge proofs (in hybrid the proofs are simulated and in real they are actual proofs). The fact that the experiments are indistinguishable is due to the formulation of zero-knowledge with respect to auxiliary inputs. In particular, it follows that for every <span class="math">s_1</span> and <span class="math">r</span>, the transcript generated by the simulator for the proof of knowledge is indistinguishable from a real proof. Furthermore, for every <span class="math">s_1, r, t_{pok}</span> and <span class="math">s_2</span> (where <span class="math">t_{pok}</span> represents a transcript from the proof of knowledge), the transcript for the second proof is indistinguishable from a real proof. We therefore have that the output of <span class="math">B_2&#x27;&#x27;</span> (in the hybrid model) is indistinguishable from the output of <span class="math">A_2</span> (in the real model).<span class="math">^{15}</span></p>

    <p class="text-gray-300">On the other hand, the probability that the (hybrid model) party <span class="math">B_1&#x27;&#x27;</span> outputs <span class="math">\\perp</span> is negligibly close to the probability that the real party <span class="math">A_1</span>, in a real execution with <span class="math">A_2</span>, aborts and outputs <span class="math">\\perp</span>. (This is because <span class="math">B_1&#x27;&#x27;</span> and <span class="math">A_1</span> output <span class="math">\\perp</span> only if <span class="math">A_2</span> sends an invalid message before Step 4. Since the only difference with respect to <span class="math">A_2</span>'s view in the settings is whether the proof is simulated or real, the probability that it sends an invalid message in both cases must be negligibly close.) Furthermore, in non-aborting executions, both <span class="math">B_1&#x27;&#x27;</span> and <span class="math">A_1</span> output <span class="math">s_1 \\oplus s_2</span>. Thus, Eq. (5) holds.</p>

    <p class="text-gray-300"><span class="math">^{14}</span>We note that this proof shows that the simulator generates an indistinguishable proof even for a false statement regarding the value committed to in <span class="math">c</span>. Otherwise, the simulator could be used to distinguish commitments.</p>

    <p class="text-gray-300"><span class="math">^{15}</span>Formally, one should consider a hybrid argument in which the first proof is real and the second is simulated. Then, by considering the indistinguishability of each simulated proof separately, the overall claim is obtained.</p>

    <p class="text-gray-300">Combining Equations (4) and (5) we obtain Eq. (3). Finally, Eq. (1) follows from Equations (2) and (3), completing the proof of Lemma 5.2.</p>

    <p class="text-gray-300">This completes the proof of Theorem 11.</p>

    <h3 id="sec-58" class="text-xl font-semibold mt-8">5.4 Comparing Protocol 2 to the Protocol of <em>[18]</em></h3>

    <p class="text-gray-300">The protocol for augmented coin-tossing presented by Goldreich <em>[18]</em> is for tossing a single bit only (i.e., where <span class="math">m=1</span>). Thus, in order to toss polynomially many coins, Goldreich suggests running the single-bit protocol many times sequentially. However, the only difference between Protocol 2 and the protocol of <em>[18]</em> is that here <span class="math">m</span> can be any value polynomial in <span class="math">n</span> and there <span class="math">m</span> is fixed at <span class="math">1</span> (i.e., by setting <span class="math">m=1</span> in our protocol, we obtain the exact protocol of <em>[18]</em>). Despite this, our proof is different and works for any <span class="math">m=\\operatorname{poly}(n)</span> whereas the proof of <em>[18]</em> relies heavily on <span class="math">m=1</span> (or at the most <span class="math">m=O(\\log n)</span>). Furthermore, there is a conceptual difference in the role of the two zero-knowledge proofs in the protocol. In <em>[18]</em>, these proofs are used in order to obtain augmented coin-tossing (and are not needed for the case that <span class="math">F</span> is the identity function). However, here these proofs are used for obtaining coin-tossing of <span class="math">m=\\operatorname{poly}(n)</span> coins in parallel, even when <span class="math">F</span> is the identity function.</p>

    <h2 id="sec-59" class="text-2xl font-bold">6 Almost Perfect Coin-Tossing</h2>

    <p class="text-gray-300">In this section we present a constant-round protocol for almost perfect coin tossing. In such a protocol, the output distribution of a real execution is guaranteed to be statistically close to the output distribution of the ideal process (rather than the distributions being only computationally indistinguishable as required by secure computation); see Theorem 12. As in the previous section, the functionality we consider is that of augmented coin tossing:</p>

    <p class="text-gray-300"><span class="math">(1^{n},1^{n})\\mapsto(U_{m},F(U_{m}))</span></p>

    <p class="text-gray-300">The protocol is almost identical to Protocol 2 except that the commitment scheme used is perfectly hiding and the zero-knowledge arguments are perfect. These primitives are known to exist assuming the existence of families of clawfree functions or collision-resistant hash functions. Thus we rely here on a (seemingly) stronger assumption than merely the existence of one-way functions. We note that Protocol 1 is a protocol for the almost perfect coin tossing of a single bit and thus almost perfect coin tossing of <span class="math">m</span> coins can be achieved in <span class="math">O(m)</span> rounds (see the proof in <em>[18]</em> which actually demonstrates statistical closeness). In this section we show that the almost perfect coin tossing of polynomially many coins can also be achieved in a constant number of rounds.</p>

    <h6 id="sec-60" class="text-base font-medium mt-4">Protocol 3 (Augmented Almost Perfect Coin-Tossing).</h6>

    <p class="text-gray-300">An augmented almost perfect coin-tossing protocol is constructed by taking Protocol 2 and making the following modifications:</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>The commitment sent by Party <span class="math">1</span> in Step <span class="math">1</span> is perfectly hiding.</li>

      <li>The argument of knowledge provided by Party <span class="math">1</span> in Step <span class="math">2</span> is perfect zero-knowledge.</li>

      <li>The proof provided by Party <span class="math">1</span> in Step <span class="math">5</span> is a perfect zero-knowledge argument of knowledge. (Recall that in Protocol 2, this proof need not be a proof of knowledge.)</li>

    </ul>

    <p class="text-gray-300">Constant-round<em> perfect zero-knowledge arguments of knowledge are known to exist assuming the existence of constant-round perfectly hiding commitment schemes </em>[10, 17]<em>. Furthermore, constant-round perfectly-hiding commitment schemes can be constructed using families of clawfree </em>[19]<em> or collision-resistant hash functions </em>[32, 13]*. These commitment schemes work by having the receiver first uniformly choose a function <span class="math">f</span> from the family designated in the protocol. The receiver then sends <span class="math">f</span> to the sender who uses it to commit to a string by sending a single message. Thus, using such a scheme, Protocol 3 begins by Party 2 choosing a function <span class="math">f</span> from a clawfree or collision-resistant family and sending it to Party 1. Then, Party 1 commits using <span class="math">f</span>.</p>

    <p class="text-gray-300">We stress the use of arguments of knowledge for <em>both</em> proofs here, whereas in Protocol 2 the proof of Step 5 need not be a proof of knowledge. The reason for this is that since the commitment is perfectly hiding, <span class="math">c</span> is essentially a valid commitment to <em>every</em> <span class="math">s_{1}\\in\\{0,1\\}^{m}</span>. Thus, every <span class="math">y</span> is “consistent” with <span class="math">c</span> and <span class="math">s_{2}</span>. Therefore, what we need to ensure is that <span class="math">y</span> is consistent with <span class="math">s_{2}</span> and the decommitment of <span class="math">c</span> that are <em>known</em> to Party 1. This can be accomplished using a proof of knowledge.</p>

    <h6 id="sec-61" class="text-base font-medium mt-4">Theorem 12</h6>

    <p class="text-gray-300">Assuming the existence of perfectly-hiding commitment schemes, Protocol 3 is a secure protocol for augmented almost perfect coin-tossing. That is, for every admissible pair of probabilistic expected polynomial-time machines for the real model <span class="math">(A_{1},A_{2})</span> there exists an admissible pair of probabilistic expected polynomial-time machines for the ideal model <span class="math">(B_{1},B_{2})</span>, such that</p>

    <p class="text-gray-300"><span class="math">\\left\\{\\text{\\sc ideal}_{f,\\overline{B}}(1^{n},1^{n})\\right\\}\\overset{\\text{s}}{=}\\left\\{\\text{\\sc real}_{\\Pi_{2},\\overline{A}}(1^{n},1^{n})\\right\\}</span></p>

    <p class="text-gray-300">where <span class="math">f</span> is the augmented coin-tossing functionality and <span class="math">\\Pi_{2}</span> denotes Protocol 3.</p>

    <p class="text-gray-300">Proof Sketch: As in the proof of Theorem 11, we separately consider the cases that <span class="math">A_{1}</span> and <span class="math">A_{2}</span> are adversarial. We first consider the case that <span class="math">A_{1}</span> is adversarial.</p>

    <h6 id="sec-62" class="text-base font-medium mt-4">Lemma 6.1</h6>

    <p class="text-gray-300">Let <span class="math">(A_{1},A_{2})</span> be a pair of probabilistic expected polynomial-time machines for the real model in which <span class="math">A_{2}</span> is <em>honest</em>. Then there exists a pair of probabilistic expected polynomial-time machines <span class="math">(B_{1},B_{2})</span> for the ideal model in which <span class="math">B_{2}</span> is honest, such that</p>

    <p class="text-gray-300"><span class="math">\\left\\{\\text{\\sc ideal}_{f,\\overline{B}}(1^{n},1^{n})\\right\\}_{n\\in\\mathsf{N}}\\overset{\\text{s}}{=}\\left\\{\\text{\\sc real}_{\\Pi_{2},\\overline{A}}(1^{n},1^{n})\\right\\}_{n\\in\\mathsf{N}}</span></p>

    <p class="text-gray-300">Proof Sketch: Loosely speaking, the only difference between here and the proof of Lemma 5.1 is that the commitment is only computationally binding. Furthermore, recall that in the proof of Lemma 5.1 we actually showed that the ideal and real distributions are statistically close, as required here. Thus, it is enough to show that this remains the case even though the commitment is only computationally binding. In the proof of Lemma 5.1, the fact that the commitment is perfectly-binding is used to show that if <span class="math">A_{1}</span> successfully proves both (zero-knowledge) proofs, then <span class="math">y=F(s_{1}\\oplus s_{2})</span>, where <span class="math">s_{1}</span> is the value <span class="math">A_{1}</span> committed to in the first step. Thus, the analogous argument here is that if <span class="math">A_{1}</span> successfully proves both (zero-knowledge) proofs of knowledge, then <span class="math">y=F(s_{1}\\oplus s_{2})</span>, where <span class="math">s_{1}</span> is the value <span class="math">A_{1}</span> <em>used</em> for the commitment in the first step. This is proven by showing that otherwise, the extractor for the proofs of knowledge can be used to extract two different decommitments from <span class="math">A_{1}</span>. This would contradict the computational binding of the commitment scheme.</p>

    <h6 id="sec-63" class="text-base font-medium mt-4">Lemma 6.2</h6>

    <p class="text-gray-300">Let <span class="math">(A_{1},A_{2})</span> be a pair of probabilistic expected polynomial-time machines for the real model in which <span class="math">A_{1}</span> is honest. Then there exists a pair of probabilistic expected polynomial-time machines <span class="math">(B_{1},B_{2})</span> for the ideal model in which <span class="math">B_{1}</span> is honest, such that</p>

    <p class="text-gray-300"><span class="math">\\left\\{\\textsc{ideal}_{f,\\overline{B}}(1^{n},1^{n})\\right\\}_{n\\in\\mathsf{N}}\\stackrel{{\\scriptstyle\\pi}}{{\\equiv}}\\left\\{\\textsc{real}_{\\Pi_{2},\\overline{A}}(1^{n},1^{n})\\right\\}_{n\\in\\mathsf{N}}</span></p>

    <p class="text-gray-300">Proof Sketch: The proof of this lemma is very similar to the proof of Lemma 5.2. That is, we define analogous mental and hybrid experiments. Recall that the mental experiment is identically distributed to the ideal execution. Furthermore, the only difference between the mental and hybrid experiments is with respect to the value of the commitment. Since the commitments here are perfectly hiding, the experiments are identically distributed. Likewise, the only difference between the hybrid experiment and the real setting is that the hybrid setting uses a simulated proof rather than a real one. Then, since we use perfect zero-knowledge here, these distributions are also identically distributed. We therefore have that the ideal and real distributions are identical (and not even just statistically close).</p>

    <p class="text-gray-300">This completes the proof of Theorem 12.</p>

    <h2 id="sec-64" class="text-2xl font-bold">7 Acknowledgements</h2>

    <p class="text-gray-300">We would like to thank Oded Goldreich for his invaluable contribution to all aspects of this work. We would also like to thank Moni Naor for his suggestion that we look at the question of almost perfect coin-tossing as well, and Jonathan Katz for pointing out an error in the proof of Lemma 3.1 that appeared in earlier versions of this paper. Finally, we are grateful to the anonymous referees for their many helpful comments.</p>

    <h2 id="sec-65" class="text-2xl font-bold">References</h2>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[1] B. Barak. How to Go Beyond the Black-Box Simulation Barrier. In 42nd FOCS, pages 106–115, 2001.</li>

      <li>[2] B. Barak and Y. Lindell. Strict Polynomial-time in Simulation and Extraction. In 34th STOC, pages 484–493, 2002.</li>

      <li>[3] D. Beaver. Foundations of Secure Interactive Computing. In CRYPTO’91, Springer-Verlag (LNCS 576), pages 377–391, 1991.</li>

      <li>[4] D. Beaver and S. Goldwasser. Multiparty Computation with Fault Majority. In CRYPTO’89, Springer-Verlag (LNCS 435), 1989.</li>

      <li>[5] D. Beaver, S. Micali and P. Rogaway. The Round Complexity of Secure Protocols. In 22nd STOC, pages 503–513, 1990.</li>

      <li>[6] M. Bellare. A Note on Negligible Functions. Journal of Cryptology, 15(4):271–284, 2002.</li>

      <li>[7] M. Bellare and O. Goldreich. On Defining Proofs of Knowledge. In CRYPTO’92, Springer-Verlag (LNCS 740), pages 390–420, 1992.</li>

    </ul>

    <p class="text-gray-300">-</p>

    <p class="text-gray-300">[8] M. Blum. Coin Flipping by Phone. IEEE Spring COMPCOM, pages 133–137, February 1982.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[9] D. Boneh and M. Franklin. Efficient generation of shared RSA keys. In CRYPTO’97, Springer-Verlag (LNCS 1233), pages 425–439, 1997.</li>

      <li>[10] G. Brassard, C. Crepeau and M. Yung. Constant-round perfect zero-knowledge computationally convincing protocols. In Theoretical Computer Science, 84(1):23–52, 1991.</li>

      <li>[11] R. Canetti. Security and Composition of Multi-party Cryptographic Protocols. Journal of Cryptology, 13(1):143–202, 2000.</li>

      <li>[12] B. Chor, O. Goldreich, E. Kushilevitz and M. Sudan. Private Information Retrieval. In 36th FOCS, pp. 41–50, 1995.</li>

      <li>[13] I. Damgard, T. Pederson and B. Pfitzmann. On the Existence of Statistically Hiding Bit Commitment Schemes and Fail-Stop Signatures. In CRYPTO’93, Springer-Verlag (LNCS 773), pages 250–265, 1993.</li>

      <li>[14] S. Even, O. Goldreich and A. Lempel. A Randomized Protocol for Signing Contracts. Communications of the ACM, 28, pages 637–647, 1985.</li>

      <li>[15] R. Fagin, M. Naor and P. Winkler. Comparing Information Without Leaking It. Communications of the ACM, 39, pages 77–85, 1996.</li>

      <li>[16] U. Feige. Alternative Models for Zero Knowledge Interactive Proofs. Ph.D. Thesis, Department of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot, Israel, 1990. Available from http://www.wisdom.weizmann.ac.il/<span class="math">\\sim</span>feige.</li>

      <li>[17] U. Feige and A. Shamir. Zero-Knowledge Proofs of Knowledge in Two Rounds. In CRYPTO’89, Springer-Verlag (LNCS 435), pages 526–544, 1989.</li>

      <li>[18] O. Goldreich. Secure Multi-Party Computation. Manuscript. Preliminary version, 1998. Available from http://www.wisdom.weizmann.ac.il/<span class="math">\\sim</span>oded/pp.html.</li>

      <li>[19] O. Goldreich. Foundations of Cryptography: Volume 1 – Basic Tools. Cambridge University Press, 2001.</li>

      <li>[20] O. Goldreich and A. Kahan. How To Construct Constant-Round Zero-Knowledge Proof Systems for NP. Journal of Cryptology, 9(3):167–189, 1996.</li>

      <li>[21] O. Goldreich and H. Krawczyk. On the Composition of Zero-Knowledge Proof Systems. SIAM Journal on Computing, 25(1):169–192, 1996.</li>

      <li>[22] S. Goldwasser and L. Levin. Fair Computation of General Functions in Presence of Immoral Majority. In CRYPTO’90, Spring-Verlag (LNCS 537), pages 77–93, 1990.</li>

      <li>[23] S. Goldwasser, S. Micali and C. Rackoff. The Knowledge Complexity of Interactive Proof Systems. SICOMP, 18(1):186–208, 1989.</li>

      <li>[24] O. Goldreich, S. Micali and A. Wigderson. Proofs that Yield Nothing but their Validity or All Languages in NP Have Zero-Knowledge Proof Systems. JACM, 38(1):691–729, 1991.</li>

    </ul>

    <p class="text-gray-300">[25] O. Goldreich, S. Micali and A. Wigderson. How to Play any Mental Game – A Completeness Theorem for Protocols with Honest Majority. In 19th STOC, pages 218–229, 1987. For details see [18].</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[26] S. Halevi and S. Micali. More on Proofs of Knowledge. Cryptology ePrint Archive, 1998/015, 1998.</li>

      <li>[27] S. Micali and P. Rogaway. Secure Computation. Unpublished manuscript, 1992. Preliminary version in CRYPTO’91, Springer-Verlag (LNCS 576), pages 392–404, 1991.</li>

      <li>[28] M. Naor. Bit Commitment using Pseudorandom Generators. Journal of Cryptology, 4(2):151–158, 1991.</li>

      <li>[29] M. Naor and K. Nissim. Communication Preserving Protocols for Secure Function Evaluation. In 33rd STOC, pages 590–599, 2001.</li>

      <li>[30] M. Naor, R. Ostrovsky, R. Venkatesan and M. Yung. Zero-Knowledge Arguments for NP can be Based on General Assumptions. Journal of Cryptology, 11(2):87–108, 1998.</li>

      <li>[31] M. Naor and B. Pinkas. Oblivious Transfer and Polynomial Evaluation. In 31st STOC, pages 245–254, 1999.</li>

      <li>[32] M. Naor and M. Yung. Universal One-Way Hash Functions and their Cryptographic Applications. In 21st STOC, pages 33–43, 1989.</li>

      <li>[33] A.C. Yao. How to Generate and Exchange Secrets. In 27th FOCS, pages 162–167, 1986.</li>

    </ul>

    <h2 id="sec-66" class="text-2xl font-bold">Appendix A Expected Polynomial-Time Adversaries and Zero-Knowledge Arguments of Knowledge</h2>

    <p class="text-gray-300">In this work we considered a slightly non-standard definition of secure computation in which the real-model adversary is allowed to run in expected polynomial-time (rather than being limited to strict polynomial-time). This means that the simulator for the zero-knowledge argument system must remain expected polynomial-time also in the case that the verifier runs in expected polynomial-time; likewise for the extractor working with an expected polynomial-time prover of knowledge. (When computing the running-time of the simulator, we include the running-time of the verifier; likewise for the extractor and prover.) However, the standard definitions of zero-knowledge arguments of knowledge (and specifically the one used by Feige and Shamir [17]) refer to strict polynomial-time adversaries only.</p>

    <p class="text-gray-300">In this appendix, we show that the [17] argument system remains a zero-knowledge argument of knowledge even when the verifier and prover may run in expected polynomial-time. However, it seems that this does not hold for all zero-knowledge arguments of knowledge. In particular, as we will see, the zero-knowledge proof of [20] seems not to remain zero-knowledge if the verifier may run in expected polynomial-time.</p>

    <h3 id="sec-67" class="text-xl font-semibold mt-8">Failure of the naive approach.</h3>

    <p class="text-gray-300">A naive approach to solving the above problem would be to simply truncate the execution of the verifier after it exceeds its expected running-time by “too much”. The intuition behind such a suggestion is that the output of the truncated verifier is very</p>

    <p class="text-gray-300">close to that of the original one. Furthermore, this truncated verifier runs in strict polynomial-time. Thus, we can simply apply the existing simulator to the truncated verifier and we are done. The conclusion would be that any zero-knowledge argument remains zero-knowledge, even when the verifier runs in expected polynomial-time. However, the above intuition is not correct in our context here. For example, consider a very simple (cheating) verifier <span class="math">V^{<em>}</span> who with probability <span class="math">1/p(n)</span>, for some polynomial <span class="math">p(\\cdot)</span>, plays the honest verifier strategy, and otherwise it aborts (not sending any message). In addition, when <span class="math">V^{</em>}</span> does not abort, it runs for <span class="math">p(n)</span> steps before sending any messages. Now, the expected running-time of <span class="math">V^{<em>}</span> equals that of the honest verifier. Thus, according to the above strategy, we would truncate <span class="math">V^{</em>}</span>’s execution after it exceeds, say, <span class="math">n^{2}</span> times the running-time of the honest verifier. The point is that if <span class="math">p(n)</span> is larger than this value, then the truncated <span class="math">V^{<em>}</span> </em>never replies<em>. Thus, the simulation of the truncated <span class="math">V^{</em>}</span> can be distinguished from real executions of <span class="math">V^{<em>}</span> with probability <span class="math">1/p(n)</span>. (Of course, this specific verifier can be simulated because <span class="math">p(n)</span> is a polynomial. Nevertheless, the example suffices for ruling out the above strategy as presented. For more discussion and examples, see </em>[16, Section 3]<em> and </em>[2]*.)</p>

    <h3 id="sec-68" class="text-xl font-semibold mt-8">A.1 The Zero-Knowledge Arguments of Knowledge of <em>[17]</em></h3>

    <p class="text-gray-300">We assume that the reader is familiar with the <em>[17]</em> system of zero-knowledge arguments of knowledge.</p>

    <h4 id="sec-69" class="text-lg font-semibold mt-6">A.1.1 Extraction from an expected polynomial-time prover</h4>

    <p class="text-gray-300">In this section, we consider a generic extractor <span class="math">K</span> that works in the following way. Let <span class="math">P^{<em>}</span> be a prover with input <span class="math">x</span> and auxiliary input <span class="math">y</span>. Then, the extractor <span class="math">K</span> chooses a uniformly distributed random-tape <span class="math">r</span> for <span class="math">P^{</em>}</span>, defining a prover <span class="math">P^{*}_{x,y,r}</span>, and works as follows:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Interact with <span class="math">P^{*}_{x,y,r}</span> and play the honest verifier <span class="math">V</span>. If <span class="math">V</span> rejects, then halt and output <span class="math">\\bot</span>. If <span class="math">V</span> accepts, then continue to the next stage.</li>

    </ol>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">2. Let <span class="math">q(\\cdot)</span> be a fixed, predetermined polynomial. Then, interact with <span class="math">P^{*}_{x,y,r}</span> and play the honest verifier <span class="math">V</span> in many interactions, until $q(</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">x</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">)<span class="math"> accepting executions are achieved. In each execution, use fresh randomness for </span>V<span class="math"> (whereas </span>P^{*}$’s randomness is fixed for all executions).</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">The extractor for the system of arguments of knowledge of <em>[17]</em> (which is actually <span class="math">n</span> parallel executions of Blum’s proof of knowledge of Hamiltonicity) works in exactly this way. For this scheme, the polynomial <span class="math">q(\\cdot)</span> of Stage 2 is <span class="math">q(n)\\equiv 1</span> (that is, one additional successful execution is enough, see <em>[16]</em>). Other proofs of knowledge can also be cast in this setting (e.g., the extractor for the <em>[24]</em> proof of knowledge based on coloring can also work in this way). For simplicity, we show the expected running-time of <span class="math">K</span> when <span class="math">q(n)\\equiv 1</span>. The general case is easily derived.</p>

    <p class="text-gray-300">We begin by analyzing the running-time of Stage 2 of the extraction procedure of <span class="math">K</span> (Stage 3 can be ignored since it merely involves a fixed polynomial-time computation). First, denote by <span class="math">t_{\\mbox{\\tiny P}}(r,s)</span>, the running-time of <span class="math">P^{<em>}</span> in an execution with the honest verifier <span class="math">V</span>, where <span class="math">P^{</em>}</span>’s random-tape equals <span class="math">r</span> and <span class="math">V</span>’s random-tape equals <span class="math">s</span> (we note that by <span class="math">P^{<em>}</span> we really mean <span class="math">P^{</em>}_{x,y}</span>). Now, notice</p>

    <p class="text-gray-300">that when <span class="math">P^{<em>}</span> interacts with the honest verifier, its messages are a deterministic function of <span class="math">r</span> and <span class="math">s</span>. Therefore, <span class="math">t_{\\text{\\tiny{P}}}(r,s)</span> is a fixed running-time. Next, let <span class="math">\\chi_{\\text{\\tiny{P}}}(r,s)=1</span> if and only if <span class="math">V</span> accepts when interacting with <span class="math">P^{</em>}</span>, when <span class="math">P^{*}</span> and <span class="math">V</span>’s respective random tapes are <span class="math">r</span> and <span class="math">s</span>. Then, the running time of Stage 2 of the extraction procedure exactly corresponds to the random variable <span class="math">T_{r}</span> in the following game, where <span class="math">r</span> is a fixed value (and the probability is taken over the extractor’s coins). Recall that <span class="math">r</span> is fixed for all of the executions, whereas <span class="math">s</span> is different in each execution. Assume, for simplicity, that the length of <span class="math">V</span>’s random-tape <span class="math">s</span> is <span class="math">n</span>.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Initialize <span class="math">T_{r}=0</span>.</li>

      <li>Iterate:</li>

    </ul>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Choose <span class="math">s\\in_{R}\\{0,1\\}^{n}</span></li>

      <li>Let <span class="math">T_{r}=T_{r}+t_{\\text{\\tiny{P}}}(r,s)</span></li>

      <li>If <span class="math">\\chi_{\\text{\\tiny{P}}}(r,s)=1</span>, then halt.</li>

    </ol>

    <p class="text-gray-300">We now compute the expected value of <span class="math">T_{r}</span>. (In the claim below, <span class="math">p</span> corresponds to the probability that <span class="math">P^{*}</span>, with randomness <span class="math">r</span>, successfully completes the proof when the random tape <span class="math">s</span> is uniformly chosen.)</p>

    <h6 id="sec-70" class="text-base font-medium mt-4">Claim A.1</h6>

    <p class="text-gray-300">For a given <span class="math">r</span>, denote by <span class="math">p</span> the probability <span class="math">(</span>over a uniform choice of <span class="math">s)</span> that <span class="math">\\chi_{\\text{\\tiny{P}}}(r,s)=1</span>. Then,</p>

    <p class="text-gray-300"><span class="math">\\mathrm{Exp}[T_{r}]=\\frac{1}{p}\\cdot\\frac{1}{2^{n}}\\sum_{s\\in\\{0,1\\}^{n}}t_{\\text{\\tiny{P}}}(r,s)</span></p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Proof: Denote the set of “good” random-tapes by <span class="math">G\\stackrel{{\\scriptstyle\\text{def}}}{{=}}\\{s\\mid\\chi_{\\text{\\tiny{P}}}(r,s)=1\\}</span>. Then, we have that $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">G</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">=p\\cdot 2^{n}<span class="math">, </span></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\overline{G}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">=(1-p)\\cdot 2^{n}$ and</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$\\mathrm{Exp}[t_{\\text{\\tiny{P}}}(r,G)]=\\frac{1}{</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">G</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}\\sum_{s\\in G}t_{\\text{\\tiny{P}}}(r,s)$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">and</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$\\mathrm{Exp}[t_{\\text{\\tiny{P}}}(r,\\overline{G})]=\\frac{1}{</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\overline{G}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">}\\sum_{s\\not\\in G}t_{\\text{\\tiny{P}}}(r,s)$</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Now, the expected number of times that the process of choosing a random-tape <span class="math">s</span> is repeated until (but not including) success equals <span class="math">\\frac{1}{p}-1</span>. Thus, we first show the following intuitive equality:</p>

    <p class="text-gray-300"><span class="math">\\mathrm{Exp}[T_{r}]=\\left(\\frac{1}{p}-1\\right)\\cdot\\mathrm{Exp}[t_{\\text{\\tiny{P}}}(r,\\overline{G})]+\\mathrm{Exp}[t_{\\text{\\tiny{P}}}(r,G)]</span> (6)</p>

    <p class="text-gray-300">Eq. (6) is shown as follows. First, notice that we can divide the expectation of <span class="math">T_{r}</span> into events relating to how many iterations of the game occur until halt is reached. That is,</p>

    <p class="text-gray-300"><span class="math">\\mathrm{Exp}[T_{r}]=\\sum_{k}\\mathrm{Pr}[k\\text{ iterations until halt}]\\cdot\\mathrm{Exp}[T_{r}\\mid k\\text{ iterations until halt}]</span></p>

    <p class="text-gray-300">Noticing further that,</p>

    <p class="text-gray-300"><span class="math">\\mathrm{Exp}[T_{r}\\mid k\\text{ iterations until halt}]=(k-1)\\cdot\\mathrm{Exp}[t_{\\text{\\tiny{P}}}(r,\\overline{G})]+\\mathrm{Exp}[t_{\\text{\\tiny{P}}}(r,G)]</span></p>

    <p class="text-gray-300">we have:</p>

    <p class="text-gray-300">$$ \\begin{array}{l} \\operatorname{Exp}[T_r] = \\sum_k (1 - p)^{k-1} p \\cdot \\left((k - 1) \\cdot \\operatorname{Exp}[t_{\\mathrm{P}}(r, \\overline{G})] + \\operatorname{Exp}[t_{\\mathrm{P}}(r, G)]\\right) \\\\ = \\sum_k (1 - p)^{k-1} p \\cdot k \\cdot \\operatorname{Exp}[t_{\\mathrm{P}}(r, \\overline{G})] - \\sum_k (1 - p)^{k-1} p \\cdot \\operatorname{Exp}[t_{\\mathrm{P}}(r, \\overline{G})] \\\\</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>\\sum_k (1 - p)^{k-1} p \\cdot \\operatorname{Exp}[t_{\\mathrm{P}}(r, G)] \\\\</li>

    </ul>

    <p class="text-gray-300">= \\left(\\frac{1}{p} - 1\\right) \\operatorname{Exp}[t_{\\mathrm{P}}(r, \\overline{G})] + \\operatorname{Exp}[t_{\\mathrm{P}}(r, G)] \\end{array} $$</p>

    <p class="text-gray-300">proving Eq. (6). Then,</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{array}{l} \\operatorname{Exp}[T_r] = \\left(\\frac{1}{p} - 1\\right) \\cdot \\operatorname{Exp}[t_{\\mathrm{P}}(r, \\overline{G})] + \\operatorname{Exp}[t_{\\mathrm{P}}(r, G)] \\\\ = \\frac{1}{p} \\left((1 - p) \\cdot \\operatorname{Exp}[t_{\\mathrm{P}}(r, \\overline{G})] + p \\cdot \\operatorname{Exp}[t_{\\mathrm{P}}(r, G)]\\right) \\\\ = \\frac{1}{p} \\left(\\frac{1}{2^n} \\sum_{s \\notin G} t_{\\mathrm{P}}(r, s) + \\frac{1}{2^n} \\sum_{s \\in G} t_{\\mathrm{P}}(r, s)\\right) \\\\ = \\frac{1}{p} \\cdot \\frac{1}{2^n} \\sum_{s \\in \\{0, 1\\}^n} t_{\\mathrm{P}}(r, s) \\end{array}</span></div>

    <p class="text-gray-300">completing the proof.</p>

    <p class="text-gray-300">Before continuing, we restate Claim A.1 in terms of the running-time of <span class="math">P^<em></span>. That is, let <span class="math">t_{\\mathrm{P}}(r)</span> denote the random variable of the running-time of <span class="math">P^</em></span>, with randomness <span class="math">r</span>, when interacting with the honest verifier (with a uniformly chosen random-tape). Then, we have that</p>

    <div class="my-4 text-center"><span class="math-block">\\operatorname{Exp}[t_{\\mathrm{P}}(r)] = \\frac{1}{2^n} \\sum_{s \\in \\{0, 1\\}^n} t_{\\mathrm{P}}(r, s)</span></div>

    <p class="text-gray-300">Therefore, by Claim A.1 we have that the expected running-time of Stage 2 is</p>

    <div class="my-4 text-center"><span class="math-block">\\operatorname{Exp}[T_r] = \\frac{1}{p} \\cdot \\operatorname{Exp}[t_{\\mathrm{P}}(r)]</span></div>

    <p class="text-gray-300">We are now ready to conclude the analysis of the overall running-time of the extractor. Recall that the extractor first (honestly) verifies the proof from the prover. If it succeeds (and this occurs with probability <span class="math">p</span>), then Stage 2 of the extraction procedure is run. Therefore, the expected running-time (when <span class="math">r</span> is fixed) equals:</p>

    <div class="my-4 text-center"><span class="math-block">\\operatorname{Exp}[t_{\\mathrm{P}}(r)] + p \\cdot \\operatorname{Exp}[T_r] = \\operatorname{Exp}[t_{\\mathrm{P}}(r)] + p \\cdot \\frac{1}{p} \\cdot \\operatorname{Exp}[t_{\\mathrm{P}}(r)] = 2 \\cdot \\operatorname{Exp}[t_{\\mathrm{P}}(r)]</span></div>

    <p class="text-gray-300">Until this point, we have considered a fixed <span class="math">r</span>. However, the extractor chooses <span class="math">P^<em></span>'s random-tape uniformly. Therefore, the expected running-time of the extractor (over all possible <span class="math">r</span>'s) equals exactly twice the expected running time of <span class="math">P^</em></span>. Recalling that <span class="math">P^*</span> is expected polynomial-time, this completes the analysis.</p>

    <p class="text-gray-300">36</p>

    <p class="text-gray-300">A.1.2 Simulation for expected polynomial-time verifiers</p>

    <p class="text-gray-300">Loosely speaking, the Feige-Shamir argument system works in two stages. In the first stage, the verifier proves that it knows some secret, using a (witness hiding) proof of knowledge. Then, in the second stage, the prover proves that it knows the witness to the input statement. However, this second proof is designed in such a way so that if the secret from the first stage is known, then the proof can be successfully proven without really knowing the witness to the input statement. Furthermore, this proof is indistinguishable from a real proof. The zero-knowledge simulator for this argument system therefore works by extracting the secret from the first stage and then using it to prove the proof of the second stage. Thus, in order to show that the simulator remains expected polynomial-time when the verifier may run in expected polynomial-time, we must first show that the extraction procedure does not take “too long”. However, in Section A.1.1, we have already shown that the extraction procedure in this setting terminates in expected polynomial-time. It therefore remains to show that the second stage of the argument system also terminates in expected polynomial-time.</p>

    <p class="text-gray-300">The basis for this claim is as follows. First, notice that the second stage of the simulation requires no rewinding and works by running a single proof with the verifier <span class="math">V^{<em>}</span>. Furthermore, and this is the key point, there exists a (not necessarily efficient) interactive machine who interacts with <span class="math">V^{</em>}</span> and generates exactly the same distribution of messages as the simulator (without being given the secret obtained by the extractor from the first stage). This machine simply extracts the secret by itself (it is not computationally bounded and can therefore do this) and uses it in the same way as the simulator. Since the verifier is expected polynomial-time when interacting with this machine, this also holds when interacting with the simulator of the second stage. We conclude that the entire simulation terminates in expected polynomial-time.</p>

    <h3 id="sec-71" class="text-xl font-semibold mt-8">A.2 The Zero-Knowledge Proof System of <em>[20]</em></h3>

    <p class="text-gray-300">In this section, we show that in contrast to the argument system of <em>[17]</em>, the simulator provided for the zero-knowledge proof system of Goldreich and Kahan <em>[20]</em> does <em>not</em> necessarily remain expected polynomial-time when simulating for an expected polynomial-time verifier. We stress that we do not claim that it is impossible to construct a <em>different</em> simulator that will have this property. However, it seems from our analysis below that it would be difficult to construct such a simulator.</p>

    <p class="text-gray-300">For this section, we assume familiarity with the proof system of <em>[20]</em>. Recall that in this proof system, the verifier begins by committing to its random query string (using a perfectly hiding commitment scheme). The parties then continue by running the zero-knowledge proof for 3-coloring of <em>[24]</em> in parallel, using the verifier’s queries from the first step. That is, the prover sends (perfectly binding) commitments to randomly permuted colorings of the graph. Then, the verifier decommits, revealing its query string. Finally, the prover answers according to the revealed queries. The exact soundness of the system depends on the number of parallel executions and is negligible. We denote the soundness of the proof system by <span class="math">\\mu(n)</span> (i.e., the probability that <span class="math">V</span> accepts and the graph is not 3-colorable is less than <span class="math">\\mu(n)</span>). We stress that the exact value of <span class="math">\\mu(n)</span> can be calculated and this does not depend on any computational assumptions.</p>

    <p class="text-gray-300">Before proceeding, we note that the prover’s commitments (to the colorings) are only computationally hiding. Therefore, given enough time, it is possible to break them and extract the committed values (which in this case equals the coloring itself). In particular, in time <span class="math">2^{n}</span> (where <span class="math">n</span> is the security parameter), it is possible to break these commitments.</p>

    <p class="text-gray-300">Loosely speaking, we will construct a verifier that with probability <span class="math">2^{-n}</span> runs for <span class="math">2^{n}</span> steps and breaks the prover’s commitments. Then, the verifier checks if these commitments are “real” or</p>

    <p class="text-gray-300">“convincing garbage”, where convincing garbage is a commitment that would convince the verifier, yet does not constitute a legal 3-coloring. Then, if it finds that it received convincing garbage, it enters a very long loop (and otherwise continues like the honest verifier). The key point is that although the simulator can generate convincing garbage, the probability that any (even all-powerful) machine can do the same is negligible. Therefore, when interacting in a real protocol execution, the verifier enters the loop with very small probability. On the other hand, the simulator always generates convincing garbage. By correctly choosing the number of steps run by the verifier in the loop, we can ensure that its overall expected-time during simulation is super-polynomial. Details follow.</p>

    <p class="text-gray-300">The Verifier <span class="math">V^{*}</span>:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Send the prover a perfectly-hiding commitment to a random query string <span class="math">q</span>, exactly according to the protocol specification.</li>

      <li>Upon receiving the prover’s commitments (to many 3-colorings) do the following:</li>

    </ol>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>With probability <span class="math">2^{-n}</span>, break the prover’s commitments and obtain the values. (This takes time at most <span class="math">2^{n}</span>.)</li>

    </ul>

    <p class="text-gray-300">If the commitments are such that none of them constitute a valid 3-coloring, yet they all answer the query string <span class="math">q</span> perfectly, then run for <span class="math">2^{n}/\\mu(n)</span> steps.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Continue in the same way as the honest verifier.</li>

    </ol>

    <p class="text-gray-300">We first claim that <span class="math">V^{<em>}</span> is an expected polynomial-time machine. This can be seen as follows. <span class="math">V^{</em>}</span> attempts to break the commitments with probability <span class="math">2^{-n}</span>. Therefore, the <span class="math">2^{n}</span> time it takes to do this contributes only a single step to its expected running-time. Furthermore, the probability that any machine sends a commitment of the form that causes <span class="math">V^{<em>}</span> to run for <span class="math">2^{n}/\\mu(n)</span> steps is at most <span class="math">\\mu(n)</span> (by the soundness of the proof system). Therefore, <span class="math">V^{</em>}</span> runs for <span class="math">2^{n}/\\mu(n)</span> steps only with probability <span class="math">2^{-n}\\cdot\\mu(n)</span> and this also contributes only a single step to its expected running-time. That is, the expected running-time of <span class="math">V^{*}</span> is at most:</p>

    <p class="text-gray-300"><span class="math">\\frac{1}{2^{n}}\\cdot\\left(2^{n}+\\mu(n)\\cdot\\frac{2^{n}}{\\mu(n)}+p(n)\\right)+\\left(1-\\frac{1}{2^{n}}\\right)\\cdot p(n)=\\mathrm{poly}(n)</span></p>

    <p class="text-gray-300">where <span class="math">p(n)</span> equals the running-time of the honest verifier.</p>

    <p class="text-gray-300">On the other hand, we claim that the <em>[20]</em> simulator runs for a super-polynomial number of steps, when simulating for this <span class="math">V^{<em>}</span>. In particular, this simulator always sends a commitment that causes <span class="math">V^{</em>}</span> to run in time <span class="math">2^{n}/\\mu(n)</span>. Therefore, the expected running time of the simulator of <span class="math">V^{*}</span> is greater than:</p>

    <p class="text-gray-300"><span class="math">\\frac{1}{2^{n}}\\cdot\\left(2^{n}+1\\cdot\\frac{2^{n}}{\\mu(n)}+p(n)\\right)+\\left(1-\\frac{1}{2^{n}}\\right)\\cdot p(n)&gt;\\frac{1}{\\mu(n)}</span></p>

    <p class="text-gray-300">Since <span class="math">\\mu(n)</span> is a negligible function, we have that the expected running-time of the simulator is super-polynomial. Therefore, the simulator presented by <em>[20]</em> for demonstrating the zero-knowledge property of their proof system is only expected polynomial-time if the verifier is limited to strict polynomial-time.</p>

    <p class="text-gray-300">We conclude with an open question that is raised by the above observation regarding the [20] proof system. That is, is it possible to construct a constant-round zero-knowledge proof system for <span class="math">\\mathcal{NP}</span> (with negligible soundness), that remains zero-knowledge for an expected polynomial-time verifier?</p>`;
---

<BaseLayout title="Parallel Coin-Tossing and Constant-Round Secure Two-Party Co... (2001/107)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2001 &middot; eprint 2001/107
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <p class="mt-4 text-xs text-gray-500">
        All content below belongs to the original authors. This page
        reproduces the paper for educational purposes. Always
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >cite the original</a>.
      </p>
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <Fragment set:html={CONTENT} />

  </article>
</BaseLayout>
