---
import BaseLayout from '../../layouts/BaseLayout.astro';
import PaperDisclaimer from '../../components/PaperDisclaimer.astro';
import PaperHistory from '../../components/PaperHistory.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2015/268';
const CRAWLER = 'marker';
const CONVERTED_DATE = '2026-02-19';
const TITLE_HTML = '7 Applying Our New Algorithms to the Simon Family of Block Ciphers';
const AUTHORS_HTML = 'The Simon family of lightweight block ciphers, presented in [4], is implemented using a balanced Feistel structure. The Simon round function is very simple and consists of only three operations: AND, XOR and constant rotations. All the ciphers in the Simon family use the same round function and differ only by the key size, the block size (which ranges from 32 to 128 bits) and the number of Feistel rounds which is dependant on the former two. As in any Feistel structure, the plaintext is divided into two blocks of size n:  $P = (L_0, R_0)$  and then every round  $1 \\le i \\le r$ :';

const CONTENT = `    <h2 id="sec-1" class="text-2xl font-bold">Improved Top-Down Techniques in Differential Cryptanalysis</h2>

    <p class="text-gray-300">Itai Dinur&lt;sup&gt;1&lt;/sup&gt; , Orr Dunkelman2,3,? , Masha Gutman&lt;sup&gt;3&lt;/sup&gt; , and Adi Shamir&lt;sup&gt;3&lt;/sup&gt;</p>

    <p class="text-gray-300">Abstract. The fundamental problem of differential cryptanalysis is to find the highest entries in the Difference Distribution Table (DDT) of a given mapping F over n-bit values, and in particular to find the highest diagonal entries which correspond to the best iterative characteristics of F. The standard bottom-up approach to this problem is to consider all the internal components of the mapping along some differential characteristic, and to multiply their transition probabilities. However, this can provide seriously distorted estimates since the various events can be dependent, and there can be a huge number of low probability characteristics contributing to the same high probability entry. In this paper we use a top-down approach which considers the given mapping as a black box, and uses only its input/output relations in order to obtain direct experimental estimates for its DDT entries which are likely to be much more accurate. In particular, we describe three new techniques which reduce the time complexity of three crucial aspects of this problem: Finding the exact values of all the diagonal entries in the DDT for small values of n, approximating all the diagonal entries which correspond to low Hamming weight differences for large values of n, and finding an accurate approximation for any DDT entry whose large value is obtained from many small contributions. To demonstrate the potential contribution of our new techniques, we apply them to the SIMON family of block ciphers, show experimentally that most of the previously published bottom-up estimates of the probabilities of various differentials are off by a significant factor, and describe new differential properties which can cover more rounds with roughly the same probability for several of its members. In addition, we show how to use our new techniques to attack a 1-key version of the iterated Even-Mansour scheme in the related key setting, obtaining the first generic attack on 4 rounds of this well-studied construction.</p>

    <p class="text-gray-300">Keywords: differential cryptanalysis, difference distribution tables, iterative characteristics, Even-Mansour, SIMON.</p>

    <p class="text-gray-300">&lt;sup&gt;1&lt;/sup&gt; D´epartement d'Informatique, Ecole Normale Sup´erieure, Paris, France ´ &lt;sup&gt;2&lt;/sup&gt; Computer Science Department, University of Haifa, Israel</p>

    <p class="text-gray-300">&lt;sup&gt;3&lt;/sup&gt; Computer Science department, The Weizmann Institute, Rehovot, Israel</p>

    <p class="text-gray-300">&lt;sup&gt;?&lt;/sup&gt; The second author was supported in part by the Israel Science Foundation through grants No. 827/12 and No. 1910/12.</p>

    <h2 id="sec-2" class="text-2xl font-bold">1 Introduction</h2>

    <p class="text-gray-300">Differential cryptanalysis, which was first proposed in [6], is one of the best known and most widely used tools for breaking the security of many types of cryptographic schemes (including block ciphers, stream ciphers, keyed and unkeyed hash functions, etc). Its main component is a Difference Distribution Table (abbreviated as DDT) which describes how many times each input difference is mapped to each output difference by a given mapping F over n-bit values. The DDT table has exponential size (with 2&lt;sup&gt;n&lt;/sup&gt; rows and 2&lt;sup&gt;n&lt;/sup&gt; columns), but we are usually interested only in its large entries: When we try to attack an existing scheme we try to find the largest DDT entry, and when we develop a new cryptographic scheme we try to demonstrate that all the DDT entries are smaller than some bound.</p>

    <p class="text-gray-300">For large value of n such as 128, it is impractical to find the exact value of even a single entry in the table, but in most cases we are only interested in finding a sufficiently good approximation of its large values. There are many proposed algorithms for computing such approximations, but almost all of them are bottom-up techniques which start by analyzing the differential properties of small components such as a single S-box, and then combine them into large components such as a reduced-round version of the full scheme. To find the best differential attack, they use the detailed description of the scheme in order to identify a consistent collection of high probability differential properties of all the small components, and then multiply all these probabilities. In order to claim that there are no high probability differentials, they lower bound the number of multiplied probabilities, e.g., by showing that any differential characteristic has a large number of active S-boxes.</p>

    <p class="text-gray-300">A second problem is that in most cases, this bottom-up approach concentrates on a single differential characteristic and describes one particular way in which the given input difference can give rise to the given output difference by specifying all the intermediate differences. Moreover, this approach is also more susceptible to variations from the Markov cipher model, where dependence between different rounds can lead to an estimation of probability which is far from the correct value.</p>

    <p class="text-gray-300">In this paper we follow a different top-down approach, in which we consider the given mapping as a black box and ignore its internal structure. In particular, we do not multiply or add a large number of of probabilities associated with its smallest components, and thus we do not suffer from the three methodological problems listed above. Our goal is to use the smallest possible number of evaluations of the given mapping in order to compute either the precise value or a sufficiently good approximation of the most interesting entries in its DDT.</p>

    <p class="text-gray-300">A straightforward black box algorithm can calculate the exact value of any particular entry in the DDT table in 2&lt;sup&gt;n&lt;/sup&gt; time by evaluating the mapping for all the pairs of inputs with the desired input difference, and counting how many times we got the desired output difference. When we want to compute a set of k entries in the DDT, we can always repeat the computation for each entry separately and thus get a k2 &lt;sup&gt;n&lt;/sup&gt; upper bound on the time complexity. However, for some large sets of entries we can do much better. In particular, we can compute all the k = 2&lt;sup&gt;n&lt;/sup&gt; entries in a single row (which corresponds to a fixed input difference and arbitrary output differences) with the same 2&lt;sup&gt;n&lt;/sup&gt; time complexity by using the same algorithm. This also implies that the whole DDT can be computed in 22&lt;sup&gt;n&lt;/sup&gt; time, whereas a naive algorithm which computes each one of the 22&lt;sup&gt;n&lt;/sup&gt; entries separately would require 23&lt;sup&gt;n&lt;/sup&gt; time. If the mapping is a permutation and we are also given its inverse as a black box, we can similarly compute each column in the DDT (which corresponds to a fixed output difference and arbitrary input differences) in 2&lt;sup&gt;n&lt;/sup&gt; time by applying the inverse black box to all the pairs with the desired output difference.</p>

    <p class="text-gray-300">Which other sets of entries in the DDT can be simultaneously computed faster than via the naive algorithm? The first result we show in this paper is a new technique called the diagonal algorithm, which can calculate the exact values of all the 2&lt;sup&gt;n&lt;/sup&gt; diagonal entries in the DDT (whose input and output differences are equal) with a total time complexity of about 2&lt;sup&gt;n&lt;/sup&gt;. These entries in the DDT are particularly interesting in differential cryptanalysis, since they describe the probabilities of all the possible iterative characteristics which can be concatenated to themselves an arbitrarily large number of times in a consistent way. For many well known cryptosystems (such as DES), the best known differential attack on the scheme is based on such iterative characteristics. We then extend the diagonal algorithm to generalized diagonals which are defined as sets of 2&lt;sup&gt;n&lt;/sup&gt; DDT entries in which the input difference and output difference are linearly related rather than equal. This can be particularly useful in schemes such as Feistel structures, in which we are often interested in output differences which are equal to the input differences but with swapped halves.</p>

    <p class="text-gray-300">In many applications of differential cryptanalysis, we can argue that only rows in the DDT which correspond to input differences with low Hamming weight can contain large values (and thus lead to efficient attacks). Our next result is a new top-down algorithm which we call the Hamming Ball algorithm, which can efficiently identify all the large diagonal entries in the DDT whose input and output differences have a low Hamming weight, and approximate their values.</p>

    <p class="text-gray-300">Our third result is a new bins-in-the-middle(BITM) algorithm for computing in a more efficient way an improved approximation for any particular DDT entry whose high value may be accumulated from a large number of differential characteristics which have much smaller probabilities. In this algorithm we assume that the given mapping is only quasi black box in the sense that it is the concatenation of two black boxes which can be computed separately. A typical example of such a situation is a cryptographic scheme which consists of many rounds, where we can choose in our analysis how many rounds we want to evaluate in the first black box, and then define the remaining rounds as the second black box.</p>

    <p class="text-gray-300">In our complexity analysis, we assume that most of the DDT entries are distributed as if the mapping is randomly chosen, but a small number of entries have unusually large values which we would like to locate and to estimate by evaluating the mapping on the smallest possible number of inputs. This is analogous to classical models of random graphs in which we try to identify some planted structure such as a large clique which was artificially added to the random graph.</p>

    <p class="text-gray-300">To demonstrate the power of our new techniques, we used the relatively new but extensively studied proposal of the Simon family of lightweight block ciphers, which was developed by a team of experienced cryptographers from the NSA. Several previous papers [1,2,9,28] tried to find the best possible differential properties of reduced-round variants of Simon with the bottom-up approach by analyzing its individual components. By using our new top-down techniques, we can provide strong experimental evidence that the previous probability estimates were inaccurate, and in fact we found new differential properties which are either longer by two rounds or have better probabilities for the same number of rounds compared to all the previously published results.</p>

    <p class="text-gray-300">The paper is organized as follows. After introducing our notation in Section 2, we survey in Section 3 the main bottom-up techniques for estimating differential probabilities which were proposed in the literature. Our three new top-down techniques are described in Section 4, Section 5, and Section 6. We describe the application of our new techniques to the Simon family of block ciphers in Section 7. Section 8 shows show how to use these top-down techniques in order to analyze the differential properties of the Even-Mansour scheme (whose random permutation is only given in the form of a black box), and to find the first generic attack on its 4-round 1-key version in the related key setting.</p>

    <h2 id="sec-3" class="text-2xl font-bold">2 Notations</h2>

    <p class="text-gray-300">In this section, we describe the notations used in the rest of this paper.</p>

    <p class="text-gray-300">Given a function  <span class="math">F: \\mathbb{GF}(2)^n \\to \\mathbb{GF}(2)^n</span> , the difference distribution table (DDT) is a  <span class="math">2^n \\times 2^n</span>  table, where  <span class="math">DDT[\\Delta_I][\\Delta_O]</span>  counts the number of input pairs to F with an n-bit difference of  <span class="math">\\Delta_I</span>  whose n-bit output difference is  <span class="math">\\Delta_O</span> . More formally we define  <span class="math">DDT[\\Delta_I, \\Delta_O] \\triangleq |\\{x \\in \\mathbb{GF}(2)^n : F(x) \\oplus F(x \\oplus \\Delta_I) = \\Delta_O\\}|</span> .</p>

    <p class="text-gray-300">We define the diagonal (DIAG) of the DDT as a vector of length  <span class="math">2^n</span>  which contains only the  <span class="math">[\\Delta_I, \\Delta_O]</span>  entries for which  <span class="math">\\Delta_O = \\Delta_I</span> , namely  <span class="math">DIAG[\\Delta] \\triangleq DDT[\\Delta, \\Delta]</span> . Given an auxiliary function  <span class="math">L: \\mathbb{GF}(2)^n \\to \\mathbb{GF}(2)^n</span> , we define the generalized diagonal (GDIAG) of the DDT as a table of size  <span class="math">2^n</span> , which contains only the  <span class="math">[\\Delta_I, \\Delta_O]</span>  entries for which  <span class="math">\\Delta_O = L(\\Delta_I)</span> , namely  <span class="math">GDIAG_L[\\Delta] \\triangleq DDT[\\Delta, L(\\Delta)]</span> . Thus, the diagonal is a particular case of the generalized diagonal for which the auxiliary function L is the identity. In this paper, we are mostly interested in generalized diagonals for linear functions L (over  <span class="math">\\mathbb{GF}(2)^n</span> ), which can be computed efficiently using our algorithms.</p>

    <p class="text-gray-300">Given an n-bit word x, we denote by ham(x) its Hamming weight. Given two n-bit words x,y, we denote by dist(x,y) their Hamming distance, i.e.  <span class="math">ham(x\\oplus y)</span> . For an integer  <span class="math">0 \\le r \\le n</span> , we denote by  <span class="math">B_r(y)</span>  the Hamming ball of radius r centered at c, namely  <span class="math">B_r(c) \\triangleq \\{x|dist(x,c) \\le r\\}</span> . The number of points in  <span class="math">B_r(c)</span>  is denoted as  <span class="math">M_r^n \\triangleq |B_r(c)| = \\sum_{i=0}^r \\binom{n}{i}</span> .</p>

    <p class="text-gray-300">We denote the <em>n</em>-bit word with bits  <span class="math">i_1, ..., i_k</span>  set to 1 and the rest set to 0 by  <span class="math">e_{i_1, ..., i_k}</span> .</p>

    <h2 id="sec-4" class="text-2xl font-bold">3 Previous Work</h2>

    <h2 id="sec-5" class="text-2xl font-bold">3.1 Bottom-Up Differential Characteristic Search</h2>

    <p class="text-gray-300">Since the early works on differential cryptanalysis (including the original work of [6]), there was a need to find good differential characteristics. This need was usually answered in the bottom-up approach: In [21] Matsui described the first general purpose differential characteristic search algorithm, which uses &quot;boundand-branch&quot; approach. Matsui's algorithm is assured to find the best characteristic, but its running time may be unbounded. Later works in the field was sometimes applied to specific ciphers (e.g., analyzing FEAL in [3]), or extending Matsui's approach using basic properties of the block cipher (notably, the byteoriented ciphers studied in [7, 8, 17, 25] or the ARX constructions studied in [10, 14, 20, 22]).</p>

    <p class="text-gray-300">Offering an upper bound on the probability of differential characteristics dates back to the early works of [26], which suggested bounds for Feistel constructions, based on bounds on the probability of differential characteristics through the round function. This method is the basis of the approach of counting the number of active S-boxes (introduced in [13]), which is widely used today. Another approach introduced in [24] is the transformation of the problem into a linear-programming problem, and solving it for constraints. This technique was later extended in [27, 28].</p>

    <p class="text-gray-300">Finally, we note that [10] also explored the concept of sampling the DDT in the context of ARX constructions. If the word size is too big to be analyzed to obtain the full DDT, one may pick a reduced set of entries and compute their probability (for ARX construction one can usually compute the probability of the transition without using input pairs).</p>

    <h4 id="sec-6" class="text-lg font-semibold mt-6">3.2 Top-Down Algorithms</h4>

    <p class="text-gray-300">The first top-down algorithm which we are aware of is due to [5] — the &quot;Shrinking&quot; algorithm that searches for impossible differentials. The main idea behind the shrinking algorithm is to take a scaled-down version of the cipher (e.g., with reduced word sizes and S-boxes). Such a scaled-down version allows evaluating the full difference distribution table, which in turn can be used to automatically identify impossible differentials. However, we note that many cryptosystems cannot be scaled down in an obvious way while maintaining properties of their DDT, and therefore the applicability of this algorithm is limited.</p>

    <h2 id="sec-7" class="text-2xl font-bold">4 The Diagonal Algorithm and its Extensions</h2>

    <h4 id="sec-8" class="text-lg font-semibold mt-6">4.1 The Diagonal Algorithm</h4>

    <p class="text-gray-300">We begin by describing our basic algorithm for calculating the exact values of all the diagonal entries in the difference distribution table with about the same time complexity as computing a single entry. The algorithm is given black box access to a function F : GF(2)&lt;sup&gt;n&lt;/sup&gt; → GF(2)n, and outputs the diagonal of the difference distribution table DIAG[∆] , DDT[∆, ∆]. The algorithm is based on the simple property that the equality x⊕y = F(x)⊕F(y) along the diagonal is equivalent to the equality x ⊕ F(x) = y ⊕ F(y). Therefore, we can efficiently identify all the (x, y) pairs with equal input and output differences ∆ = x ⊕ y (which contribute to the DIAG table) by searching for all the collisions between values of x ⊕ F(x) and y ⊕ F(y).</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>Initialize all the entries of the table DIAG to zero, and set DIAG[0] to 2n.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>For each n-bit value x:</li>
    </ol>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>(a) Compute x ⊕ F(x), and store the pair (x ⊕ F(x), x) in a hash table H, i.e., add x to the set of values stored at H[x ⊕ F(x)].</li>
    </ul></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>For each n-bit value b:</li>
    </ol>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>(a) For each pair (x, y) of distinct values such that x, y ∈ H[b], increment DIAG[x ⊕ y] by 1.</li>
    </ul></li>
    </ul>

    <p class="text-gray-300">The time complexity of Steps 1 and 2 is 2&lt;sup&gt;n&lt;/sup&gt; each, and the time complexity of Step 3 is proportional to D, which denotes the total number of pairs (x, y) such that x ⊕ y = F(x) ⊕ F(y) (which is the same as the sum of all the entries in DIAG). Note that for a random function F, the expected value of D is about 2 &lt;sup&gt;2&lt;/sup&gt;n−&lt;sup&gt;n&lt;/sup&gt; = 2&lt;sup&gt;n&lt;/sup&gt; as we have about 2&lt;sup&gt;2&lt;/sup&gt;&lt;sup&gt;n&lt;/sup&gt; (x, y) pairs, and the probability that a pair satisfies the n-bit equality is 2&lt;sup&gt;−&lt;/sup&gt;&lt;sup&gt;n&lt;/sup&gt;. Consequently, the expected time complexity of the algorithm for a random function is about 2&lt;sup&gt;n&lt;/sup&gt;, and the total memory complexity is also 2&lt;sup&gt;n&lt;/sup&gt;, which is the size of the hash table H and the output table DIAG.</p>

    <p class="text-gray-300">We note that there are several previous algorithms whose general structure resembles the diagonal algorithm. One such algorithm is impossible differential cryptanalysis of Feistel structures [18] and its various extensions, which use a data structure similar to H to iterate over pairs with related input and output differences. However, in these algorithms H is used in order to filter pairs required to attack specific cryptosystems, and not to explicitly calculate the DDT (as we do in Step 3.(a)).</p>

    <h4 id="sec-9" class="text-lg font-semibold mt-6">4.2 The Generalized Diagonal Algorithm</h4>

    <p class="text-gray-300">We now extend the diagonal algorithm to compute a generalized diagonal GDIAG&lt;sup&gt;L&lt;/sup&gt; for any given linear function L over GF(2)&lt;sup&gt;n&lt;/sup&gt;. In this case, we are interested in (x, y) pairs such that L(F(x)⊕F(y)) = x⊕y, which is equivalent to the equality x⊕L(F(x)) = y ⊕L(F(y)), since L is linear. Therefore, the generalized diagonal algorithm is very similar to the diagonal algorithm above, and only differs in Step 2.(a), where we store the pair (x⊕L(F(x)), x) in the hash table H (instead of storing the pair (x ⊕ F(x), x)). The complexity analysis of the generalized diagonal algorithm is essentially identical to the basic diagonal algorithm.</p>

    <h2 id="sec-10" class="text-2xl font-bold">5 The Hamming Ball Algorithm</h2>

    <p class="text-gray-300">The (generalized) diagonal algorithm computes the exact value of the (generalized) diagonal of the DDT of the function F in about 2&lt;sup&gt;n&lt;/sup&gt; time, which is practical for n = 32 but marginal for n = 64. In fact, it is easy to show that information theoretically, the only way to compute the precise value of a single DDT entry is to test all the 2&lt;sup&gt;n&lt;/sup&gt; relevant pairs of inputs or outputs. However, if we assume that we only want to find large entries on the diagonal and to approximate their values, we can do much better.</p>

    <p class="text-gray-300">Assume that there exists some entry DDT[∆, L(∆)] with a value of p · 2 n (where 0 &lt; p ≤ 1 is the probability of an input pair with difference ∆ to have an output difference of L(∆)) for a fixed linear function L. A trivial adaptation to the (generalized) diagonal algorithm evaluates and stores the pairs (x ⊕ L(F(x)), x) for only 0 &lt; C ≤ 2 &lt;sup&gt;n&lt;/sup&gt; random values of x. Clearly, we do not expect to generate a non-zero value in entry DDT[∆, L(∆)] before evaluating at least p −1 (x, x ⊕ ∆) pairs. This gives a lower bound on C and on the complexity of the algorithm, since after the evaluation of C arbitrary values x, we expect to have about C 2 · 2 &lt;sup&gt;−&lt;/sup&gt;&lt;sup&gt;n&lt;/sup&gt; pairs with randomly scattered input differences, and thus we require C 2 · 2 &lt;sup&gt;−&lt;/sup&gt;&lt;sup&gt;n&lt;/sup&gt; ≥ p &lt;sup&gt;−&lt;/sup&gt;&lt;sup&gt;1&lt;/sup&gt; or C ≥ 2 n/2 · p −1/2 . Therefore, the time and memory complexity of our adaptation are still somewhat large for big domains, and in particular it is barely practical for n = 128 even when p is close to 1 (as C ≥ 2 n/&lt;sup&gt;2&lt;/sup&gt; = 2&lt;sup&gt;64&lt;/sup&gt;).</p>

    <p class="text-gray-300">We now describe a more efficient adaptation that requires the stronger assumption that the high probability entries DDT[∆, L(∆)] occur at ∆'s which have (relatively) low Hamming weight. The motivation behind this assumption is that we are interested in applying our algorithms to concrete cryptosystems in which a high probability entry DDT[∆&lt;sup&gt;I&lt;/sup&gt; , ∆O] typically indicates the existence of a high probability differential characteristic with the corresponding input-output differences. Such high probability characteristics in SP networks are likely to have a small number of active Sboxes, and thus ∆&lt;sup&gt;I&lt;/sup&gt; and ∆&lt;sup&gt;O&lt;/sup&gt; are likely to have low Hamming weights.</p>

    <p class="text-gray-300">In order to consider only DDT[∆, L(∆)] entries where ∆ is of small Hamming weight, we pick an arbitrary center c and a small radius r, and evaluate F only for inputs inside the Hamming ball Br(c). All the pairs of points inside the Hamming ball have a small Hamming distance, and thus for a carefully chosen value of r, we will obtain a quadratic number of relevant pairs from a linear number of values which have small Hamming distances d.</p>

    <p class="text-gray-300">It is easy to see that the raw estimates we get with this approach for the entries in the DDT are biased, since the Hamming ball has more pairs which differ only in their least significant bit than pairs which differ in their d least significant bits for d &gt; 1.&lt;sup&gt;1&lt;/sup&gt; Given a difference ∆ such that ham(∆) = d, an important measure which is used by our Hamming ball algorithm is the number</p>

    <p class="text-gray-300">&lt;sup&gt;1&lt;/sup&gt; This claim can be easily supported by the fact that as more bits are changed, the probability that the new computed value is outside the ball increases.</p>

    <p class="text-gray-300">of pairs with difference  <span class="math">\\Delta</span>  in  <span class="math">B_r(c)</span> . This measure, which we denote by  <span class="math">P_{r,d}^n</span>  (it does not depend on the actual values of c or  <span class="math">\\Delta</span> ), is used in order to create from the experimental data unbiased estimates for the values of the entries  <span class="math">DDT[\\Delta_I, \\Delta_O]</span> , as described below.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>Initialize the entries of the table  <span class="math">GDIAG_L</span>  to zero.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>For each <em>n</em>-bit value  <span class="math">x \\in B_r(c)</span> :</li>
    </ol>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>(a) Compute  <span class="math">x \\oplus L(F(x))</span> , and store the pair  <span class="math">(x \\oplus L(F(x)), x)</span>  in a hash table H, i.e., add x to the set of values stored at  <span class="math">H[x \\oplus L(F(x))]</span> .</li>
    </ul></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>For each n-bit value b such that H[b] contains at least 2 values:</li>
    </ol>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>(a) For each pair (x, y) such that  <span class="math">x, y \\in H[b]</span> , increment  <span class="math">GDIAG_L[x \\oplus y]</span>  by 1.</li>
    </ul></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>For each <em>n</em>-bit value  <span class="math">\\Delta</span>  such that  <span class="math">GDIAG_L[\\Delta] &gt; 0</span> :</li>
    </ol>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>(a) Denote  <span class="math">ham(\\Delta) = d</span>  and normalize the entry  <span class="math">GDIAG_L[\\Delta]</span>  by setting  <span class="math">GDIAG_L[\\Delta] \\leftarrow GDIAG_L[\\Delta] \\cdot (2^n/P_{r,d}^n)</span> .</li>
    </ul></li>
    </ul>

    <p class="text-gray-300">The time and memory complexities of Step 2 are  <span class="math">M_r^n</span> . The time and memory complexities of steps 3 and 4 are determined by the number of collisions in the hash table H, which depends on F. For a random function, we expect to have  <span class="math">(M_r^n)^2 \\cdot 2^{-n} \\leq M_r^n</span>  such collisions, and therefore we generally do not expect steps 3 and 4 to dominate the time or memory complexities of the attack (especially for large domains where we select a small r implying that  <span class="math">M_r^n \\ll 2^n</span>  and thus  <span class="math">(M_r^n)^2 \\cdot 2^{-n} \\ll M_r^n</span> ).</p>

    <p class="text-gray-300">In order to detect an entry  <span class="math">DDT[\\Delta, L(\\Delta)]</span>  with  <span class="math">ham(\\Delta) = d</span>  whose probability is p, the most efficient method (assuming that we have sufficient memory) is to select a r such that  <span class="math">B_r(c)</span>  contains about  <span class="math">p^{-1}</span>  pairs of points with input different  <span class="math">\\Delta</span> , or  <span class="math">P_{r,d}^n \\geq p^{-1}</span> .</p>

    <p class="text-gray-300">The efficiency of our algorithm for low Hamming weights is derived from the fact that Hamming balls are relatively closed under XOR's - pairs of points which are close to the origin are also close to each other. Similar efficiencies can be obtained for other sets with similar closure properties, such as arbitrary linear subspaces and sets of points which have short Hamming distance to linear subspaces.</p>

    <h4 id="sec-11" class="text-lg font-semibold mt-6">5.1 Analyzing Keyed Functions</h4>

    <p class="text-gray-300">The algorithms described so far analyze a keyless function F. In order to obtain meaningful results for a keyed function  <span class="math">F_K</span> , we assume the existence of high probability entries  <span class="math">DDT[\\Delta_I, \\Delta_O]</span> , which are common to a large fraction of the keys. Such common high probability entries are typically the result of a high probability differential characteristics (with the corresponding input-output differences) in iterated block ciphers where the round keys are XORed into the state.&lt;sup&gt;3&lt;/sup&gt;</p>

    <p class="text-gray-300">&lt;sup&gt;&amp;&lt;/sup&gt;lt;sup&gt;2&lt;/sup&gt; The computation of  <span class="math">P_{r,d}^n</span>  is discussed in Appendix 10.</p>

    <p class="text-gray-300">&lt;sup&gt;&amp;&lt;/sup&gt;lt;sup&gt;3&lt;/sup&gt; In such cases, the probability of the characteristic can be estimated independently of the round keys, assuming the input values are selected at random.</p>

    <p class="text-gray-300">Based on this assumption, we can select a few keys  <span class="math">K_i</span>  at random, and independently run our algorithms on  <span class="math">F_{K_i}</span>  for each  <span class="math">K_i</span> . Then, we look for high probability entries  <span class="math">DDT[\\Delta_I, \\Delta_O]</span>  which are common to several keys. An additional possibility is to first run our algorithms on  <span class="math">F_{K_1}</span> , and then to test the obtained high probability entries  <span class="math">DDT[\\Delta_I, \\Delta_O]</span>  on  <span class="math">F_{K_i}</span>  for i &gt; 1, by encrypting sufficiently many pairs with input difference  <span class="math">\\Delta_I</span>  for each key.</p>

    <h2 id="sec-12" class="text-2xl font-bold">6 Improved Approximation of a Single Large DDT Entry</h2>

    <p class="text-gray-300">We now turn our attention to a related problem. Assume that we found a pair of input/output differences  <span class="math">(\\Delta_I, \\Delta_O)</span>  which are somehow related. For example, this can occur when an iterative characteristic is repeated several times. Given  <span class="math">(\\Delta_I, \\Delta_O)</span> , we wish to estimate the probability of the transition  <span class="math">\\Delta_I \\xrightarrow{r} \\Delta_O</span>  (where r is the number of rounds in the differential). The standard method to estimate this probability is to take many pairs with input difference  <span class="math">\\Delta_I</span>  and check how many of them have output difference  <span class="math">\\Delta_O</span>  (again, trying multiple keys). If the probability of the differential is p, a good estimation requires  <span class="math">O(p^{-1})</span>  queries to the encryption algorithm.</p>

    <p class="text-gray-300">Now, assume that the cipher (or the rounds) for which we analyze this transition, can be divided into two (roughly equal) parts. In such a case, we can discuss the transition from  <span class="math">\\Delta_I</span>  to some  <span class="math">\\Delta_M</span>  after about r/2 rounds, and from  <span class="math">\\Delta_M</span>  to  <span class="math">\\Delta_O</span>  in the the remaining rounds. In other words, we look at  <span class="math">\\Delta_M</span>  after r' rounds, and use the fact that:</p>

    <p class="text-gray-300"><span class="math">$\\Pr[\\Delta_I \\xrightarrow{r} \\Delta_O] = \\sum_{\\Delta_M} \\Pr[\\Delta_I \\xrightarrow{r&#x27;} \\Delta_M \\xrightarrow{r-r&#x27;} \\Delta_O] \\tag{1}</span>$</p>

    <p class="text-gray-300">which by the stochastic equivalence assumption (see [19]) we can re-write as</p>

    <p class="text-gray-300"><span class="math">$\\Pr[\\Delta_I \\xrightarrow{r} \\Delta_O] = \\sum_{\\Delta_M} \\Pr[\\Delta_I \\xrightarrow{r&#x27;} \\Delta_M] \\cdot \\Pr[\\Delta_M \\xrightarrow{r-r&#x27;} \\Delta_O]</span>$
(2)</p>

    <p class="text-gray-300">To correctly evaluate the probability suggested by Equation (2), one needs to go over all possible  <span class="math">\\Delta_M</span>  values (which is usually infeasible for common block sizes), and for each one of them evaluate the probability of two shorter differentials,  <span class="math">\\Delta_I \\xrightarrow{r&#x27;} \\Delta_M</span>  and  <span class="math">\\Delta_M \\xrightarrow{r-r&#x27;} \\Delta_O</span>  (which in itself may be a hard task).</p>

    <p class="text-gray-300">Luckily, it was already observed in [6] that (in most cases) a high probability differential characteristic has several &quot;close&quot; high probability neighbors. This is explained by taking slightly different transitions through the active S-boxes with probability which is only slightly lower than the highest possible probability (used in the high probability characteristic). Similar behavior sometimes happen for differentials (especially for differentials which are based on a few &quot;strong&quot; characteristics, each having a few high probability &quot;neighbors&quot;).</p>

    <p class="text-gray-300">Hence, to give a lower bound on the value suggested by Equation (2), we can use the following computation:</p>

    <p class="text-gray-300"><span class="math">$\\Pr[\\Delta_I \\xrightarrow{r} \\Delta_O] \\ge \\sum_{\\Delta_M \\in S} \\Pr[\\Delta_I \\xrightarrow{r&#x27;} \\Delta_M] \\cdot \\Pr[\\Delta_M \\xrightarrow{r-r&#x27;} \\Delta_O] \\tag{3}</span>$</p>

    <p class="text-gray-300">where the set S contains all the  <span class="math">\\Delta_M</span>  values for which the differentials  <span class="math">\\Delta_I \\xrightarrow{r&#x27;} \\Delta_M</span>  and  <span class="math">\\Delta_M \\xrightarrow{r-r&#x27;} \\Delta_O</span>  have a sufficiently high probability.&lt;sup&gt;4&lt;/sup&gt;</p>

    <p class="text-gray-300">Obviously, this approximation relies on the fact that the two parts of the cipher are independent of each other. When taking into consideration a Markov-cipher assumption or the Stochastic Equivalence assumption (see [19] for more details), then the independence assumption immediately holds. However, in real life, one needs to verify it.</p>

    <p class="text-gray-300">One advantage of the Bins-in-the-Middle algorithm which is presented next over the standard analytical approach is the fact that we &quot;reduce&quot; the independence assumption only to the transition between the two parts of the cipher. This is to be compared with an analytical approach that computes the probability of each round independently, and then simply multiplies the probabilities of each round (i.e., approaches that assume that each round is independent of others). In the Bins-in-the-Middle algorithm, the probabilities which are multiplied are the sampled probabilities of differentials, i.e., probabilities that were experimentally verified.&lt;sup&gt;5&lt;/sup&gt;</p>

    <h2 id="sec-13" class="text-2xl font-bold">6.1 The Bins-in-the-Middle (BITM) Algorithm</h2>

    <p class="text-gray-300">We now present an algorithm that finds all the &quot;good&quot;  <span class="math">\\Delta_M</span>  values in the set S and experimentally estimates the probability of the two differentials  <span class="math">\\Delta_I \\xrightarrow{r&#x27;} \\Delta_M</span>  and  <span class="math">\\Delta_M \\xrightarrow{r-r&#x27;} \\Delta_O</span> . The algorithm requires that the last r-r' rounds are invertible (and thus, can be used only on permutations).</p>

    <p class="text-gray-300">The algorithm's basic idea is to actually produce a list of plausible  <span class="math">\\Delta_M</span>  by sampling random pairs with input difference  <span class="math">\\Delta_I</span>  (for the first r' rounds) and a corresponding list by sampling random pairs with output difference  <span class="math">\\Delta_O</span>  (for the last r-r') rounds. We shall denote the two lists,  <span class="math">L_1</span>  and  <span class="math">L_2</span> , respectively. The first list,  <span class="math">L_1</span> , contains pairs of the form  <span class="math">(\\Delta_{M_i}, p_i)</span>  (i.e., the difference  <span class="math">\\Delta_{M_i}</span>  appears with probability  <span class="math">p_i</span>  given an input difference  <span class="math">\\Delta_I</span> ). Similarly, the second list,  <span class="math">L_2</span> , contains pairs of the form  <span class="math">(\\Delta_{M_i}, q_j)</span> .</p>

    <p class="text-gray-300">Given these two lists, we can define the set S as all the differences which appear both in  <span class="math">L_1</span>  and  <span class="math">L_2</span>  with sufficiently high probability (which we denote by  <span class="math">p_b</span> ). Then, by using Equation (3), and the estimations for the  <span class="math">p_i</span> 's and  <span class="math">q_j</span> 's, we can compute an estimation for the probability of the differential  <span class="math">\\Delta_I \\stackrel{r}{\\to} \\Delta_O</span> :</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>Pick N plaintext pairs&lt;sup&gt;6&lt;/sup&gt; of the form  <span class="math">(x, x \\oplus \\Delta_I)</span> , and obtain their partial encryption after r' rounds, (z, z').</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>Collect the differences  <span class="math">z \\oplus z&#x27;</span> , and produce  <span class="math">L_1</span> .</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>Pick N ciphertext pairs of the form  <span class="math">(y, y \\oplus \\Delta_O)</span> , and obtain their partial decryption after r r' rounds, (w, w').</li>
    </ol></li>
    </ul>

    <p class="text-gray-300">&lt;sup&gt;&amp;&lt;/sup&gt;lt;sup&gt;4&lt;/sup&gt; When using BITM to calculate the probability of a differential, one can choose the meeting round in a variety of ways. Usually setting  <span class="math">r&#x27; \\approx r/2</span>  gives the optimal results.</p>

    <p class="text-gray-300"><span class="math">&lt;sup&gt;^{5}&lt;/sup&gt;</span>  Of course, we still need to assume independence between the two parts of the cipher.</p>

    <p class="text-gray-300"><span class="math">&lt;sup&gt;^{6}&lt;/sup&gt;</span>  The value of N is discussed later.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>Collect the differences  <span class="math">w \\oplus w&#x27;</span> , and produce  <span class="math">L_2</span> .</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>For all the differences that appear with probability above some bound  <span class="math">p_b</span>  in both  <span class="math">L_1</span>  and  <span class="math">L_2</span> , compute the sum of all products  <span class="math">(p_i \\cdot q_i)</span> .</li>
    </ol></li>
    </ul>

    <p class="text-gray-300">First, it is easy to see that both  <span class="math">L_1</span>  and  <span class="math">L_2</span>  contain two types of differences: High-probability differences (e.g., differences that appear with probability higher than  <span class="math">p_b</span> ) as well as low-probability differences that got sampled by chance. For an n-bit block cipher, after sampling N pairs, we expect low probability differences  <span class="math">\\Delta_M</span>  to be encountered only once (both in  <span class="math">L_1</span>  and in  <span class="math">L_2</span> ) as long as  <span class="math">N &lt; 2^{n/2}</span> . Moreover, as we later discuss, estimating the probabilities  <span class="math">p_i</span> 's and  <span class="math">q_j</span> 's can be done over many keys, offering a better estimation.</p>

    <p class="text-gray-300">Now, given  <span class="math">p_b</span> , we wish to assure that we sample the high probability differences. This can be done, by looking for differences that appear at least twice during Steps 1–2 (for  <span class="math">L_1</span> ) or Steps 3–4 (for  <span class="math">L_2</span> ). Given that the number of &quot;appearances&quot; of an output difference follows the Poisson distribution, we need to take  <span class="math">N = \\alpha/p_b</span>  pairs, where  <span class="math">\\alpha</span>  determines the quality of our sampling. For example, if we pick  <span class="math">\\alpha = 4</span> , i.e., we expect 4 pairs that follow the differential  <span class="math">\\Delta_I \\xrightarrow{r&#x27;} \\Delta_M</span> , then with probability of 90%,  <span class="math">\\Delta_M</span>  would appear at least twice in Steps 1–2. Increasing the value of  <span class="math">\\alpha</span>  (and/or sampling using more keys) improves the quality of the values in  <span class="math">L_1</span>  and  <span class="math">L_2</span> . For example, for  <span class="math">\\alpha = 10</span> , the probability that a good  <span class="math">\\Delta_M</span>  will not appear at least twice is less than 0.5%.</p>

    <p class="text-gray-300">It is important to note that differences of low probability do not affect the overall estimation. This follows from the fact that we count only differences that appear in both lists  <span class="math">L_1</span>  and  <span class="math">L_2</span> . Hence, even though there are some low probability differences in each list, it is extremely unlikely that the same low probability difference will appear in both lists simultaneously. Even in the extreme case that there are N low probability  <span class="math">\\Delta_M</span>  values in each list, expected number of low probability  <span class="math">\\Delta_M</span>  appearing in both lists is  <span class="math">N^2/2^n</span> , which is less than 1.</p>

    <p class="text-gray-300">We recall that similarly to all approaches that estimate the probability of differentials, we need to rely on some randomness assumptions. A round-by-round approach relies on the cipher being Markovian, whereas an experimental verification of the full differential does not require any assumption. The independence assumption needed by the BITM algorithm lies between these two extremes. We need to assume that the transition between the two parts of the cipher does not affect the probability estimations. In other words, even though the actual pairs in  <span class="math">L_1</span>  and  <span class="math">L_2</span>  are different, we can use a (reduced) Markov-cipher assumption to obtain an estimate for the total probability of the differential  <span class="math">\\Delta_I \\xrightarrow{r} \\Delta_O</span> .</p>

    <p class="text-gray-300">We note that one can take more pairs, but as we later show,  <span class="math">N = O(1/p_b)</span> , i.e., as long as  <span class="math">p_b</span>  is above  <span class="math">2^{-n/2}</span>  the algorithm is expected to work. Moreover, if both  <span class="math">\\Delta_I \\xrightarrow{r&#x27;} \\Delta_M</span>  and  <span class="math">\\Delta_M \\xrightarrow{r-r&#x27;} \\Delta_O</span>  have probability lower than  <span class="math">p_b</span> , the overall contribution of the characteristic  <span class="math">\\Delta_I \\xrightarrow{r&#x27;} \\Delta_M \\xrightarrow{r-r&#x27;} \\Delta_O</span>  to the probability we estimate is at most  <span class="math">p_b^2</span> . Picking  <span class="math">p_b &lt; 2^{-n/2}</span>  suggests that the contribution is less than  <span class="math">2^{-n}</span> . Such a low probability is usually of little interest in cryptanalysis, and requires a very careful analysis.</p>

    <p class="text-gray-300">As mentioned earlier, as  <span class="math">\\alpha</span>  increases (or if the probability of the difference we check is higher than  <span class="math">p_b</span> ) the quality of the estimation of the probabilities in  <span class="math">L_1</span>  and  <span class="math">L_2</span>  improves. This is explained by the fact that we estimate the probability of an event which follows a Poisson distribution. If  <span class="math">X \\sim Poi(\\lambda)</span> , then  <span class="math">E[X] = Var[X] = \\lambda</span> , so the larger  <span class="math">\\lambda</span>  is, the closer X is to its mean.</p>

    <p class="text-gray-300">Moreover, we note that the use of multiple keys can significantly improve the quality of the estimation. If we repeat the experiment with t different keys, the expected number of times  <span class="math">\\Delta_M</span>  appeared in all t experiments is increased by a factor t. As the sum of Poisson random variables is itself a Poisson random variable, we obtain a significantly better estimate for the actual probability of the difference.&lt;sup&gt;8&lt;/sup&gt;</p>

    <p class="text-gray-300">Hence, after sampling sufficiently many keys, one can obtain a better estimation of the actual probabilities of the various differences in  <span class="math">L_1</span>  and  <span class="math">L_2</span> , and discard the low probability differences. These probabilities can then be combined to offer a higher quality estimate of the probability of the differential  <span class="math">\\Delta_I \\xrightarrow{r} \\Delta_O</span> .</p>

    <p class="text-gray-300"><strong>A few improvements</strong> We first note that there is no need to actually store  <span class="math">L_2</span> . One can generate  <span class="math">L_1</span> , and for each  <span class="math">w \\oplus w&#x27;</span>  value of Steps 3–4, to increment the counter if  <span class="math">w \\oplus w&#x27;</span>  happens to be in  <span class="math">L_1</span> .</p>

    <p class="text-gray-300">We now turn our attention to the generation of  <span class="math">L_1</span> . It is easy to see that  <span class="math">L_1</span>  can take at most O(N) memory cells. As N increases this may be a practical bottleneck. Hence, once the used memory reaches the machine's limit (or the process' limit), we suggest to &quot;extract&quot; all the high probability differences encountered so far into a shorter list  <span class="math">L&#x27;_1</span> . Then, we sample more random pairs, but this time, we only deal with those pairs whose &quot;output&quot; difference is in the short list  <span class="math">L&#x27;_1</span> . The main advantage is now that we use almost no memory (as  <span class="math">L&#x27;_1</span>  tends to be small), we can actually increase the number of queries, thus obtaining a more accurate estimate.</p>

    <p class="text-gray-300">The final improvement in this front is to perform the previous idea in steps. We first sample many pairs, and store the differences  <span class="math">z \\oplus z&#x27;</span>  in a hash table (with less than N bins). After finding the bins which were suggested more than others, we can dive into them by re-sampling more pairs.</p>

    <p class="text-gray-300">Comparison with Meet in the Middle Attacks We note that while the <em>BITM</em> algorithm is is superficially similar to the meet in the middle (MITM) algorithm, it is quite different. In the MITM algorithm, we typically try to find some common value between the two parts of a cipher, and use this value to find the key (depending on the cryptanalytic task at hand, we may search for</p>

    <p class="text-gray-300">&lt;sup&gt;&amp;&lt;/sup&gt;lt;sup&gt;8&lt;/sup&gt; For  <span class="math">\\alpha=4</span>  and t=32 (expecting four pairs in 32 experiments), the total number of times  <span class="math">\\Delta_M</span>  appears in all experiments follows a Poisson distribution with a mean of 128. Hence, with probability 95%, counting over all experiments will suggest  <span class="math">\\Delta_M</span>  somewhere between 105 and 151 times (in all 32 experiments). In other words, taking the number of times  <span class="math">\\Delta_M</span>  appears (divided by 32N) as an estimate for the actual probability will be accurate within 18% of the correct probability with probability 95%.</p>

    <p class="text-gray-300">all the common values). In the BITM algorithm our goal is not to find these values, but to estimate the probability that they exist, in order to choose the best differential attack on the scheme.</p>

    <h2 id="sec-14" class="text-2xl font-bold">6.2 The Advantages of the BITM Algorithm</h2>

    <p class="text-gray-300">The main advantage of the BITM algorithm over a pure top-down algorithm which evaluates the full mapping is its greatly improved efficiency. Indeed, in order to estimate the differential  <span class="math">\\Delta_I \\xrightarrow{r} \\Delta_O</span>  requires  <span class="math">O(p^{-1})</span>  pairs. However, if we pick  <span class="math">r&#x27; \\approx r/2</span> , and under the assumption that both parts are roughly of the same strength, we obtain  <span class="math">p_b = O(\\sqrt{p})</span> . This is extremely important for the cases where a time complexity of  <span class="math">p_b^{-1}</span>  is still feasible but  <span class="math">p_b^{-2}</span>  is not (e.g., when  <span class="math">p_b \\approx 2^{-40}</span> ).</p>

    <p class="text-gray-300">Another advantage of the BITM algorithm over bottom-up algorithms is that we take into account all the high probability differential characteristics simultaneously. Hence, the estimation for the differential probability is closer to the actual probability than an estimation which is based on the multiplication of many probabilities along a single differential characteristic.</p>

    <p class="text-gray-300">Finally, this method offers some experimental verification of the stochastic equivalence assumption. Indeed, for most ciphers (and most of the keys), the stochastic equivalence assumption tends to hold (or seem to &quot;work&quot; most of the time). However, when we discuss a single long characteristic, we may encounter some inconsistencies between different parts of the characteristic. In these situations, the real probability and computed probability will differ. Once we take into consideration multiple differential characteristics, the estimation becomes more resilient (though not 100% full-proof), as a &quot;failure&quot; of one of the longer characteristics does not invalidate the full differential. In addition, by running the algorithm with several different r' can also help in validating the probability of the transition between the top half and the bottom half.</p>

    <p class="text-gray-300">The Simon family of lightweight block ciphers, presented in [4], is implemented using a balanced Feistel structure. The Simon round function is very simple and consists of only three operations: AND, XOR and constant rotations. All the ciphers in the Simon family use the same round function and differ only by the key size, the block size (which ranges from 32 to 128 bits) and the number of Feistel rounds which is dependant on the former two. As in any Feistel structure, the plaintext is divided into two blocks of size n:  <span class="math">P = (L_0, R_0)</span>  and then every round  <span class="math">1 \\le i \\le r</span> :</p>

    <p class="text-gray-300"><span class="math">$L_i = R_{i-1} \\oplus F(L_{i-1}) \\oplus K_{i-1}; \\quad R_i = L_{i-1}</span>$</p>

    <p class="text-gray-300">where the ciphertext is  <span class="math">C = (L_r, R_r)</span>  and F is the Simon round function:</p>

    <p class="text-gray-300"><span class="math">$F(x) = ((x \\ll 1) \\land (x \\ll 8)) \\oplus (x \\ll 2)</span>$</p>

    <p class="text-gray-300">An illustration of the round function is depicted in Figure 1.</p>

    <p class="text-gray-300">    <img src="_page_13_Figure_1.jpeg" alt="" class="my-4 max-w-full" />
</p>

    <p class="text-gray-300">Fig. 1. The SIMON round function</p>

    <p class="text-gray-300">In this section we present the best differentials for Simon64 Simon96 and Simon128 we found using our various diagonal estimation algorithms. We also describe more accurate BITM-based estimates for the differential probabilities of previously presented Simon characteristics, which are substantially different from the original estimates.</p>

    <h4 id="sec-15" class="text-lg font-semibold mt-6">7.1 Applying BITM to previously known Simon differentials</h4>

    <p class="text-gray-300">We applied the BITM algorithm to some of the previously known differentials for Simon which were published in [1,9,27]. Since testing probabilities  <span class="math">&lt; 2^{-80}</span>  is too expensive (it requires throwing  <span class="math">&gt; 2^{40}</span>  balls into bins from each side), the longer differentials were evaluated by breaking the characteristic into the smallest possible number of parts, evaluating each part separately, and taking the product of the probabilities as the result. This is not a pure BITM approach, but it is much closer to a top-down computation compared to the bottom-up approach used by other researchers.</p>

    <p class="text-gray-300">For example, the 41-round Simon128 characteristic from [1] was divided into a 9-round differential  <span class="math">(e_{12},e_{6,10,14})</span>   <span class="math">\\xrightarrow{9R}</span>   <span class="math">(e_{6,10,14},e_{12})</span>  and a 7-round differential  <span class="math">(e_{6,10,14},e_{12})</span>   <span class="math">\\xrightarrow{7R}</span>   <span class="math">(e_{12},e_{6,10,14})</span> . The probabilities for the differentials were tested many times:  <span class="math">2^{35}</span>  balls were thrown from each side for 30 different keys, and  <span class="math">2^{30}-2^{32}</span>  balls were thrown from each side for &gt;100 different keys. The results for the 9-round differential were in the range  <span class="math">[2^{-18.61}, 2^{-18.59}]</span>  with an average of  <span class="math">2^{-18.6}</span> . The results for the 7-round differential were in the range  <span class="math">[2^{-32.92}, 2^{-32.25}]</span>  with an average of  <span class="math">2^{-32.77}</span> . When testing the entire 16-round characteristic (first the 9-round differential, then the 7-round one) with the same number of experiments, the probability range was  <span class="math">[2^{-51.2}, 2^{-48.4}]</span>  with an average  <span class="math">2^{-49.9}</span> . This means that the entire 41-round characteristic has probability in the approximate range of  <span class="math">[2^{-121}, 2^{-115.6}]</span>  with an average of  <span class="math">\\approx 2^{-118.6}</span> .</p>

    <p class="text-gray-300">The advantage of using the BITM algorithm to evaluate the probabilities is the fact we do not consider what happens in the intermediate rounds, but only take interest in the probabilities between the first, last and middle round. Additionally, we do not make any independence assumptions about the round keys since the BITM experiments use the actual SIMON key-schedule.</p>

    <p class="text-gray-300">Table 1 compares all the previously published bottom-up estimates of the probabilities of various differential transitions with our experimentally obtained top-down results (where the numbers are the log to the base 2 of the probabilities). All the results (including those presented in the next subsection) were obtained by the same method as described for the SIMON128 differential (Many experiments with as many as 2&lt;sup&gt;35&lt;/sup&gt; balls thrown from each side with a narrow result range, and the average value is taken as the final probability).</p>

    <p class="text-gray-300">Table 1. The original and our improved estimates of the probabilities of the best previously published differentials</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Cipher</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Rounds</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Presented prob.</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">BITM prob.</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Source</th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">SIMON64</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">21</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">-60.53</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">-56.05</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">[9]</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">SIMON64</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">21</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">-61.01</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">-56.05</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">[1]</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">SIMON64</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">21</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">-60.21</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">-59</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">[27]</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">SIMON96</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">30</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">-92.20</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">-88.5</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">[1]</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">SIMON128</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">41</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">-124.6</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">-118.6</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">[1]</td>
          </tr>
        </tbody>
      </table>
    </div>

    <p class="text-gray-300">This table shows that the previous estimates were too pessimistic (sometimes by a significant factor of 2&lt;sup&gt;6&lt;/sup&gt; = 64) since they considered only a limited number of differential characteristics. Since some of the differential probabilities that appear in the mentioned papers are significantly lower than the probabilities estimated by our BITM algorithm, we can extend the differentials to a larger number of rounds, while maintaining the probability above 2&lt;sup&gt;−&lt;/sup&gt;&lt;sup&gt;n&lt;/sup&gt;. &lt;sup&gt;9&lt;/sup&gt; However, even without extending the characteristics, the results of Table 1 automatically translate into better key recovery attacks on the SIMON members, as the previous attacks only depended on differentials for SIMON (and not on the internal characteristics).</p>

    <h2 id="sec-16" class="text-2xl font-bold">7.2 Improved SIMON differentials found using GDIAG&lt;sup&gt;L&lt;/sup&gt;</h2>

    <p class="text-gray-300">We applied the GDIAG&lt;sup&gt;L&lt;/sup&gt; algorithm to SIMON, followed by estimating the probabilities using BITM. The result is an improvement by two rounds of the best previously known differential from [1] by 2 rounds while maintaining roughly the same probability.</p>

    <p class="text-gray-300">&lt;sup&gt;9&lt;/sup&gt; All these differential characteristics could be theoretically extended to cover more rounds, but in order to break an n-bit block cipher, the probabilities generally need to be higher than 2&lt;sup&gt;−&lt;/sup&gt;&lt;sup&gt;n&lt;/sup&gt; (otherwise we do not expect to find more than a single accidental pair, even when we try the full code book).</p>

    <p class="text-gray-300">The application of  <span class="math">GDIAG_L</span>  was done with the function</p>

    <p class="text-gray-300"><span class="math">$L(x) = (x \\lll n)</span>$</p>

    <p class="text-gray-300">(which is a half block rotation that swaps its two halves). The result of this function is some differential families of the type  <span class="math">AB \\rightarrow BA</span>  for various numbers of rounds. After applying a search for complementing differentials pairs  <span class="math">AB \\rightarrow BA \\rightarrow AB</span> , the most probable ones we found were:</p>

    <p class="text-gray-300"><span class="math">$\\Pr\\left[ (e_{i,i+4}, e_{i+6}) \\xrightarrow{\\text{7R}} (e_{i+6}, e_{i,i+4}) \\right] \\approx 2^{-14.6}</span>$
(4)</p>

    <p class="text-gray-300"><span class="math">$\\Pr\\left[ (e_{i+6}, e_{i,i+4}) \\xrightarrow{9R} (e_{i,i+4}, e_{i+6}) \\right] \\approx 2^{-35.6}</span>$
(5)</p>

    <p class="text-gray-300"><span class="math">$\\Pr\\left[\\left(e_{i+2}, e_{i,i+4}\\right) \\xrightarrow{\\text{5R}} \\left(e_{i,i+4}, e_{i+2}\\right)\\right] = 2^{-8}</span>$
(6)</p>

    <p class="text-gray-300"><span class="math">$\\Pr\\left[\\left(e_{i,i+4}, e_{i+2}\\right) \\xrightarrow{11R} \\left(e_{i+2}, e_{i,i+4}\\right)\\right] \\approx 2^{-41.5}</span>$
(7)</p>

    <p class="text-gray-300">Both pairs result in a 16-round iterated differential.</p>

    <p class="text-gray-300">Simon 64 In order to construct the full characteristic we additionally use the following differential:</p>

    <p class="text-gray-300"><span class="math">$\\Pr\\left[ (e_{i,i+4}, e_{i+6}) \\xrightarrow{\\text{6R}} (e_{i,i+4}, e_{i+2}) \\right] \\approx 2^{-11.3}</span>$
(8)</p>

    <p class="text-gray-300">Concatenating differentials (4) and (5) results in a 16-round characteristic with probability of  <span class="math">\\approx 2^{-48.6}</span> , thus the full characteristic</p>

    <p class="text-gray-300"><span class="math">$(e_{i,i+4}, e_{i+6}) \\xrightarrow[2^{-48.6}]{16R} (e_{i,i+4}, e_{i+6}) \\xrightarrow[2^{-11.3}]{6R} (e_{i,i+4}, e_{i+2})</span>$</p>

    <p class="text-gray-300">has 22 rounds and probability of  <span class="math">\\approx 2^{-59.9}</span> .</p>

    <p class="text-gray-300">A different 22-round characteristic can be obtained by using (6), (7), (6) and an additional round. Combining (6) with (7) results in a 16-round differential with probability  <span class="math">\\approx 2^{-48}</span> , and adding  <span class="math">(e_{i,i+4}, e_{i+2}) \\xrightarrow{1R} (e_{i+6}, e_{i,i+4})</span>  after (6) results in a 6-round differential with probability  <span class="math">\\approx 2^{-11.3}</span> . The entire 22-round characteristic</p>

    <p class="text-gray-300"><span class="math">$(e_{i+2}, e_{i,i+4}) \\xrightarrow[2^{-48}]{16R} (e_{i+2}, e_{i,i+4}) \\xrightarrow[2^{-11.3}]{6R} (e_{i+6}, e_{i,i+4})</span>$</p>

    <p class="text-gray-300">has probability  <span class="math">\\approx 2^{-59.3}</span> .</p>

    <p class="text-gray-300">Note that both characteristics can be extended one round further  <span class="math">((e_{i+6}, e_{i,i+4,i+8}) \\xrightarrow{1R} (e_{i,i+4}, e_{i+6})</span>  and  <span class="math">(e_{i+6}, e_{i,i+4}) \\xrightarrow{1R} (e_{i,i+4,i+8}, e_{i+6})</span> , respectively) while maintaining a probability which is above  <span class="math">2^{-64}</span> . The resultant 23-round differential is longer than all the previously found differentials for SIMON64.</p>

    <p class="text-gray-300">Simon128 The longest previously found differential for Simon128 had 41 rounds. We can find a longer 43-round differential which is based on differentials (4), (5) and another short differential:</p>

    <p class="text-gray-300"><span class="math">$\\Pr\\left[\\left(e_{i+6}, e_{i,i+4}\\right) \\xrightarrow{\\text{3R}} \\left(e_{i,i+8,i+12}, e_{i+2,i+10}\\right)\\right] = 2^{-12}</span>$
(9)</p>

    <p class="text-gray-300">Combining (4) with (9) results in a 9-round characteristic of probability  <span class="math">\\approx 2^{-22.4}</span> . The full 43-round characteristic</p>

    <p class="text-gray-300"><span class="math">$(e_{i+6}, e_{i,i+4,i+8}) \\xrightarrow[2^{-2}]{1R} (e_{i,i+4}, e_{i+6}) \\xrightarrow[2^{-97.2}]{32R} (e_{i,i+4}, e_{i+6}) \\xrightarrow[2^{-26.4}]{10R} (e_{i,i+8,i+12}, e_{i+2,i+10})</span>$</p>

    <p class="text-gray-300">has probability of  <span class="math">\\approx 2^{-125.6}</span> .</p>

    <p class="text-gray-300">A different 43-round characteristic will require the same differentials that were used for Simon64, combined with a 4-round differential as follows:</p>

    <p class="text-gray-300"><span class="math">$\\Pr\\left[\\left(e_{i+2,i+10}, e_{i,i+8,i+12}\\right) \\xrightarrow{4R} \\left(e_{i+2}, e_{i,i+4}\\right)\\right] = 2^{-15.1}</span>$
(10)</p>

    <p class="text-gray-300">and finishing with a single round  <span class="math">(e_{i+6}, e_{i,i+4}) \\xrightarrow{1R} (e_{i,i+4,i+8}, e_{i+6})</span> . The full characteristic is:</p>

    <p class="text-gray-300"><span class="math">$(e_{i+2,i+10},e_{i,i+8,i+12}) \\xrightarrow[2^{-15.1}]{\\text{4R}} (e_{i+2},e_{i,i+4}) \\xrightarrow[2^{-96}]{\\text{32R}} (e_{i+2},e_{i,i+4}) \\xrightarrow[2^{-13.3}]{\\text{7R}} (e_{i,i+4,i+8},e_{i+6})</span>$</p>

    <p class="text-gray-300">and it has probability of  <span class="math">\\approx 2^{-124.4}</span> .</p>

    <p class="text-gray-300"><strong>Table 2.</strong> Summary of the  <span class="math">GDIAG_L</span>  results for SIMON</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Cipher</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Differential family</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Rounds</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Prob. <span class="math">(\\log_2)</span></th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Simon64</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">(e_{i,i+4}, e_{i+6}) \\to (e_{i,i+4}, e_{i+2})</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">22</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">-59.9</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Simon64</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">(e_{i+2}, e_{i,i+4}) \\to (e_{i+6}, e_{i,i+4})</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">22</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">-59.3</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Simon64</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">(e_{i+6}, e_{i,i+4,i+8}) \\to (e_{i,i+4}, e_{i+2})</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">23</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">-61.9</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Simon64</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">(e_{i+2}, e_{i,i+4}) \\to (e_{i,i+4,i+8}, e_{i+6})</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">23</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">-61.3</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">SIMON128</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">(e_{i+6}, e_{i,i+4,i+8}) \\rightarrow (e_{i,i+8,i+12}, e_{i+2,i+10})</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">43</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">-125.6</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">SIMON128</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"><span class="math">(e_{i+2,i+10}, e_{i,i+8,i+12}) \\rightarrow (e_{i,i+4,i+8}, e_{i+6})</span></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">43</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">-124.4</td>
          </tr>
        </tbody>
      </table>
    </div>

    <h2 id="sec-17" class="text-2xl font-bold">8 Application to Iterated Even-Mansour</h2>

    <p class="text-gray-300">The Even-Mansour (EM) block cipher was proposed at Asiacrypt 1991 [16]. It uses a single publicly known random permutation P on n-bit values and two secret n-bit keys  <span class="math">K_1</span>  and  <span class="math">K_2</span> , and defines the encryption of the n-bit plaintext m as  <span class="math">E(m) = P(m \\oplus K_1) \\oplus K_2</span> . The decryption of the n-bit ciphertext c is</p>

    <p class="text-gray-300">similarly defined as D(c) = P −1 (c ⊕ K2) ⊕ K1. It can be naturally generalized into an r-round iterated EM encryption function (a.k.a. a key-alternating scheme in [11]), which is defined using r permutations P1, P2, . . . , P&lt;sup&gt;r&lt;/sup&gt; and r + 1 keys K1, K2, . . . Kr+1 as E(m) = Pr(. . . P2(P1(m ⊕ K1)⊕ K2)⊕ K&lt;sup&gt;3&lt;/sup&gt; . . .⊕ Kr)⊕ Kr+1, where decryption is defined in an analogous way.</p>

    <p class="text-gray-300">At Asiacrypt 2013, Dinur et al. [15] analyzed several instances of iterated EM schemes, and in particular showed that the scheme in which all round keys are equal to K (shown in Figure 2) must have at least 4 rounds in order to provide perfect n-bit security in the single-key model. In this section, we extend this result to the related-key model, and show that the scheme must have at least 5 rounds in order to provide perfect (n−1)-bit security against related-key attacks that use 2 related keys.&lt;sup&gt;10&lt;/sup&gt; This is done by presenting a related-key differential attack, which is applicable to (almost) any 4-round EM scheme.</p>

    <p class="text-gray-300">    <img src="_page_17_Picture_2.jpeg" alt="" class="my-4 max-w-full" />
</p>

    <p class="text-gray-300">Fig. 2. An iterated EM with one key</p>

    <p class="text-gray-300">Our starting point is the attack of Mendel et al. [23] on the block cipher LED-64, which is a specific instance of the iterated EM scheme shown in Figure 2. This attack assumes that we have an iterative differential characteristic for P&lt;sup&gt;2&lt;/sup&gt; with probability p, where the input and output difference is ∆, and extends it to a 3-round related-key characteristic with that same probability p. This is done by choosing both the key difference and the plaintext difference as ∆, which implies that the input difference to P&lt;sup&gt;4&lt;/sup&gt; is ∆ with probability p. The 3-round characteristic can be used to attack 4 rounds of the scheme with time, data and memory complexities of about 2n/&lt;sup&gt;2&lt;/sup&gt; · p &lt;sup&gt;−&lt;/sup&gt;1/&lt;sup&gt;2&lt;/sup&gt; using an extension of the attack of Daemen on the original EM scheme [12]. The full details of the attack are given in [23] and are not required in order to understand this section.</p>

    <p class="text-gray-300">The most relevant component of the attack of [23] to our analysis is the iterative differential characteristic for P2. In [23], such a characteristic was efficiently found using the internal properties of LED, but here we notice that we can apply a similar attack to essentially any 4-round EM scheme with one key. Our general framework is similar to the one of [15], which analyzed the public permutations of an EM scheme in order to detect some property which is useful for attacking the full cipher. More specifically, we can use our new diagonal algorithm of Section 4.1 in order to analyze the specific choice of P&lt;sup&gt;2&lt;/sup&gt; in a particular</p>

    <p class="text-gray-300">&lt;sup&gt;10&lt;/sup&gt; Exhaustive search given data encrypted with two related keys can be performed with time complexity of 2&lt;sup&gt;n&lt;/sup&gt;−&lt;sup&gt;1&lt;/sup&gt; .</p>

    <p class="text-gray-300">incarnation of the Even-Mansour scheme, and find in it the highest probability iterative characteristic. However, applying the full diagonal algorithm results in an attack with complexity of at least  <span class="math">2^n</span> , which is not faster than exhaustive search. Therefore, we apply the algorithm by evaluating only a fraction of the input space of  <span class="math">P_2</span> . More specifically, we evaluate an arbitrary linear subspace of inputs X, such that |X| = S (for a parameter S), and look for the entry  <span class="math">\\Delta</span>  of the (partial) DIAG with a maximal value, denoted by t. The value of S is carefully chosen in order to optimize the total time complexity of the attack, which is about  <span class="math">S + 2^{n/2} \\cdot p^{-1/2}</span> , where  <span class="math">p = t/2^n</span> .</p>

    <p class="text-gray-300">In order to calculate t as a function of S, we first compute the expected value of an arbitrary cell of the (partial) DIAG. Assuming that  <span class="math">P_2</span>  is a random permutation, then the function  <span class="math">x \\oplus P_2(x)</span>  is (very close to) a random function, and therefore we expect about  <span class="math">S^2 \\cdot 2^{-n}</span>  collision in the hash table H in Step 1 of the diagonal algorithm. Each such collision will contribute once to a DIAG entry, but since we selected X as a linear subspace, we know in advance that there are only S relevant DIAG entries (those which belong to the closed subspace X). Therefore, the average value of a relevant entry is  <span class="math">S^2 \\cdot 2^{-n}/S = S \\cdot 2^{-n}</span>  (whereas the other  <span class="math">2^n - S</span>  entries have zero values).</p>

    <p class="text-gray-300">Similarly to the analysis of [15], based on some randomness assumptions on  <span class="math">P_2</span> , the value of an entry in the (partial) DIAG is distributed according to the Poisson distribution with an expectation  <span class="math">\\lambda</span> , which is equal to the average value  <span class="math">\\lambda = S \\cdot 2^{-n}</span> . Given a parameter t, the probability that an arbitrary entry of the partial DIAG will have a value of t is estimated as  <span class="math">(\\lambda^t e^{-\\lambda})/t!</span> . We have S elements in the range, implying that we expect that about  <span class="math">(S \\cdot \\lambda^t e^{-\\lambda})/t!</span>  entries will have a value of t. If we equate this number to 1, consider large values of  <span class="math">S \\approx 2^n/n</span> , and ignore low order terms, we can deduce that the largest expected entry value t satisfies  <span class="math">t \\cdot \\log(t) = n</span> , and thus t is approximately equal to  <span class="math">n/\\log(n)</span> . When plugging  <span class="math">S \\approx 2^n/n</span>  and  <span class="math">p = t \\cdot 2^{-n} \\approx n/(2^n \\cdot \\log(n))</span>  into the complexity of the attack, we obtain  <span class="math">S + 2^{n/2} \\cdot p^{-1/2} \\approx 2^n/n + 2^n/\\sqrt{n} \\approx 2^n/\\sqrt{n}</span> . In other words, we obtain a related key attack on any particular incarnation of a 4-round EM which is about  <span class="math">\\sqrt{n}</span>  times faster than exhaustive search, even if we have to include the complexity of analyzing the particular choice of  <span class="math">P_2</span>  in the total time complexity.</p>

    <p class="text-gray-300">We note that the analysis provided in this section can be used to improve the complexity of the original 4-round related-key attack on LED-64 [23]. However, the improvement factor is rather small and the main significance of our analysis is theoretical, namely, describing the first generic attack on the 4-round 1-key EM scheme which is (slightly) better than exhaustive search.</p>

    <h2 id="sec-18" class="text-2xl font-bold">9 Conclusions</h2>

    <p class="text-gray-300">In this paper we described and motivated the top-down approach to differential cryptanalysis, which tries to compute or approximate certain DDT values without looking at the internal structure of the given mapping. We introduced three novel techniques which can compute three types of interesting entries (on</p>

    <p class="text-gray-300">the diagonal, in low Hamming weight entries on the diagonal, and arbitrarily located entries with large values) with improved efficiency. We then applied the new BITM technique to SIMON in order to obtain more accurate estimates of the probabilities of all the previously published differentials and combined it with the generalized diagonal algorithm to find better differentials for a larger number of rounds. This improves the best known cryptanalytic results for this scheme and demonstrates the power and versatility of our new top-down techniques. Finally, in Section 8 we described how to use our new algorithms to efficiently locate the highest diagonal entry in any given incarnation of the four round version of Even-Mansour scheme, in order to break the scheme with a related key attack which is faster than exhaustive search.</p>

    <h2 id="sec-19" class="text-2xl font-bold">References</h2>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>F. Abed, E. List, J. Wenzel, and S. Lucks. Differential Cryptanalysis of roundreduced Simon and Speck, 2014. Presented at FSE 2014. To Appear in Lecture Notes in Computer Science.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>H. A. Alkhzaimi and M. M. Lauridsen. Cryptanalysis of the SIMON Family of Block Ciphers. Cryptology ePrint Archive, Report 2013/543, 2013.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>K. Aoki, K. Kobayashi, and S. Moriai. Best Differential Characteristic Search of FEAL. In Fast Software Encryption, FSE '97, volume 1267 of Lecture Notes in Computer Science, pages 41–53. Springer, 1997.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>R. Beaulieu, D. Shors, J. Smith, S. Treatman-Clark, B. Weeks, and L. Wingers. The SIMON and SPECK Families of Lightweight Block Ciphers. Cryptology ePrint Archive, Report 2013/404, 2013.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>E. Biham, A. Biryukov, and A. Shamir. Cryptanalysis of Skipjack Reduced to 31 Rounds Using Impossible Differentials. In Advances in Cryptology - EUROCRYPT '99, volume 1592 of Lecture Notes in Computer Science, pages 12–23. Springer, 1999.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>E. Biham and A. Shamir. Differential cryptanalysis of DES-like cryptosystems. Journal of CRYPTOLOGY, 4(1):3–72, 1991.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>A. Biryukov and I. Nikolic. Automatic Search for Related-Key Differential Characteristics in Byte-Oriented Block Ciphers: Application to AES, Camellia, Khazad and Others. In Advances in Cryptology - EUROCRYPT 2010, volume 6110 of Lecture Notes in Computer Science, pages 322–344. Springer, 2010.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>A. Biryukov and I. Nikolic. Search for Related-Key Differential Characteristics in DES-Like Ciphers. In Fast Software Encryption, FSE 2011, volume 6733 of Lecture Notes in Computer Science, pages 18–34. Springer, 2011.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>A. Biryukov, A. Roy, and V. Velichkov. Differential Analysis of Block Ciphers SIMON and SPECK, 2014. Presented at FSE 2014. To Appear in Lecture Notes in Computer Science.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>A. Biryukov and V. Velichkov. Automatic Search for Differential Trails in ARX Ciphers. In Topics in Cryptology - CT-RSA 2014, volume 8366 of Lecture Notes in Computer Science, pages 227–250. Springer, 2014.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>A. Bogdanov, L. R. Knudsen, G. Leander, F. Standaert, J. P. Steinberger, and E. Tischhauser. Key-Alternating Ciphers in a Provable Setting: Encryption Using a Small Number of Public Permutations - (Extended Abstract). In Advances in Cryptology - EUROCRYPT 2012, volume 7237 of Lecture Notes in Computer Science, pages 45–62. Springer, 2012.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>J. Daemen. Limitations of the Even-Mansour Construction. In Advances in Cryptology - ASIACRYPT '91, volume 739 of Lecture Notes in Computer Science, pages 495–498. Springer, 1993.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>J. Daemen, R. Govaerts, and J. Vandewalle. A New Approach to Block Cipher Design. In R. J. Anderson, editor, Fast Software Encryption, Cambridge Security Workshop, Cambridge, UK, December 9-11, 1993, Proceedings, volume 809 of Lecture Notes in Computer Science, pages 18–32. Springer, 1993.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>C. De Canni\`ere and C. Rechberger. Finding SHA-1 Characteristics: General Results and Applications. In Advances in Cryptology - ASIACRYPT 2006, volume 4284 of Lecture Notes in Computer Science, pages 1–20. Springer, 2006.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>I. Dinur, O. Dunkelman, N. Keller, and A. Shamir. Key Recovery Attacks on 3 round Even-Mansour, 8-step LED-128, and Full AES&lt;sup&gt;2&lt;/sup&gt; . In Advances in Cryptology - ASIACRYPT 2013, volume 8269 of Lecture Notes in Computer Science, pages 337–356. Springer, 2013.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>S. Even and Y. Mansour. A Construction of a Cipher from a Single Pseudorandom Permutation. J. Cryptology, 10(3):151–162, 1997.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>P. Fouque, J. Jean, and T. Peyrin. Structural Evaluation of AES and Chosen-Key Distinguisher of 9-Round AES-128. In Advances in Cryptology - CRYPTO 2013, volume 8042 of Lecture Notes in Computer Science, pages 183–203. Springer, 2013.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>L. Knudsen. DEAL A 128-bit Block Cipher, 1998. NIST AES Proposal.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>X. Lai and J. L. Massey. Markov Ciphers and Differentail Cryptanalysis. In Advances in Cryptology - EUROCRYPT '91, volume 547 of Lecture Notes in Computer Science, pages 17–38. Springer, 1991.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>G. Leurent. Analysis of Differential Attacks in ARX Constructions. In Advances in Cryptology - ASIACRYPT 2012, volume 7658 of Lecture Notes in Computer Science, pages 226–243. Springer, 2012.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>M. Matsui. On Correlation Between the Order of S-boxes and the Strength of DES. In Advances in Cryptology - EUROCRYPT '94, volume 950 of Lecture Notes in Computer Science, pages 366–375. Springer, 1995.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>F. Mendel, T. Nad, and M. Schl¨affer. Finding SHA-2 Characteristics: Searching through a Minefield of Contradictions. In Advances in Cryptology - ASIACRYPT 2011, volume 7073 of Lecture Notes in Computer Science, pages 288–307. Springer, 2011.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>F. Mendel, V. Rijmen, D. Toz, and K. Varici. Differential Analysis of the LED Block Cipher. In Advances in Cryptology - ASIACRYPT 2012, volume 7658 of Lecture Notes in Computer Science, pages 190–207. Springer, 2012.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>N. Mouha, Q. Wang, D. Gu, and B. Preneel. Differential and Linear Cryptanalysis Using Mixed-Integer Linear Programming. In Information Security and Cryptology - Inscrypt 2011, volume 7537 of Lecture Notes in Computer Science, pages 57–76. Springer, 2012.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>I. Nikolic. Tweaking AES. In Selected Areas in Cryptography SAC 2010, volume 6544 of Lecture Notes in Computer Science, pages 198–210. Springer, 2011.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>K. Nyberg and L. R. Knudsen. Provable Security Against Differential Cryptanalysis. In Advances in Cryptology - CRYPTO '92, volume 740 of Lecture Notes in Computer Science, pages 566–574. Springer, 1993.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>S. Sun, L. Hu, M. Wang, P. Wang, K. Qiao, X. Ma, D. Shi, and L. Song. Automatic Enumeration of (Related-key) Differential and Linear Characteristics with Predefined Properties and Its Applications. 2014.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>S. Sun, L. Hu, P. Wang, K. Qiao, X. Ma, and L. Song. Automatic Security Evaluation and (Related-key) Differential Characteristic Search: Application to SIMON,</li>
    </ol></li>
    </ul>

    <p class="text-gray-300">PRESENT, LBlock, DES(L) and Other Bit-oriented Block Ciphers. Cryptology ePrint Archive, Report 2013/676, 2013. Accepted to ASIACRYPT 2014.</p>

    <h2 id="sec-20" class="text-2xl font-bold">10 Calculating <span class="math">P_{r,d}^n</span></h2>

    <p class="text-gray-300">The Hamming ball algorithm of Section 5 relies on the value of  <span class="math">P_{r,d}^n</span> . We compute this value by distinguishing between two cases: when d&gt;2r, then  <span class="math">P_{r,d}^n=0</span> , as the largest Hamming distance between points in  <span class="math">B_r(c)</span>  is 2r. Otherwise,  <span class="math">d\\leq 2r</span> , and we consider the conditions on a point x such that both  <span class="math">x\\in B_r(c)</span>  and  <span class="math">x\\oplus\\Delta\\in B_r(c)</span> . We partition the coordinates of  <span class="math">x\\oplus c</span>  which are set to 1 into two groups: the  <span class="math">d_1\\leq \\min(r,d)</span>  coordinates which are common to  <span class="math">x\\oplus c</span>  and  <span class="math">\\Delta\\oplus c</span> , and the remaining  <span class="math">d_2\\leq \\min(r,n-d)</span>  coordinates. Thus, we have  <span class="math">dist(x,c)=d_1+d_2</span>  and  <span class="math">dist(x\\oplus\\Delta,c)=d+d_2-d_1</span> , implying that  <span class="math">d_1+d_2\\leq r</span>  and  <span class="math">d+d_2-d_1\\leq r</span> . In particular, the last equality implies that  <span class="math">d_1\\geq \\max(d-r,0)</span> , and so  <span class="math">\\max(d-r,0)\\leq d_1\\leq \\min(r,d)</span> , while  <span class="math">0\\leq d_2\\leq \\min(r-d_1,r+d_1-d,n-d)</span> . Therefore, we obtain</p>

    <p class="text-gray-300"><span class="math">$P_{r,d}^{n} = \\sum_{d_{1}=m_{1}}^{m_{2}} \\sum_{d_{2}=0}^{m_{3}} \\binom{d}{d_{1}} \\binom{n-d}{d_{2}}</span>$</p>

    <p class="text-gray-300">where  <span class="math">m_1 = max(d-r, 0)</span> ,  <span class="math">m_2 = min(r, d)</span>  and  <span class="math">m_3 = min(r-d_1, r+d_1-d, n-d)</span> .</p>

`;
---

<BaseLayout title="7 Applying Our New Algorithms to the Simon Family of Block C... (2015/268)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2015 &middot; eprint 2015/268
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <PaperDisclaimer eprintUrl={EPRINT_URL} />
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <Fragment set:html={CONTENT} />

    <PaperHistory slug="7-applying-our-new-algorithms-to-the-simon-family-of-block-2015" />
  </article>
</BaseLayout>
