---
import BaseLayout from '../../layouts/BaseLayout.astro';
import PaperDisclaimer from '../../components/PaperDisclaimer.astro';
import PaperHistory from '../../components/PaperHistory.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2024/1399';
const CRAWLER = 'marker';
const CONVERTED_DATE = '2026-02-16';
const TITLE_HTML = 'A Note on Ligero and Logarithmic Randomness';
const AUTHORS_HTML = 'Guillermo Angeris, Alex Evans, Gyumin Roh';

const CONTENT = `    <section id="abstract" class="mb-10">
      <h2 class="text-2xl font-bold">Abstract</h2>
      <p class="text-gray-300">We revisit the Ligero proximity test, and its logarithmic randomness variant, in the framework of [EA23] and show a simple proof that improves the soundness error of the original logarithmic randomness construction of [DP23] by a factor of two. This note was originally given as a presentation in ZK Summit 11.</p>
      <p class="text-gray-300"><strong>Keywords:</strong> ligero &middot; linear algebra &middot; succinct proofs &middot; zero knowledge</p>
    </section>

    <p class="text-gray-300">We use the same notation and conventions as [EA23], which we quickly review here. For (much) more, including proofs of these statements, see [EA23, &sect;1].</p>

    <p class="text-gray-300">Probabilistic implications. Given some statements P<sup>r</sup> and Q<sup>r</sup> &prime; each depending on some random variables r and r &prime; , we say that</p>

    <p class="text-gray-300"><span class="math">$P_r \\Longrightarrow_p Q_{r&#x27;},</span>$</p>

    <p class="text-gray-300"><sup>&lowast;</sup>Authors are listed in alphabetical order.</p>

    <p class="text-gray-300">if  <span class="math">Pr(P_r \\wedge \\neg Q_{r&#x27;}) \\leq p</span> , where we call p the error probability (sometimes known as the soundness error). This is a relaxation of traditional logic, which is the case where the error probability p = 0. Note that the distribution from which r and r' are drawn is otherwise arbitrary and is specified in the text. (In almost all cases, we will either have r' = r, or r and r' independently drawn.) A simple exercise shows that</p>

    <p class="text-gray-300"><span class="math">$P_r \\Longrightarrow_p Q_{r&#x27;}</span>$
and  <span class="math">Q_{r&#x27;} \\Longrightarrow_{p&#x27;} T_{r&#x27;&#x27;}</span> ,</p>

    <p class="text-gray-300">then</p>

    <p class="text-gray-300"><span class="math">$P_r \\Longrightarrow_{p+p&#x27;} T_{r&#x27;&#x27;},</span>$</p>

    <p class="text-gray-300">where we make no assumptions about the distribution from which r, r', and r'' are drawn. In general, we sometimes call probabilistic implications tests since they greatly strengthen our belief about the conclusion, but do not guarantee it; i.e., they are a type of statistical test about an object which we might not have direct access to.</p>

      <h3 id="sec-1.1" class="text-xl font-semibold mt-8">1.1 Linear algebra</h3>

    <p class="text-gray-300">All linear algebra in this note will be over some finite field  <span class="math">\\mathbf{F}</span> .</p>

    <p class="text-gray-300"><strong>Basic results.</strong> We will only rely on two important results from linear algebra over finite fields. The first is that any vector (sub)space  <span class="math">V \\subseteq \\mathbf{F}^m</span>  has a basis which forms the columns of a matrix  <span class="math">T \\in \\mathbf{F}^{m \\times n}</span>  such that, for every vector  <span class="math">y \\in V</span> , there is a unique  <span class="math">x \\in \\mathbf{F}^n</span>  such that</p>

    <p class="text-gray-300"><span class="math">$y = Tx</span>$
.</p>

    <p class="text-gray-300">(In this case n is often called the <em>dimension</em> of the subspace V.) We write the range of T as  <span class="math">\\mathcal{R}(T)</span>  which is the set of all linear combination of the columns of T. The fact above can then be restated as: for any subspace  <span class="math">V \\subseteq \\mathbf{F}^m</span> , there exists a matrix  <span class="math">T \\in \\mathbf{F}^{m \\times n}</span>  with linearly-independent columns such that  <span class="math">\\mathcal{R}(T) = V</span> . The second fact is that, if some matrix T has linearly-independent columns then it is <em>injective</em>. As a reminder, T has linearly independent columns if</p>

    <p class="text-gray-300"><span class="math">$Tx = 0</span>$
implies  <span class="math">x = 0</span> .</p>

    <p class="text-gray-300">Such a matrix T must be injective since, if Tx = Ty, then</p>

    <p class="text-gray-300"><span class="math">$T(x-y) = 0</span>$
implies  <span class="math">x = y</span> .</p>

    <p class="text-gray-300"><strong>Weight.</strong> We will also define the (Hamming) weight of a vector  <span class="math">x \\in \\mathbf{F}^n</span>  as</p>

    <p class="text-gray-300"><span class="math">$||x|| = |\\{i = 1, \\dots, n \\mid x_i \\neq 0\\}|.</span>$</p>

    <p class="text-gray-300">In English: the weight of a vector x, written ||x||, is the number of nonzero entries of x. Note that the weight  <span class="math">||\\cdot||</span>  is almost, but not quite, a norm as it satisfies the triangle inequality: for  <span class="math">x, y \\in \\mathbf{F}^m</span>  we have</p>

    <p class="text-gray-300"><span class="math">$||x + y|| \\le ||x|| + ||y||,</span>$</p>

    <p class="text-gray-300">definiteness,</p>

    <p class="text-gray-300"><span class="math">$||x|| = 0</span>$
if, and only if,  <span class="math">x = 0</span> ,</p>

    <p class="text-gray-300">and 0-homogeneity,</p>

    <p class="text-gray-300"><span class="math">$\\|\\alpha x\\| = \\|x\\|,</span>$</p>

    <p class="text-gray-300">for any nonzero &alpha; &isin; F. (The weight is not quite a norm since norms must satisfy 1 homogeneity.)</p>

    <p class="text-gray-300">Weight of a matrix. It will be very useful in what follows to overload notation slightly and define the weight for a matrix X &isin; F m&times;n , which we write</p>

    <p class="text-gray-300"><span class="math">$||X|| = \\text{number of nonzero rows of } X.</span>$</p>

    <p class="text-gray-300">This definition also satisfies the triangle inequality since, for X, Y &isin; F m&times;n ,</p>

    <p class="text-gray-300"><span class="math">$||X + Y|| \\le ||X|| + ||Y||,</span>$</p>

    <p class="text-gray-300">along with definiteness and 0-homogeneity. It also satisfies the following two useful facts. First, for any vector z &isin; F n , we have that</p>

    <p class="text-gray-300"><span class="math">$||Xz|| \\le ||X||. \\tag{1}</span>$</p>

    <p class="text-gray-300">(The symbols here must be parsed very carefully: on the left hand side we are taking the weight of a vector Xz, resulting from a matrix-vector product, while on the right hand side we are taking the weight of a matrix X as defined above.) Second, if we interpret some n vector x &isin; F <sup>n</sup> as an n-by-1 matrix, then the definition of weight for the vector x and its corresponding n-by-1 matrix coincide exactly. This justifies overloading the notation &#8741; &middot; &#8741; to stand for both the matrix and the vector cases.</p>

    <p class="text-gray-300">Weight of a set. For convenience, we define the weight of a set S (composed of either vectors or matrices) to be</p>

    <p class="text-gray-300"><span class="math">$||S|| = \\min_{x \\in S} ||x||.</span>$</p>

    <p class="text-gray-300">In other words, the weight of a set S is the weight of its 'smallest' element. If we write z &minus; S = {z &minus; x | x &isin; S} for the (Minkowski) difference of z and S, then we can interpret</p>

    <p class="text-gray-300"><span class="math">$||z-S||</span>$
,</p>

    <p class="text-gray-300">as the distance between the vector z and its 'nearest' vector in the subset S.</p>

    <p class="text-gray-300">Subspace matrices. Finally, given a vector subspace V &sube; F <sup>m</sup>, we write V n for the set of m &times; n matrices whose columns lie in V . Since this is a subspace, note that any linear combination of matrices in V n is also a matrix with columns in V .</p>

      <h3 id="sec-1.2" class="text-xl font-semibold mt-8">1.2 Error correcting codes</h3>

    <p class="text-gray-300">We will not use any deep results from error correcting codes in this note except for two basic definitions.</p>

    <p class="text-gray-300">Distance. We define the distance d &ge; 0 of a code (which we write as a matrix G &isin; F m&times;n ) to be</p>

    <p class="text-gray-300"><span class="math">$d = \\min_{x \\neq 0} \\|Gx\\|.</span>$</p>

    <p class="text-gray-300">(Note that the matrix G is injective if, and only if, its distance d &gt; 0.) We similarly define the distance d of a vector subspace V &sube; F <sup>m</sup> as the smallest nonzero element in the subspace</p>

    <p class="text-gray-300"><span class="math">$d = \\min_{v \\in V \\setminus \\{0\\}} ||v||.</span>$</p>

    <p class="text-gray-300">Note that when G has linearly independent columns, the distance of G (as a matrix/code) and the distance of its range R(G) (as a subspace) are the same. One useful consequence of this definition is the fact that, given two vectors x, y &isin; F n if we can show, for some matrix G with distance d &gt; 0 that</p>

    <p class="text-gray-300"><span class="math">$||G(x-y)|| &lt; d, (2)</span>$</p>

    <p class="text-gray-300">then, necessarily, we know that x &minus; y = 0, or that x = y, since G(x &minus; y) = 0 and G must be injective since its distance d is strictly positive.</p>

    <p class="text-gray-300">Distance to a subspace. We will constantly make use of the following (nearly-obvious) fact in the proof of this paper. Let V be a subspace with distance at least d, then, any matrix X &isin; V <sup>n</sup> with columns in V , which has weight smaller than d,</p>

    <p class="text-gray-300">&#8741;X&#8741; &lt; d,</p>

    <p class="text-gray-300">must be the zero matrix, X = 0. The proof follows from applying the fact that, if some vector x &isin; V has weight smaller than d then x = 0, to each column of X. One consequence of this fact is that, if any matrix Y &isin; F m&times;n satisfies</p>

    <p class="text-gray-300"><span class="math">$||Y - V^n|| &lt; d/2,</span>$</p>

    <p class="text-gray-300">then there exists a unique matrix X &isin; V <sup>n</sup> with</p>

    <p class="text-gray-300"><span class="math">$||Y - X|| &lt; d/2. (3)</span>$</p>

    <p class="text-gray-300">This is an easy consequence since, letting X&prime; &isin; V n satisfy &#8741;Y &minus; X&prime;&#8741; &lt; d/2 implies that</p>

    <p class="text-gray-300"><span class="math">$\\|X - X&#x27;\\| \\le \\|Y - X\\| + \\|Y - X&#x27;\\| &lt; d</span>$</p>

    <p class="text-gray-300">so, since X &minus; X&prime; &isin; V n , we know that X &minus; X&prime; = 0, or that X = X&prime; . This fact is often called 'unique decoding' since there exists a unique matrix in V n closest to Y if the distance between Y and V n is less than d/2.</p>

      <h3 id="sec-1.3" class="text-xl font-semibold mt-8">1.3 Sparsity checks</h3>

    <p class="text-gray-300">An important 'primitive' we will use repeatedly in what follows is the ability to probabilistically check that two vectors are 'close' under the norm &#8741; &middot; &#8741;. In particular, to check that two vectors x, y &isin; F <sup>m</sup> are no further apart than, say, some distance q, it suffices to randomly sample a few entries of x and y, and verify that these entries are equal. We can write this in the language of probabilistic implications as</p>

    <p class="text-gray-300"><span class="math">$(x - y)_S = 0 \\quad \\Longrightarrow \\quad ||x - y|| &lt; q, \\tag{4}</span>$</p>

    <p class="text-gray-300">where</p>

    <p class="text-gray-300"><span class="math">$p \\le \\left(1 - \\frac{q}{m}\\right)^{|S|},\\tag{5}</span>$</p>

    <p class="text-gray-300">and S &sube; {1, . . . , m} is a set of uniformly randomly sampled indices of x and y. Here, |S| denotes the cardinality of S; i.e., the number of entries that were randomly sampled and checked. In particular, for any given probability p, this implies that the number of checks |S| required to achieve this error probability p is no more than</p>

    <p class="text-gray-300"><span class="math">$|S| \\le \\left\\lceil \\frac{m}{q} \\log \\left( \\frac{1}{p} \\right) \\right\\rceil. \\tag{6}</span>$</p>

    <p class="text-gray-300">Here, we have used the bound that log(1 &minus; t) &le; &minus;t for t &lt; 1.</p>

    <p class="text-gray-300">Discussion. Note that the above implication is only in one direction: in particular, this implication does not say that, if &#8741;x&minus;y&#8741; &lt; q, then (x&minus;y)<sup>S</sup> = 0 with high probability. (Indeed, if &#8741;x &minus; y&#8741; = q &minus; 1, then the probability of failure is roughly the same as if &#8741;x &minus; y&#8741; = q, even though the conclusion is satisfied in the former.) This plays particularly nicely in the case that x and y are known to belong to a subspace V with some distance d. In this case, x and y are equal only when they differ in less than d entries. If so, then it suffices only to randomly sample entries of x and y and check equality of these randomly sampled entries. If these checks all pass, then we know that, with high probability, not only are these vectors close, but, in fact, they must be equal everywhere. This 'amplification of differences' is the reason why error correcting codes are extremely natural in the construction of succinct proofs.</p>

      <h3 id="sec-1.4" class="text-xl font-semibold mt-8">1.4 Interaction model</h3>

    <p class="text-gray-300">Throughout this note we will use one simple interaction model. In this model, we send and receive vectors, and we are allowed to access only a small number of rows of some fixed, but otherwise opaque, matrix X &isin; F m&times;n . These models can be practically realized in a number of ways: for example, one could use a Merkle commitment to commit to the rows of X, publish this commitment, and only 'open' up a small number of rows, or one could provide X to a trusted third party who only allows a bounded number of queries.</p>

    <p class="text-gray-300">Matrix-vector products. An important consequence of this interaction model is that we can efficiently query a small number of indices of the matrix-vector product Xv: if we wish to query, say entry i of Xv, it suffices to query the ith row of X, which we will write as &tilde;x T i and take the inner product with v since</p>

    <p class="text-gray-300"><span class="math">$(Xv)_i = \\tilde{x}_i^T v. (7)</span>$</p>

    <p class="text-gray-300">By assumption on the interaction model, it is easy to query a specific row of the matrix X, so this computation is easy to perform. At a high level, one of our goals will be to make sure that the number of rows we query of the matrix X is small&mdash;much smaller than the total number of rows m of X.</p>

    <p class="text-gray-300">Distance of a matrix-vector product. Using the above observation, it is not hard to see that, in this interaction model, we can easily certify that the matrix-vector product Xv is not 'too far' from a given vector, say y, using the sparsity check presented in (4). To do this, pick a random subset S &sube; {1, . . . , m} and then simply verify that</p>

    <p class="text-gray-300"><span class="math">$(Xv)_S = y_S,</span>$</p>

    <p class="text-gray-300">which will imply that &#8741;Xv &minus; y&#8741; &le; q with error probability no more than that given in (5). From the observation provided in (7), we know that to compute (Xv)S, it suffices only to request rows S of X, meaning that we only need |S| queries to verify this statement. As an illustrative example, in what follows, q will be roughly m/3 and m will be reasonably large, roughly m &sim; 2 <sup>10</sup>. Making the probability of failure very small, say p &sim; 2 <sup>&minus;</sup><sup>80</sup>, means that the number of rows we need to query is no more than around</p>

    <p class="text-gray-300"><span class="math">$\\lceil 3\\log(2^{80}) \\rceil \\approx 167,</span>$</p>

    <p class="text-gray-300">queries, which is much smaller than the na&uml;&#305;ve deterministic test requiring 2m/3 queries, which would be on the order of &sim; 2 10!</p>

    <section id="sec-2" class="mb-10">
      <h2 class="text-2xl font-bold">2 Proximity testing</h2>

    <p class="text-gray-300">The second tool we will use in this note is that of a proximity test. In a proximity test, we are provided with some vector space V &sube; F <sup>m</sup> with distance d &gt; 0, and we would like to make sure that the columns of some matrix X &isin; F <sup>m</sup>&times;<sup>n</sup> are 'close' to the vector space; i.e., we would like to verify that</p>

    <p class="text-gray-300"><span class="math">$||X - V^n|| \\le q, (8)</span>$</p>

    <p class="text-gray-300">for some proximity parameter q &gt; 0. Note that, if q &lt; d/2 and G is injective and generates the subspace V (i.e., that R(G) = V ) then we immediately know that there exists a unique(!) matrix X&tilde; such that</p>

    <p class="text-gray-300"><span class="math">$||X - G\\tilde{X}|| \\le q.</span>$</p>

    <p class="text-gray-300">We will show that there is a very simple probabilistic implication that ensures (8) holds with high probability.</p>

    <p class="text-gray-300">Succinct proximity test. The succinct test is as follows. Let  <span class="math">G&#x27; \\in \\mathbf{F}^{m&#x27; \\times n}</span>  be a code with distance d' &gt; 0, and let  <span class="math">g_r&#x27;^T</span>  be its rth row. Then, the following probabilistic implication is true:</p>

    <p class="text-gray-300"><span class="math">$||Xg_r&#x27; - V|| \\le q \\quad \\Longrightarrow_{p} \\quad ||X - V^n|| \\le q, \\tag{9}</span>$</p>

    <p class="text-gray-300">where the row index r is uniformly randomly chosen from  <span class="math">1, \\ldots, m&#x27;</span> , while the error probability p depends on the distance of G' and the proximity parameter q. We give a concrete bound on p in what follows. Note that, at no point, does the whole matrix G' (which may be extremely large) ever have to be formed: it suffices only to be able to form a specific (randomly sampled) row of G'.</p>

    <p class="text-gray-300"><strong>Discussion.</strong> We can write the test of (9) in English: to verify that all of the columns of some matrix X are q-close to a vector space V, it suffices to verify that a structured random linear combination of the columns of X (with the randomly chosen coefficients given by  <span class="math">g&#x27;_r</span> ) is q-close to the vector space V, so long as we are willing to accept a probability of error no larger than p. We may view this as a succinct proximity test as we have reduced checking that all of the columns of some matrix X are q-close to a vector subspace to checking that a single vector is q-close to a vector subspace, while potentially taking on some (small) probability of error, p, which we bound next.</p>

    <p class="text-gray-300"><strong>This note.</strong> In this note, we will show that when the code matrix G' can be written as</p>

    <p class="text-gray-300"><span class="math">$G&#x27; = \\underbrace{\\tilde{G} \\otimes \\tilde{G} \\otimes \\cdots \\otimes \\tilde{G}}_{k \\text{ times}}, \\tag{10}</span>$</p>

    <p class="text-gray-300">where  <span class="math">\\tilde{G} \\in \\mathbf{F}^{|\\mathbf{F}| \\times 2}</span>  is the matrix whose rows are of the form (1 - t, t) for each  <span class="math">t \\in \\mathbf{F}</span> , the proximity test (9) satisfies</p>

    <p class="text-gray-300"><span class="math">$p \\le \\frac{k(q+1)}{|\\mathbf{F}|},</span>$</p>

    <p class="text-gray-300">for any q &lt; d/3. This fact was first discovered by [DP23] and used to construct a protocol to succinctly evaluate a multilinear polynomial at a given point. The bound found in that work,  <span class="math">p \\le 2kq/|\\mathbf{F}|</span> , is slightly weaker, by a factor of around 2, from the bound above. (While writing this note, the bound was improved to the case of q &lt; d/2 when the vector space V has certain properties, with slightly worse probability bound,  <span class="math">p \\le kn/|\\mathbf{F}|</span> , by [DG24,DP24] using techniques from the proof presented below, originally from our presentation [EA24] and private communication.) Our proof of the tighter bound is short, fitting in about a page, and relatively straightforward. For now, we will take the above test (9) as given, describe the protocol, and prove this bound for p later in &sect;4. We will use generic parameters to describe the protocol in what follows.</p>

    <p class="text-gray-300"><strong>Extension.</strong> We note that the proof provided also has an immediate extension to the slightly more general case where the matrices  <span class="math">\\tilde{G}</span>  are not all the same and are otherwise</p>

    <p class="text-gray-300">arbitrary codes. In this case, if we have matrices  <span class="math">\\tilde{G}_i \\in \\mathbf{F}^{m_i \\times 2}</span>  for i = 1, ..., k, each with distance  <span class="math">d_i &gt; 0</span> , and  <span class="math">G&#x27; = \\tilde{G}_1 \\otimes \\cdots \\otimes \\tilde{G}_k</span> , then</p>

    <p class="text-gray-300"><span class="math">$p \\le (q+1) \\sum_{i=1}^k \\left(1 - \\frac{d_i}{m_i}\\right).</span>$</p>

    </section>

    <section id="sec-3" class="mb-10">
      <h2 class="text-2xl font-bold">3 The protocol</h2>

    <p class="text-gray-300">In this section we give a simple explanation of Ligero [AHIV17], using the framework of [EA23], as a protocol that can be used to prove that the matrix-vector product for a given matrix X and vector v was correctly computed, up to some (very small) error probability. For the remainder of this section, we will set  <span class="math">V \\subseteq \\mathbf{F}^m</span>  to be some vector subspace with generator matrix  <span class="math">G \\in \\mathbf{F}^{m \\times k}</span>  with distance d &gt; 0; <em>i.e.</em>, the vector space is the range of the matrix,  <span class="math">V = \\mathcal{R}(G)</span> .</p>

      <h3 id="sec-3.1" class="text-xl font-semibold mt-8">3.1 High level description</h3>

    <p class="text-gray-300">We will give a short description of two conditions, which, once verified, show that a certain matrix-vector product has been correctly performed. We will then use these conditions to construct a protocol which allows a player, who we call the <em>prover</em>, to convince another, who we call the <em>verifier</em>, that any desired matrix-vector product has been correctly computed. More interestingly, it will allow the verifier to be certain of this with very high probability while requiring only a very small amount of communication and a relatively small amount of computation. (Indeed, the amount of communication and computation will be far smaller than having the prover send the complete matrix so the verifier can compute and then check the resulting matrix-vector product.)</p>

    <p class="text-gray-300"><strong>Conditions.</strong> Given a matrix  <span class="math">X \\in \\mathbf{F}^{m \\times n}</span> , if we know its columns are no further than q &lt; d/2 to the vector space V, <em>i.e.</em>,</p>

    <p class="text-gray-300"><span class="math">$||X - V^n|| \\le q,\\tag{11}</span>$</p>

    <p class="text-gray-300">then we know that there exists a unique matrix  <span class="math">\\tilde{X} \\in \\mathbf{F}^{k \\times n}</span>  such that</p>

    <p class="text-gray-300"><span class="math">$||X - G\\tilde{X}|| \\le q.</span>$</p>

    <p class="text-gray-300">(This follows from the discussion in &sect;2.) Intuitively, this condition guarantees that there is some unique matrix  <span class="math">\\tilde{X}</span>  from which X must have been derived by encoding the columns of  <span class="math">\\tilde{X}</span>  using G. From (1) we also know that, for any  <span class="math">v \\in \\mathbf{F}^n</span> ,</p>

    <p class="text-gray-300"><span class="math">$\\|(X - G\\tilde{X})v\\| \\le q,\\tag{12}</span>$</p>

    <p class="text-gray-300">by the definition of the weight of a matrix. Keep this fact in mind as we will use it soon. Now, if we verify that Xv is closer than (d &minus; q) to some vector y &isin; V , or, equivalently, there is some vector &tilde;y &isin; F k such that Gy&tilde; is close to Xv, we can write</p>

    <p class="text-gray-300"><span class="math">$||Xv - G\\tilde{y}|| &lt; d - q. \\tag{13}</span>$</p>

    <p class="text-gray-300">Finally, putting it all together, we get</p>

    <p class="text-gray-300"><span class="math">$||G\\tilde{X}v - G\\tilde{y}|| \\le ||(X - G\\tilde{X})v|| + ||Xv - G\\tilde{y}|| &lt; q + (d - q) = d,</span>$</p>

    <p class="text-gray-300">where there first inequality follows from the triangle inequality and we have used observation (12) and inequality (13) in the strict inequality. Now, since G has distance at least d, we know that the left hand side, which satisfies</p>

    <p class="text-gray-300"><span class="math">$||G(\\tilde{X}v - \\tilde{y})|| &lt; d,</span>$</p>

    <p class="text-gray-300">will immediately imply that</p>

    <p class="text-gray-300"><span class="math">$\\tilde{X}v = \\tilde{y}.\\tag{14}</span>$</p>

    <p class="text-gray-300">(This is simply an application of (2) and the surrounding discussion.)</p>

    <p class="text-gray-300">Discussion of conditions. One way of looking at this set of claims is that if we can verify condition (11) and condition (13), then we immediately know that Xv &tilde; = &tilde;y, where X&tilde; is the unique matrix whose encoding is closest to X. Of course, we know that condition (11) can be succinctly tested using the proximity test of &sect;2, while condition (13), given &tilde;y and the ability to query rows of X, is also easily verified using the sparsity check of &sect;1.3. We will use these two checks to verify both conditions succinctly and give bounds on the probability of error, leading to a succinct proof of the matrix-vector product (14). We will then show how to use this to construct a succinct protocol which convinces a verifier that a matrix-vector product was correctly performed for any given X&tilde;.</p>

      <h3 id="sec-3.2" class="text-xl font-semibold mt-8">3.2 Checks and the final protocol</h3>

    <p class="text-gray-300">In this subsection, we outline a simple probability bound on the checks and show how this results in a protocol, played by two parties, which allows one to show the other that a certain matrix-vector product was correctly computed, using only a very small number of queries and a very small amount of computation.</p>

    <p class="text-gray-300">Protocol. The protocol involves two parties: a prover who wishes to show that the matrixvector product of some matrix X&tilde; (known only to the prover) and vector v has been correctly computed, and a verifier who wishes to check this claim. The steps are as follows:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>The prover constructs a matrix X = GX&tilde;, which simply encodes the columns of X&tilde; by G and commits to the rows of the matrix. (The rows of X are fixed and accessible to the verifier, who will only query a small number of these rows. This can be practically achieved via a Merkle commitment to the rows of X.)</li>
    </ol>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>The verifier selects a random row r of G', denoted  <span class="math">g&#x27;_r</span> , and sends it to the prover. (As a reminder, the matrix G' is defined in the proximity test &sect;2.)</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>The prover computes and returns  <span class="math">\\tilde{z}_r = \\tilde{X}g&#x27;_r</span> .</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>The verifier checks, using the sparsity check in &sect;1.3, whether  <span class="math">||Xg&#x27;_r G\\tilde{z}_r|| \\leq q</span> . If this condition holds, the verifier can conclude that  <span class="math">||X G\\tilde{X}|| \\leq q</span> , or that X is close to a correctly encoded matrix, failing with probability no more than p, based on the result in (9).</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>The verifier sends the desired vector v to the prover to compute the matrix-vector product.</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>The prover computes and sends back the 'correct' matrix vector product  <span class="math">\\tilde{y} = \\tilde{X}v</span> .</li>
    </ol></li>
      <li><ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">
      <li>The verifier checks whether  <span class="math">||Xv G\\tilde{y}|| &lt; d q</span> . If this inequality holds, the verifier concludes that the matrix-vector product  <span class="math">\\tilde{X}v = \\tilde{y}</span>  has been correctly computed by the reasoning in &sect;3.1.</li>
    </ol></li>
    </ul>

    <p class="text-gray-300">In fact, in some practical cases, steps 5-7 are not required, such as when Ligero is used as a multilinear polynomial commitment scheme, since then we have  <span class="math">v = g&#x27;_r</span>  and the result is given by  <span class="math">\\tilde{z}_r</span> , as was originally proposed in [DP23,DP24]. From this description and the proofs below, it is not hard to see that the same proofs apply and result in less communication and total work.</p>

    <p class="text-gray-300"><strong>Correctness.</strong> The fact that the verifier always accepts if the prover is honest is immediate from the protocol description. If the prover correctly encodes  <span class="math">X = G\\tilde{X}</span>  and commits to this matrix, then every check passes by definition: the first because  <span class="math">Xg&#x27;_r = Gz_r = G\\tilde{X}g&#x27;_r</span>  so the sparsity check does not fail, while the second is true for a similar reason.</p>

    <p class="text-gray-300"><strong>Soundness.</strong> The soundness of this protocol&mdash;i.e., the verifier falsely concludes the product was computed correctly with low probability&mdash;follows directly from combining probabilistic implications and the discussions of &sect;1.3 and &sect;2.</p>

    <p class="text-gray-300">To see this, note the verifier samples r uniformly from  <span class="math">1, \\ldots, m&#x27;</span>  and a subset  <span class="math">S \\subseteq \\{1, \\ldots, m\\}</span>  of fixed size |S|, to get</p>

    <p class="text-gray-300"><span class="math">$(Xg_r&#x27; - G\\tilde{z}_r)_S = 0 \\quad \\Longrightarrow_{p&#x27;} \\quad ||Xg_r&#x27; - G\\tilde{z}_r|| \\le q, \\tag{15}</span>$</p>

    <p class="text-gray-300">where  <span class="math">p&#x27; \\leq (1 - (q+1)/m)^{|S|}</span>  and the randomness here is over S, with r independent. Since, by definition of G and V at the beginning of this section, we have  <span class="math">G\\tilde{z}_r \\in V</span> , then this is the same as saying</p>

    <p class="text-gray-300"><span class="math">$||Xg_r&#x27; - V|| \\le ||Xg_r&#x27; - G\\tilde{z}_r|| \\le q,</span>$</p>

    <p class="text-gray-300">for uniformly randomly sampled r. From &sect;2 we know that</p>

    <p class="text-gray-300"><span class="math">$||Xg_r&#x27; - V|| \\le q \\quad \\Longrightarrow_p \\quad ||X - G\\tilde{X}|| \\le q, \\tag{16}</span>$</p>

    <p class="text-gray-300">with the randomness again over r uniform from  <span class="math">1, \\ldots, m</span>  and where  <span class="math">\\tilde{X} \\in \\mathbf{F}^{k \\times n}</span>  is some (unique) matrix. From before,  <span class="math">p \\leq kq/|\\mathbf{F}|</span> . Finally, the verifier checks that</p>

    <p class="text-gray-300"><span class="math">$(Xv - G\\tilde{y})_{S&#x27;} = 0 \\quad \\Longrightarrow_{p&#x27;&#x27;} \\quad ||Xv - G\\tilde{y}|| &lt; d - q, \\tag{17}</span>$</p>

    <p class="text-gray-300">with  <span class="math">p&#x27;&#x27; \\leq (1 - (d - q)/m)^{|S&#x27;|}</span> , where  <span class="math">S&#x27; \\subseteq \\{1, \\ldots, m\\}</span>  uniformly randomly chosen of fixed size |S'|.</p>

    <p class="text-gray-300">From the high level description in &sect;3.1, the conclusions of (15), (16), and (17), and the basic probabilistic implications, we must have</p>

    <p class="text-gray-300"><span class="math">$\\tilde{X}v = y</span>$</p>

    <p class="text-gray-300">except with probability no more than</p>

    <p class="text-gray-300"><span class="math">$p + p&#x27; + p&#x27;&#x27; \\le \\left(1 - \\frac{q+1}{m}\\right)^{|S|} + \\frac{k(q+1)}{|\\mathbf{F}|} + \\left(1 - \\frac{d-q}{m}\\right)^{|S&#x27;|}.</span>$
(18)</p>

      <h3 id="sec-3.3" class="text-xl font-semibold mt-8">3.3 Proof size and total work</h3>

    <p class="text-gray-300">In this subsection, we compare the total proof size and computational work necessary for both the prover and the verifier in the 'na&iuml;ve' case where the prover simply sends the complete matrix  <span class="math">\\tilde{X}</span>  and the verifier computes the matrix-vector product, compared to the succinct case, where the prover and verifier perform the protocol presented in &sect;3.2.</p>

    <p class="text-gray-300"><strong>Na&iuml;ve proof.</strong> A 'na&iuml;ve' proof of this statement would involve sending the complete matrix  <span class="math">\\tilde{X} \\in \\mathbf{F}^{k \\times n}</span>  which has a total of kn elements, each of size  <span class="math">\\log(|\\mathbf{F}|)</span> . The verifier simply computes the complete matrix-vector product  <span class="math">\\tilde{X}v</span> , which takes  <span class="math">\\sim kn</span>  operations.</p>

    <p class="text-gray-300"><strong>Succinct proof.</strong> In comparison, set a threshold probability  <span class="math">0 &lt; \\varepsilon &lt; 1</span>  of failure and set q = d/3 - 1. Using (6) gives</p>

    <p class="text-gray-300"><span class="math">$|S| \\ge \\frac{3m}{d} \\log \\left(\\frac{1}{\\varepsilon}\\right), \\qquad |S&#x27;| \\ge \\frac{3m}{2d} \\log \\left(\\frac{1}{\\varepsilon}\\right)</span>$
(19)</p>

    <p class="text-gray-300">samples are sufficient. For now, we assume that the field size  <span class="math">|\\mathbf{F}|</span>  is large enough such that  <span class="math">k(q+1)/|\\mathbf{F}| \\leq \\varepsilon</span> . In this case, the total error probability (18) is no more than  <span class="math">3\\varepsilon</span> , by construction.</p>

    <p class="text-gray-300">Communication for succinct proof. Following the protocol above, the verifier sends randomness r of size  <span class="math">\\log_2(m&#x27;)</span>  (as a reminder, G' has dimensions  <span class="math">m&#x27; \\times n</span> ) and v which is n field elements. The verifier sends the indices corresponding to the chosen rows S and S', which totals  <span class="math">(|S| + |S&#x27;|) \\log_2(m)</span>  bits. (We do not include this in the total as  <span class="math">|S| \\log_2(m) \\ll n</span>  for reasonable parameters.) The prover sends back |S| rows of X, each of size m field elements, committed to by some commitment opening of size, say, C for each row (a similar thing is true for S') along with  <span class="math">\\tilde{z}_r</span>  and  <span class="math">\\tilde{y}</span> , which are of size k field elements, each. The total number of field elements is, from the prover, 2k + Cm(|S| + |S'|), while the verifier sends n field elements (for v) and  <span class="math">\\log_2(m&#x27;)</span>  bits for the random row of G'.</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Communication</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Work</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"></th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Prover</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Verifier</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Prover</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Verifier</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Na&uml;&#305;ve</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">knF</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">0</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">0</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">kn</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Succinct</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">&prime;<br>(2k<br>+<br>Cm( S <br>+<br> S<br> ))F</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">(m&prime;<br>nF<br>+ log2<br>)</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">nkm</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">&prime;<br>( S <br>+<br> S<br> )(k<br>+<br>n)</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
          </tr>
        </tbody>
      </table>
    </div>

    <p class="text-gray-300">Table 1: Comparison of na&uml;&#305;ve and succinct protocols in the general setting, where F = log<sup>2</sup> (|F|).</p>

    <div class="overflow-x-auto my-4">
      <table class="min-w-full text-sm text-gray-300">
        <thead>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Communication</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"></th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left">Work</th>
            <th class="px-3 py-2 border-b border-gray-600 font-semibold text-left"></th>
        </thead>
        <tbody>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left"></td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Prover</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Verifier</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Prover</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Verifier</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Na&uml;&#305;ve</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">2F<br>n</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">0</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">0</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">2<br>n</td>
          </tr>
          <tr>
            <td class="px-3 py-2 border-b border-gray-700 text-left">Succinct</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">180n<br>log2<br>(n)F</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">(m&prime;<br>nF<br>+ log2<br>)</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">3<br>2n</td>
            <td class="px-3 py-2 border-b border-gray-700 text-left">360n</td>
          </tr>
        </tbody>
      </table>
    </div>

    <p class="text-gray-300">Table 2: Comparison of na&uml;&#305;ve and succinct protocols in the concrete setting.</p>

    <p class="text-gray-300">Computational work for succinct proof. Finally, the total computational work for the prover comes from computing GX&tilde; which is around nkm operations for general G (but can be much lower for more structured matrices G), committing to the result's rows, which we assume is much smaller than the nkm operations, and computing the matrix-vector products of X&tilde; with g &prime; r and v, which are km &#8810; nkm operations each, which means that the total prover work is around &sim; nkm. The verifier's work comes from computing |S| + |S &prime; | inner products of rows of X with g &prime; r and v, each of which are a total of k operations, along with computing |S| + |S &prime; | inner products of rows of G with &tilde;z<sup>r</sup> and &tilde;y, respectively, each of which are a total of n operations, totaling up to around &sim; (|S| + |S &prime; |)(k + n) operations. (We assume that checking the commitment is negligible relative to this number.)</p>

    <p class="text-gray-300">Putting it all together. We can see the results in table 1 for 'essentially' general parameters m, k, n, and fields F so long as the field size is large enough. For slightly more concrete parameters, set the matrix dimensions of X&tilde; to be square, i.e., k = n, and the block size as m = 2n, such that G &isin; F 2n&times;n . If we use a maximum-distance-separable code, such as a Reed&ndash;Solomon code, we have that d = m &minus; n = n. Setting the parameter &epsilon; = e <sup>&minus;</sup><sup>20</sup> &asymp; 2 &minus;29 such that the probability of failure is no more than 3&epsilon;, we then have that |S| = 120 and |S &prime; | = 60. (Here, we have used (19).) Finally, let the commitment be a Merkle commitment such that C &sim; log<sup>2</sup> (n). We place these 'more concrete' results in table 2. Note that, in this setting, the succinct protocol has both lower communication and total work for the verifier, whenever n &#8811; 360; indeed, the verifier work only grows linearly(!) in n, the side length of the square matrix X&tilde; &isin; F n&times;n . In many practical applications, we have n &sim; 2 <sup>10</sup> making the total communication and verification time considerably smaller than those of the na&uml;&#305;ve protocol. In the particular case of a Reed&ndash;Solomon code, we also have that the work necessary for the prover is also much smaller: the matrix G is structured so GX&tilde; can be computed in  <span class="math">\\sim n^2 \\log_2(n)</span>  operations, versus  <span class="math">2n^3</span> , as is the case here.</p>

    <p class="text-gray-300"><strong>Discussion.</strong> In general, both tables and the corresponding results show a particularly interesting pattern in succinct proofs: often, the prover has to do some additional work over the na&iuml;ve protocol, yet the result is that the succinct proof is considerably smaller than the na&iuml;ve proof in both computation for the verifier and total communication. We note that it is also possible to squeeze out slightly more performance in a number of places, but we have not done so here. (For example, in the protocol description, S and S' are sampled independently, yet the bound applies even in the case where they are not. Choosing  <span class="math">S&#x27; \\subseteq S</span>  reduces the communication overhead, so long as the order of operations ensures that the prover does not learn S before sending the proposed result  <span class="math">\\tilde{y}</span> .)</p>

    </section>

    <section id="sec-4" class="mb-10">
      <h2 class="text-2xl font-bold">4 Proof of test</h2>

    <p class="text-gray-300">In this section, we prove the proximity testing claim given at the end of &sect;2 along with the provided bounds. In particular, we will show that, letting  <span class="math">G&#x27; \\in \\mathbf{F}^{|\\mathbf{F}|^k \\times 2^k}</span>  be the Kronecker product code of (10), and letting  <span class="math">g&#x27;_r</span>  be its rth row, the following probabilistic implication is true, for any matrix  <span class="math">X \\in \\mathbf{F}^{m \\times 2^k}</span>  and any vector space  <span class="math">V \\subseteq \\mathbf{F}^m</span>  with distance d &gt; 0:</p>

    <p class="text-gray-300"><span class="math">$||Xg&#x27;_r - V|| \\le q \\quad \\Longrightarrow_p \\quad ||X - V^n|| \\le q,</span>$</p>

    <p class="text-gray-300">whenever q &lt; d/3 while  <span class="math">p \\le k(q+1)/|\\mathbf{F}|</span> . The proof provided in this section was originally presented by the authors at [EA24], and has been subsequently used in a number of improved results [DG24] during the preparation of this note.</p>

    <p class="text-gray-300"><strong>Proof outline.</strong> We will prove the result in two steps. We will show that, given two matrices  <span class="math">X_1, X_2 \\in \\mathbf{F}^{m \\times n}</span>  then the following probabilistic implication is true:</p>

    <p class="text-gray-300"><span class="math">$||r&#x27;X_1 + (1 - r&#x27;)X_2 - V^n|| \\le q \\quad \\Longrightarrow_{p&#x27;} \\quad ||[X_1 \\ X_2] - V^{2n}|| \\le q,</span>$
(20)</p>

    <p class="text-gray-300">where  <span class="math">r&#x27; \\in \\mathbf{F}</span>  is uniformly randomly chosen, q &lt; d/3, and  <span class="math">p&#x27; \\le (q+1)/|\\mathbf{F}|</span> . In other words, if we take a random linear combination of  <span class="math">X_1</span>  and  <span class="math">X_2</span> , and this random linear combination is q-close to a vector space V, then the matrix formed by concatenating both  <span class="math">X_1</span>  and  <span class="math">X_2</span>  must also be q-close to the vector space. Then, the easy part of the proof follows essentially by induction and the definition of the Kronecker product. We prove this 'easy' part, assuming the probabilistic implication (20) is true, next.</p>

    <p class="text-gray-300"><strong>Proof of second statement.</strong> The bound essentially directly follows from the probabilistic implications given in the introduction and a basic application of induction. First, note that</p>

    <p class="text-gray-300"><span class="math">$g&#x27;_r = (1 - r_1, r_1) \\otimes (1 - r_2, r_2) \\otimes \\cdots \\otimes (1 - r_k, r_k),</span>$
(21)</p>

    <p class="text-gray-300">for  <span class="math">r_1, \\ldots, r_k \\in \\mathbf{F}</span>  sampled uniformly and independently. We would like to show that</p>

    <p class="text-gray-300"><span class="math">$||Xg&#x27;_r - V|| \\le q \\quad \\Longrightarrow_{kp&#x27;} \\quad ||X - V^{2^k}|| \\le q,</span>$
(22)</p>

    <p class="text-gray-300">for any k, where  <span class="math">p&#x27; \\leq (q+1)/|\\mathbf{F}|</span>  as before. Assume the claim is true for k-1. Note that, if  <span class="math">X = [X_1 X_2]</span>  and  <span class="math">X_1, X_2 \\in \\mathbf{F}^{m \\times 2^{k-1}}</span> , then</p>

    <p class="text-gray-300"><span class="math">$Xg&#x27;_r = ((1 - r_k)X_1 + r_kX_2)\\tilde{g}&#x27;_{\\tilde{r}},</span>$</p>

    <p class="text-gray-300">where  <span class="math">\\tilde{g}&#x27;_{\\tilde{r}}</span>  is the tensor product (21) with the last term,  <span class="math">(1 - r_k, r_k)</span> , removed and  <span class="math">r = (\\tilde{r}, r_k)</span> ; <em>i.e.</em>,</p>

    <p class="text-gray-300"><span class="math">$\\tilde{g}&#x27;_{\\tilde{r}} = (1 - r_1, r_1) \\otimes (1 - r_2, r_2) \\otimes \\cdots \\otimes (1 - r_{k-1}, r_{k-1}).</span>$</p>

    <p class="text-gray-300">If claim (22) is true for k-1, then we have that, for any  <span class="math">r_k \\in \\mathbf{F}</span> :</p>

    <p class="text-gray-300"><span class="math">$\\|((1-r_k)X_1+r_kX_2)y_{\\tilde{r}}&#x27;-V\\| \\le q \\quad \\Longrightarrow_{(k-1)p} \\quad \\|(1-r_k)X_1+r_kX_2-V^{2^{k-1}}\\| \\le q,</span>$</p>

    <p class="text-gray-300">with  <span class="math">\\tilde{r} \\in \\mathbf{F}^{k-1}</span>  uniformly and independently sampled. But, from the original claim (20), we know</p>

    <p class="text-gray-300"><span class="math">$\\|(1-r_k)X_1 + r_kX_2 - V^{2^{k-1}}\\| \\le q \\quad \\Longrightarrow \\quad \\|[X_1 \\ X_2] - V^{2^k}\\| \\le q,</span>$</p>

    <p class="text-gray-300">for  <span class="math">r_k \\in \\mathbf{F}</span>  uniformly sampled, so we recover the final result that  <span class="math">p \\leq kp&#x27; \\leq k(q+1)/|\\mathbf{F}|</span> , by chaining the probabilistic implications.</p>

      <h3 id="sec-4.1" class="text-xl font-semibold mt-8">4.1 Proof of the first statement</h3>

    <p class="text-gray-300">Here, we prove the first statement given in the general proof outline (20).</p>

    <p class="text-gray-300"><strong>Outline.</strong> The proof's main goal will be to reduce this check to the matrix sparsity check of [EA23, &sect;3.2.2], and we provide a simple, self-contained proof of the special case used here in appendix A. Now, let R be the set of  <span class="math">r&#x27; \\in \\mathbf{F}</span>  such that  <span class="math">\\|(1-r&#x27;)X_1 + r&#x27;X_2 - V^n\\| \\le q</span> , and, for notational convenience, define</p>

    <p class="text-gray-300"><span class="math">$Z_{r&#x27;} = (1 - r&#x27;)X_1 + r&#x27;X_2, (23)</span>$</p>

    <p class="text-gray-300">for every  <span class="math">r&#x27; \\in \\mathbf{F}</span> . We can then rewrite the set R to be  <span class="math">R = \\{r&#x27; \\in \\mathbf{F} \\mid ||Z_{r&#x27;} - V^n|| \\leq q\\}</span> . The first part of the proof will be to show that, given two elements  <span class="math">r&#x27;, r&#x27;&#x27; \\in R</span>  with  <span class="math">r&#x27; \\neq r&#x27;&#x27;</span> , then we can always write the error matrix of  <span class="math">Z_{\\bar{r}}</span>  for some other index  <span class="math">\\bar{r} \\in R</span>  as a (fixed) linear combination of the errors in  <span class="math">Z_{r&#x27;}</span>  and  <span class="math">Z_{r&#x27;&#x27;}</span> . The second part will then show that if |R| &gt; q + 1, then it must be the case that there are at most q errors in the combined matrix  <span class="math">[Z_{r&#x27;} Z_{r&#x27;&#x27;}]</span> . This will mean that any linear combination of the errors of  <span class="math">Z_{r&#x27;}</span>  and  <span class="math">Z_{r&#x27;&#x27;}</span>  also has at most q errors. Finally, we then note that we can write the error matrices for  <span class="math">X_1</span>  and  <span class="math">X_2</span>  as linear combinations of the error matrices of  <span class="math">Z_{r&#x27;}</span>  and  <span class="math">Z_{r&#x27;&#x27;}</span> , which means that the combined matrix must also have no more than q errors when |R| &gt; q + 1, which completes the claim since this</p>

    <p class="text-gray-300">is the same as saying that, if there are more than q errors, the number of entries in which this implication fails is  <span class="math">|R| \\le q + 1</span> , which leads to the error bound  <span class="math">p&#x27; = |R|/|\\mathbf{F}| \\le (q+1)/|\\mathbf{F}|</span> .</p>

    <p class="text-gray-300">Purely in the probabilistic implication language, we will show that</p>

    <p class="text-gray-300"><span class="math">$\\|(1-\\bar{r})X_1+\\bar{r}X_2-V^n\\| \\le q \\implies \\|a_{\\bar{r}}\\xi+b_{\\bar{r}}\\xi&#x27;\\| \\le q \\implies \\|[\\xi\\xi&#x27;]\\| \\le q \\implies \\|[X_1X_2]-V^{2n}\\| \\le q,</span>$</p>

    <p class="text-gray-300">where  <span class="math">\\xi</span>  and  <span class="math">\\xi&#x27;</span>  are some (in fact, as we will show, the) error matrices corresponding to  <span class="math">Z_{r&#x27;}</span>  and  <span class="math">Z_{r&#x27;&#x27;}</span> , respectively with  <span class="math">\\bar{r}</span>  uniformly drawn from  <span class="math">\\mathbf{F}</span>  and  <span class="math">a_{\\bar{r}}, b_{\\bar{r}} \\in \\mathbf{F}</span>  some coefficients to be determined later.</p>

    <p class="text-gray-300"><strong>Notation.</strong> For notational convenience, for any  <span class="math">r&#x27; \\in \\mathbf{F}</span> , let  <span class="math">Y_{r&#x27;} \\in V^n</span>  be a closest matrix, with columns in V, to  <span class="math">Z_{r&#x27;}</span> . Define  <span class="math">\\xi_{r&#x27;} = Z_r - Y_r</span>  to be an 'error' matrix for index r. There may be many error matrices  <span class="math">\\xi_{r&#x27;}</span>  (as there may be many 'closest' matrices Y) but we will only really make use of these within the unique decoding radius such that  <span class="math">\\xi_{r&#x27;}</span>  is unique.</p>

    <p class="text-gray-300"><strong>Part one.</strong> In the first part of the proof, we will show that the error matrices for an index  <span class="math">\\bar{r} \\in R</span> , written, from before,</p>

    <p class="text-gray-300"><span class="math">$\\xi_{\\bar{r}} = Z_{\\bar{r}} - Y_{\\bar{r}},</span>$</p>

    <p class="text-gray-300">has at most q nonzero rows and can be written as a linear combination of the error matrices  <span class="math">\\xi</span>  and  <span class="math">\\xi&#x27;</span>  of any two indices  <span class="math">r&#x27;, r&#x27;&#x27; \\in R</span>  with  <span class="math">r \\neq r&#x27;</span> . (We fix indices r' and r'' in what follows and assume that |R| &gt; 1 since, otherwise, the claim is trivial.) Because these indices will be fixed for the remainder of the proof, and we will reference their corresponding matrices often, write</p>

    <p class="text-gray-300"><span class="math">$Z = Z_{r&#x27;}</span>$
and  <span class="math">Z&#x27; = Z_{r&#x27;&#x27;}</span> ,</p>

    <p class="text-gray-300">and similarly for Y, Y', and  <span class="math">\\xi, \\xi&#x27;</span> , where r' and r'' are the (fixed) indices above.</p>

    <p class="text-gray-300">Note that, for each  <span class="math">\\bar{r} \\in \\mathbf{F}</span> , there are coefficients  <span class="math">a_{\\bar{r}}, b_{\\bar{r}} \\in \\mathbf{F}</span>  such that</p>

    <p class="text-gray-300"><span class="math">$(1 - r&#x27;)a_{\\bar{r}} + (1 - r&#x27;&#x27;)b_{\\bar{r}} = 1 - \\bar{r}</span>$
<span class="math">$r&#x27;a_{\\bar{r}} + r&#x27;&#x27;b_{\\bar{r}} = \\bar{r}.</span>$</p>

    <p class="text-gray-300">(This linear system has a solution since  <span class="math">r&#x27; \\neq r&#x27;&#x27;</span> .) Because this is true for each  <span class="math">\\bar{r} \\in \\mathbf{F}</span>  we have, using the definitions of Z and Z' along with (23):</p>

    <p class="text-gray-300"><span class="math">$Z_{\\bar{r}} = a_{\\bar{r}}Z + b_{\\bar{r}}Z&#x27;.</span>$</p>

    <p class="text-gray-300">Now, since  <span class="math">\\bar{r} \\in R</span> , then, by definition of the set R, we know  <span class="math">\\xi_{\\bar{r}} = Z_{\\bar{r}} - Y_{\\bar{r}}</span>  has at most q nonzero rows. Finally, note that</p>

    <p class="text-gray-300"><span class="math">$\\xi_{\\bar{r}} - (a_{\\bar{r}}\\xi + b_{\\bar{r}}\\xi&#x27;) = a_{\\bar{r}}Y + b_{\\bar{r}}Y&#x27; - Y_{\\bar{r}},</span>$</p>

    <p class="text-gray-300">from the definition of  <span class="math">a_{\\bar{r}}</span> ,  <span class="math">b_{\\bar{r}}</span> , and the  <span class="math">\\xi_r</span> . Note that this matrix has no more than 3q &lt; d nonzero rows, as  <span class="math">\\xi_{\\bar{r}}</span> ,  <span class="math">\\xi_r</span> , and  <span class="math">\\xi_{r&#x27;}</span>  each have at most q nonzero rows, by definition of the set R. On the other hand, the right hand side has columns lying in V&mdash;as each of Y, Y', and</p>

    <p class="text-gray-300"><span class="math">Y_{\\bar{r}}</span>  has columns in V&mdash;and this subspace V has distance at least d, so the matrix must be identically zero. The errors therefore satisfy</p>

    <p class="text-gray-300"><span class="math">$\\xi_{\\bar{r}} = a_{\\bar{r}}\\xi + b_{\\bar{r}}\\xi&#x27;,</span>$</p>

    <p class="text-gray-300">for every  <span class="math">\\bar{r} \\in R</span> , which means that</p>

    <p class="text-gray-300"><span class="math">$||a_{\\bar{r}}\\xi + b_{\\bar{r}}\\xi&#x27;|| \\le q</span>$</p>

    <p class="text-gray-300">for each  <span class="math">\\bar{r} \\in R</span> .</p>

    <p class="text-gray-300"><strong>Part two.</strong> We will now show that, if |R| &gt; q + 1, then the matrix  <span class="math">[\\xi \\xi&#x27;]</span>  has at most q nonzero entries.</p>

    <p class="text-gray-300">We can view the vector  <span class="math">(a_{\\bar{r}}, b_{\\bar{r}})</span>  as the  <span class="math">\\bar{r}</span> th row of an  <span class="math">|\\mathbf{F}|</span>  by 2 generator matrix. We will show that the distance of this code is at least  <span class="math">|\\mathbf{F}| - 1</span> , which will imply that, by the matrix sparsity check of appendix A, if |R| &gt; q + 1 then</p>

    <p class="text-gray-300"><span class="math">$\\|[\\xi \\ \\xi&#x27;]\\| \\le q.</span>$</p>

    <p class="text-gray-300">To see that the generator matrix with rows  <span class="math">(a_{\\bar{r}}, b_{\\bar{r}})</span>  has distance  <span class="math">|\\mathbf{F}| - 1</span> , we will show that, for any  <span class="math">\\bar{r} \\neq \\bar{r}&#x27;</span> , we have</p>

    <p class="text-gray-300"><span class="math">$\\begin{bmatrix} a_{\\bar{r}} &amp; b_{\\bar{r}} \\\\ a_{\\bar{r}&#x27;} &amp; b_{\\bar{r}&#x27;} \\end{bmatrix} x = 0, \\tag{24}</span>$</p>

    <p class="text-gray-300">then x = 0. (In other words, if any two distinct symbols of the encoding of x are zero, then x must be zero.) This will immediately imply that either at most one entry of the codeword is zero, or the whole codeword is equal to zero; <em>i.e.</em>, that the code has distance at least  <span class="math">|\\mathbf{F}| - 1</span> .</p>

    <p class="text-gray-300">The proof of this fact is nearly immediate: note that, using the definition of the coefficients  <span class="math">a_{\\bar{r}}, b_{\\bar{r}}</span> , we can write</p>

    <p class="text-gray-300"><span class="math">$\\begin{bmatrix} a_{\\bar{r}} &amp; b_{\\bar{r}} \\\\ a_{\\bar{r}&#x27;} &amp; b_{\\bar{r}&#x27;} \\end{bmatrix} \\begin{bmatrix} (1-r&#x27;) &amp; r&#x27; \\\\ (1-r&#x27;&#x27;) &amp; r&#x27;&#x27; \\end{bmatrix} = \\begin{bmatrix} 1-\\bar{r} &amp; \\bar{r} \\\\ 1-\\bar{r}&#x27; &amp; \\bar{r}&#x27; \\end{bmatrix}.</span>$</p>

    <p class="text-gray-300">Since  <span class="math">r&#x27; \\neq r&#x27;&#x27;</span>  (which are the fixed coefficients of part one) and  <span class="math">\\bar{r} \\neq \\bar{r}&#x27;</span> , then the matrix on the right hand side has linearly independent columns, while the second matrix on the left hand side is invertible. This immediately implies that the columns of the first matrix are also linearly independent, implying (24).</p>

    <p class="text-gray-300"><strong>Part three.</strong> Finally, when we know that  <span class="math">[\\xi \\xi&#x27;]</span>  has at most q nonzero rows, then certainly any linear combination of  <span class="math">\\xi</span>  and  <span class="math">\\xi&#x27;</span>  also has at most q nonzero rows. Since we know that</p>

    <p class="text-gray-300"><span class="math">$X_1 = aZ_r + bZ_{r&#x27;},</span>$</p>

    <p class="text-gray-300">for some  <span class="math">a, b \\in \\mathbf{F}</span> , then the errors of  <span class="math">X_1</span> , which we write as  <span class="math">\\xi_1</span> , can be written as  <span class="math">\\xi_1 = a\\xi + b\\xi&#x27;</span> . (This follows from the fact that q &lt; d/2 so we're in unique decoding and the nonzero entries</p>

    <p class="text-gray-300">of  <span class="math">\\xi</span>  and  <span class="math">\\xi&#x27;</span>  are aligned.) Similarly, we can write the error matrix of  <span class="math">X_2</span> ,  <span class="math">\\xi_2</span> , as a linear combination of  <span class="math">\\xi</span>  and  <span class="math">\\xi&#x27;</span> , which means that  <span class="math">[\\xi_1 \\ \\xi_2]</span>  also has at most the number of nonzero rows as  <span class="math">[\\xi \\ \\xi&#x27;]</span> , which is at most q. This implies, finally, that</p>

    <p class="text-gray-300"><span class="math">$||[X_1 \\ X_2] - V^{2n}|| \\le ||[\\xi_1 \\ \\xi_2]|| \\le q,</span>$</p>

    <p class="text-gray-300">if |R| &gt; q + 1. The claim then is immediate from the definition of probabilistic implications.</p>

    <p class="text-gray-300"><strong>Extensions.</strong> The proof technique here was used for a code G' whose rows are given by Kronecker products of pairs (1 - t, t) for each  <span class="math">t \\in \\mathbf{F}</span> . (Equivalently, when G' is a matrix which is the Kronecker product of k matrices, each with rows of the form (1 - t, t) for each  <span class="math">t \\in \\mathbf{F}</span> .) On the other hand, note that the argument applies very generally to any code G of the form</p>

    <p class="text-gray-300"><span class="math">$G&#x27; = \\tilde{G}_1 \\otimes \\cdots \\otimes \\tilde{G}_k,</span>$</p>

    <p class="text-gray-300">where  <span class="math">\\tilde{G}_i \\in \\mathbf{F}^{m_i \\times 2}</span>  and has distance  <span class="math">d_i &gt; 0</span> . In this case, the probabilistic implication is</p>

    <p class="text-gray-300"><span class="math">$||Xg_r&#x27; - V|| \\Longrightarrow ||X - V^{2^k}||,</span>$</p>

    <p class="text-gray-300">with r uniformly chosen from  <span class="math">1, \\ldots, \\prod_i m_i</span>  and</p>

    <p class="text-gray-300"><span class="math">$p \\le (q+1) \\sum_{i} \\left( 1 - \\frac{d_i}{m_i} \\right).</span>$</p>

    </section>

    <section id="references" class="mb-10">
      <h2 class="text-2xl font-bold">References</h2>

    <ul class="space-y-2 text-gray-400 text-sm list-none">
      <li>[AHIV17] Scott Ames, Carmit Hazay, Yuval Ishai, and Muthuramakrishnan Venkitasubramaniam. Ligero: Lightweight sublinear arguments without a trusted setup. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pages 2087&ndash;2104, Dallas Texas USA, October 2017. ACM.</li>
      <li>[DG24] Benjamin E. Diamond and Angus Gruen. Proximity gaps in interleaved codes. Cryptology ePrint Archive, Paper 2024/1351, 2024.</li>
      <li>[DP23] Benjamin E. Diamond and Jim Posen. Proximity testing with logarithmic randomness. Cryptology ePrint Archive, Paper 2023/630, 2023.</li>
      <li>[DP24] Benjamin E. Diamond and Jim Posen. Polylogarithmic proofs for multilinears over binary towers. Cryptology ePrint Archive, Paper 2024/504, 2024.</li>
      <li>[EA23] Alex Evans and Guillermo Angeris. Succinct proofs and linear algebra. Cryptology ePrint Archive, Paper 2023/1478, 2023.</li>
      <li>[EA24] Alex Evans and Guillermo Angeris. Folding, codes, and linear algebra. Talk at ZK Summit 11, Athens, Greece, April 2024.</li>
    </ul>

    </section>

    <section id="app-a" class="mb-10">
      <h2 class="text-2xl font-bold">A Matrix sparsity check</h2>

    <p class="text-gray-300">We show that, given two matrices  <span class="math">\\xi_1, \\xi_2 \\in \\mathbf{F}^{m&#x27; \\times n}</span>  and a code  <span class="math">G \\in \\mathbf{F}^{m \\times 2}</span>  with distance d, that</p>

    <p class="text-gray-300"><span class="math">$||G_{r1}\\xi_1 + G_{r2}\\xi_2|| \\le q \\quad \\Longrightarrow \\quad ||[\\xi_1 \\ \\xi_2]|| \\le q,</span>$</p>

    <p class="text-gray-300">with r uniformly chosen from  <span class="math">1, \\ldots, m</span> , and  <span class="math">p \\leq (q+1)(1-d/m)</span> . The proof is essentially that of [EA23, &sect;3.2.2], but we reproduce it here for completeness. Additionally, note that this is equivalent to saying: if there are at least (q+1)(m-d) indices r satisfying  <span class="math">||G_{r1}\\xi_1+G_{r2}\\xi_2|| \\leq q</span> , then it must be the case that  <span class="math">||[\\xi_1, \\xi_2]|| \\leq q</span> . This is the form we use in the proof above.</p>

    <p class="text-gray-300"><strong>Proof.</strong> This is easy to see. Let  <span class="math">[\\xi_1 \\ \\xi_2]</span>  have more than q nonzero rows and pick any q+1 of them to form some reduced matrix  <span class="math">[\\hat{\\xi}_1 \\ \\hat{\\xi}_2]</span>  with q+1 rows, all nonzero. It is clear that</p>

    <p class="text-gray-300"><span class="math">$||G_{r1}\\xi_1 + G_{r2}\\xi_2|| \\ge ||G_{r1}\\hat{\\xi}_1 + G_{r2}\\hat{\\xi}_2||,</span>$</p>

    <p class="text-gray-300">so we will show that</p>

    <p class="text-gray-300"><span class="math">$||G_{r1}\\hat{\\xi}_1 + G_{r2}\\hat{\\xi}_2|| \\le q</span>$</p>

    <p class="text-gray-300">with probability no more than p. A simple observation is that, if  <span class="math">x_1^T</span>  and  <span class="math">x_2^T</span>  are two row vectors, then</p>

    <p class="text-gray-300"><span class="math">$G_{r1}x_1^T + G_{r2}x_2^T = 0 \\implies [x_1^T \\ x_2^T] = 0,</span>$</p>

    <p class="text-gray-300">where  <span class="math">p&#x27; \\leq 1 - d/m</span> , which follows from the definition of the distance of G. But it can only be the case that  <span class="math">||G_{r1}\\hat{\\xi}_1 + G_{r2}\\hat{\\xi}_2|| \\leq q</span>  if at least one of the linear combinations of the rows of  <span class="math">\\hat{\\xi}_1</span>  and  <span class="math">\\hat{\\xi}_2</span>  is equal to zero, so, by the union bound,  <span class="math">p \\leq (q+1)p&#x27; \\leq (q+1)(1-d/m)</span> , as required.</p>

    <p class="text-gray-300"><strong>Special case.</strong> A special, but very useful, case is when the matrix  <span class="math">G \\in \\mathbf{F}^{m \\times 2}</span>  has rows of the form (1-t,t) for each  <span class="math">t \\in \\mathbf{F}</span> . In this case it is easy to prove that the distance is  <span class="math">|\\mathbf{F}| - 1</span>  and  <span class="math">m = |\\mathbf{F}|</span> . The linear combination</p>

    <p class="text-gray-300"><span class="math">$G_{r1}\\xi_1 + G_{r2}\\xi_2,</span>$</p>

    <p class="text-gray-300">where r is uniformly randomly sampled from  <span class="math">1, \\ldots, m</span>  is the same as the random linear combination</p>

    <p class="text-gray-300"><span class="math">$(1-t)\\xi_1+t\\xi_2,</span>$</p>

    <p class="text-gray-300">with  <span class="math">t \\in \\mathbf{F}</span>  uniformly drawn, and gives the direct implication</p>

    <p class="text-gray-300"><span class="math">$\\|(1-t)\\xi_1 + t\\xi_2\\| \\le q \\quad \\Longrightarrow_p \\quad \\|[\\xi_1 \\ \\xi_2]\\| \\le q,</span>$</p>

    <p class="text-gray-300">with
<span class="math">$p \\le (q+1)(1-d/m) = (q+1)/|\\mathbf{F}|</span>$
.</p>

    </section>
`;
---

<BaseLayout title="A Note on Ligero and Logarithmic Randomness (2024/1399)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2024 &middot; eprint 2024/1399
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <PaperDisclaimer eprintUrl={EPRINT_URL} />
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <nav id="toc" class="mb-10 p-6 rounded-lg" style="background: rgba(255,255,255,0.03); border: 1px solid rgba(255,255,255,0.06);">
      <h2 class="text-lg font-bold mb-4">Table of Contents</h2>
      <ol class="space-y-1 text-sm text-gray-300
        list-decimal list-inside">
        <li><a href="#abstract" class="hover:text-white">Abstract</a></li>
        <li>
          <a href="#sec-1" class="hover:text-white">Notation and conventions</a>
          <ol class="ml-6 mt-1 space-y-1 list-decimal
            list-inside text-gray-400">
            <li><a href="#sec-1.1" class="hover:text-white">Linear algebra</a></li>
            <li><a href="#sec-1.2" class="hover:text-white">Error correcting codes</a></li>
            <li><a href="#sec-1.3" class="hover:text-white">Sparsity checks</a></li>
            <li><a href="#sec-1.4" class="hover:text-white">Interaction model</a></li>
          </ol>
        </li>
        <li><a href="#sec-2" class="hover:text-white">Proximity testing</a></li>
        <li>
          <a href="#sec-3" class="hover:text-white">The protocol</a>
          <ol class="ml-6 mt-1 space-y-1 list-decimal
            list-inside text-gray-400">
            <li><a href="#sec-3.1" class="hover:text-white">High level description</a></li>
            <li><a href="#sec-3.2" class="hover:text-white">Checks and the final protocol</a></li>
            <li><a href="#sec-3.3" class="hover:text-white">Proof size and total work</a></li>
          </ol>
        </li>
        <li>
          <a href="#sec-4" class="hover:text-white">Proof of test</a>
          <ol class="ml-6 mt-1 space-y-1 list-decimal
            list-inside text-gray-400">
            <li><a href="#sec-4.1" class="hover:text-white">Proof of the first statement</a></li>
          </ol>
        </li>
      </ol>
      <p class="text-xs text-gray-500 mt-4 mb-1 font-semibold">
        Appendices
      </p>
      <ol class="space-y-1 text-sm text-gray-400
        list-[upper-alpha] list-inside">
        <li><a href="#app-a" class="hover:text-white">Matrix sparsity check</a></li>
      </ol>
      <p class="text-xs text-gray-500 mt-4 mb-1 font-semibold">
        Additional
      </p>
      <ul class="space-y-1 text-sm text-gray-400
        list-disc list-inside">
        <li><a href="#references" class="hover:text-white">References</a></li>
      </ul>
    </nav>


    <Fragment set:html={CONTENT} />

    <PaperHistory slug="a-note-on-ligero-and-logarithmic-randomness-2024" />
  </article>
</BaseLayout>
