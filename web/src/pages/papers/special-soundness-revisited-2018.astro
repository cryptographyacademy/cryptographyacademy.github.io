---
import BaseLayout from '../../layouts/BaseLayout.astro';

const EPRINT_URL = 'https://eprint.iacr.org/2018/1157';
const CRAWLER = 'mistral';
const CONVERTED_DATE = '2026-02-16';
const TITLE_HTML = 'Special Soundness Revisited';
const AUTHORS_HTML = 'Douglas Wikström';

const CONTENT = `    <p class="text-gray-300">Douglas Wikström</p>

    <p class="text-gray-300">KTH Royal Institute of Technology dog@kth.se</p>

    <p class="text-gray-300">November 27, 2018</p>

    <p class="text-gray-300">Abstract. We generalize and abstract the problem of extracting a witness from a prover of a special sound protocol into a combinatorial problem induced by a sequence of matroids and a predicate, and present a parametrized algorithm for solving this problem.</p>

    <p class="text-gray-300">The parametrization provides a tight tradeoff between the running time and the extraction error of the algorithm, which allows optimizing the parameters to minimize: the soundness error for interactive proofs, or the extraction time for proofs of knowledge.</p>

    <p class="text-gray-300">In contrast to previous work we bound the distribution of the running time and not only the expected running time. Tail bounds give a tighter analysis when applied recursively and concentrated running time.</p>

    <p class="text-gray-300">A three-message public-coin protocol [6,1] is defined to be <em>special sound</em> by Cramer et al. [4] if a witness can be computed efficiently from two accepting transcripts with a common first prover message, but distinct verifier messages. This notion generalizes a property of Schnorr's proof of knowledge of a discrete logarithm [8].</p>

    <p class="text-gray-300">In the natural generalization of Cramer et al.'s notion we require that the accepting transcripts form a tree, i.e., the executions are identical to start with and then successively branch to form a tree, where the messages at each branching point are "independent". The notion of independence is protocol dependent, but it is readily captured using matroids and usually corresponds to inequality [8] or linear independence [2].</p>

    <p class="text-gray-300">Recall that in proofs of knowledge [3] we consider the prover as a <em>deterministic</em> next-message function to allow rewinding. Moreover, in public coin protocols the verifier's messages do not depend on the prover's messages, so we can consider the prover and verifier jointly as a predicate on a sequence of verifier messages.</p>

    <p class="text-gray-300">We construct and analyze an algorithm that extracts a tree such that every path satisfies the predicate and the children of each node is a basis relative to a given matroid. This reduces the problem of constructing a knowledge extractor for a special-sound protocol to the inherently protocol-dependent construction of a procedure that computes a witness from such a tree of accepting transcripts.</p>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>IACR is granted a non-exclusive and irrevocable license to distribute this work under the Creative Commons Attribution-NonCommercial 3.0 Unported license.</li>

    </ul>

    <p class="text-gray-300">2 Contributions</p>

    <p class="text-gray-300">Our work is motivated by, and addresses, a real-world need to give an exact proof of security for a complete electronic voting system. We are not aware of prior work that is sufficiently rigorous, general, flexible, and exact to be used as the toolbox we need, but given how well researched the area is the main contribution is perhaps the complete and coherent nature of our treatment with applications in both theory and practice.</p>

    <h2 id="sec-3" class="text-2xl font-bold">3 The Extraction Problem</h2>

    <p class="text-gray-300">Given are matroids <span class="math">\\mathbb{M}_{0},\\ldots,\\mathbb{M}_{r}</span>, with ground sets <span class="math">S_{0},\\ldots,S_{r}</span> respectively. We consider an unordered complete tree such that the children of a node at depth <span class="math">i-1</span> are identified (or more precisely labeled) with the elements of <span class="math">S_{i}</span>. To ensure that this is a tree and not a forest we require that <span class="math">S_{0}</span> is a singleton set. The element it contains is mostly used as a placeholder for the root, but it is essential that it remains a variable for applications in general settings.</p>

    <p class="text-gray-300">The predicate that captures both the computations performed by the prover during execution and the computations performed by the verifier to reach a verdict then has the form <span class="math">\\rho:\\prod_{i\\in[0,r]}S_{i}\\to\\{0,1\\}</span>. An explicit description parametrized by a prover is given in Definition 13.</p>

    <p class="text-gray-300">The goal is to find a subtree such that: for every inner node at depth <span class="math">i-1</span> its children is a basis of <span class="math">\\mathbb{M}_{i}</span>, and the predicate <span class="math">\\rho</span> evaluates to <span class="math">1</span> on every path in the subtree from the root to a leaf.</p>

    <h3 id="sec-4" class="text-xl font-semibold mt-8">3.1 What Can We Expect?</h3>

    <p class="text-gray-300">The required tree structure implies that any extractor must find at least <span class="math">d=\\prod_{i\\in[r]}d_{i}</span> accepting executions, where <span class="math">d_{i}</span> is the rank of <span class="math">\\mathbb{M}_{i}</span>. If we treat <span class="math">\\rho</span> as an oracle, then a first guess might be that the expected number of queries of an optimal extractor is <span class="math">O\\left(d/\\Delta\\right)</span>, where <span class="math">\\Delta=\\Pr\\left[\\rho(v)=1\\right]</span> for a randomly chosen <span class="math">v\\in\\prod_{i\\in[0,r]}S_{i}</span>.</p>

    <p class="text-gray-300">However, we also need to take into account the restrictions imposed by the matroids on the nodes at each level. Consider a node <span class="math">u</span> at depth <span class="math">i-1</span>. In general we cannot expect to simply pick a basis of <span class="math">\\mathbb{M}_{i}</span> to be children of <span class="math">u</span> and extend them to paths accepted by <span class="math">\\rho</span>, since the conditional probabilities of success for the children are not necessarily sufficiently large. One natural idea is to sequentially identify children and build both a basis <span class="math">B</span> for <span class="math">\\mathbb{M}_{i}</span> and recursively build subtrees for which the children are the roots.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Suppose that we are in the process of doing this and have an independence set <span class="math">B</span> of <span class="math">j&lt;d_{i}</span> children for which we have extracted subtrees. Then the next child of <span class="math">u</span> must be chosen in <span class="math">S_{i}\\setminus\\mathsf{span}(B)</span>, so without a deeper understanding of the distribution of accepting paths we must accept an additive loss of $\\omega_{\\mathbb{M}_{i},j}=</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\mathsf{span}(B)</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">/</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">S_{i}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$ in the success probability. Collecting statistics about the</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">distribution of accepting paths precise enough to avoid this has similar complexity as solving the extraction problem itself and is therefore too costly. Thus, a somewhat more realistic goal for an efficient extractor is an expected running time of <span class="math">O\\left(d/(\\Delta-\\epsilon)\\right)</span>, where <span class="math">\\epsilon=\\sum_{i\\in[r]}\\omega_{\\mathbb{M}_{i}}</span> and <span class="math">\\omega_{\\mathbb{M}_{i}}=\\max_{j\\in[d_{i}]}\\omega_{\\mathbb{M}_{i},j}</span>.</p>

    <p class="text-gray-300">Minimizing <span class="math">\\epsilon</span> is important for protocols where we do not care about the running time of the extractor and only use it as a probabilistic proof to argue that a witness exists. This establishes <span class="math">\\epsilon</span> as the soundness error of the protocol viewed as an interactive <em>proof</em>. However, if we view the protocol as a proof of <em>knowledge</em>, then the running time of the extractor plays an important role, since it influences the running time of a security reduction of an invoking protocol, which in turn determines the running time of an algorithm that breaks a complexity assumption. Minimizing <span class="math">\\epsilon</span> aggressively increases the constant factor of the running time drastically.</p>

    <h3 id="sec-5" class="text-xl font-semibold mt-8">3.2 On the Distribution of the Running Time</h3>

    <p class="text-gray-300">In the discussion above we have only considered the <em>expected</em> running time <span class="math">\\mu</span> of a potential extractor <span class="math">\\mathcal{X}</span>. When this is not enough, the standard approach is to apply Markov’s inequality and conclude that <span class="math">\\mathcal{X}</span> completes within time <span class="math">2\\mu</span> with probability at least <span class="math">1/2</span>, so the number of attempts we need to extract a witness has geometric distribution with probability <span class="math">1/2</span>.</p>

    <p class="text-gray-300">However, the discussion above suggests that the running time of an extractor may be quite concentrated from scratch due to the large number of relatively independent samples needed to extract the tree. To see this, consider a tree where the accepting paths are uniformly distributed. Then we would expect the number of samples needed by the extractor to have (almost) negative binomial distribution with parameter <span class="math">d=\\prod_{i\\in[r]}d_{i}</span> and probability <span class="math">\\Delta-\\epsilon</span>.</p>

    <p class="text-gray-300">Unfortunately, the tree is constructed by the adversary, so for many nodes the conditional probability <span class="math">\\Delta^{\\prime}</span> of finding an extension to an accepting path may be very low. Any unsupervised attempt to simply call a recursive routine that extracts a subtree at such a point will give a running time that is at best geometrically distributed with probability <span class="math">\\Delta^{\\prime}</span>. Given even a moderately large <span class="math">d</span>, the probability of encountering such a node is high, and the total running time would then be a sum that is dominated by the running times of the extractions from such nodes (see Jansson <em>[7]</em> for how bounds on sums of geometric distributions with different probabilities behave).</p>

    <p class="text-gray-300">In other words, every strategy must have a mechanism to interrupt <em>all</em> subroutine calls that take too long to complete. Thus, we may hope that the number of queries made is essentially a constant times a random variable with negative binomial distribution with parameter <span class="math">d_{1}</span> and probability <span class="math">\\Delta-\\epsilon</span>, where the scaling factor depends on the cost of a recursive call, and this is the type of result we achieve.</p>

    <p class="text-gray-300">3.3 Strategy</p>

    <p class="text-gray-300">The basic case at depth <span class="math">r-1</span> for a depth <span class="math">r</span> tree consists of simply sampling accepting leafs. It is easy to see that this requires a number of queries that has negative binomial distribution.</p>

    <p class="text-gray-300">The general strategy is relatively natural and recursive, so we describe it as if we start at the root. We sample paths until we find an accepting path <span class="math">v</span>. Consider the child <span class="math">u</span> of the root in such a path. If the conditional probability of finding an accepting path through <span class="math">u</span> is <span class="math">\\delta_{u}</span>, then Markov’s inequality implies that it is at least <span class="math">\\alpha\\Delta</span> with probability <span class="math">1-\\alpha</span> for <span class="math">\\alpha\\in(0,1)</span>.</p>

    <p class="text-gray-300">We can invoke the algorithm recursively using <span class="math">u</span> as a root and interrupt the execution if it takes too long, but if the recursive call is costly it is worthwhile to first validate that <span class="math">u</span> is reasonably good.</p>

    <p class="text-gray-300">We can do this by sampling random paths through <span class="math">u</span> and sample a new node if we do not encounter sufficiently many accepting paths within a given number of attempts. We then balance the cost of sampling against the cost of failed attempts to execute the strategy recursively. For nodes close to the root sampling is relatively cheap compared to the cost of an interrupted execution.</p>

    <p class="text-gray-300">This strategy gives a number of parameters for each recursive call. A parameter <span class="math">\\alpha</span> determines how close to <span class="math">\\Delta</span> we want <span class="math">\\delta_{u}</span> to be. A parameter <span class="math">\\beta</span> captures the additional loss we have if we validate the candidate, since we cannot do this perfectly. The probability that validation gives the right result is determined by a parameter <span class="math">\\gamma</span>, and finally a parameter <span class="math">\\lambda</span> determines the probability that a recursive call completes.</p>

    <p class="text-gray-300">We derive expressions for the extraction error and the expected value, and give a tail bound for the number of queries made by the extractor in terms of these parameters. This allows choosing good parameters for an exact security analysis of any special-sound protocol.</p>

    <h2 id="sec-6" class="text-2xl font-bold">4 Matroids and Trees</h2>

    <p class="text-gray-300">We denote a matroid with ground set <span class="math">S</span> and independence sets <span class="math">I</span> by <span class="math">\\mathbb{M}=(S,I)</span>. For completeness we provide one standard way of formalizing matroids in Section A. We denote the matroid with a singleton ground set <span class="math">\\{u\\}</span> and independence set <span class="math">\\{\\varnothing,\\{u\\}\\}</span> by <span class="math">\\{u\\}</span>. The two most common examples of matroids in the literature are essentially vector spaces over finite fields and matroids that capture inequality, but the ground sets may be restricted for practical reasons.</p>

    <h6 id="sec-7" class="text-base font-medium mt-4">Example 1 (Vector Space as Matroid)</h6>

    <p class="text-gray-300">A vector space <span class="math">\\mathbb{Z}_{q}^{N}</span> over a finite field <span class="math">\\mathbb{Z}_{q}</span>, where <span class="math">q</span> is prime, can be viewed as a matroid <span class="math">\\big{(}\\mathbb{Z}_{q}^{N},I\\big{)}</span>, where <span class="math">I</span> is the set of all sets of linearly independent vectors.</p>

    <h6 id="sec-8" class="text-base font-medium mt-4">Example 2 (Inequality Matroid)</h6>

    <p class="text-gray-300">The inequality matroid <span class="math">(S,I)</span> over a ground set <span class="math">S</span> has as independent set <span class="math">I</span> the set of all subsets of <span class="math">S</span> of size at most two.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">In the above examples every submatroid of the same rank has the same cardinality, which means that the fraction $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">/</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">S</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> is the same for every flat </span>A$ of</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">a given rank. In our applications we need this fraction to remain exponentially small, but we relax the requirement to make room for oddities introduced in cryptographic protocols.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><strong>Definition 1 (Subdensity).</strong> Let <span class="math">\\mathbb{M} = (S, I)</span> be a matroid of rank <span class="math">d</span>. Then its <span class="math">i</span>th subdensity is <span class="math">\\omega_{\\mathbb{M},i}</span> if $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">/</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">S</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\leq \\omega_{\\mathbb{M},i}<span class="math"> for every flat </span>A<span class="math"> of rank </span>i - 1<span class="math">, and it has maximal subdensity </span>\\omega_{\\mathbb{M}} = \\omega_{\\mathbb{M},d}$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Note that we have <span class="math">\\omega_{\\mathbb{M},1} = 0</span> for every non-trivial matroid <span class="math">\\mathbb{M}</span>, since <span class="math">\\operatorname{span}(\\varnothing) = \\varnothing</span>. In Example 1 the <span class="math">i</span>th subdensity is <span class="math">q^{i - N - 1}</span> and in Example 2 the 2nd subdensity is $1 /</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">S</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">$. We introduce some additional notation that allows us to consider a list of matroids as a tree.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><strong>Definition 2 (Matroid Tree).</strong> The matroid tree associated with a list of matroids <span class="math">\\mathbb{M} = (\\{v_0\\}, \\mathbb{M}_1, \\ldots, \\mathbb{M}_r)</span>, is the vertex-labeled rooted unordered directed tree of depth <span class="math">r</span> such that: the root is labeled <span class="math">v_0</span> and every node at depth <span class="math">i - 1</span> has edges to $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">S_i</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math"> children which are uniquely labeled with the elements of </span>S_i$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Although a matroid tree is unordered, the children of each node are labeled uniquely, so we may abuse notation and identify a node with its label. We also use <span class="math">\\mathbb{M}</span> to denote both the matroid tree and the list of matroids with which it is associated.</p>

    <p class="text-gray-300"><strong>Definition 3 (Basis).</strong> A basis of a matroid tree <span class="math">\\mathbb{M}</span> of depth <span class="math">r</span> is a maximal subgraph such that for every <span class="math">i \\in [r]</span> the set of children of every node at depth <span class="math">i - 1</span> is a basis of <span class="math">\\mathbb{M}_i</span>.</p>

    <p class="text-gray-300">As explained above, we abstract from the details of protocols by capturing the computations performed by the prover and verifier in a protocol to reach a verdict as a predicate. Definition 13 gives a concrete predicate for any given prover.</p>

    <p class="text-gray-300"><strong>Definition 4 (Predicate).</strong> An <span class="math">\\mathbb{M}</span>-predicate, where <span class="math">\\mathbb{M}</span> is a matroid tree, is a function of the form <span class="math">\\rho : \\prod_{i \\in [0,r]} S_i \\to \\{0,1\\}</span>.</p>

    <p class="text-gray-300"><strong>Definition 5 (Accepting Basis).</strong> A basis <span class="math">B</span> of a matroid tree <span class="math">\\mathbb{M}</span> is <span class="math">\\rho</span>-accepting for an <span class="math">\\mathbb{M}</span>-predicate <span class="math">\\rho</span> if <span class="math">\\rho(v) = 1</span> for each path <span class="math">v</span> of maximal length in <span class="math">B</span>.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">For a matroid tree <span class="math">\\mathbb{M}</span> we let <span class="math">S = \\prod_{i\\in [0,r]}S_i</span>, where <span class="math">\\mathbb{M}_i = (S_i,I_i)</span> and <span class="math">S_0 = \\{v_0\\}</span> for some <span class="math">v_{0}</span>. We define $\\varDelta_{\\rho}(\\mathbb{M})=</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\{\\rho(v)=1\\mid v\\in S\\}</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">/</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">S</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math">, and when </span>\\mathbb{M}<span class="math"> is clear from the context we drop </span>\\mathbb{M}<span class="math"> and write </span>\\varDelta_{\\rho}$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Let <span class="math">\\mathsf{D}</span> and <span class="math">\\mathsf{D}&#x27;</span> be distributions over <span class="math">\\mathbb{N}</span>. We say that <span class="math">\\mathsf{D}</span> is bounded by <span class="math">\\mathsf{D}&#x27;</span> if <span class="math">\\mathsf{D}</span> stochastically dominates <span class="math">\\mathsf{D}&#x27;</span>, i.e., if for random variables <span class="math">X</span> and <span class="math">X&#x27;</span> distributed according to <span class="math">\\mathsf{D}</span> and <span class="math">\\mathsf{D}&#x27;</span>, respectively, and every <span class="math">x \\in \\mathbb{N}</span>: <span class="math">\\operatorname<em>{Pr}[X \\leq x] \\geq \\operatorname</em>{Pr}[X&#x27; \\leq x]</span>.</p>

    <h6 id="sec-11" class="text-base font-medium mt-4">Definition 6 (Accepting Basis Extractor)</h6>

    <p class="text-gray-300">A probabilistic polynomial time oracle algorithm <span class="math">\\mathcal{X}_{\\kappa}</span> parametrized by <span class="math">\\kappa\\in\\{0,1\\}^{*}</span> is a <span class="math">(\\epsilon_{\\kappa},\\mathsf{D}_{\\kappa}(\\Delta))</span>-accepting basis extractor with extraction error <span class="math">\\epsilon_{\\kappa}</span> for a matroid tree <span class="math">\\mathbb{M}</span>, where <span class="math">\\mathsf{D}_{\\kappa}(\\Delta)</span> for fixed <span class="math">\\kappa</span> is a family of distributions on <span class="math">\\mathbb{N}</span> parametrized by <span class="math">\\Delta\\in[0,1]</span>, if for every <span class="math">\\mathbb{M}</span>-predicate <span class="math">\\rho:S\\to\\{0,1\\}</span> and <span class="math">\\Delta_{\\rho}(\\mathbb{M})\\geq\\Delta_{0}&gt;\\epsilon_{\\kappa}</span>: <span class="math">\\mathcal{X}_{\\kappa}^{\\rho(\\cdot)}(\\mathbb{M},\\Delta_{0})</span> outputs a <span class="math">\\rho</span>-accepting basis of <span class="math">\\mathbb{M}</span>, where the distribution of the number of <span class="math">\\rho(\\cdot)</span>-queries is bounded by <span class="math">\\mathsf{D}_{\\kappa}(\\Delta_{0})</span>.</p>

    <p class="text-gray-300">This definition is more precise than the definition of a proof of knowledge in that it bounds the distribution, and not only the expected value, of the number of queries made by the extractor. It also allows modifying the extractor and the corresponding extraction error <span class="math">\\epsilon_{\\kappa}</span> and distribution <span class="math">\\mathsf{D}_{\\kappa}(\\Delta_{0})</span> using the parameter <span class="math">\\kappa</span>, but we require a lower bound <span class="math">\\Delta_{0}</span> on the accept probability <span class="math">\\Delta_{\\rho}</span> as an explicit input to the extractor to guarantee the expected behavior.</p>

    <p class="text-gray-300">We stress that the extraction error <span class="math">\\epsilon_{\\kappa}</span> is a property related to a particular algorithm and parameter <span class="math">\\kappa</span> and is neither necessarily the soundness error nor the knowledge error of the protocol from which the matroid tree is derived. It merely provides an upper bound on both when the running time is not too large and a witness can be efficiently computed from an extracted accepting basis.</p>

    <h2 id="sec-12" class="text-2xl font-bold">7 Constructing Accepting Basis Extractors</h2>

    <p class="text-gray-300">We split the description of extractors into subroutines and analyze them separately to emphasize the structure of the main algorithm and the interplay between the parameters that we consider below. When convenient we use generating functions to describe distributions, e.g., a distribution <span class="math">\\mathsf{D}</span> over <span class="math">\\mathbb{N}</span> has probability generating function <span class="math">\\mathcal{G}_{\\mathsf{D}}(z)</span>.</p>

    <h3 id="sec-13" class="text-xl font-semibold mt-8">7.1 Notation for Bounds</h3>

    <p class="text-gray-300">Consider a random variable <span class="math">X</span> taking values in <span class="math">\\mathbb{N}</span> that has distribution <span class="math">\\mathsf{D}(s,\\Delta)</span> parametrized by <span class="math">s\\in\\mathbb{N}^{+}</span> and <span class="math">\\Delta\\in[0,1]</span>. Recall that the negative binomial distribution is parametrized in this way and that its tail bound does not depend on <span class="math">\\Delta</span>. This property is shared by the distributions we encounter, so we denote by <span class="math">t_{s}^{0}(k)</span> a tail bound that satisfies <span class="math">\\Pr\\left[X\\geq k\\mu_{\\mathsf{D}(s,\\Delta)}\\right]\\leq t_{s}^{0}(k)</span>, where <span class="math">\\mu_{\\mathsf{D}(s,\\Delta)}</span> is the expected value of <span class="math">\\mathsf{D}(s,\\Delta)</span>. We similarly think of <span class="math">h_{s}^{0}(k)=1-t_{s}^{0}(k)</span> as a head bound.</p>

    <p class="text-gray-300">We need to express optimal parameters to head bounds as functions. We denote the smallest possible <span class="math">k</span> that satisfies a certain lower bound <span class="math">\\lambda</span> by</p>

    <p class="text-gray-300"><span class="math">k_{s}^{0}(\\lambda)=\\min\\{k\\in(1,\\infty)\\mid h_{s}^{0}(k)\\geq\\lambda\\}\\enspace.</span></p>

    <p class="text-gray-300">When <span class="math">s</span> is fixed we drop it from our notation and consider the distribution to carry this information, e.g., <span class="math">\\mathsf{D}_{0}(\\Delta)</span> could be defined as <span class="math">\\mathsf{D}(s,\\Delta)</span> in which case <span class="math">t^{0_{0}}(k)=t_{s}^{0}(k)</span> and similarly for other quantities.</p>

    <p class="text-gray-300">However, if instead the value of <span class="math">k</span> is fixed, and <span class="math">s</span> appears as a parameter, then we can increase <span class="math">h_{s}^{\\mathrm{D}}(k)</span> by increasing <span class="math">s</span> which gives</p>

    <p class="text-gray-300"><span class="math">s_{\\beta}^{\\mathrm{D}}(\\gamma)=\\min\\{s\\in\\mathbb{N}^{+}\\mid h_{s}^{\\mathrm{D}}(1/\\beta)\\geq\\gamma\\}\\enspace,</span></p>

    <p class="text-gray-300">where we replace <span class="math">k</span> by <span class="math">\\beta=1/k</span> for notational convenience. Note that changing <span class="math">s</span> changes the distribution. We have the following two concrete tail bounds</p>

    <p class="text-gray-300"><span class="math">t_{s}^{\\mathtt{NB}}(k)</span> <span class="math">=e^{-(1-1/k)^{2}ks/2}</span> <span class="math">t_{s}^{\\mathtt{CG}}(k)</span> <span class="math">=e^{-(k-1-\\ln k)s}</span></p>

    <p class="text-gray-300">for the negative binomial distribution <span class="math">\\mathtt{NB}(s,\\Delta)</span> for some probability <span class="math">\\Delta</span>, and a product of <span class="math">s</span> compound geometric distributions (see Definition 29), respectively. The (shifted) geometric distribution is denoted <span class="math">\\mathsf{Geo}(\\Delta)</span>. At one point we also need to bound below the expected value, i.e., we need a bound of the form <span class="math">\\Pr\\left[X\\leq\\mu_{\\mathtt{NB}(s,\\Delta)}/k\\right]\\leq n_{s}^{\\mathtt{NB}}(k)</span>, where <span class="math">n_{s}^{\\mathtt{NB}}(k)=e^{-(k-1)^{2}\\frac{s}{2k}}</span>. Due to asymmetry this bound is slightly weaker. (See Theorem 8 and Theorem 7.)</p>

    <h3 id="sec-14" class="text-xl font-semibold mt-8">7.2 Basic Algorithms</h3>

    <h6 id="sec-15" class="text-base font-medium mt-4">Definition 7 (Basic Extractor).</h6>

    <p class="text-gray-300">The <em>basic extractor</em> oracle algorithm <span class="math">\\mathcal{B}</span> takes input <span class="math">(\\mathbb{M},\\Delta_{0})</span>, where <span class="math">\\mathbb{M}=(\\mathbb{M}_{0},\\mathbb{M}_{1})</span>, and proceeds as follows:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Set <span class="math">B=\\varnothing</span>.</li>

    </ol>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">2. Repeat while $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">B</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><d_{1}$:</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">(a) Sample <span class="math">v\\in S_{0}\\times(S_{1}\\setminus\\mathsf{span}(B))</span> randomly. (b) If <span class="math">\\rho(v)=1</span>, then set <span class="math">B=B\\cup\\{v_{1}\\}</span>.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Return <span class="math">B</span>.</li>

    </ol>

    <h6 id="sec-16" class="text-base font-medium mt-4">Lemma 1 (Basic Extractor).</h6>

    <p class="text-gray-300">The algorithm <span class="math">\\mathcal{B}</span> is a <span class="math">(\\omega_{\\mathbb{M}_{1}},\\mathtt{NB}(d_{1},\\Delta_{1}^{\\prime}))</span>-accepting basis extractor for <span class="math">\\mathbb{M}</span>, where <span class="math">\\Delta_{1}^{\\prime}=\\Delta_{0}-\\omega_{\\mathbb{M}_{1}}</span>.</p>

    <h6 id="sec-17" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">The probability that a randomly sampled <span class="math">v</span> satisfies <span class="math">\\rho(v)=1</span> is at least <span class="math">\\Delta_{0}-\\omega_{\\mathbb{M}_{1}}&gt;0</span> by assumption from which the claimed distribution of the number of queries made follows immediately.</p>

    <h6 id="sec-18" class="text-base font-medium mt-4">Remark 1.</h6>

    <p class="text-gray-300">The algorithm ignores the input <span class="math">\\Delta_{0}</span> and the result could be sharpened to say that the distribution of the number of queries to the oracle is bounded by <span class="math">\\mathtt{NB}(d_{1},\\Delta_{\\rho}-\\omega_{\\mathbb{M}_{1}})</span>. We choose the above exposition to keep a uniform interface and structure with the algorithms that follow below.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">In the analysis of the basic extractor and the algorithms below we need to consider the conditional probability that a node can be extended to an accepting path. Thus, we define $\\delta_{u}=\\Pr\\left[\\rho(V)=1\\,</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">V_{1}=u\\right]<span class="math">, where </span>V<span class="math"> is uniformly distributed in </span>S$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h6 id="sec-19" class="text-base font-medium mt-4">Definition 8 (Basic Sampler).</h6>

    <p class="text-gray-300">The <em>basic sampler</em> oracle algorithm <span class="math">\\mathcal{BS}</span> takes input <span class="math">(\\mathbb{M},B,\\Delta_{0})</span>, where <span class="math">B\\in I_{1}</span> is not a basis and <span class="math">\\Delta_{0}\\in(0,1]</span>, and repeats:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Sample <span class="math">v \\in S_0 \\times (S_1 \\setminus \\operatorname{span}(B)) \\times \\prod_{i \\in [2,r]} S_i</span> randomly.</li>

      <li>If <span class="math">\\rho(v) = 1</span>, then return <span class="math">v_1</span>.</li>

    </ol>

    <p class="text-gray-300"><strong>Lemma 2 (Basic Sampler).</strong> If <span class="math">\\Delta_{\\rho} \\geq \\Delta_0 &amp;gt; \\omega_{\\mathbb{M}_1}</span>, then the distribution of the number of queries to <span class="math">\\rho(\\cdot)</span> made by <span class="math">\\mathcal{BS}^{\\rho(\\cdot)}(\\mathbb{M}, B, \\Delta_0)</span> is bounded by <span class="math">\\mathcal{G}_{\\mathrm{BS}(\\mathbb{M}, \\Delta_0)}(z) = \\mathcal{G}_{\\mathrm{Geo}(\\Delta_1&#x27;)}(z)</span>, where <span class="math">\\Delta_1&#x27; = \\Delta_0 - \\omega_{\\mathbb{M}_1}</span>. Furthermore, if <span class="math">U</span> denotes its output, then <span class="math">\\operatorname*{Pr}\\left[\\delta_U \\geq \\alpha \\Delta_1&#x27;\\right] \\geq 1 - \\alpha</span>.</p>

    <p class="text-gray-300"><strong>Proof.</strong> A sample satisfies <span class="math">\\rho(v) = 1</span> with probability at least <span class="math">\\Delta_1&#x27;</span>, so the number of samples needed is bounded by <span class="math">\\mathrm{Geo}(\\Delta_1&#x27;)</span>. For the second claim we let <span class="math">V</span> be uniformly distributed in <span class="math">S</span> and let <span class="math">U</span> be the node denoted by <span class="math">v_1</span> in the algorithm. Set <span class="math">B_{\\perp} = S_1 \\setminus \\operatorname{span}(B)</span>. Then by definition we have</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{array}{l} \\operatorname{Pr} [U = u] = \\operatorname{Pr} \\left[ V_1 = u \\mid \\rho(V) = 1 \\wedge V_1 \\in B_{\\perp} \\right] \\quad \\text{and} \\\\ \\delta_u = \\operatorname{Pr} \\left[ \\rho(V) = 1 \\mid V_1 = u \\wedge V_1 \\in B_{\\perp} \\right] \\end{array}</span></div>

    <p class="text-gray-300">so</p>

    <div class="my-4 text-center"><span class="math-block">\\operatorname{Pr} \\left[ U = u \\right] / \\delta_u = \\frac{\\operatorname{Pr} \\left[ V_1 = u \\wedge V_1 \\in B_{\\perp} \\right]}{\\operatorname{Pr} \\left[ \\rho(V) = 1 \\wedge V_1 \\in B_{\\perp} \\right]}</span></div>

    <p class="text-gray-300">which implies</p>

    <div class="my-4 text-center"><span class="math-block">\\operatorname{E} \\left[ 1 / \\delta_U \\right] = \\sum_{u \\in B_{\\perp}} \\operatorname{Pr} \\left[ U = u \\right] / \\delta_u \\leq \\frac{1}{\\Delta_1&#x27;} \\sum_{u \\in B_{\\perp}} \\operatorname{Pr} \\left[ V_1 = u \\right] \\leq \\frac{1}{\\Delta_1&#x27;}.</span></div>

    <p class="text-gray-300">Markov's inequality then implies that <span class="math">\\operatorname<em>{Pr}\\left[\\delta_U &amp;lt; \\alpha \\Delta_1&#x27;\\right] = \\operatorname</em>{Pr}\\left[1 / \\delta_U &amp;gt; 1 / (\\alpha \\Delta_1&#x27;)\\right] \\leq \\alpha</span> so <span class="math">\\operatorname*{Pr}\\left[\\delta_U \\geq \\alpha \\Delta_1&#x27;\\right] \\geq 1 - \\alpha</span>.</p>

    <p class="text-gray-300"><strong>Definition 9 (Sample Validator).</strong> The sample validator oracle algorithm <span class="math">\\mathcal{V}_{s,k}</span> proceeds as follows on input <span class="math">(\\mathbb{M}, \\Delta_0)</span>, where <span class="math">s \\in \\mathbb{N}^+</span>, <span class="math">k \\in (1, \\infty)</span>, and <span class="math">\\Delta_0 \\in (0, 1]</span>:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Set <span class="math">h = 0</span> and <span class="math">c = 0</span>.</li>

      <li>While <span class="math">h &amp;lt; s</span> and <span class="math">c &amp;lt; sk / \\Delta_0</span>:</li>

    </ol>

    <p class="text-gray-300">(a) Sample <span class="math">v \\in \\prod_{i \\in [0, r]} S_i</span> randomly. (b) Set <span class="math">h = h + \\rho(v)</span> and <span class="math">c = c + 1</span>.</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>If <span class="math">h = s</span>, then return 1 and otherwise return 0.</li>

    </ol>

    <p class="text-gray-300"><strong>Lemma 3 (Sample Validator).</strong> The number of queries made by <span class="math">\\mathcal{V}_{s,k}^{\\rho(\\cdot)}(\\mathbb{M}, \\Delta_0)</span> is bounded by <span class="math">sk / \\Delta_0</span>. If <span class="math">\\Delta_{\\rho} \\geq \\Delta_0</span>, then <span class="math">\\operatorname<em>{Pr}\\left[\\mathcal{V}_{s,k}^{\\rho(\\cdot)}(\\mathbb{M}, \\Delta_0) = 1\\right] \\geq h_s^{\\mathrm{NB}}(k)</span>, and if <span class="math">\\Delta_{\\rho} &amp;lt; \\frac{\\Delta_0}{k}</span>, then <span class="math">\\operatorname</em>{Pr}\\left[\\mathcal{V}_{s,k}^{\\rho(\\cdot)}(\\mathbb{M}, \\Delta_0) = 1\\right] \\leq n_s^{\\mathrm{NB}}(k)</span>.</p>

    <p class="text-gray-300"><strong>Proof.</strong> Note that if the bound <span class="math">c &amp;lt; sk / \\Delta_0</span> is removed, then the number of oracle calls has distribution bounded by <span class="math">\\mathsf{NB}(s, \\Delta_{\\rho})</span>, since each sample <span class="math">v</span> satisfies <span class="math">\\rho(v) = 1</span> with probability at least <span class="math">\\Delta_{\\rho}</span> and we only exit the loop when <span class="math">h = s</span>. The claims now follow directly from the tail and head bounds of the negative binomial distribution (see Theorem 8).</p>

    <p class="text-gray-300">The validating sampling algorithm repeatedly samples paths until an accepting path is found. This is repeated until the first element of the found path is considered good by the validation algorithm.</p>

    <p class="text-gray-300"><strong>Definition 10 (Validating Sampler).</strong> The validating sampler oracle algorithm <span class="math">\\mathcal{VS}_{\\alpha,\\beta,\\gamma}</span> proceeds as follows on input <span class="math">(\\mathbb{M}, B, \\Delta_0)</span>, where <span class="math">\\alpha, \\beta, \\gamma \\in (0,1)</span>, <span class="math">\\Delta_0 \\in (0,1]</span>, and <span class="math">B \\in I_1</span> is not a basis:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Define <span class="math">\\Delta_1&#x27; = \\Delta_0 - \\omega_{\\mathbb{M}_1}</span>, <span class="math">k = 1 / \\beta</span> and <span class="math">s = s_{\\beta}^{\\mathrm{HB}}(\\gamma)</span>.</li>

      <li>Repeat:</li>

    </ol>

    <p class="text-gray-300">(a) Compute <span class="math">v_{1} = \\mathcal{BS}^{\\rho(\\cdot)}(\\mathbb{M}, B, \\Delta_{0})</span>. (b) If <span class="math">\\mathcal{V}_{s,k}^{\\rho(v_0,\\cdot)}\\big((\\{v_1\\}, \\mathbb{M}_2, \\ldots, \\mathbb{M}_r), \\alpha \\Delta_1&#x27;\\big) = 1</span>, then return <span class="math">v_{1}</span>.</p>

    <p class="text-gray-300"><strong>Lemma 4 (Validating Sampler).</strong> If <span class="math">\\Delta_{\\rho} \\geq \\Delta_0 &amp;gt; \\omega_{\\mathbb{M}_1}</span>, then the distribution of the number of queries of <span class="math">\\mathcal{VS}_{\\alpha, \\beta, \\gamma}^{\\rho(\\cdot)}(\\mathbb{M}, B, \\Delta_0)</span> is bounded by</p>

    <div class="my-4 text-center"><span class="math-block">\\mathcal {G} _ {\\mathrm {V S} (\\mathbb {M}, \\alpha , \\beta , \\gamma , \\Delta_ {0})} (z) = \\mathcal {G} _ {\\mathrm {G e o} ((1 - \\alpha) \\gamma)} \\left(\\mathcal {G} _ {\\mathrm {B S} (\\mathbb {M}, \\Delta_ {0})} (z) z ^ {s _ {\\beta} ^ {\\mathrm {H B}} (\\gamma) / \\Delta_ {1}}\\right),</span></div>

    <p class="text-gray-300">where <span class="math">\\Delta_1&#x27; = \\Delta_0 - \\omega_{\\mathbb{M}_1}</span> and <span class="math">\\Delta_1 = \\alpha \\beta \\Delta_1&#x27;</span>, and its output <span class="math">U</span> satisfies <span class="math">\\operatorname*{Pr}\\left[\\delta_U\\geq \\Delta_1\\right]\\geq \\phi (\\alpha ,\\beta ,\\gamma)</span>, where <span class="math">\\phi (\\alpha ,\\beta ,\\gamma) = 1 - \\alpha \\beta n_s^{\\mathrm{HB}}(1 / \\beta) / ((1 - \\alpha)\\gamma)</span>.</p>

    <p class="text-gray-300"><strong>Proof.</strong> We know from Lemma 2 that the distribution of the number of queries made by <span class="math">\\mathcal{BS}^{\\rho(\\cdot)}</span> in a given iteration is bounded by <span class="math">\\mathsf{BS}(\\mathbb{M}, \\Delta_0)</span>. Lemma 3 implies that the number of queries made by <span class="math">\\mathcal{V}_{s,k}^{\\rho(v_0, \\cdot)}</span> is upper bounded by <span class="math">ks / (\\alpha \\Delta_1&#x27;) = s_{\\beta}^{\\mathrm{HB}}(\\gamma) / \\Delta_1</span> so the distribution of the total number of queries in the <span class="math">i</span>th iteration is bounded by</p>

    <div class="my-4 text-center"><span class="math-block">f (z) = \\mathcal {G} _ {\\mathrm {B S} (\\mathbb {M}, \\Delta_ {0})} (z) z ^ {s _ {\\beta} ^ {\\mathrm {H B}} (\\gamma) / \\Delta_ {1}}.</span></div>

    <p class="text-gray-300">Lemma 2 implies that if <span class="math">U_{i}</span> denotes the output of <span class="math">\\mathcal{BS}^{\\rho(\\cdot)}</span> in the <span class="math">i</span>th iteration, then <span class="math">\\operatorname*{Pr}\\left[\\delta_{U_i} \\geq \\alpha \\Delta_1&#x27;\\right] \\geq 1 - \\alpha</span>. From Lemma 3 and how <span class="math">k</span> and <span class="math">s</span> are defined in the algorithm we know that provided that <span class="math">\\delta_{U_i} \\geq \\alpha \\Delta_1&#x27;</span> the validator outputs 1 with probability at least <span class="math">h_s^{\\mathrm{HB}}(k) \\geq \\gamma</span>. Thus, the distribution of the number of samples considered is bounded by <span class="math">\\mathsf{Geo}((1 - \\alpha)\\gamma)</span>. This implies that <span class="math">\\mathcal{G}_{\\mathsf{Geo}((1 - \\alpha)\\gamma)}(f(z))</span> bounds the distribution of the total number of queries as claimed.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Similarly to the previous claim we have <span class="math">\\operatorname<em>{Pr}\\left[\\delta_{U_i} &amp;lt; \\alpha \\beta \\Delta_1&#x27;\\right] &amp;lt; \\alpha \\beta</span> for every <span class="math">i</span> from Lemma 2. Denote by <span class="math">A_{i}</span> the output of <span class="math">\\mathcal{V}_{s,k}^{\\rho(v_0,\\cdot)}\\big((\\{U_i\\},\\mathbb{M}_2,\\ldots ,\\mathbb{M}_r),\\alpha \\Delta_1&#x27;\\big)</span>, i.e., it is the indicator variable for the event that <span class="math">\\mathcal{VS}_{\\alpha ,\\beta ,\\gamma}</span> returns <span class="math">U_{i}</span>. Then Lemma 3 implies that $\\operatorname</em>{Pr}\\left[A_i = 1</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\delta_{U_i} &lt; \\alpha \\beta \\Delta_1'\\right]\\leq n_s^{\\mathrm{HB}}(k)$ which gives</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <div class="my-4 text-center"><span class="math-block">\\Pr \\left[ \\delta_ {U _ {i}} &amp;lt;   \\alpha \\beta \\Delta_ {1} ^ {\\prime} \\mid A _ {i} = 1 \\right] &amp;lt;   \\frac {\\alpha \\beta n _ {s} ^ {\\mathrm {H B}} (1 / \\beta)}{\\Pr \\left[ A _ {i} = 1 \\right]} \\leq \\frac {\\alpha \\beta n _ {s} ^ {\\mathrm {H B}} (1 / \\beta)}{(1 - \\alpha) \\gamma}.</span></div>

    <h2 id="sec-20" class="text-2xl font-bold">7.3 Recursive Algorithm</h2>

    <p class="text-gray-300">We now have the subroutines we need. We sequentially sample good candidates for roots of accepting bases of subtrees and make sure that the roots are independent with respect to <span class="math">\\mathbb{M}_1</span>. If extracting an accepting basis for a subtree takes too long, then we interrupt the execution and find a new candidate.</p>

    <p class="text-gray-300">To be able to seamlessly talk about both the basic sampler and the validating sampler, the distributions of queries, and bounds on their outputs we use the following notation. The sampling algorithm is defined by:</p>

    <div class="my-4 text-center"><span class="math-block">\\mathcal {S} _ {\\alpha , \\beta , \\gamma} ^ {\\rho (\\cdot)} (\\mathbb {M}, B, \\Delta) = \\left\\{ \\begin{array}{l l} \\mathcal {B S} ^ {\\rho (\\cdot)} (\\mathbb {M}, B, \\Delta) &amp;amp; \\text {if} \\beta = 1 \\\\ \\mathcal {V S} _ {\\alpha , \\beta , \\gamma} ^ {\\rho (\\cdot)} (\\mathbb {M}, B, \\Delta) &amp;amp; \\text {otherwise} \\end{array} \\right.</span></div>

    <p class="text-gray-300">The distribution of the number of queries (for a fixed oracle) is denoted by:</p>

    <div class="my-4 text-center"><span class="math-block">\\mathsf {S} (\\mathbb {M}, \\alpha , \\beta , \\gamma , \\varDelta) = \\left\\{ \\begin{array}{l l} \\mathsf {B S} (\\mathbb {M}, \\varDelta) &amp;amp; \\text {if} \\beta = 1 \\\\ \\mathsf {V S} (\\mathbb {M}, \\alpha , \\beta , \\gamma , \\varDelta) &amp;amp; \\text {otherwise} \\end{array} \\right.</span></div>

    <p class="text-gray-300">The domain of the bounding function  <span class="math">\\phi (\\alpha ,\\beta ,\\gamma)</span>  is extended to  <span class="math">\\beta \\in (0,1]</span>  by setting  <span class="math">\\phi (\\alpha ,\\beta ,\\gamma) = 1 - \\alpha</span>  when  <span class="math">\\beta = 1</span></p>

    <p class="text-gray-300"><strong>Definition 11 (Recursive Extractor).</strong> Let  <span class="math">\\mathbb{M} = (\\mathbb{M}_0, \\ldots, \\mathbb{M}_r)</span>  be a matroid tree and assume that  <span class="math">\\mathcal{R}</span>  is a  <span class="math">(\\epsilon_1, \\mathsf{D}_1(\\Delta))</span> -accepting basis extractor for matroid trees of the form  <span class="math">(\\{v_1\\}, \\mathbb{M}_2, \\ldots, \\mathbb{M}_r)</span> , where  <span class="math">v_1 \\in S_1</span> . The recursive extractor  <span class="math">\\mathcal{R}_{\\kappa}[\\mathcal{R}]</span> , where  <span class="math">\\kappa = (\\alpha, \\beta, \\gamma, \\lambda)</span> ,  <span class="math">\\alpha, \\lambda, \\gamma \\in (0,1)</span> ,  <span class="math">\\beta \\in (0,1]</span> , proceeds as follows on input  <span class="math">(\\mathbb{M}, \\Delta_0)</span> .</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Set  <span class="math">\\varDelta_{1}=\\alpha\\beta(\\varDelta_{0}-\\omega_{\\mathbb{M}_{1}})</span> ,  <span class="math">k=k^{0_{1}}(\\lambda)</span> , and  <span class="math">\\mu=\\mu_{\\mathsf{D}_{1}(\\varDelta_{1})}</span> .</li>

      <li>Set  <span class="math">B = \\varnothing</span> , and  <span class="math">T = \\varnothing</span> .</li>

    </ol>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">3. While  $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">B</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">&lt; d_{1}$ :</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">(a) Compute  <span class="math">v_{1} = \\mathcal{S}_{\\alpha, \\beta, \\gamma}^{\\rho(\\cdot)}(\\mathbb{M}, B, \\Delta_{0})</span> . (b) Extract subtree  <span class="math">t = \\mathcal{R}^{\\rho(v_0, \\cdot)}\\big((\\{v_1\\}, \\mathbb{M}_2, \\ldots, \\mathbb{M}_r), \\Delta_1\\big)</span> , but interrupt the execution and set  <span class="math">t = \\bot</span>  if it attempts to make more than  <span class="math">k\\mu</span>  queries. (c) If  <span class="math">t \\neq \\bot</span> , then set  <span class="math">B = B \\cup \\{v_1\\}</span>  and  <span class="math">T = T \\cup \\{t\\}</span> .</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>Return the accepting basis tree  <span class="math">T</span> .</li>

    </ol>

    <p class="text-gray-300"><strong>Remark 2 (Reusing Samples).</strong> The accepting paths with common prefix drawn by the sampling algorithm may be re-used by the extractor, but this will only make a difference deep down in the tree due to the relatively large number of additional accepting paths that need to be found higher up in the tree and the fact that paths from the sampler may have to be discarded. To keep the presentation simple we only use this fact in the proof of Theorem 1 to get slightly better results.</p>

    <p class="text-gray-300"><strong>Lemma 5 (Recursive Extractor).</strong> The algorithm  <span class="math">\\mathcal{R}_{\\kappa}[\\mathcal{R}]</span>  is a  <span class="math">(\\epsilon_0, \\mathsf{D}_0(\\varDelta))</span> -accepting basis extractor, where  <span class="math">\\epsilon_0 = \\epsilon_1 / (\\alpha \\beta) + \\omega_{\\mathbb{M}_1}</span>  and</p>

    <div class="my-4 text-center"><span class="math-block">\\mathcal {G} _ {\\mathrm {D} _ {0} (\\Delta_ {0})} (z) = \\prod_ {i = 1} ^ {d _ {1}} \\mathcal {G} _ {\\mathrm {G e o} (\\phi \\lambda)} \\left(\\mathcal {G} _ {\\mathrm {S} (\\mathbb {M}, \\alpha , \\beta , \\gamma , \\Delta_ {0})} (z) z ^ {k ^ {0 _ {1}} (\\lambda) \\mu_ {\\mathrm {D} _ {1} (\\Delta_ {1})}}\\right),</span></div>

    <p class="text-gray-300">defined by  <span class="math">\\phi = \\phi (\\alpha ,\\beta ,\\gamma)</span>  and  <span class="math">\\varDelta_{1}=\\alpha\\beta(\\varDelta_{0}-\\omega_{\\mathbb{M}_{1}})</span> .</p>

    <p class="text-gray-300">Proof. From Lemma 2 and Lemma 4 we know that the number of queries made in Step 3a has distribution bounded by <span class="math">\\mathsf{S}(\\mathbb{M},\\alpha ,\\beta ,\\gamma ,\\varDelta_0)</span>.</p>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">Denote the candidate node in the <span class="math">i</span>th iteration by <span class="math">U_{i}</span>, i.e., the value denoted <span class="math">v_{1}</span> in the algorithm, and denote the output of <span class="math">\\mathcal{R}^{\\rho(v_0,\\cdot)}\\big((\\{U_i\\},\\mathbb{M}_2,\\ldots ,\\mathbb{M}_r),\\varDelta_1\\big)</span> in the <span class="math">i</span>th iteration (or <span class="math">\\perp</span> if it is interrupted) by <span class="math">T_{i}</span>. Then both Lemma 2 and Lemma 4 imply that we have <span class="math">\\operatorname<em>{Pr}\\left[\\delta_{U_i}\\geq \\varDelta_1\\right]\\geq \\phi</span>, and by our choice of scalar <span class="math">k</span> we have $\\operatorname</em>{Pr}\\left[T_i\\neq \\perp</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">\\delta_{U_i}\\geq \\varDelta_1\\right]\\geq \\lambda<span class="math">, so </span>\\operatorname*{Pr}\\left[T_i\\neq \\perp \\right]\\geq \\phi \\lambda<span class="math">. This means that the distribution of the number of iterations is bounded by </span>\\mathsf{Geo}(\\phi \\lambda)<span class="math">, and we need </span>d_{1}$ successes.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <p class="text-gray-300">Corollary 1 (Recursive Extractor). The distribution <span class="math">\\mathsf{D}_0(\\varDelta)</span> satisfies</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{array}{l} \\mu_{\\mathsf{D}_0(\\varDelta_0)} = \\left\\{ \\begin{array}{ll} \\frac{d_1}{(1 - \\alpha) \\lambda} \\left( \\alpha \\frac{1}{\\varDelta_1} + k^{\\flat_1}(\\lambda) \\mu_{\\mathsf{D}_1(\\varDelta_1)} \\right) &amp;amp; \\text{if } \\beta = 1 \\\\ \\frac{d_1}{\\phi \\lambda} \\left( \\frac{\\alpha \\beta + s_{\\beta}^{\\mathrm{NR}}(\\gamma)}{(1 - \\alpha) \\gamma} \\frac{1}{\\varDelta_1} + k^{\\flat_1}(\\lambda) \\mu_{\\mathsf{D}_1(\\varDelta_1)} \\right) &amp;amp; \\text{otherwise} \\end{array} \\right. \\\\ t_{d_1}^{\\flat_0}(k) \\leq t_{d_1}^{\\mathrm{co}}(k) \\quad \\text{for } k \\in (1, \\infty). \\end{array}</span></div>

    <p class="text-gray-300">Proof. The expected value follows from linearity and Wald's equation [10] (or directly from Lemma 8). More precisely, it follows from the equalities <span class="math">\\mu_{\\mathsf{Geo}(\\phi \\lambda)} = 1 / (\\phi \\lambda)</span>, <span class="math">\\varDelta_1^{\\prime}=\\varDelta_0-\\omega_{\\mathbb{M}_1}</span>, <span class="math">\\varDelta_1=\\alpha\\beta\\varDelta_1^{\\prime}</span>, and</p>

    <div class="my-4 text-center"><span class="math-block">\\mu_{\\mathsf{S}(\\mathbb{M},\\alpha,\\beta,\\gamma,\\varDelta_0)} = \\left\\{ \\begin{array}{ll} \\frac{1}{\\varDelta_1^{\\prime}} &amp;amp; \\text{if } \\beta = 1 \\\\ \\frac{1}{(1 - \\alpha) \\gamma} \\left( \\frac{1}{\\varDelta_1^{\\prime}} + \\frac{s_{\\beta}^{\\mathrm{NR}}(\\gamma)}{\\varDelta_1} \\right) &amp;amp; \\text{otherwise} \\end{array} \\right.</span></div>

    <p class="text-gray-300">and the fact that <span class="math">\\phi = 1 - \\alpha</span> when <span class="math">\\beta = 1</span>. The tail bound follows directly from Theorem 7 for this type of compound geometric distribution.</p>

    <h2 id="sec-21" class="text-2xl font-bold">7.4 Accepting Basis Extractor</h2>

    <p class="text-gray-300">We now let the recursive extractor <span class="math">\\mathcal{R}</span> invoke itself recursively until it suffices to invoke the basic extractor <span class="math">\\mathcal{B}</span>. For each additional recursive call needed, there is growth in the extraction error, but this only depends on the quantity <span class="math">\\nu = 1 / (\\alpha \\beta)</span>, apart from the matroid subdensity which is fixed. Given a fixed <span class="math">\\nu</span> we may optimize all other parameters to minimize the expected number of queries, since our tail bound does not depend on the expected value. All we need to do this is the rank of the matroid and a bound on the distribution of the number of queries needed in the next recursive call.</p>

    <p class="text-gray-300">Theorem 1 (Extractor). For every <span class="math">\\nu_{1},\\ldots ,\\nu_{r - 1} &amp;gt; 1</span> there exist parameters <span class="math">\\kappa_{i} = (\\alpha_{i},\\beta_{i},\\gamma_{i},\\lambda_{i})</span> such that the oracle algorithm <span class="math">\\mathcal{X}_{\\kappa} = \\mathcal{R}_{\\kappa_1}[\\mathcal{R}_{\\kappa_2}[\\dots \\mathcal{R}_{\\kappa_{r - 2}}[\\mathcal{B}]\\dots ]]</span>,</p>

    <p class="text-gray-300">is a <span class="math">(\\epsilon_0,\\mathsf{D}(\\varDelta_0))</span>-accepting basis extractor for matroid trees of depth <span class="math">r</span> where:</p>

    <div class="my-4 text-center"><span class="math-block">\\epsilon_ {0} = \\sum_ {i \\in [ r ]} \\omega_ {\\mathbb {M} _ {i}} \\prod_ {j \\in [ i - 1 ]} \\nu_ {j} \\quad \\text {(extraction error)} \\tag {1}</span></div>

    <div class="my-4 text-center"><span class="math-block">\\mu_ {\\mathrm {D} _ {0} \\left(\\Delta_ {0}\\right)} \\leq \\frac {c _ {0} \\prod_ {j \\in [ r ]} d _ {j}}{\\Delta_ {0} - \\epsilon_ {0}} \\quad \\text {(expected number of queries)} \\tag {2}</span></div>

    <div class="my-4 text-center"><span class="math-block">t _ {d _ {1}} ^ {0 _ {0}} (k) \\leq t _ {d _ {1}} ^ {c _ {0}} (k) \\quad \\text {for } k &amp;gt; 1, \\quad \\text {(tail bound)} \\tag {3}</span></div>

    <p class="text-gray-300">where the constant <span class="math">c_0</span> is defined by <span class="math">c_{r-1} = 1</span> and <span class="math">c_i = f_{\\mathsf{S}}(d_{i+1}, \\nu_{i+1}, c_{i+1})</span> for <span class="math">i = r-2, \\ldots, 0</span>, using</p>

    <div class="my-4 text-center"><span class="math-block">f _ {\\mathsf {B S}} (d, \\nu , c) = \\frac {\\nu^ {2}}{(\\nu - 1)} \\cdot \\min  \\{k / h _ {d} ^ {c _ {0}} (k) \\} \\cdot c</span></div>

    <div class="my-4 text-center"><span class="math-block">f _ {\\mathsf {V S}} (d, \\nu , c) = \\min  _ {\\alpha , s, k} \\left\\{\\frac {\\nu}{\\phi (\\alpha , \\nu \\alpha , \\gamma)} \\left(\\frac {1 + s}{(1 - \\alpha) h _ {s} ^ {\\mathrm {n m}} (\\nu \\alpha) h _ {d} ^ {c _ {0}} (k)} \\cdot \\frac {d _ {i}}{D _ {i + 1 , r}} + \\frac {k}{h _ {d} ^ {c _ {0}} (k)} \\cdot c\\right) \\right\\}</span></div>

    <div class="my-4 text-center"><span class="math-block">f _ {\\mathsf {S}} (d, \\nu , c) = \\min  \\left\\{f _ {\\mathsf {B S}} (d, \\nu , c), f _ {\\mathsf {V S}} (d, \\nu , c) \\right\\},</span></div>

    <p class="text-gray-300">with <span class="math">D_{i,r} = \\prod_{j\\in [i,r]}d_j</span>, <span class="math">\\alpha \\in (0,1 / \\nu)</span>, <span class="math">s\\in \\mathbb{N}^+</span>, and <span class="math">k\\in (1,\\infty)</span>.</p>

    <p class="text-gray-300">Both strategies give convoluted expressions, but we choose to not simplify, since they tell a story and are readily computed numerically. The first factor in <span class="math">f_{\\mathsf{BS}}(d,\\nu ,c)</span> represents how aggressively we use Markov's bound, i.e., how good we want a sample to be. The second factor represents the tradeoff between the number of attempts needed to complete a recursive call and how long it is allowed to run. This factor appears as a term in the second factor of the validating sampling strategy as well, but here it is balanced with the first term where <span class="math">s</span> represents how many samples are used for validation. This is only worthwhile when <span class="math">D_{i + 1,r}</span> and <span class="math">c</span> are large.</p>

    <p class="text-gray-300">Note that it is easy to compute optimal parameters for the algorithm for any concrete protocol. We prove a slightly stronger result where we exploit the special properties of leaves.</p>

    <p class="text-gray-300">Proof (Theorem 1). The tail bound follows directly from Corollary 1.</p>

    <p class="text-gray-300">Bounding the extraction error. If we set <span class="math">\\zeta_{i} = \\alpha_{i}\\beta_{i}</span> for <span class="math">i\\in [r - 1]</span> and <span class="math">\\zeta_r = 1</span>, then we have <span class="math">\\varDelta_{i}=\\zeta_{i}(\\varDelta_{i-1}-\\omega_{\\mathbb{M}_{i}})</span> for <span class="math">i\\in[r]</span> which expands to</p>

    <div class="my-4 text-center"><span class="math-block">\\varDelta_ {r} = \\varDelta_ {0} \\prod_ {i \\in [ r ]} \\zeta_ {i} - \\sum_ {i \\in [ r ]} \\omega_ {\\mathbb {M} _ {i}} \\prod_ {j \\in [ i, r ]} \\zeta_ {j}.</span></div>

    <p class="text-gray-300">The basic extractor requires that <span class="math">\\varDelta_{r-1}-\\omega_{\\mathbb{M}_r}=\\varDelta_r&amp;gt;0</span> to work, so the extraction error is given by</p>

    <div class="my-4 text-center"><span class="math-block">\\epsilon_ {0} = \\sum_ {i \\in [ r ]} \\frac {\\omega_ {\\mathbb {M} _ {i}}}{\\prod_ {j \\in [ i - 1 ]} \\zeta_ {j}} = \\sum_ {i \\in [ r ]} \\omega_ {\\mathbb {M} _ {i}} \\prod_ {j \\in [ i - 1 ]} \\nu_ {j}.</span></div>

    <p class="text-gray-300">Deriving parameters. Next we consider the problem of deriving <span class="math">\\alpha_{i}</span>, <span class="math">\\beta_{i}</span>, <span class="math">\\gamma_{i}</span>, and <span class="math">\\lambda_{i}</span> from <span class="math">\\zeta_{i}</span>. Define <span class="math">\\mathcal{X}_{\\kappa,i} = \\mathcal{R}_{\\kappa_i}[\\mathcal{R}_{\\kappa_{i + 1}}[\\cdot \\cdot \\cdot \\mathcal{R}_{\\kappa_{r - 2}}[\\mathcal{B}]\\cdot \\cdot \\cdot ]</span>. We will express the expected running time of <span class="math">\\mathcal{X}_{\\kappa,i}</span> on the form <span class="math">c_{i}D_{i + 1,r} / \\Delta_{i}</span> for a constant <span class="math">c_{i}</span> provided that its oracle <span class="math">\\rho_{i}(\\cdot)</span> and input <span class="math">(N_{i},\\Delta_{i})</span> are reasonably good. More precisely, it is called with an oracle of the form <span class="math">\\rho_{i}(\\cdot) = \\rho (v_{0},\\ldots ,v_{i - 2},\\cdot)</span> and a matroid tree <span class="math">N_{i} = (\\{v_{i - 1}\\} ,\\mathbb{M}_{i},\\dots ,\\mathbb{M}_{r})</span> for some values <span class="math">v_{i}\\in S_{i}</span>. Denote by <span class="math">\\Delta_0</span> the original estimated probability used as input to <span class="math">\\mathcal{X}_{\\kappa ,1}</span> and define</p>

    <div class="my-4 text-center"><span class="math-block">\\Delta_ {i} ^ {\\prime} = \\Delta_ {i - 1} - \\omega_ {\\mathbb {M} _ {i}} \\quad , \\quad \\Delta_ {i} = \\zeta_ {i} \\Delta_ {i} ^ {\\prime} \\quad \\text {and} \\quad \\epsilon_ {i} = \\zeta_ {i} (\\epsilon_ {i - 1} - \\omega_ {\\mathbb {M} _ {i}})</span></div>

    <p class="text-gray-300">for <span class="math">i\\in [r]</span>.</p>

    <p class="text-gray-300">Basic sampling strategy. Consider first the strategy where the non-validating sampler is used, i.e., we have <span class="math">\\beta_{i} = 1</span> and <span class="math">\\zeta_{i} = \\alpha_{i}</span>. If we exploit the fact that the path through the sampled node can be re-used we have</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{array}{l} \\mu_ {\\mathsf {D} _ {i - 1} (\\Delta_ {i - 1})} = \\frac {d _ {i}}{(1 - \\alpha_ {i}) \\lambda_ {i}} k ^ {\\mathrm {D} _ {i}} (\\lambda_ {i}) \\mu_ {\\mathsf {D} _ {i} (\\Delta_ {i})} \\\\ = \\frac {d _ {i} k ^ {\\mathrm {D} _ {i}} (\\lambda_ {i})}{(1 - \\alpha_ {i}) \\lambda_ {i}} \\cdot \\frac {c _ {i} D _ {i + 1 , r}}{\\Delta_ {i} - \\epsilon_ {i}} \\\\ = \\frac {k ^ {\\mathrm {D} _ {i}} (\\lambda_ {i})}{(1 - \\alpha_ {i}) \\lambda_ {i}} \\cdot c _ {i} \\cdot \\frac {D _ {i , r}}{\\alpha_ {i} (\\Delta_ {i - 1} - \\epsilon_ {i - 1})} \\\\ = \\frac {1}{\\alpha_ {i} (1 - \\alpha_ {i})} \\cdot \\frac {k ^ {\\mathrm {D} _ {i}} (\\lambda_ {i})}{\\lambda_ {i}} \\cdot c _ {i} \\cdot \\frac {D _ {i , r}}{\\Delta_ {i - 1} - \\epsilon_ {i - 1}}. \\\\ \\end{array}</span></div>

    <p class="text-gray-300">If we choose an optimal <span class="math">\\lambda_{i}</span>, then</p>

    <div class="my-4 text-center"><span class="math-block">c _ {i - 1} = \\frac {1}{\\alpha_ {i} (1 - \\alpha_ {i})} \\cdot \\frac {k ^ {\\mathrm {D} _ {i}} (\\lambda_ {i})}{\\lambda_ {i}} \\cdot c _ {i} = f _ {\\mathsf {B S}} (d _ {i}, \\nu_ {i}, c _ {i}).</span></div>

    <p class="text-gray-300">Validating sampling strategy. Next we consider the strategy where samples are validated before use and we are not extracting leaves in the recursive call, i.e., we have <span class="math">i &amp;lt; r - 2</span>, <span class="math">\\beta_{i} &amp;lt; 1</span>, and <span class="math">\\zeta_{i} = \\alpha_{i}\\beta_{i}</span>. In this case re-using leaves has limited value and we have</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{array}{l} \\mu_ {\\mathsf {D} _ {i - 1} (\\Delta_ {i - 1})} = \\frac {d _ {i}}{\\phi_ {i} \\lambda_ {i}} \\left(\\frac {\\zeta_ {i} + s _ {\\beta_ {i}} ^ {\\mathrm {u p}} (\\gamma_ {i})}{(1 - \\alpha_ {i}) \\gamma_ {i}} \\frac {1}{\\Delta_ {i}} + k ^ {\\mathrm {D} _ {i}} (\\lambda_ {i}) \\mu_ {\\mathsf {D} _ {i} (\\Delta_ {i})}\\right) \\\\ = \\frac {d _ {i}}{\\zeta_ {i} \\phi_ {i} \\lambda_ {i}} \\left(\\frac {\\zeta_ {i} + s _ {\\beta_ {i}} ^ {\\mathrm {u p}} (\\gamma_ {i})}{(1 - \\alpha_ {i}) \\gamma_ {i}} \\frac {1}{\\Delta_ {i - 1} - \\epsilon_ {i - 1}} + k ^ {\\mathrm {D} _ {i}} (\\lambda_ {i}) \\frac {c _ {i} D _ {i + 1 , r}}{\\Delta_ {i - 1} - \\epsilon_ {i - 1}}\\right) \\\\ = \\frac {1}{\\zeta_ {i} \\phi_ {i}} \\left(\\frac {\\zeta_ {i} + s _ {\\beta_ {i}} ^ {\\mathrm {u p}} (\\gamma_ {i})}{(1 - \\alpha_ {i}) \\gamma_ {i} \\lambda_ {i}} \\cdot \\frac {d _ {i}}{D _ {i + 1 , r}} + \\frac {k ^ {\\mathrm {D} _ {i}} (\\lambda_ {i})}{\\lambda_ {i}} \\cdot c _ {i}\\right) \\frac {D _ {i , r}}{\\Delta_ {i - 1} - \\epsilon_ {i - 1}} \\\\ \\end{array}</span></div>

    <p class="text-gray-300">If we choose parameters optimally, then we have</p>

    <div class="my-4 text-center"><span class="math-block">c _ {i - 1} = \\frac {1}{\\zeta_ {i} \\phi_ {i}} \\left(\\frac {\\zeta_ {i} + s _ {\\beta_ {i}} ^ {\\mathrm {u p}} (\\gamma_ {i})}{(1 - \\alpha_ {i}) \\gamma_ {i} \\lambda_ {i}} \\cdot \\frac {d _ {i}}{D _ {i + 1 , r}} + \\frac {k ^ {\\mathrm {D} _ {i}} (\\lambda_ {i})}{\\lambda_ {i}} \\cdot c _ {i}\\right) \\leq f _ {\\mathsf {V S}} (d _ {i}, \\nu_ {i}, c _ {i}).</span></div>

    <p class="text-gray-300">Leaves allow sharper bounds.</p>

    <p class="text-gray-300">Recursive calls deep in the tree are special in two ways. Firstly, the distribution of the number of queries is negative binomial, so a slightly sharper head bound <span class="math">h^{\\text{\\tiny{BB}}}_{d_{r}}(k)</span>, instead of <span class="math">h^{\\text{\\tiny{CC}}}_{d_{r}}(k)</span>, can be used to bound the probability that <span class="math">\\mathcal{B}</span> completes in time <span class="math">k\\mu_{\\text{\\tiny{D}}_{r-1}(\\Delta_{r-1})}</span>.</p>

    <p class="text-gray-300">Secondly, and more importantly for the second strategy it is worthwile to re-use paths from the sampler in the recursive call if <span class="math">d_{r}</span> is large. This gives a mutual dependency between the parameters of recursive calls at depth <span class="math">r-2</span> and <span class="math">r-1</span>, but fortunately the advantage of re-use diminishes quickly, so it suffices to consider this for <span class="math">\\mathcal{X}_{\\nu,r-2}</span>. In this case we have <span class="math">c_{r-1}=\\max\\{1,d_{r}-1-s^{\\text{\\tiny{BB}}}_{\\beta_{r-2}}(\\gamma_{r-2})\\}/d_{r}</span>.</p>

    <h3 id="sec-22" class="text-xl font-semibold mt-8">7.5 Interpretation</h3>

    <p class="text-gray-300">In the following we assume that <span class="math">\\Delta_{0}</span> is a tight lower bound of <span class="math">\\Delta_{\\rho}</span>, since this is a setup assumption in our approach. The expected running time of the extractor is <span class="math">\\poly/(\\Delta_{0}-\\epsilon_{0})</span> as expected for a proof of knowledge with knowledge error <span class="math">\\epsilon_{0}</span>.</p>

    <p class="text-gray-300">We may choose <span class="math">\\nu_{i}</span> arbitrarily close to one and conclude that a witness can be extracted provided that <span class="math">\\Delta_{0}</span> is slightly larger than <span class="math">\\epsilon=\\sum_{i\\in[r]}\\omega_{\\mathbb{M}_{i}}</span>, which coincides with our intuition about the soundness of special-sound protocols in general, i.e., to convince a verifier of a false statement it suffices in general to guess a challenge value correctly in at least one round. This is optimal in the sense that it is necessary to exploit dependencies between the rounds in the protocol to establish a smaller soundness error.</p>

    <p class="text-gray-300">Note that Bellare and Goldreich’s definition of a proof of knowledge <em>[3]</em> is satisfied regardless of how small we make <span class="math">\\nu_{i}&gt;1</span>. However, if we choose <span class="math">\\nu_{i}</span> based on a given <span class="math">\\Delta_{0}</span> such that <span class="math">\\epsilon_{0}&lt;\\Delta_{0}</span>, then the expected number of queries of the extractor has the form <span class="math">f(\\Delta_{0})/(\\Delta_{0}-\\epsilon_{0})</span>, where <span class="math">f(\\Delta_{0})</span> grows superexponentially when <span class="math">\\Delta_{0}</span> approaches <span class="math">\\epsilon</span>. Conversely, we may set <span class="math">\\nu_{i}\\approx 2</span> to minimize the expected running time of the extractor and accept an extraction error of the form <span class="math">\\sum_{i\\in[r]}2^{i-1}\\omega_{\\mathbb{M}_{i}}</span>. This begs the question: What is the knowledge error of the protocol?</p>

    <p class="text-gray-300">A protocol is said to be a proof of knowledge with knowledge error <span class="math">\\epsilon_{<em>}</span> if there is an extractor that outputs the witness in expected time <span class="math">\\poly/(\\Delta_{\\rho}-\\epsilon_{</em>})</span>. On the one hand we can make <span class="math">\\epsilon_{<em>}</span> arbitrarily close to <span class="math">\\epsilon</span> (and it cannot be smaller), but on the other hand this causes a drastic loss in security in terms of the running time of the extractor. Squeezing <span class="math">\\epsilon_{</em>}</span> in this way is arguably an abuse of the definition, but we still think that it is more natural to view the knowledge error as a property of the extractor and not of the protocol.</p>

    <h3 id="sec-23" class="text-xl font-semibold mt-8">7.6 Counting Predicate Queries Suffices</h3>

    <p class="text-gray-300">We have no control over how the prover distributes its running time over the execution of the protocol. Counting queries may be viewed as the worst case where the vast majority of the work is performed right before the last round.</p>

    <p class="text-gray-300">Above we have ignored all overhead costs in the extraction algorithms and focused on the number of oracle queries. The only potentially non-linear operation performed by the algorithms that is not already captured by the evaluation</p>

    <p class="text-gray-300">of the predicate is sampling from the complement of a flat in a matroid. Note that small subdensity does not imply that verifying independence is efficient. Consider the following definition.</p>

    <p class="text-gray-300"><strong>Definition 12 (Sampling Cost).</strong> Let <span class="math">\\mathbb{M}</span> be a matroid of rank <span class="math">d</span>. Then <span class="math">\\mathbb{M}</span> has sampling cost <span class="math">c_{\\mathbb{M}}</span> if there exists a probabilistic algorithm <span class="math">\\mathsf{Alg}</span> with running time <span class="math">c_{\\mathbb{M}}</span> such that setting <span class="math">a_0 = \\varnothing</span> and computing <span class="math">(a_i, b_i) = \\mathsf{Alg}(a_{i-1})</span> for <span class="math">i = 1, \\ldots, d</span> gives a uniformly distributed basis <span class="math">\\{b_1, \\ldots, b_d\\}</span> of <span class="math">\\mathbb{M}</span>.</p>

    <p class="text-gray-300">The value <span class="math">a_i</span> is used to store any pre-computation used by <span class="math">\\mathsf{Alg}</span> to complete the task within the required time. One can give more precise running times for the extractors by including the cost for sampling in the analysis, but we choose to not do this, since for the matroids of protocols the running time of the sampling algorithm is typically linear in <span class="math">i</span> with a unit cost that is a multiplication in a field or similar.</p>

    <h2 id="sec-24" class="text-2xl font-bold">8 Special Soundness</h2>

    <p class="text-gray-300">We can now express a general form of special soundness using matroid trees. Note that the message spaces and what usually appears as the knowledge error in concrete presentations are captured by the ground sets, ranks, and the subdensities of the matroids.</p>

    <p class="text-gray-300">Recall that <span class="math">\\langle \\mathcal{P}^<em>, \\mathcal{V}_c \\rangle(x)</span> denotes the verdict of <span class="math">\\mathcal{V}</span> regarding an interaction with a prover <span class="math">\\mathcal{P}^</em></span> on common input <span class="math">x</span> and using a random tape <span class="math">c = (v_1, \\ldots, v_r)</span> of challenges. The following definition instantiates our abstract predicate.</p>

    <p class="text-gray-300"><strong>Definition 13 (Prover Predicate).</strong> The prover predicate <span class="math">\\rho[\\mathcal{P}^<em>]</span> for a public-coin protocol <span class="math">(\\mathcal{P}, \\mathcal{V})</span> is defined by <span class="math">\\rho\\mathcal{P}^</em> = \\langle \\mathcal{P}^*, \\mathcal{V}_c \\rangle(v_0)</span>, where <span class="math">c = (v_1, \\ldots, v_r)</span>.</p>

    <p class="text-gray-300"><strong>Definition 14 (Accepting Transcript Tree).</strong> A rooted unordered directed tree <span class="math">T</span> with vertex labels <span class="math">\\ell(\\cdot)</span> is an accepting transcript tree for <span class="math">\\mathcal{V}</span> if every leaf has depth <span class="math">r</span> and for every path <span class="math">(u_0, \\ldots, u_r)</span> in <span class="math">T</span>: <span class="math">(v_{u_0}, a_{u_0}, \\ldots, v_{u_r}, a_{u_r})</span> is accepting, and <span class="math">\\ell(u_i) = (v_{u_i}, a_{u_i})</span>.</p>

    <p class="text-gray-300">Note that <span class="math">v_{u_0}</span> corresponds to the instance of the execution. This notation makes more sense when one considers the protocol as embedded into a larger protocol where the instance is chosen as the result of a random process under the influence of the adversary. We need a convenient notation to project the labels of the tree to their verifier message parts to allow us to state conditions on accepting transcript trees.</p>

    <p class="text-gray-300"><strong>Definition 15 (Challenge Tree).</strong> The challenge tree <span class="math">\\mathcal{C}(T)</span> of an accepting transcript tree <span class="math">T</span> with vertex labels <span class="math">\\ell(\\cdot)</span> has the same nodes and vertices, but labels defined by <span class="math">\\ell&#x27;(u) = v</span>, where <span class="math">\\ell(u) = (v, a)</span>.</p>

    <p class="text-gray-300"><strong>Definition 16 (Special Soundness).</strong> A <span class="math">(2r + 1)</span>-message public coin-protocol <span class="math">(\\mathcal{P}, \\mathcal{V})</span> is <span class="math">((\\mathbb{M}_1, \\ldots, \\mathbb{M}_r), p)</span>-special-sound for an NP relation <span class="math">\\mathsf{R}</span>, where <span class="math">\\mathbb{M}_i = (v, a)</span> is the vertex label of the <span class="math">i</span>-th vertex.</p>

    <p class="text-gray-300"><span class="math">(S_{i},I_{i})</span> is a matroid, if the <span class="math">i</span>th message of <span class="math">\\mathcal{V}</span> is chosen randomly from <span class="math">S_{i}</span>, and there exists a <em>witness extraction algorithm</em> <span class="math">\\mathcal{W}</span> that given an accepting transcript tree <span class="math">T</span> such that <span class="math">\\mathcal{C}(T)</span> is basis subtree of <span class="math">(\\{x\\},\\mathbb{M}_{1},\\ldots,\\mathbb{M}_{r})</span> outputs a witness <span class="math">w</span> such that <span class="math">(x,w)\\in\\mathsf{R}</span> in time <span class="math">p</span>.</p>

    <h2 id="sec-25" class="text-2xl font-bold">9 Piece-wise Special Soundness</h2>

    <p class="text-gray-300">Some protocols require multiple rounds of extraction because matroids and values that need to be extracted may depend on what have been extracted so far. This is often the case where multiple witnesses can be extracted in principle, but only one witness can be extracted without violating a computational assumption, e.g., proofs of shuffles <em>[5, 9]</em>.</p>

    <p class="text-gray-300">This does not quite fit into the framework we have presented, since the matroids are fixed, but we can still capture the extraction properties of such protocols in a way that has the same flavor as special soundness. To this end we decompose an NP relation.</p>

    <h6 id="sec-26" class="text-base font-medium mt-4">Definition 17 (Decomposable NP Relation).</h6>

    <p class="text-gray-300">An NP relation <span class="math">\\mathsf{R}</span> has a <em>decomposition</em> <span class="math">(\\mathsf{R}_{1}[\\cdot],\\ldots,\\mathsf{R}_{k}[\\cdot])</span>, where <span class="math">\\mathsf{R}_{j}[\\cdot]</span> is a family of NP relations if <span class="math">(x,w)\\in\\mathsf{R}</span> if and only if there exists <span class="math">y_{1},\\ldots,y_{k}</span> such that <span class="math">(x,y_{j})\\in\\mathsf{R}_{j}[y_{1},\\ldots,y_{j-1}]</span> for <span class="math">j\\in[k]</span>.</p>

    <p class="text-gray-300">The idea is now that we can think of a protocol as special-sound if we can decompose the NP relation into a number of steps and device an extractor for each step using the approach already presented. This may seem complicated, but turns out to be convenient and preserves the strong properties of special soundness.</p>

    <h6 id="sec-27" class="text-base font-medium mt-4">Definition 18 (Piece-wise Special Soundness).</h6>

    <p class="text-gray-300">A <span class="math">(2r+1)</span>-message public coin-protocol <span class="math">(\\mathcal{P},\\mathcal{V})</span> is <em>piece-wise</em> <span class="math">(\\mathbb{M}[\\cdot],p)</span>-special-sound for an NP relation <span class="math">\\mathsf{R}</span> with decomposition <span class="math">(\\mathsf{R}_{1}[\\cdot],\\ldots,\\mathsf{R}_{k}[\\cdot])</span>, where <span class="math">\\mathbb{M}_{j}[\\cdot]</span> is a family of matroid trees of depth <span class="math">r</span> if <span class="math">(\\mathcal{P},\\mathcal{V})</span> is <span class="math">(\\mathbb{M}_{j}[z],p_{j})</span>-special-sound for <span class="math">\\mathsf{R}_{j}[z]</span> for every <span class="math">z=(y_{1},\\ldots,y_{j})</span> such that <span class="math">(x,y_{l})\\in\\mathsf{R}_{l}[y_{1},\\ldots,y_{l-1}]</span> for <span class="math">l\\in[j]</span>, and <span class="math">p=\\sum_{j\\in[k]}p_{j}</span>.</p>

    <p class="text-gray-300">Although the parametrized matroids have the same ground sets, the sets of independence sets may differ. It is natural to abuse notation and think of a piece-wise special sound protocol as being special sound, but the decomposition of the NP relation and parametrized matroids must be provided along with the algorithms that compute a witnesses from accepting transcript trees.</p>

    <h2 id="sec-28" class="text-2xl font-bold">References</h2>

    <ul class="list-disc list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>[1] L. Babai. Trading group theory for randomness. In R. Sedgewick, editor, Proceedings of the 17th Annual ACM Symposium on Theory of Computing, May 6-8, 1985, Providence, Rhode Island, USA, pages 421–429. ACM, 1985.</li>

      <li>[</li>

    </ul>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>M. Bellare, J. A. Garay, and T. Rabin. Fast batch verification for modular exponentiation and digital signatures. In K. Nyberg, editor, Advances in Cryptology - EUROCRYPT ’98, International Conference on the Theory and Application of Cryptographic Techniques, Espoo, Finland, May 31 - June 4, 1998, Proceeding, volume 1403 of Lecture Notes in Computer Science, pages 236–250. Springer, 1998.</li>

      <li>M. Bellare and O. Goldreich. On defining proofs of knowledge. In E. F. Brickell, editor, Advances in Cryptology - CRYPTO ’92, 12th Annual International Cryptology Conference, Santa Barbara, California, USA, August 16-20, 1992, Proceedings, volume 740 of Lecture Notes in Computer Science, pages 390–420. Springer, 1992.</li>

      <li>R. Cramer, I. Damgård, and B. Schoenmakers. Proofs of partial knowledge and simplified design of witness hiding protocols. In Y. Desmedt, editor, Advances in Cryptology - CRYPTO ’94, 14th Annual International Cryptology Conference, Santa Barbara, California, USA, August 21-25, 1994, Proceedings, volume 839 of Lecture Notes in Computer Science, pages 174–187. Springer, 1994.</li>

      <li>J. Furukawa and K. Sako. An efficient scheme for proving a shuffle. In J. Kilian, editor, Advances in Cryptology - CRYPTO 2001, 21st Annual International Cryptology Conference, Santa Barbara, California, USA, August 19-23, 2001, Proceedings, volume 2139 of Lecture Notes in Computer Science, pages 368–387. Springer, 2001.</li>

      <li>S. Goldwasser, S. Micali, and C. Rackoff. The knowledge complexity of interactive proof systems. SIAM J. Comput., 18(1):186–208, 1989.</li>

      <li>S. Janson. Tail bounds for sums of geometric and exponential variables, 2017.</li>

      <li>C. Schnorr. Efficient signature generation by smart cards. J. Cryptology, 4(3):161–174, 1991.</li>

      <li>B. Terelius and D. Wikström. Proofs of restricted shuffles. In D. J. Bernstein and T. Lange, editors, Progress in Cryptology - AFRICACRYPT 2010, Third International Conference on Cryptology in Africa, Stellenbosch, South Africa, May 3-6, 2010. Proceedings, volume 6055 of Lecture Notes in Computer Science, pages 100–113. Springer, 2010.</li>

      <li>A. Wald. On cumulative sums of random variables. Ann. Math. Statist., 15(3):283–296, 09 1944.</li>

    </ol>

    <h2 id="sec-29" class="text-2xl font-bold">Appendix A Matroids</h2>

    <p class="text-gray-300">We recall one set of definitions for matroids and some of their properties.</p>

    <h6 id="sec-30" class="text-base font-medium mt-4">Definition 19 (Matroid).</h6>

    <p class="text-gray-300">A <em>matroid</em> is a pair <span class="math">(S,I)</span> of a <em>ground set</em> <span class="math">S</span> and a set <span class="math">I\\subset 2^{S}</span> of <em>independence sets</em> such that:</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li><span class="math">I</span> is non-empty,</li>

      <li>if <span class="math">A\\in I</span> and <span class="math">B\\subset A</span>, then <span class="math">B\\in I</span>, and</li>

    </ol>

    <div class="overflow-x-auto my-4">

      <table class="min-w-full text-sm text-gray-300">

        <thead>

          <tr>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">3. if <span class="math">A,B\\in I</span> and $</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">A</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">></th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600">B</th>

            <th class="px-3 py-2 text-left font-semibold border-b border-gray-600"><span class="math">, then there exists an element </span>a\\in A\\setminus B<span class="math"> such that </span>\\{a\\}\\cup B\\in I$.</th>

          </tr>

        </thead>

        <tbody>

        </tbody>

      </table>

    </div>

    <h6 id="sec-31" class="text-base font-medium mt-4">Definition 20 (Submatroid).</h6>

    <p class="text-gray-300">Let <span class="math">(S,I)</span> be a matroid and <span class="math">S^{\\prime}\\subset S</span>. The <em>submatroid</em> induced by <span class="math">S^{\\prime}</span> is the pair <span class="math">(S^{\\prime},I^{\\prime})</span> defined by <span class="math">I^{\\prime}=I\\cap 2^{S^{\\prime}}</span>.</p>

    <h6 id="sec-32" class="text-base font-medium mt-4">Definition 21 (Basis).</h6>

    <p class="text-gray-300">Let <span class="math">(S,I)</span> be a matroid. A set <span class="math">B\\in I</span> such that <span class="math">B\\cup\\{x\\}\\not\\in I</span> for every <span class="math">x\\in S\\setminus B</span> is a *basis</p>

    <p class="text-gray-300">Definition 22 (Rank). The rank of a matroid <span class="math">(S,I)</span> is the unique cardinality of each basis in <span class="math">I</span>.</p>

    <p class="text-gray-300">Definition 23 (Rank of Set). Let <span class="math">(S,I)</span> be a matroid and <span class="math">A\\subset S</span>. The rank <span class="math">\\operatorname{rank}(A)</span> of <span class="math">A</span> is the rank of the submatroid induced by <span class="math">A</span>.</p>

    <p class="text-gray-300">Definition 24 (Span and Flats). Let <span class="math">(S,I)</span> be a matroid and <span class="math">A\\subset S</span>. The span of <span class="math">A</span> is defined by <span class="math">\\operatorname{span}(A)=\\{x\\in S\\mid\\operatorname{rank}(A\\cup\\{x\\})=\\operatorname{rank}(A)\\}</span> and <span class="math">A</span> is a <em>flat</em> if <span class="math">\\operatorname{span}(A)=A</span>.</p>

    <h2 id="sec-33" class="text-2xl font-bold">Appendix B Generic Bounds on Random Variables</h2>

    <p class="text-gray-300">We use two standard bounds in this paper depending on how much we know about a distribution and how important it is to give a tight bound. Markov’s inequality can be applied to any distribution for which the expected value can be estimated. There are many variations of Chernoff’s bound depending on what is most convenient and how precisely it is stated. We state the bounds in their traditional forms.</p>

    <p class="text-gray-300">Theorem 2 (Markov’s Inequality). Let <span class="math">X</span> be a non-negative random variable over <span class="math">\\mathbb{R}</span> with expected value <span class="math">\\mu</span> and let <span class="math">k\\in(1,\\infty)</span>. Then <span class="math">\\Pr\\left[X\\geq k\\mu\\right]\\leq\\frac{1}{k}</span>.</p>

    <p class="text-gray-300">Theorem 3 (Chernoff’s Inequalities). Let <span class="math">X_{1},\\ldots,X_{n}</span> be independent binary random variables such that <span class="math">\\Pr\\left[X_{i}=1\\right]=p</span> and define <span class="math">X=\\sum_{i=1}^{n}X_{i}</span>. Then for every <span class="math">\\delta\\in(0,1)</span>:</p>

    <p class="text-gray-300"><span class="math">\\Pr\\left[X&lt;(1-\\delta)np\\right]&lt;e^{-\\frac{\\delta^{2}e}{3}n}\\text{ and}</span> <span class="math">\\Pr\\left[X&gt;(1+\\delta)np\\right]&lt;e^{-\\frac{\\delta^{2}e}{3}n}\\text{ .}</span></p>

    <p class="text-gray-300">We remark that the slight asymmetry due to the factors <span class="math">1/2</span> and <span class="math">1/3</span> in the exponents of the bounds is necessary.</p>

    <h2 id="sec-34" class="text-2xl font-bold">Appendix C Generating Functions</h2>

    <p class="text-gray-300">Probability and moment generating functions are convenient ways to describe distributions and derive bounds. They can be viewed as tools for manipulation of formal power series, but they also have an analytic meaning where they converge.</p>

    <p class="text-gray-300">Definition 25 (Probability Generating Function). The <em>probability generating function</em> of a random variable <span class="math">X</span> over <span class="math">\\mathbb{N}</span> is defined by <span class="math">\\mathcal{G}_{X}(z)=\\operatorname{E}\\left[z^{X}\\right]</span> for all <span class="math">z\\in\\mathbb{R}</span> for which this converges.</p>

    <p class="text-gray-300">Theorem 4 (Properties of Probability Generating Functions).</p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>If <span class="math">X</span> is a random variable over <span class="math">\\mathbb{N}</span>, then <span class="math">\\mathsf{P}_{X}\\left(k\\right)=\\left(\\frac{1}{k!}\\right)\\mathcal{G}_{X}^{\\left(k\\right)}(0)</span>, i.e., the probability generating function determines <span class="math">\\mathsf{P}_{X}</span> uniquely.</li>

    </ol>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>If <span class="math">X</span> and <span class="math">Y</span> are independent random variables over <span class="math">\\mathbb{N}</span> with probability generating functions <span class="math">\\mathcal{G}_X(z)</span> and <span class="math">\\mathcal{G}_Y(\\cdot)</span>, then <span class="math">\\mathcal{G}_X(z)\\mathcal{G}_Y(z)</span> is the probability generating function of the sum <span class="math">X + Y</span>.</li>

    </ol>

    <p class="text-gray-300"><strong>Definition 26 (Moment Generating Function).</strong> The moment generating function of a random variable <span class="math">X</span> over <span class="math">\\mathbb{N}</span> is defined by <span class="math">\\mathcal{M}_X(\\theta) = \\operatorname{E}\\left[e^{\\theta X}\\right]</span> for all <span class="math">\\theta \\in \\mathbb{R}</span> for which this converges.</p>

    <p class="text-gray-300"><strong>Theorem 5 (Properties of Moment Generating Functions).</strong></p>

    <ol class="list-decimal list-inside space-y-1 text-gray-300 my-2 ml-4">

      <li>If <span class="math">X</span> is a random variable over <span class="math">\\mathbb{N}</span>, then <span class="math">\\operatorname{E}\\left[X^k\\right] = \\mathcal{M}_X^{(k)}(0)</span>, i.e., the moment generating function determines all moments of <span class="math">\\mathsf{P}_X</span> uniquely, which in turn determines <span class="math">\\mathsf{P}_X</span> uniquely.</li>

      <li>If <span class="math">X</span> and <span class="math">Y</span> are independent random variables over <span class="math">\\mathbb{N}</span> with moment generating functions <span class="math">\\mathcal{M}_X(\\theta)</span> and <span class="math">\\mathcal{M}_Y(\\theta)</span>, then <span class="math">\\mathcal{M}_X(\\theta)\\mathcal{M}_Y(\\theta)</span> is the moment generating function of <span class="math">X + Y</span>.</li>

    </ol>

    <p class="text-gray-300">The following is a general form of Markov's inequality from which Chernoff's inequality follows using the right choice of <span class="math">\\theta</span> for the binomial distribution.</p>

    <p class="text-gray-300"><strong>Theorem 6 (Cramér's Theorem).</strong> Let <span class="math">X_1, \\ldots, X_n</span> be identically and independently distributed random variables, and define <span class="math">Y = \\sum_{i=1}^{n} X_i</span>. Then for every <span class="math">a &amp;gt; \\mu</span> and <span class="math">\\theta \\in (0, \\infty)</span> such that <span class="math">\\mathcal{M}_{X_1}(\\theta)</span> is finite:</p>

    <div class="my-4 text-center"><span class="math-block">\\Pr \\left[ Y \\geq n a \\right] \\leq \\left(\\frac {\\mathcal {M} _ {X _ {1}} (\\theta)}{e ^ {\\theta a}}\\right) ^ {n}.</span></div>

    <p class="text-gray-300"><strong>Proof.</strong> We apply Markov's inequality and independence to get</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{array}{l} \\Pr \\left[ Y \\geq n a \\right] = \\Pr \\left[ e ^ {\\theta Y} \\geq e ^ {\\theta n a} \\right] \\leq \\frac {\\operatorname {E} \\left[ e ^ {\\theta Y} \\right]}{e ^ {\\theta n a}} = \\frac {\\mathcal {M} _ {Y} (\\theta)}{e ^ {\\theta n a}} \\\\ = \\frac {\\prod_ {i \\in [ n ]} \\mathcal {M} _ {X _ {i}} (\\theta)}{e ^ {\\theta n a}} = \\left(\\frac {\\mathcal {M} _ {X _ {1}} (\\theta)}{e ^ {\\theta a}}\\right) ^ {n}. \\end{array}</span></div>

    <h2 id="sec-35" class="text-2xl font-bold">D Distributions</h2>

    <p class="text-gray-300">We are mainly interested in two related types of distributions: geometric distributions and negative binomial distributions, where the latter appears as a sum of the former, but we exploit the exponential distribution to bound compound distributions. We write <span class="math">X \\sim \\mathsf{D}</span> if a random variable <span class="math">X</span> has distribution <span class="math">\\mathsf{D}</span>.</p>

    <h2 id="sec-36" class="text-2xl font-bold">D.1 Exponential Distribution</h2>

    <p class="text-gray-300">The exponential distribution is the archetypal continuous distribution with an exponentially decreasing tail.</p>

    <p class="text-gray-300">19</p>

    <p class="text-gray-300">Definition 27 (Exponential Distribution). The exponential distribution <span class="math">\\mathsf{Exp}(\\lambda)</span> over <span class="math">(0,\\infty)</span> is given by its cumulative distribution function <span class="math">F(x,\\lambda) = 1 - e^{-\\lambda x}</span>, i.e., a random variable <span class="math">X \\sim \\mathsf{Exp}(\\lambda)</span> satisfies <span class="math">\\operatorname*{Pr}\\left[X \\leq x\\right] = 1 - e^{-\\lambda x}</span>.</p>

    <p class="text-gray-300">Lemma 6 (Properties of the Exponential Distribution). If <span class="math">X \\sim \\mathsf{Exp}(\\lambda)</span>, then</p>

    <div class="my-4 text-center"><span class="math-block">\\operatorname {E} [ X ] = \\lambda^ {- 1}, \\quad \\operatorname {V a r} [ X ] = \\lambda^ {- 2}, \\text{ and } \\mathcal {M} _ {X} (\\theta) = \\frac {\\lambda}{\\lambda - \\theta} \\quad \\text{for } \\theta &amp;lt; \\lambda .</span></div>

    <h2 id="sec-37" class="text-2xl font-bold">D.2 Geometric Distribution</h2>

    <p class="text-gray-300">Consider some experiment that succeeds with probability <span class="math">p</span>, and fails with probability <span class="math">1 - p</span>. A random variable with unshifted geometric distribution with probability <span class="math">p</span> represents the number of failures before a successful attempt. When we refer to the geometric distribution we mean the shifted variation, i.e., we count the total number of attempts including the successful attempt.</p>

    <p class="text-gray-300">Definition 28 (Geometric Distribution). A random variable <span class="math">X</span> has geometric distribution over <span class="math">\\{x \\in \\mathbb{N} \\mid x &amp;gt; 0\\}</span> with probability <span class="math">p \\in [0,1]</span>, denoted <span class="math">\\mathsf{Geo}(p)</span>, if <span class="math">\\mathsf{P}_X(x) = (1 - p)^{x - 1}p</span>.</p>

    <p class="text-gray-300">Lemma 7 (Properties of the Geometric Distribution). If <span class="math">X \\sim \\mathsf{Geo}(p)</span>, then</p>

    <div class="my-4 text-center"><span class="math-block">\\operatorname {E} [ X ] = \\frac {1}{p}</span></div>

    <div class="my-4 text-center"><span class="math-block">\\operatorname {V a r} [ X ] = \\frac {1 - p}{p ^ {2}}</span></div>

    <div class="my-4 text-center"><span class="math-block">\\mathsf {F} _ {X} (x) = 1 - (1 - p) ^ {x - 1}</span></div>

    <div class="my-4 text-center"><span class="math-block">\\mathcal {G} _ {X} (z) = \\frac {p z}{1 - (1 - p) z}</span></div>

    <div class="my-4 text-center"><span class="math-block">\\mathcal {M} _ {X} (\\theta) = \\frac {p e ^ {\\theta}}{1 - (1 - p) e ^ {\\theta}} \\quad \\text{for } \\theta &amp;lt; - \\ln (1 - p).</span></div>

    <h2 id="sec-38" class="text-2xl font-bold">D.3 A Compound Geometric Distribution</h2>

    <p class="text-gray-300">Distributions formed by letting the parameters of one distribution be chosen according to another are called compound distributions. In general compound distributions can only be bounded, but in some cases they can be described concisely.</p>

    <p class="text-gray-300">Definition 29 (Compound Geometric Distribution). A random variable <span class="math">X</span> has compound geometric distribution <span class="math">\\mathsf{CG}(c,p)</span> where <span class="math">c\\in \\mathbb{N}^k</span>, <span class="math">c_{1} = 0</span>, and <span class="math">p\\in (0,1]^k</span>, if its probability generating function <span class="math">g_{1}(z)</span> is defined by the equations</p>

    <div class="my-4 text-center"><span class="math-block">g _ {k} (z) = z ^ {c _ {k}} f _ {k} (z)</span></div>

    <div class="my-4 text-center"><span class="math-block">g _ {i - 1} (z) = z ^ {c _ {i - 1}} f _ {i - 1} \\left(g _ {i} (z)\\right)</span></div>

    <p class="text-gray-300">where <span class="math">f_{i}(z) = \\mathcal{G}_{\\mathrm{Geo}(p_{i})}(z)</span>.</p>

    <p class="text-gray-300">This distribution emerges naturally in Section 8 as the running time of an algorithm that recursively identifies a sparse subtree which satisfies certain properties at each level. The geometric distribution captures the number of attempts needed and the constants represent the added work needed after successful attempts.</p>

    <p class="text-gray-300"><strong>Lemma 8 (Properties of Compound Geometric Distribution).</strong> If <span class="math">X \\sim \\mathsf{CG}(c, p)</span> and we let <span class="math">c_{k+1} = 1</span>, then</p>

    <div class="my-4 text-center"><span class="math-block">\\mathrm{E} [ X ] = \\sum_ {i \\in [ k ]} \\prod_ {j \\in [ t ]} \\frac {1}{p _ {j}} c _ {i + 1} \\quad \\text{and}</span></div>

    <div class="my-4 text-center"><span class="math-block">\\mathcal {G} _ {X} (z) = \\frac {z ^ {\\sum_ {i \\in [ k + 1 ]} c _ {i}} \\prod_ {i \\in [ k ]} p _ {i}}{1 - \\sum_ {i \\in [ k ]} q _ {i} \\prod_ {j \\in [ i + 1 , k ]} p _ {j} z ^ {\\sum_ {l \\in [ i + 1 , k + 1 ]} c _ {l}}}.</span></div>

    <p class="text-gray-300"><strong>Proof.</strong> The claim about the expected value follows immediately from Wald's equation [10] (or by conditional expected values):</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{array}{l} \\operatorname {E} [ X ] = \\frac {1}{p _ {1}} \\left(\\frac {1}{p _ {2}} \\left(\\dots \\frac {1}{p _ {k - 1}} \\left(\\frac {1}{p _ {k}} + c _ {k}\\right) + c _ {k - 1} \\dots\\right) + c _ {2}\\right) + c _ {1} \\\\ = \\sum_ {i \\in [ k ]} \\prod_ {j \\in [ i ]} \\frac {1}{p _ {j}} c _ {i + 1}. \\\\ \\end{array}</span></div>

    <p class="text-gray-300">We adopt the notation from Definition 29 and set <span class="math">q_{t} = 1 - p_{t}</span>. We aim to derive <span class="math">g_{t}(z) = a_{t}(z) / b_{t}(z)</span> for <span class="math">t = k, \\ldots, 1</span>. Note that we have</p>

    <div class="my-4 text-center"><span class="math-block">a _ {k} (z) = z ^ {c _ {k}} p _ {k} z = p _ {k} z ^ {c _ {k} + c _ {k + 1}}</span></div>

    <div class="my-4 text-center"><span class="math-block">b _ {k} (z) = 1 - q _ {k} z = 1 - q _ {k} z ^ {c _ {k + 1}}</span></div>

    <p class="text-gray-300">and in general we have the relation</p>

    <div class="my-4 text-center"><span class="math-block">g _ {t - 1} (z) = \\frac {p _ {t - 1} z ^ {c _ {t - 1}} g _ {t} (z)}{1 - q _ {t - 1} g _ {t} (z)}</span></div>

    <p class="text-gray-300">from which we conclude</p>

    <div class="my-4 text-center"><span class="math-block">g _ {t - 1} (z) = \\frac {p _ {t - 1} z ^ {c _ {t - 1}} a _ {t} (z)}{b _ {t} (z)} \\bigg / \\left(1 - \\frac {q _ {t - 1} a _ {t} (z)}{b _ {t} (z)}\\right) = \\frac {p _ {t - 1} z ^ {c _ {t - 1}} a _ {t} (z)}{b _ {t} (z) - q _ {t - 1} a _ {t} (z)}.</span></div>

    <p class="text-gray-300">This defines <span class="math">a_{t}(z) = z^{\\sum_{l\\in [t,k + 1]}c_{l}}\\prod_{j\\in [t,k]}p_{j}</span>. Resolving the recursion gives</p>

    <div class="my-4 text-center"><span class="math-block">b _ {t} (z) = 1 - \\sum_ {i \\in [ t, k ]} q _ {i} \\prod_ {j \\in [ i + 1, k ]} p _ {j} z ^ {\\sum_ {l \\in [ i + 1, k + 1 ]} c _ {l}}</span></div>

    <p class="text-gray-300">which concludes the proof.</p>

    <p class="text-gray-300"><strong>Lemma 9.</strong> If <span class="math">0 &amp;lt; a \\leq b</span>, then <span class="math">e^a &amp;lt; 1 + e^b a</span>.</p>

    <p class="text-gray-300">21</p>

    <p class="text-gray-300">Proof. The proof follows by considering the Taylor expansion of <span class="math">e^a</span>:</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{array}{l} e^a - 1 = a + \\frac{a^2}{2} + \\frac{a^3}{3!} + \\frac{a^4}{4!} + \\dots \\\\ = a \\left(1 + \\frac{a}{2} + \\frac{a^2}{3!} + \\frac{a^3}{4!} + \\dots\\right) &amp;lt; a e^a \\leq a e^b. \\end{array}</span></div>

    <p class="text-gray-300">Lemma 10 (Compound Geometric Distribution). If <span class="math">X \\sim \\mathsf{CG}(c,p)</span> and <span class="math">Y \\sim \\mathsf{Exp}(\\lambda)</span>, where <span class="math">\\mu = \\operatorname{E}[X]</span> and <span class="math">\\lambda = 1 / \\mu</span>, then <span class="math">\\mathcal{M}_X(\\theta) &amp;lt; \\mathcal{M}_Y(\\theta)</span> for <span class="math">\\theta &amp;lt; 1 / \\mu</span>.</p>

    <p class="text-gray-300">Proof. Set <span class="math">\\Lambda = \\sum_{l\\in [2,k + 1]}c_l</span>. We use Lemma 9 (setting <span class="math">a = \\theta \\sum_{l\\in [i + 1,k + 1]}c_l</span> and <span class="math">b = \\theta \\Lambda</span>) to bound the statement from Lemma 12 in its form as a moment generating function</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{array}{l} e^{\\theta \\Lambda} \\mathcal{M}_X(\\theta)^{-1} = \\frac{1 - \\sum_{i\\in[k]}q_i\\prod_{j\\in[i + 1,k]}p_je^{\\theta\\sum_{l\\in[i + 1,k + 1]}c_l}}{\\prod_{i\\in[k]}p_i} \\\\ &amp;gt; \\prod_{i \\in [k]} \\frac{1}{p_i} - \\sum_{i \\in [k]} q_i \\prod_{j \\in [i]} \\frac{1}{p_j} \\left(1 + \\theta e^{\\theta \\Lambda} \\sum_{l \\in [i + 1, k + 1]} c_l\\right) \\\\ = \\prod_{i \\in [k]} \\frac{1}{p_i} - \\sum_{i \\in [k]} (1 - p_i) \\prod_{j \\in [i]} \\frac{1}{p_j} - \\theta e^{\\theta \\Lambda} \\sum_{i \\in [k]} (1 - p_i) \\prod_{j \\in [i]} \\frac{1}{p_j} \\sum_{l \\in [i + 1, k + 1]} c_l. \\end{array}</span></div>

    <p class="text-gray-300">The constant term is essentially a telescoping sum which sums to one, i.e., we have</p>

    <div class="my-4 text-center"><span class="math-block">\\prod_{j \\in [k]} \\frac{1}{p_j} + \\sum_{i \\in [k]} \\left(\\prod_{j \\in [i - 1]} \\frac{1}{p_j} - \\prod_{j \\in [i]} \\frac{1}{p_j}\\right) = 1.</span></div>

    <p class="text-gray-300">The multiple of <span class="math">\\theta e^{\\theta \\Lambda}</span> can be expressed similarly</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{array}{l} \\sum_{i \\in [k]} \\sum_{l \\in [i + 1, k + 1]} c_l \\left(\\prod_{j \\in [i]} \\frac{1}{p_j} - \\prod_{j \\in [i - 1]} \\frac{1}{p_j}\\right) \\\\ = \\sum_{l \\in [2, k + 1]} c_l \\sum_{i \\in [l - 1]} \\left(\\prod_{j \\in [i]} \\frac{1}{p_j} - \\prod_{j \\in [i - 1]} \\frac{1}{p_j}\\right) \\\\ = \\sum_{l \\in [2, k + 1]} c_l \\left(\\prod_{j \\in [l - 1]} \\frac{1}{p_j} - 1\\right) \\\\ = \\sum_{i \\in [k]} \\prod_{j \\in [i]} \\frac{1}{p_j} c_{i + 1} - \\sum_{l \\in [2, k + 1]} c_l \\\\ = \\mu - \\Lambda. \\end{array}</span></div>

    <p class="text-gray-300">Thus, we have</p>

    <div class="my-4 text-center"><span class="math-block">e ^ {\\theta \\Lambda} \\mathcal {M} _ {X} (\\theta) ^ {- 1} &amp;gt; 1 - \\theta e ^ {\\theta \\Lambda} (\\mu - \\Lambda)</span></div>

    <p class="text-gray-300">which, using  <span class="math">e^{\\theta} &amp;gt; 1 + \\theta</span>  for  <span class="math">\\theta &amp;gt;0</span> , finally gives the bound</p>

    <div class="my-4 text-center"><span class="math-block">\\begin{array}{l} \\mathcal {M} _ {X} (\\theta) ^ {- 1} &amp;gt; e ^ {- \\theta \\Lambda} - \\theta (\\mu - \\Lambda) \\\\ &amp;gt; 1 - \\theta \\Lambda - \\theta (\\mu - \\Lambda) = 1 - \\theta \\mu \\\\ \\end{array}</span></div>

    <p class="text-gray-300">which can be restated as  <span class="math">\\mathcal{M}_X(\\theta) &amp;lt; 1 / (1 - \\theta \\mu) = \\lambda / (\\lambda - \\theta)</span>  as claimed.</p>

    <p class="text-gray-300">Theorem 7 (Cramér's Theorem for Compound Geometric Distributions). If  <span class="math">X_{i} \\sim \\mathsf{CG}(c,p)</span>  for  <span class="math">i \\in [n]</span>  are independently distributed and  <span class="math">Y = \\sum_{i \\in [n]} X_{i}</span>  with  <span class="math">\\mu = \\operatorname{E}[X_1]</span> , then for every  <span class="math">k \\in (1,\\infty)</span> :</p>

    <div class="my-4 text-center"><span class="math-block">\\Pr \\left[ Y \\geq n k \\mu \\right] &amp;lt;   e ^ {- n (k - 1 - \\ln k)}.</span></div>

    <p class="text-gray-300">Proof. Theorem 6 implies that for every  <span class="math">\\theta &amp;lt; 1 / \\mu</span> :</p>

    <div class="my-4 text-center"><span class="math-block">\\Pr \\left[ Y \\geq k n \\mu \\right] \\leq \\left(\\frac {\\mathcal {M} _ {X _ {1}} (\\theta)}{e ^ {\\theta k \\mu}}\\right) ^ {n}.</span></div>

    <p class="text-gray-300">From Lemma 10 we know that  <span class="math">\\mathcal{M}_{X_1}(\\theta) &amp;lt; \\lambda / (\\lambda - \\theta)</span>  for all  <span class="math">\\theta &amp;lt; \\lambda</span> , where  <span class="math">\\lambda = 1 / \\mu</span> , and if we set  <span class="math">\\theta = (1 - k^{-1}) / \\mu</span>  the claim follows.</p>

    <h2 id="sec-39" class="text-2xl font-bold">D.4 Negative Binomial Distribution</h2>

    <p class="text-gray-300">Consider some experiment that succeeds with probability  <span class="math">p</span> , and fails with probability  <span class="math">q = 1 - p</span> . A random variable  <span class="math">X</span>  with negative binomial distribution with probability  <span class="math">p</span>  and success parameter  <span class="math">s</span>  represents how many attempts are needed to succeed  <span class="math">s</span>  times.</p>

    <p class="text-gray-300">Definition 30 (Negative Binomial Distribution). A random variable  <span class="math">X</span>  has negative binomial distribution, denoted  <span class="math">\\mathsf{NB}(s,p)</span> , over  <span class="math">\\{x\\in \\mathbb{N}\\mid x\\geq s\\}</span>  with probability  <span class="math">p</span>  and success parameter  <span class="math">s</span>  if  <span class="math">\\mathsf{P}_X(k) = \\binom{k-1}{s-1}(1-p)^{k-s}p^s</span> .</p>

    <p class="text-gray-300">Lemma 11 (Properties of Negative Binomial Distribution). If  <span class="math">X \\sim \\mathsf{NB}(s,p)</span> , then</p>

    <div class="my-4 text-center"><span class="math-block">\\operatorname {E} [ X ] = s / p</span></div>

    <div class="my-4 text-center"><span class="math-block">\\operatorname {V a r} [ X ] = s (1 - p) / p ^ {2}, \\quad \\text{and}</span></div>

    <div class="my-4 text-center"><span class="math-block">\\mathcal {M} _ {X} (\\theta) = \\left(\\frac {p e ^ {\\theta}}{1 - (1 - p) e ^ {\\theta}}\\right) ^ {s} \\quad \\text{for} \\ \\theta &amp;lt;   - \\ln (1 - p).</span></div>

    <p class="text-gray-300">Lemma 12 (Sum of Geometric Distributions). If  <span class="math">X_{i} \\sim \\mathsf{Geo}(p)</span>  for  <span class="math">i \\in [s]</span>  are independently distributed and  <span class="math">X = \\sum_{i=1}^{s} X_{i}</span> , then  <span class="math">X \\sim \\mathsf{NB}(s, p)</span> .</p>

    <p class="text-gray-300">Proof.</p>

    <p class="text-gray-300">The multiplicative property of moment generating functions implies that</p>

    <p class="text-gray-300"><span class="math">\\mathcal{M}_{X}(\\theta)=\\prod_{i=1}^{s}\\mathcal{M}_{X_{i}}(\\theta)=\\left(\\frac{pe^{\\theta}}{1-(1-p)e^{\\theta}}\\right)^{s}\\enspace,</span></p>

    <p class="text-gray-300">which is the moment generating function of a binomial distribution with probability <span class="math">p</span> and success parameter <span class="math">s</span>.</p>

    <p class="text-gray-300">Chernoff’s lower and upper bounds hold for negative binomial distribution similarly to the binomial distribution, but the asymmetry in the bounds is reversed since a lower bound on a random variable of the former distribution corresponds to an upper bound of one of the latter.</p>

    <h6 id="sec-40" class="text-base font-medium mt-4">Theorem 8 (Chernoff’s Inequalities for Negative Binomial Distribution).</h6>

    <p class="text-gray-300">If <span class="math">X\\sim\\mathsf{NB}(s,p)</span> and <span class="math">\\mu=s/p</span>, then for every <span class="math">k&gt;1</span></p>

    <p class="text-gray-300"><span class="math">\\Pr\\left[X&gt;k\\mu\\right]&lt;e^{-\\left(1-\\frac{1}{k}\\right)^{2}\\frac{ks}{2}}\\quad\\text{and}</span> (4) <span class="math">\\Pr\\left[X&lt;\\mu/k\\right]&lt;e^{-\\left(k-1\\right)^{2}\\frac{s}{2k}}\\enspace.</span> (5)</p>

    <h6 id="sec-41" class="text-base font-medium mt-4">Proof.</h6>

    <p class="text-gray-300">To prove the first inequality we set <span class="math">m=k\\mu=ks/p</span>, let <span class="math">Y_{1},\\ldots,Y_{m}</span> be independent binary random variables such that <span class="math">\\Pr\\left[Y_{i}=1\\right]=p</span>, and define <span class="math">Y=\\sum_{i=1}^{m}Y_{i}</span>. Then</p>

    <p class="text-gray-300"><span class="math">\\Pr\\left[X&gt;k\\mu\\right]=\\Pr\\left[Y&lt;s\\right]\\enspace,</span></p>

    <p class="text-gray-300">and the latter expression is of a convenient form to bound using Theorem 3. We have <span class="math">\\operatorname{E}\\left[Y\\right]=mp=ks</span>, so <span class="math">s=\\mu^{\\prime}/k</span>, where <span class="math">\\mu^{\\prime}=\\operatorname{E}\\left[Y\\right]</span>. Thus, we set <span class="math">1-\\delta=1/k</span> and conclude that</p>

    <p class="text-gray-300"><span class="math">\\Pr\\left[Y&lt;s\\right]=\\Pr\\left[Y&lt;\\mu^{\\prime}/k\\right]&lt;e^{-\\frac{\\delta^{2}\\mu^{\\prime}}{\\delta}}=e^{-\\left(1-\\frac{1}{k}\\right)^{2}\\frac{ks}{2}}\\enspace.</span></p>

    <p class="text-gray-300">The second inequality is proved similarly by instead setting <span class="math">m=\\mu/k=s/(pk)</span>, which gives <span class="math">s=k\\mu^{\\prime}</span> with correspondingly defined random variables <span class="math">Y_{1},\\ldots,Y_{m}</span>. Setting <span class="math">1+\\delta=k</span> then implies the inequality</p>

    <p class="text-gray-300"><span class="math">\\Pr\\left[X&lt;\\mu/k\\right]=\\Pr\\left[Y&gt;s\\right]=\\Pr\\left[Y&gt;k\\mu^{\\prime}\\right]&lt;e^{-\\frac{\\delta^{2}\\mu^{\\prime}}{\\delta}}=e^{-\\left(k-1\\right)^{2}\\frac{s}{2k}}\\enspace.</span></p>

    <h2 id="sec-42" class="text-2xl font-bold">Appendix E Stochastic Dominance and Bounding Distributions</h2>

    <p class="text-gray-300">One approach to compare distributions is to not only bound expected values, variances, or probabilities of certain events, but instead find families of distributions that are ordered stochastically. The advantage of this, when possible, is that more structural information about the original distribution can be retained. We only need first-order stochastic dominance over <span class="math">\\mathbb{N}</span>.</p>

    <h6 id="sec-43" class="text-base font-medium mt-4">Definition 31 (Stochastic Dominance).</h6>

    <p class="text-gray-300">Let <span class="math">X</span> and <span class="math">Y</span> be random variables over <span class="math">\\mathbb{N}</span>. Then <span class="math">Y</span> stochastically dominates <span class="math">X</span>, denoted <span class="math">X\\preceq Y</span>, if <span class="math">\\mathsf{F}_{X}(z)\\leq\\mathsf{F}_{Y}(z)</span> for every <span class="math">z\\in\\mathbb{N}</span>.</p>

    <p class="text-gray-300">######</p>

    <p class="text-gray-300">This is a somewhat confusing definition when the random variables encode running times of algorithms, since providing a bound of a running time with distribution <span class="math">\\mathsf{D}_X</span> amounts to defining a distribution <span class="math">\\mathsf{D}_Y</span> such that <span class="math">Y</span> is stochastically dominated by <span class="math">X</span>. This motivates the following more natural definition.</p>

    <p class="text-gray-300"><strong>Definition 32 (Bounding Distribution).</strong> Let <span class="math">\\mathsf{D}_X</span> and <span class="math">\\mathsf{D}_Y</span> be distributions over <span class="math">\\mathbb{N}</span>. Then <span class="math">\\mathsf{D}_X</span> is bounded by <span class="math">\\mathsf{D}_Y</span> if <span class="math">Y \\preceq X</span>.</p>

    <p class="text-gray-300">25</p>`;
---

<BaseLayout title="Special Soundness Revisited (2018/1157)">
  <article class="max-w-4xl mx-auto article-prose">
    <nav class="mb-8">
      <a href="/papers" class="text-blue-400 hover:text-blue-300">
        &larr; Back to Papers
      </a>
    </nav>

    <header class="mb-12">
      <h1 class="text-3xl font-bold mb-4"
        set:html={TITLE_HTML} />
      <p class="text-gray-400 mb-2"
        set:html={AUTHORS_HTML} />
      <p class="text-gray-500 text-sm mb-4">
        2018 &middot; eprint 2018/1157
      </p>
      <div class="flex gap-4 text-sm">
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >
          Paper (eprint) &rarr;
        </a>
      </div>
      <p class="mt-4 text-xs text-gray-500">
        All content below belongs to the original authors. This page
        reproduces the paper for educational purposes. Always
        <a
          href={EPRINT_URL}
          target="_blank"
          rel="noopener noreferrer"
          class="text-blue-400 hover:text-blue-300"
        >cite the original</a>.
      </p>
      <p class="mt-1 text-xs text-gray-600">
        Converted with: {CRAWLER} &middot; {CONVERTED_DATE}
      </p>
    </header>

    <Fragment set:html={CONTENT} />

  </article>
</BaseLayout>
